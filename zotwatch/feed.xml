<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 08 Feb 2026 03:58:39 +0000</lastBuildDate><item><title>Adaptive Weighted Mutual Nearest Neighbor Network with Support-Query Collaborative Feature Reconstruction for Few-Shot SAR Target Classification</title><link>https://doi.org/10.1109/tcsvt.2026.3661977</link><guid>10.1109/tcsvt.2026.3661977</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Jia Zheng</dc:creator><dc:creator>Ming Li</dc:creator><dc:creator>Hongmeng Chen</dc:creator><dc:creator>Peng Zhang</dc:creator><dc:creator>Yan Wu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3661977</prism:doi><description>Limited by observation conditions, the availability of synthetic aperture radar (SAR) image samples is constrained, posing challenges for deep learning-based SAR target recognition. The SAR target recognition algorithms based on few-shot learning (FSL) have made significant progress, and recent studies indicate that local descriptors outperform image-level feature representations. However, the similarity in SAR target backgrounds makes some irrelevant local descriptors severely affect the accuracy of the metric. To address this issue, we propose a novel metric-based FSL framework for SAR target recognition, i.e., adaptive weighted mutual nearest neighbor network with support-query collaborative feature reconstruction. First, by proposing the support-query collaborative feature reconstruction module, calculating the similarity between the reconstructed support features and the support feature values allows the model to better capture intra-class common features and enhance intra-class consistency. Meanwhile, calculating the similarity between the reconstructed query features and the query feature values helps the model identify effective distinguishing features, highlight target saliency, and increase inter-class differentiation. Secondly, an adaptive weighted mutual nearest neighbor module is designed, where the weight of query descriptors is adjusted to highlight distinctive features and reduce background interference. Finally, a metric fusion module is proposed, which not only computes image-to-class metrics based on local descriptors but also integrates similarity from the support set to the query set as well as from the query set to the support set, enabling a discriminative similarity measure. Experimental results on three public SAR datasets demonstrate that our proposed algorithm performs better classification than other FSL algorithms.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.856 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jia Zheng; Ming Li; Hongmeng Chen; Peng Zhang; Yan Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3661977"&gt;10.1109/tcsvt.2026.3661977&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.856 (must_read)&lt;/p&gt;
&lt;p&gt;Limited by observation conditions, the availability of synthetic aperture radar (SAR) image samples is constrained, posing challenges for deep learning-based SAR target recognition. The SAR target recognition algorithms based on few-shot learning (FSL) have made significant progress, and recent studies indicate that local descriptors outperform image-level feature representations. However, the similarity in SAR target backgrounds makes some irrelevant local descriptors severely affect the accuracy of the metric. To address this issue, we propose a novel metric-based FSL framework for SAR target recognition, i.e., adaptive weighted mutual nearest neighbor network with support-query collaborative feature reconstruction. First, by proposing the support-query collaborative feature reconstruction module, calculating the similarity between the reconstructed support features and the support feature values allows the model to better capture intra-class common features and enhance intra-class consistency. Meanwhile, calculating the similarity between the reconstructed query features and the query feature values helps the model identify effective distinguishing features, highlight target saliency, and increase inter-class differentiation. Secondly, an adaptive weighted mutual nearest neighbor module is designed, where the weight of query descriptors is adjusted to highlight distinctive features and reduce background interference. Finally, a metric fusion module is proposed, which not only computes image-to-class metrics based on local descriptors but also integrates similarity from the support set to the query set as well as from the query set to the support set, enabling a discriminative similarity measure. Experimental results on three public SAR datasets demonstrate that our proposed algorithm performs better classification than other FSL algorithms.&lt;/p&gt;</content:encoded></item><item><title>Dynamic High-frequency Convolution for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tcsvt.2026.3661285</link><guid>10.1109/tcsvt.2026.3661285</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Ruojing Li</dc:creator><dc:creator>Chao Xiao</dc:creator><dc:creator>Qian Yin</dc:creator><dc:creator>Wei An</dc:creator><dc:creator>Nuo Chen</dc:creator><dc:creator>Xinyi Ying</dc:creator><dc:creator>Miao Li</dc:creator><dc:creator>Yingqian Wang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3661285</prism:doi><description>Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zerocentered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a dropin replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.836 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruojing Li; Chao Xiao; Qian Yin; Wei An; Nuo Chen; Xinyi Ying; Miao Li; Yingqian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3661285"&gt;10.1109/tcsvt.2026.3661285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.836 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zerocentered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a dropin replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.&lt;/p&gt;</content:encoded></item><item><title>Domain Adaptation for Object Detection Based on Domain-aware Prompting in Remote Sensing Imagery</title><link>https://doi.org/10.1109/tgrs.2026.3661721</link><guid>10.1109/tgrs.2026.3661721</guid><pubDate>Fri, 06 Feb 2026 20:49:16 +0000</pubDate><dc:creator>Xiufei Zhang</dc:creator><dc:creator>Xiwen Yao</dc:creator><dc:creator>Gong Cheng</dc:creator><dc:creator>Junwei Han</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3661721</prism:doi><description>Domain Adaptation (DA) is of critical importance in practical remote sensing object detection, aiming to address the performance degradation caused by significant domain discrepancies between a labeled source domain and an unlabeled target domain. The DA method enables the alignment of cross-domain features, thus improving detection performance in the target domain while avoiding the high cost of manual annotation. However, most existing methods focus on mitigating domain bias by optimizing discriminative visual encoders. These coarse feature alignment strategies often overlook domain-aware semantic consistency, which leads to feature misalignment, especially for the remote sensing scene with small objects and diverse backgrounds. To address this problem, we propose a novel domain-aware prompting framework to learn domain-invariant semantics by modeling the learnable domain-aware prompts. Specifically, we first design a reciprocal prompt interaction learning module to generate domain-aware semantics for each domain by modeling reciprocal interactions between textual and image modalities. Subsequently, we develop a prototype-driven domain prompt alignment module to capture shared domain prompt semantics from both local and global levels by constructing domain prompt prototypes. Extensive experiments demonstrate the efficacy of our proposed approach, achieving superior performance on challenging remote sensing datasets. The code is publicly accessible at https://github.com/XZhang878/DAP/.
Published: 2026-02-06T20:49:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiufei Zhang; Xiwen Yao; Gong Cheng; Junwei Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3661721"&gt;10.1109/tgrs.2026.3661721&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Domain Adaptation (DA) is of critical importance in practical remote sensing object detection, aiming to address the performance degradation caused by significant domain discrepancies between a labeled source domain and an unlabeled target domain. The DA method enables the alignment of cross-domain features, thus improving detection performance in the target domain while avoiding the high cost of manual annotation. However, most existing methods focus on mitigating domain bias by optimizing discriminative visual encoders. These coarse feature alignment strategies often overlook domain-aware semantic consistency, which leads to feature misalignment, especially for the remote sensing scene with small objects and diverse backgrounds. To address this problem, we propose a novel domain-aware prompting framework to learn domain-invariant semantics by modeling the learnable domain-aware prompts. Specifically, we first design a reciprocal prompt interaction learning module to generate domain-aware semantics for each domain by modeling reciprocal interactions between textual and image modalities. Subsequently, we develop a prototype-driven domain prompt alignment module to capture shared domain prompt semantics from both local and global levels by constructing domain prompt prototypes. Extensive experiments demonstrate the efficacy of our proposed approach, achieving superior performance on challenging remote sensing datasets. The code is publicly accessible at https://github.com/XZhang878/DAP/.&lt;/p&gt;</content:encoded></item><item><title>SA-CMF: A Secondary Attention-Based Cross-Modal Fusion Framework for Deep Space Target Recognition in Remote Sensing</title><link>https://doi.org/10.1109/tgrs.2026.3661448</link><guid>10.1109/tgrs.2026.3661448</guid><pubDate>Fri, 06 Feb 2026 20:49:16 +0000</pubDate><dc:creator>Na Li</dc:creator><dc:creator>Shuoyu Zhang</dc:creator><dc:creator>Huijie Zhao</dc:creator><dc:creator>Wen Ou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3661448</prism:doi><description>Deep space infrared target recognition is significantly challenged by similar interference sources and complex backgrounds, which degrade detection accuracy and robustness. While detection has shifted from single-detector to multi-detector coordinated systems, current algorithms fail to consider both the dependencies between different features and the complementarity of the unique characteristics inherent to each modality, limiting fusion quality. To address this, we propose a Secondary Attention-Based Cross-Modal Fusion(SA-CMF) framework. The first attention stage applies modality-specific attention to enhance the discriminative ability of individual features, fully exploiting each modality’s unique characteristics, while the second stage employs cross-attention to capture interdependencies among modal features. Additionally, a dynamic mutual information adjustment strategy is introduced to suppress redundant information and mitigate cross-modal distribution discrepancies. Experimental results demonstrate that SA-CMF achieves 94.7% accuracy, high F1 scores, and robust performance under diverse deep space conditions. These results validate the framework’s effectiveness in improving both feature quality and target recognition, highlighting its potential for complex infrared remote sensing applications.
Published: 2026-02-06T20:49:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Na Li; Shuoyu Zhang; Huijie Zhao; Wen Ou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3661448"&gt;10.1109/tgrs.2026.3661448&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;Deep space infrared target recognition is significantly challenged by similar interference sources and complex backgrounds, which degrade detection accuracy and robustness. While detection has shifted from single-detector to multi-detector coordinated systems, current algorithms fail to consider both the dependencies between different features and the complementarity of the unique characteristics inherent to each modality, limiting fusion quality. To address this, we propose a Secondary Attention-Based Cross-Modal Fusion(SA-CMF) framework. The first attention stage applies modality-specific attention to enhance the discriminative ability of individual features, fully exploiting each modality’s unique characteristics, while the second stage employs cross-attention to capture interdependencies among modal features. Additionally, a dynamic mutual information adjustment strategy is introduced to suppress redundant information and mitigate cross-modal distribution discrepancies. Experimental results demonstrate that SA-CMF achieves 94.7% accuracy, high F1 scores, and robust performance under diverse deep space conditions. These results validate the framework’s effectiveness in improving both feature quality and target recognition, highlighting its potential for complex infrared remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>SPDTT: Synergizing Prior-Spectrum-Guided Diffusion Model and Target-Aware Transformer for Hyperspectral Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3661488</link><guid>10.1109/tgrs.2026.3661488</guid><pubDate>Fri, 06 Feb 2026 20:49:16 +0000</pubDate><dc:creator>Mingchao Xue</dc:creator><dc:creator>Zitong Xu</dc:creator><dc:creator>Yang Xu</dc:creator><dc:creator>Le Sun</dc:creator><dc:creator>Zhihui Wei</dc:creator><dc:creator>Zebin Wu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3661488</prism:doi><description>The performance of deep learning models in hyperspectral target detection (HTD) is significantly hampered by two primary challenges: the scarcity of labeled data and the intrinsic spectral variability of targets. To overcome these limitations, this paper introduces a novel HTD framework, termed SPDTT. First, a prior-spectrum-guided diffusion model (PDM) is designed for data augmentation. By incorporating a customized spectral angle mapping loss, our PDM generates high-fidelity and diverse samples that effectively simulate spectral variations, providing sufficient data for robust model training. Second, a target-aware transformer network is developed for discriminative feature extraction. By utilizing cross-attention, the proposed network deeply embeds prior target knowledge within the feature learning stage. This design guides the model to selectively prioritize the most salient spectral attributes, thereby enhancing its discriminative capability. Furthermore, a composite loss function is introduced to synergistically optimize the network. It enhances inter-class separability via a hardness-aware metric, enforces a clear decision boundary through classification supervision, and promotes intra-class compactness to counteract spectral variability. The superiority of the proposed SPDTT method is demonstrated through comprehensive experiments on five publicly available hyperspectral datasets. Results indicate that our approach substantially surpasses both classical and state-of-the-art methods in target detection performance, thereby confirming its effectiveness and robustness.
Published: 2026-02-06T20:49:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingchao Xue; Zitong Xu; Yang Xu; Le Sun; Zhihui Wei; Zebin Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3661488"&gt;10.1109/tgrs.2026.3661488&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;The performance of deep learning models in hyperspectral target detection (HTD) is significantly hampered by two primary challenges: the scarcity of labeled data and the intrinsic spectral variability of targets. To overcome these limitations, this paper introduces a novel HTD framework, termed SPDTT. First, a prior-spectrum-guided diffusion model (PDM) is designed for data augmentation. By incorporating a customized spectral angle mapping loss, our PDM generates high-fidelity and diverse samples that effectively simulate spectral variations, providing sufficient data for robust model training. Second, a target-aware transformer network is developed for discriminative feature extraction. By utilizing cross-attention, the proposed network deeply embeds prior target knowledge within the feature learning stage. This design guides the model to selectively prioritize the most salient spectral attributes, thereby enhancing its discriminative capability. Furthermore, a composite loss function is introduced to synergistically optimize the network. It enhances inter-class separability via a hardness-aware metric, enforces a clear decision boundary through classification supervision, and promotes intra-class compactness to counteract spectral variability. The superiority of the proposed SPDTT method is demonstrated through comprehensive experiments on five publicly available hyperspectral datasets. Results indicate that our approach substantially surpasses both classical and state-of-the-art methods in target detection performance, thereby confirming its effectiveness and robustness.&lt;/p&gt;</content:encoded></item><item><title>MDANet: A Lightweight Multi-Task Dynamic Adaptive Network for Real-Time Visual Perception in Autonomous Driving</title><link>https://doi.org/10.1109/tits.2026.3657786</link><guid>10.1109/tits.2026.3657786</guid><pubDate>Fri, 06 Feb 2026 20:52:07 +0000</pubDate><dc:creator>Xiao Ke</dc:creator><dc:creator>Jingyi Fang</dc:creator><dc:creator>Chaoying Chen</dc:creator><dc:creator>Huanqi Wu</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2026.3657786</prism:doi><description>Accurate and efficient multi-task perception remains a core challenge in autonomous driving, particularly under real-world constraints such as limited computational resources and dynamic environmental conditions. This paper presents MDANet, a lightweight multi-task dynamic adaptive network that jointly addresses three essential visual perception tasks within a unified framework: traffic object detection, drivable area segmentation, and lane detection. The proposed approach incorporates three key modules: a Triple-Dynamic Sampling strategy that enhances structured feature reconstruction through decoupled upsampling across spatial, channel, and semantic dimensions; a Geometric-Aware Dynamic Convolution module that separately models the elastic deformation of lane lines and the rigid structure of traffic objects to alleviate geometric conflicts; and a Scene-Adaptive Dynamic Fusion module that adjusts feature importance based on environmental context to improve cross-task alignment. Experiments on the BDD100K benchmark show that MDANet achieves a strong balance between accuracy and efficiency, with the lightweight MDANet-n (3.59M parameters) and the larger MDANet-s (13.22M parameters) both demonstrating competitive performance across all tasks. Moreover, cross-architecture evaluations indicate that integrating the proposed modules into other models consistently yields performance improvements, confirming their architectural generalizability and transferability. Qualitative visualizations and ablation studies further validate the robustness and practical value of the proposed design in complex and dynamic driving scenarios.
Published: 2026-02-06T20:52:07+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiao Ke; Jingyi Fang; Chaoying Chen; Huanqi Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2026.3657786"&gt;10.1109/tits.2026.3657786&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate and efficient multi-task perception remains a core challenge in autonomous driving, particularly under real-world constraints such as limited computational resources and dynamic environmental conditions. This paper presents MDANet, a lightweight multi-task dynamic adaptive network that jointly addresses three essential visual perception tasks within a unified framework: traffic object detection, drivable area segmentation, and lane detection. The proposed approach incorporates three key modules: a Triple-Dynamic Sampling strategy that enhances structured feature reconstruction through decoupled upsampling across spatial, channel, and semantic dimensions; a Geometric-Aware Dynamic Convolution module that separately models the elastic deformation of lane lines and the rigid structure of traffic objects to alleviate geometric conflicts; and a Scene-Adaptive Dynamic Fusion module that adjusts feature importance based on environmental context to improve cross-task alignment. Experiments on the BDD100K benchmark show that MDANet achieves a strong balance between accuracy and efficiency, with the lightweight MDANet-n (3.59M parameters) and the larger MDANet-s (13.22M parameters) both demonstrating competitive performance across all tasks. Moreover, cross-architecture evaluations indicate that integrating the proposed modules into other models consistently yields performance improvements, confirming their architectural generalizability and transferability. Qualitative visualizations and ablation studies further validate the robustness and practical value of the proposed design in complex and dynamic driving scenarios.&lt;/p&gt;</content:encoded></item><item><title>Visual language models show widespread visual deficits on neuropsychological tests</title><link>https://doi.org/10.1038/s42256-026-01179-y</link><guid>10.1038/s42256-026-01179-y</guid><pubDate>Fri, 06 Feb 2026 10:03:29 +0000</pubDate><dc:creator>Gene Tangtartharakul</dc:creator><dc:creator>Katherine R. Storrs</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-026-01179-y</prism:doi><description>Visual language models (VLMs) show remarkable performance in visual reasoning tasks, successfully tackling college-level challenges that require a high-level understanding of images. However, some recent reports of VLMs struggling to reason about elemental visual concepts such as orientation, position, continuity and occlusion suggest a potential gulf between human and VLM vision. Currently, few assessments enable a direct comparison between human and VLM performance, which limits our ability to measure alignment between the two systems. Here we use the toolkit of neuropsychology to systematically evaluate the capabilities of three state-of-the-art VLMs across low, mid and high visual domains. Using 51 tests drawn from 6 clinical and experimental psychology batteries, we characterize the visual abilities of leading VLMs relative to normative performance in healthy adults. While the models excel in straightforward object recognition tasks, we find widespread deficits in low- and mid-level visual abilities that would be considered clinically significant in humans. These selective deficits, profiled through validated test batteries, suggest that an artificial system can achieve complex object recognition without developing foundational visual concepts that in humans require no explicit training. Tangtartharakul and Storrs use standardized neuropsychological tests to compare human visual abilities with those of visual language models (VLMs). They report that while VLMs excel in high-level object recognition, they show deficits in low- and mid-level visual abilities.
Published: 2026-02-06T10:03:29+00:00
Venue: Nature Machine Intelligence
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gene Tangtartharakul; Katherine R. Storrs&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-026-01179-y"&gt;10.1038/s42256-026-01179-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Visual language models (VLMs) show remarkable performance in visual reasoning tasks, successfully tackling college-level challenges that require a high-level understanding of images. However, some recent reports of VLMs struggling to reason about elemental visual concepts such as orientation, position, continuity and occlusion suggest a potential gulf between human and VLM vision. Currently, few assessments enable a direct comparison between human and VLM performance, which limits our ability to measure alignment between the two systems. Here we use the toolkit of neuropsychology to systematically evaluate the capabilities of three state-of-the-art VLMs across low, mid and high visual domains. Using 51 tests drawn from 6 clinical and experimental psychology batteries, we characterize the visual abilities of leading VLMs relative to normative performance in healthy adults. While the models excel in straightforward object recognition tasks, we find widespread deficits in low- and mid-level visual abilities that would be considered clinically significant in humans. These selective deficits, profiled through validated test batteries, suggest that an artificial system can achieve complex object recognition without developing foundational visual concepts that in humans require no explicit training. Tangtartharakul and Storrs use standardized neuropsychological tests to compare human visual abilities with those of visual language models (VLMs). They report that while VLMs excel in high-level object recognition, they show deficits in low- and mid-level visual abilities.&lt;/p&gt;</content:encoded></item><item><title>Localized Background-aware Generative Distillation for Enhanced Remote Sensing Object Detection</title><link>https://doi.org/10.1109/tcsvt.2026.3661231</link><guid>10.1109/tcsvt.2026.3661231</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Chao Wang</dc:creator><dc:creator>Yanguang Sun</dc:creator><dc:creator>Jian Yang</dc:creator><dc:creator>Lei Luo</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3661231</prism:doi><description>Feature-based knowledge distillation has attracted significant attention in remote sensing object detection. The main challenge in this method is that feature distillation may misguide the detection of tiny remote sensing objects due to the lack of local background priors. To address this issue, this paper proposes the Localized Background-aware Generative Distillation (LBGD) method, which incorporates two key components: the lightweight diffusion reconstructor (LDR) and the patch-wise channel distillation (PCD) loss. LDR dynamically adjusts the receptive field to effectively capture the local background information surrounding the target. Meanwhile, PCD emphasizes the most salient patch regions in each channel, reducing the impact of global background information. To the best of our knowledge, localized background-aware generative distillation mechanisms have not been previously explored in remote sensing object detection. Numerous experimental results demonstrate that LBGD brings significant performance improvements, for example, SODA-A (+1.9% mAP), and DIOR (+2.8% mAP). The dataset and code are available at: https://github.com/wchao0601/LBGD.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Wang; Yanguang Sun; Jian Yang; Lei Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3661231"&gt;10.1109/tcsvt.2026.3661231&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Feature-based knowledge distillation has attracted significant attention in remote sensing object detection. The main challenge in this method is that feature distillation may misguide the detection of tiny remote sensing objects due to the lack of local background priors. To address this issue, this paper proposes the Localized Background-aware Generative Distillation (LBGD) method, which incorporates two key components: the lightweight diffusion reconstructor (LDR) and the patch-wise channel distillation (PCD) loss. LDR dynamically adjusts the receptive field to effectively capture the local background information surrounding the target. Meanwhile, PCD emphasizes the most salient patch regions in each channel, reducing the impact of global background information. To the best of our knowledge, localized background-aware generative distillation mechanisms have not been previously explored in remote sensing object detection. Numerous experimental results demonstrate that LBGD brings significant performance improvements, for example, SODA-A (+1.9% mAP), and DIOR (+2.8% mAP). The dataset and code are available at: https://github.com/wchao0601/LBGD.&lt;/p&gt;</content:encoded></item><item><title>Dual-Path Self-Supervised Spatiotemporal Learning for Satellite Video Scene Understanding</title><link>https://doi.org/10.1109/tgrs.2026.3660976</link><guid>10.1109/tgrs.2026.3660976</guid><pubDate>Fri, 06 Feb 2026 20:49:16 +0000</pubDate><dc:creator>Yuanhang Wang</dc:creator><dc:creator>Jiameng Li</dc:creator><dc:creator>Guoming Gao</dc:creator><dc:creator>Yanfeng Gu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3660976</prism:doi><description>Satellite video scene understanding (SVSU) is critical for dynamic Earth observation but faces challenges from the distinctive spatiotemporal patterns and the limited publicly available data. While self-supervised learning (SSL) has advanced representation learning for remote sensing imagery, its application to SVSU remains challenging due to the need for temporal motion modeling and data scarcity. To address these limitations, we propose a novel dual-path self-supervised spatiotemporal learning framework that disentangles spatial and temporal representation learning. A frozen Vision Transformer (ViT) backbone, pretrained on large-scale remote sensing images, preserves robust spatial semantics, whereas a trainable temporal encoder is introduced to capture motion dynamics specific to satellite videos. A feature interaction adapter (FIA) is integrated to fuse the information from both paths. To empower temporal modeling, we devise a dual reconstruction objective that jointly optimizes masked spatial appearance reconstruction and dynamic motion reconstruction during self-supervised pretraining. Experimental results demonstrate the superior performance of the proposed method.
Published: 2026-02-06T20:49:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanhang Wang; Jiameng Li; Guoming Gao; Yanfeng Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3660976"&gt;10.1109/tgrs.2026.3660976&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Satellite video scene understanding (SVSU) is critical for dynamic Earth observation but faces challenges from the distinctive spatiotemporal patterns and the limited publicly available data. While self-supervised learning (SSL) has advanced representation learning for remote sensing imagery, its application to SVSU remains challenging due to the need for temporal motion modeling and data scarcity. To address these limitations, we propose a novel dual-path self-supervised spatiotemporal learning framework that disentangles spatial and temporal representation learning. A frozen Vision Transformer (ViT) backbone, pretrained on large-scale remote sensing images, preserves robust spatial semantics, whereas a trainable temporal encoder is introduced to capture motion dynamics specific to satellite videos. A feature interaction adapter (FIA) is integrated to fuse the information from both paths. To empower temporal modeling, we devise a dual reconstruction objective that jointly optimizes masked spatial appearance reconstruction and dynamic motion reconstruction during self-supervised pretraining. Experimental results demonstrate the superior performance of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Identifying spatial single-cell-level interactions with graph transformer</title><link>https://doi.org/10.1038/s42256-026-01191-2</link><guid>10.1038/s42256-026-01191-2</guid><pubDate>Fri, 06 Feb 2026 10:02:54 +0000</pubDate><dc:creator>Xiangzheng Cheng</dc:creator><dc:creator>Suoqin Jin</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-026-01191-2</prism:doi><description>Identifying cell–cell interactions from imaging-based spatial transcriptomics suffers from limited gene panels. A new self-supervised graph transformer-based method can resolve spatial single-cell-level interactions without requiring known ligand–receptor pairs.
Published: 2026-02-06T10:02:54+00:00
Venue: Nature Machine Intelligence
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangzheng Cheng; Suoqin Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-026-01191-2"&gt;10.1038/s42256-026-01191-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Identifying cell–cell interactions from imaging-based spatial transcriptomics suffers from limited gene panels. A new self-supervised graph transformer-based method can resolve spatial single-cell-level interactions without requiring known ligand–receptor pairs.&lt;/p&gt;</content:encoded></item><item><title>Dynamic Fusion of Hyperspectral and LiDAR Data for Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3661274</link><guid>10.1109/tgrs.2026.3661274</guid><pubDate>Fri, 06 Feb 2026 20:49:16 +0000</pubDate><dc:creator>Xukun Zang</dc:creator><dc:creator>Lina Xu</dc:creator><dc:creator>Yuxiang Zhang</dc:creator><dc:creator>Yanni Dong</dc:creator><dc:creator>Qian Shi</dc:creator><dc:creator>Lei Fang</dc:creator><dc:creator>Bo Du</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3661274</prism:doi><description>Hyperspectral target detection has achieved significant progress by exploiting the rich spectral and spatial information. However, the spectral similarity between different types of land cover and the interference of shadows still limit the target detection performance. To address these issues, we introduce LiDAR data to distinguish spectrally similar and shadow-affected land covers by using elevation differences. We propose a cross-modal dynamic fusion method (DFTD) for target detection. This method constructs a cross-modal sample selection module based on the spectral information divergence and k-means algorithms, which applies the priori target spectra from HSI to select target and background training samples for HSI and LiDAR data. Additionally, the method develops a cross-modal feature interactive extraction module by a dual-branch spatial sequence Mamba encoder and a cross-modal interactive attention module to obtain encoded features with enhanced discriminability and robustness. Furthermore, the method proposes a cross-modal dynamic feature fusion module via a designed mask deep deterministic policy gradient algorithm, which dynamically generates modality-specific masks based on the varying discriminative capacities of HSI and LiDAR data to effectively fuse cross-modal encoded features. The fused features are fed into a detection head with fully connected layers to achieve target detection results. Extensive experiments on three publicly available HSI-LiDAR datasets demonstrate that DFTD outperforms state-of-the-art methods.
Published: 2026-02-06T20:49:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xukun Zang; Lina Xu; Yuxiang Zhang; Yanni Dong; Qian Shi; Lei Fang; Bo Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3661274"&gt;10.1109/tgrs.2026.3661274&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral target detection has achieved significant progress by exploiting the rich spectral and spatial information. However, the spectral similarity between different types of land cover and the interference of shadows still limit the target detection performance. To address these issues, we introduce LiDAR data to distinguish spectrally similar and shadow-affected land covers by using elevation differences. We propose a cross-modal dynamic fusion method (DFTD) for target detection. This method constructs a cross-modal sample selection module based on the spectral information divergence and k-means algorithms, which applies the priori target spectra from HSI to select target and background training samples for HSI and LiDAR data. Additionally, the method develops a cross-modal feature interactive extraction module by a dual-branch spatial sequence Mamba encoder and a cross-modal interactive attention module to obtain encoded features with enhanced discriminability and robustness. Furthermore, the method proposes a cross-modal dynamic feature fusion module via a designed mask deep deterministic policy gradient algorithm, which dynamically generates modality-specific masks based on the varying discriminative capacities of HSI and LiDAR data to effectively fuse cross-modal encoded features. The fused features are fed into a detection head with fully connected layers to achieve target detection results. Extensive experiments on three publicly available HSI-LiDAR datasets demonstrate that DFTD outperforms state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>MDSLCA: Multi-scale Dilated Spatial and Local Channel Attention for LiDAR Point Cloud Semantic Segmentation</title><link>https://doi.org/10.1109/tcsvt.2026.3662054</link><guid>10.1109/tcsvt.2026.3662054</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Jinzheng Guang</dc:creator><dc:creator>Qianyi Zhang</dc:creator><dc:creator>Jingtai Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662054</prism:doi><description>LiDAR point cloud semantic segmentation is a fundamental task in 3D perception, essential for applications like autonomous driving and mobile robotics. However, the inherent sparsity and irregular distribution of point cloud data pose significant challenges to achieving high segmentation accuracy. To address these issues, we propose a Multi-scale Dilated Spatial and Local Channel Attention (MDSLCA) network, which integrates sparse convolutional operations with advanced attention mechanisms for accurate and efficient 3D semantic segmentation. The MDSLCA network features three core components: the Heterogeneous Convolution Skip Connection (HCSC) architecture, which bridges the semantic gap between the encoder and decoder; the Multi-Scale Local Attention (MSLA) module, which enhances focus on salient spatial regions and important feature channels while suppressing irrelevant information; and the Multi-Scale Dilated Spatial Attention (MSDSA) module, which improves spatial feature learning through multi-scale dilated convolutions that effectively capture long-range spatial dependencies in point cloud data. Extensive experiments conducted on the SemanticKITTI dataset demonstrate that our MDSLCA achieves a mean Intersection-over-Union (mIoU) of 73.7%, surpassing current global modeling approaches such as Point Transformer and achieving a new state-of-the-art (SOTA) performance. These results validate the effectiveness of our method in capturing fine-grained local features and modeling high-level semantic context, demonstrating its strong capability to handle the inherent challenges of point cloud data. Our project is publicly available at https://github.com/jinzhengguang/MDSLCA.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinzheng Guang; Qianyi Zhang; Jingtai Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662054"&gt;10.1109/tcsvt.2026.3662054&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;LiDAR point cloud semantic segmentation is a fundamental task in 3D perception, essential for applications like autonomous driving and mobile robotics. However, the inherent sparsity and irregular distribution of point cloud data pose significant challenges to achieving high segmentation accuracy. To address these issues, we propose a Multi-scale Dilated Spatial and Local Channel Attention (MDSLCA) network, which integrates sparse convolutional operations with advanced attention mechanisms for accurate and efficient 3D semantic segmentation. The MDSLCA network features three core components: the Heterogeneous Convolution Skip Connection (HCSC) architecture, which bridges the semantic gap between the encoder and decoder; the Multi-Scale Local Attention (MSLA) module, which enhances focus on salient spatial regions and important feature channels while suppressing irrelevant information; and the Multi-Scale Dilated Spatial Attention (MSDSA) module, which improves spatial feature learning through multi-scale dilated convolutions that effectively capture long-range spatial dependencies in point cloud data. Extensive experiments conducted on the SemanticKITTI dataset demonstrate that our MDSLCA achieves a mean Intersection-over-Union (mIoU) of 73.7%, surpassing current global modeling approaches such as Point Transformer and achieving a new state-of-the-art (SOTA) performance. These results validate the effectiveness of our method in capturing fine-grained local features and modeling high-level semantic context, demonstrating its strong capability to handle the inherent challenges of point cloud data. Our project is publicly available at https://github.com/jinzhengguang/MDSLCA.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Class-Incremental SAR Target Recognition Based on Dynamic Task-Adaptive Classifier</title><link>https://doi.org/10.3390/rs18030527</link><guid>10.3390/rs18030527</guid><pubDate>Fri, 06 Feb 2026 11:10:38 +0000</pubDate><dc:creator>Dan Li</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Yong Li</dc:creator><dc:creator>Wei Cheng</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030527</prism:doi><description>Current synthetic aperture radar automatic target recognition (SAR ATR) tasks face challenges including limited training samples and poor generalization capability to novel classes. To address these issues, few-shot class-incremental learning (FSCIL) has emerged as a promising research direction. Few-shot learning facilitates the expedited adaptation to novel tasks utilizing a limited number of labeled samples, whereas incremental learning concentrates on the continuous refinement of the model as new categories are incorporated without eradicating previously learned knowledge. Although both methodologies present potential resolutions to the challenges of sample scarcity and class evolution in SAR target recognition, they are not without their own set of difficulties. Fine-tuning with emerging classes can perturb the feature distribution of established classes, culminating in catastrophic forgetting, while training exclusively on a handful of new samples can induce bias towards older classes, leading to distribution collapse and overfitting. To surmount these limitations and satisfy practical application requirements, we propose a Few-Shot Class-Incremental SAR Target Recognition method based on a Dynamic Task-Adaptive Classifier (DTAC). This approach underscores task adaptability through a feature extraction module, a task information encoding module, and a classifier generation module. The feature extraction module discerns both target-specific and task-specific characteristics, while the task information encoding module modulates the network parameters of the classifier generation module based on pertinent task information, thereby improving adaptability. Our innovative classifier generation module, honed with task-specific insights, dynamically assembles classifiers tailored to the current task, effectively accommodating a variety of scenarios and novel class samples. Our extensive experiments on SAR datasets demonstrate that our proposed method generally outperforms the baselines in few-shot class incremental SAR target recognition.
Published: 2026-02-06T11:10:38+00:00
Venue: Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dan Li; Feng Zhao; Yong Li; Wei Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030527"&gt;10.3390/rs18030527&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Current synthetic aperture radar automatic target recognition (SAR ATR) tasks face challenges including limited training samples and poor generalization capability to novel classes. To address these issues, few-shot class-incremental learning (FSCIL) has emerged as a promising research direction. Few-shot learning facilitates the expedited adaptation to novel tasks utilizing a limited number of labeled samples, whereas incremental learning concentrates on the continuous refinement of the model as new categories are incorporated without eradicating previously learned knowledge. Although both methodologies present potential resolutions to the challenges of sample scarcity and class evolution in SAR target recognition, they are not without their own set of difficulties. Fine-tuning with emerging classes can perturb the feature distribution of established classes, culminating in catastrophic forgetting, while training exclusively on a handful of new samples can induce bias towards older classes, leading to distribution collapse and overfitting. To surmount these limitations and satisfy practical application requirements, we propose a Few-Shot Class-Incremental SAR Target Recognition method based on a Dynamic Task-Adaptive Classifier (DTAC). This approach underscores task adaptability through a feature extraction module, a task information encoding module, and a classifier generation module. The feature extraction module discerns both target-specific and task-specific characteristics, while the task information encoding module modulates the network parameters of the classifier generation module based on pertinent task information, thereby improving adaptability. Our innovative classifier generation module, honed with task-specific insights, dynamically assembles classifiers tailored to the current task, effectively accommodating a variety of scenarios and novel class samples. Our extensive experiments on SAR datasets demonstrate that our proposed method generally outperforms the baselines in few-shot class incremental SAR target recognition.&lt;/p&gt;</content:encoded></item><item><title>Pioneering Video Semantic Segmentation with Light Field Imaging and Spatial-Angular Temporal Fusion</title><link>https://doi.org/10.1109/tcsvt.2026.3662156</link><guid>10.1109/tcsvt.2026.3662156</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Chen Jia</dc:creator><dc:creator>Hui Liu</dc:creator><dc:creator>Fan Shi</dc:creator><dc:creator>Xu Cheng</dc:creator><dc:creator>Shengyong Chen</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662156</prism:doi><description>Video semantic segmentation aims to assign a semantic label to each pixel in a video by jointly exploiting spatial context and temporal coherence, and it is essential for applications such as autonomous perception and intelligent surveillance. However, conventional RGB videos lack plenoptic cues, making segmentation unreliable in visually complex conditions, including occlusion, low-light environments, and transparent surfaces. Light field imaging captures both spatial and angular information via a micro-lens array, providing multi-view geometric cues that can enhance scene representation and improve segmentation robustness. Motivated by these advantages, we investigate light field video semantic segmentation and propose the Light Field Spatial-Angular Complementary Network (LFCNet) for precise and efficient segmentation under challenging visual settings. LFCNet first employs an efficient pooling strategy to extract multi-scale macro-pixel spatial context features, and then introduces the Angular Modeling Module (AMM) and Context Change Module (CCM) to capture angular cues and handle view-dependent contextual variations. Furthermore, we design a Temporal-Channel Correlation Module (TCCM) to enhance temporal feature consistency by selectively refining channel-wise representations across frames. To support training and evaluation, we construct a light field video dataset based on macro-pixel representations as a benchmark for this task. Extensive experiments on four datasets demonstrate that LFCNet achieves superior segmentation accuracy and competitive efficiency.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Jia; Hui Liu; Fan Shi; Xu Cheng; Shengyong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662156"&gt;10.1109/tcsvt.2026.3662156&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Video semantic segmentation aims to assign a semantic label to each pixel in a video by jointly exploiting spatial context and temporal coherence, and it is essential for applications such as autonomous perception and intelligent surveillance. However, conventional RGB videos lack plenoptic cues, making segmentation unreliable in visually complex conditions, including occlusion, low-light environments, and transparent surfaces. Light field imaging captures both spatial and angular information via a micro-lens array, providing multi-view geometric cues that can enhance scene representation and improve segmentation robustness. Motivated by these advantages, we investigate light field video semantic segmentation and propose the Light Field Spatial-Angular Complementary Network (LFCNet) for precise and efficient segmentation under challenging visual settings. LFCNet first employs an efficient pooling strategy to extract multi-scale macro-pixel spatial context features, and then introduces the Angular Modeling Module (AMM) and Context Change Module (CCM) to capture angular cues and handle view-dependent contextual variations. Furthermore, we design a Temporal-Channel Correlation Module (TCCM) to enhance temporal feature consistency by selectively refining channel-wise representations across frames. To support training and evaluation, we construct a light field video dataset based on macro-pixel representations as a benchmark for this task. Extensive experiments on four datasets demonstrate that LFCNet achieves superior segmentation accuracy and competitive efficiency.&lt;/p&gt;</content:encoded></item><item><title>Contrastive Decoupling: Dynamic Regularization for Enhanced Fine-Grained Image Classification</title><link>https://doi.org/10.1109/tcsvt.2026.3661760</link><guid>10.1109/tcsvt.2026.3661760</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Zheyuan Wang</dc:creator><dc:creator>Tingyao Li</dc:creator><dc:creator>Yiming Qin</dc:creator><dc:creator>Bin Sheng</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3661760</prism:doi><description>Fine-grained visual classification remains challenging due to subtle inter-class differences and significant intra-class variations. We solve this problem from the representation space perspective and propose Contrastive Decoupled Regularization (CoDeR), a module-level regularization method that guides representation learning using class prototypes as anchors without introducing any learnable parameters, steering hierarchical representations toward more discriminative directions. Specifically, for a target module Bi, we maintain an independent cache that collects the module’s outputs and corresponding class labels during each training epoch. At the end of each epoch, features are aggregated by class and mapped onto a hypersphere to compute cluster centers as class prototypes. In the next epoch, these prototypes guide updates in module Bi, pulling representations toward their ground-truth class prototypes and pushing them away from others. This strengthens inter-class separation in the representation space and directly addresses the core challenge of fine-grained recognition. Furthermore, we apply CoDeR in parallel across multiple modules to accelerate information propagation to earlier layers. This enables shallow layers to learn semantically meaningful representations earlier in training and mitigates the delayed representation update problem. Overall, CoDeR provides a simple, general, and effective supervised regularization mechanism that demonstrates the value of imposing constraints on high-dimensional representations. We conduct extensive experiments on ImageNet, six fundus medical imaging datasets, and two standard semi-supervised learning benchmarks. Consistent improvements across all settings validate the effectiveness and cross-domain applicability of our method.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheyuan Wang; Tingyao Li; Yiming Qin; Bin Sheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3661760"&gt;10.1109/tcsvt.2026.3661760&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-grained visual classification remains challenging due to subtle inter-class differences and significant intra-class variations. We solve this problem from the representation space perspective and propose Contrastive Decoupled Regularization (CoDeR), a module-level regularization method that guides representation learning using class prototypes as anchors without introducing any learnable parameters, steering hierarchical representations toward more discriminative directions. Specifically, for a target module Bi, we maintain an independent cache that collects the module’s outputs and corresponding class labels during each training epoch. At the end of each epoch, features are aggregated by class and mapped onto a hypersphere to compute cluster centers as class prototypes. In the next epoch, these prototypes guide updates in module Bi, pulling representations toward their ground-truth class prototypes and pushing them away from others. This strengthens inter-class separation in the representation space and directly addresses the core challenge of fine-grained recognition. Furthermore, we apply CoDeR in parallel across multiple modules to accelerate information propagation to earlier layers. This enables shallow layers to learn semantically meaningful representations earlier in training and mitigates the delayed representation update problem. Overall, CoDeR provides a simple, general, and effective supervised regularization mechanism that demonstrates the value of imposing constraints on high-dimensional representations. We conduct extensive experiments on ImageNet, six fundus medical imaging datasets, and two standard semi-supervised learning benchmarks. Consistent improvements across all settings validate the effectiveness and cross-domain applicability of our method.&lt;/p&gt;</content:encoded></item><item><title>Enhancing shape bias for object detection</title><link>https://doi.org/10.1016/j.neucom.2026.132931</link><guid>10.1016/j.neucom.2026.132931</guid><pubDate>Fri, 06 Feb 2026 17:06:53 +0000</pubDate><dc:creator>Jiwen Tang</dc:creator><dc:creator>Gu Wang</dc:creator><dc:creator>Ruida Zhang</dc:creator><dc:creator>Xiangyang Ji</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132931</prism:doi><description>Convolutional Neural Networks (CNNs) are widely used for object detection tasks, whereas recent studies have shown that they rely more on texture rather than shape for object recognition, a phenomenon known as texture bias. This bias makes them vulnerable to image corruptions, domain shifts, and adversarial perturbations, posing significant challenges for real-world deployment, especially in safety-critical and industrial applications. Despite its significance, texture bias in object detection remains largely underexplored. To address this gap, we first conduct a comprehensive analysis of texture bias across multiple widely-used CNN-based detection architectures, demonstrating the widespread presence and detrimental impact of this issue. Motivated by these findings, we propose a simple yet effective method, TexDrop, to increase shape bias in CNNs and therefore improve their accuracy and robustness. Specifically, TexDrop randomly drops out the texture and color of the training images through straightforward edge detection, forcing models to learn to detect objects based on their shape, thus increasing shape bias. Unlike prior approaches that require architectural modifications, extensive additional training data or complex regularization schemes, TexDrop is model-agnostic, easy to integrate into existing training pipelines, and incurs negligible computational overhead. Intensive experiments on Pascal VOC, COCO, and various corrupted COCO datasets demonstrate that TexDrop not only improves detection performance across multiple architectures but also consistently enhances robustness against various image corruptions and texture variations. Our study provides empirical insights into texture dependence in object detectors and contributes a practical solution for developing more robust and reliable object detection systems in real-world applications.
Published: 2026-02-06T17:06:53+00:00
Venue: Neurocomputing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiwen Tang; Gu Wang; Ruida Zhang; Xiangyang Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132931"&gt;10.1016/j.neucom.2026.132931&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) are widely used for object detection tasks, whereas recent studies have shown that they rely more on texture rather than shape for object recognition, a phenomenon known as texture bias. This bias makes them vulnerable to image corruptions, domain shifts, and adversarial perturbations, posing significant challenges for real-world deployment, especially in safety-critical and industrial applications. Despite its significance, texture bias in object detection remains largely underexplored. To address this gap, we first conduct a comprehensive analysis of texture bias across multiple widely-used CNN-based detection architectures, demonstrating the widespread presence and detrimental impact of this issue. Motivated by these findings, we propose a simple yet effective method, TexDrop, to increase shape bias in CNNs and therefore improve their accuracy and robustness. Specifically, TexDrop randomly drops out the texture and color of the training images through straightforward edge detection, forcing models to learn to detect objects based on their shape, thus increasing shape bias. Unlike prior approaches that require architectural modifications, extensive additional training data or complex regularization schemes, TexDrop is model-agnostic, easy to integrate into existing training pipelines, and incurs negligible computational overhead. Intensive experiments on Pascal VOC, COCO, and various corrupted COCO datasets demonstrate that TexDrop not only improves detection performance across multiple architectures but also consistently enhances robustness against various image corruptions and texture variations. Our study provides empirical insights into texture dependence in object detectors and contributes a practical solution for developing more robust and reliable object detection systems in real-world applications.&lt;/p&gt;</content:encoded></item><item><title>Collaborative Model and Data Adaptation at Test Time</title><link>https://doi.org/10.1109/tcsvt.2026.3661181</link><guid>10.1109/tcsvt.2026.3661181</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Chunyun Zhang</dc:creator><dc:creator>Fujun Yang</dc:creator><dc:creator>Chaoran Cui</dc:creator><dc:creator>Shuai Gong</dc:creator><dc:creator>Wenna Wang</dc:creator><dc:creator>Xue Lin</dc:creator><dc:creator>Yonggang Qi</dc:creator><dc:creator>Lei Zhu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3661181</prism:doi><description>Traditional Test-Time Adaptation (TTA) methods primarily focus on updating the parameters of a pre-trained source model to better fit the target domain. In contrast, recent diffusion-driven TTA approaches leverage an unconditional diffusion model trained on the source domain to map target samples towards the source distribution, without modifying the model parameters. In this paper, we propose to combine the strengths of model adaptation and data adaptation to achieve more effective alignment between the source model and target data. Unlike existing two-stage methods that perform model and data adaptation independently, we introduce a unified Collaborative Model and Data Adaptation (CMDA) framework that integrates the two processes in a mutually beneficial manner. Specifically, model predictions on synthetic target samples serve as category-discriminative signals to guide the reverse diffusion process during data adaptation. Conversely, the synthetic data generated through data adaptation are used to progressively update and refine the source model. This bidirectional collaboration between model and data adaptation occurs iteratively, progressively aligning the source model with the target data. To further enhance prediction accuracy, we designed a lightweight and learnable aggregation network that ensembles predictions from the source and adapted models on both the original and synthetic target samples. This network dynamically integrates complementary predictions, improving the robustness and confidence of the final outputs. Extensive experiments on four benchmark datasets demonstrate that CMDA achieves state-of-the-art performance under the TTA setting.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunyun Zhang; Fujun Yang; Chaoran Cui; Shuai Gong; Wenna Wang; Xue Lin; Yonggang Qi; Lei Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3661181"&gt;10.1109/tcsvt.2026.3661181&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Traditional Test-Time Adaptation (TTA) methods primarily focus on updating the parameters of a pre-trained source model to better fit the target domain. In contrast, recent diffusion-driven TTA approaches leverage an unconditional diffusion model trained on the source domain to map target samples towards the source distribution, without modifying the model parameters. In this paper, we propose to combine the strengths of model adaptation and data adaptation to achieve more effective alignment between the source model and target data. Unlike existing two-stage methods that perform model and data adaptation independently, we introduce a unified Collaborative Model and Data Adaptation (CMDA) framework that integrates the two processes in a mutually beneficial manner. Specifically, model predictions on synthetic target samples serve as category-discriminative signals to guide the reverse diffusion process during data adaptation. Conversely, the synthetic data generated through data adaptation are used to progressively update and refine the source model. This bidirectional collaboration between model and data adaptation occurs iteratively, progressively aligning the source model with the target data. To further enhance prediction accuracy, we designed a lightweight and learnable aggregation network that ensembles predictions from the source and adapted models on both the original and synthetic target samples. This network dynamically integrates complementary predictions, improving the robustness and confidence of the final outputs. Extensive experiments on four benchmark datasets demonstrate that CMDA achieves state-of-the-art performance under the TTA setting.&lt;/p&gt;</content:encoded></item><item><title>Multi-feature collaboration with spatial-frequency learning guided by vision foundation model for remote sensing image captioning</title><link>https://doi.org/10.1016/j.neucom.2026.132936</link><guid>10.1016/j.neucom.2026.132936</guid><pubDate>Fri, 06 Feb 2026 16:29:34 +0000</pubDate><dc:creator>Yijie Zhang</dc:creator><dc:creator>Jian Cheng</dc:creator><dc:creator>Ziying Xia</dc:creator><dc:creator>Siyu Liu</dc:creator><dc:creator>Changjian Deng</dc:creator><dc:creator>Zunni Zhu</dc:creator><dc:creator>Zichong Chen</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132936</prism:doi><description>Remote Sensing Image Captioning (RSIC) refers to the core task of utilizing technology to automatically generate natural language text, accurately describing the types, spatial distribution, and complex semantic relationships of ground objects in remote sensing images. However, the spatial semantic relationships of ground objects in remote sensing images are complex and their scales are variable, making it challenging to achieve accurate descriptions. In this paper, we propose a multi-feature collaborative network (MFC-Net) with spatial-frequency learning guided by vision foundation model (VFM). MFC-Net follows an encoder-decoder architecture: the encoder combines multi-feature representations in both spatial and frequency domains, while the decoder utilizes a transformer-based structure for caption generation. To better comprehend the unique geographical attributes and spatial relationships of ground objects, we leverage domain knowledge captured by the pre-trained Remote Sensing Vision Foundation Model (RS-VFM) to collaboratively guide the visual features generated by the CLIP image encoder and ResNet-50 branch through the Contextual Semantic Guidance Model (CSGM) and the Contextual Detail Guidance Model (CDGM). Furthermore, to better understand the multi-scale variations of ground objects, we transform multi-level features into the frequency domain using wavelet transform, efficiently extracting multi-scale features through partitioning them by frequency bands. Extensive experiments on two benchmark RSIC datasets demonstrate the superiority of our approach.
Published: 2026-02-06T16:29:34+00:00
Venue: Neurocomputing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yijie Zhang; Jian Cheng; Ziying Xia; Siyu Liu; Changjian Deng; Zunni Zhu; Zichong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132936"&gt;10.1016/j.neucom.2026.132936&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Remote Sensing Image Captioning (RSIC) refers to the core task of utilizing technology to automatically generate natural language text, accurately describing the types, spatial distribution, and complex semantic relationships of ground objects in remote sensing images. However, the spatial semantic relationships of ground objects in remote sensing images are complex and their scales are variable, making it challenging to achieve accurate descriptions. In this paper, we propose a multi-feature collaborative network (MFC-Net) with spatial-frequency learning guided by vision foundation model (VFM). MFC-Net follows an encoder-decoder architecture: the encoder combines multi-feature representations in both spatial and frequency domains, while the decoder utilizes a transformer-based structure for caption generation. To better comprehend the unique geographical attributes and spatial relationships of ground objects, we leverage domain knowledge captured by the pre-trained Remote Sensing Vision Foundation Model (RS-VFM) to collaboratively guide the visual features generated by the CLIP image encoder and ResNet-50 branch through the Contextual Semantic Guidance Model (CSGM) and the Contextual Detail Guidance Model (CDGM). Furthermore, to better understand the multi-scale variations of ground objects, we transform multi-level features into the frequency domain using wavelet transform, efficiently extracting multi-scale features through partitioning them by frequency bands. Extensive experiments on two benchmark RSIC datasets demonstrate the superiority of our approach.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Alignment and Fusion: A Survey</title><link>https://doi.org/10.1007/s11263-025-02667-1</link><guid>10.1007/s11263-025-02667-1</guid><pubDate>Fri, 06 Feb 2026 03:27:20 +0000</pubDate><dc:creator>Songtao Li</dc:creator><dc:creator>Hao Tang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02667-1</prism:doi><description>This survey provides a comprehensive overview of recent advances in multimodal alignment and fusion within the field of machine learning, driven by the increasing availability and diversity of data modalities such as text, images, audio, and video. Unlike previous surveys that often focus on specific modalities or limited fusion strategies, our work presents a structure-centric and method-driven framework that emphasizes generalizable techniques. We systematically categorize and analyze key approaches to alignment and fusion through both structural perspectives—data-level, feature-level, and output-level fusion—and methodological paradigms—including statistical, kernel-based, graphical, generative, contrastive, attention-based, and large language model (LLM)-based methods, drawing insights from an extensive review of over 260 relevant studies. Furthermore, this survey highlights critical challenges such as cross-modal misalignment, computational bottlenecks, data quality issues, and the modality gap, along with recent efforts to address them. Applications ranging from social media analysis and medical imaging to emotion recognition and embodied AI are explored to illustrate the real-world impact of robust multimodal systems. The insights provided aim to guide future research toward optimizing multimodal learning systems for improved scalability, robustness, and generalizability across diverse domains.
Published: 2026-02-06T03:27:20+00:00
Venue: International Journal of Computer Vision
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songtao Li; Hao Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02667-1"&gt;10.1007/s11263-025-02667-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;This survey provides a comprehensive overview of recent advances in multimodal alignment and fusion within the field of machine learning, driven by the increasing availability and diversity of data modalities such as text, images, audio, and video. Unlike previous surveys that often focus on specific modalities or limited fusion strategies, our work presents a structure-centric and method-driven framework that emphasizes generalizable techniques. We systematically categorize and analyze key approaches to alignment and fusion through both structural perspectives—data-level, feature-level, and output-level fusion—and methodological paradigms—including statistical, kernel-based, graphical, generative, contrastive, attention-based, and large language model (LLM)-based methods, drawing insights from an extensive review of over 260 relevant studies. Furthermore, this survey highlights critical challenges such as cross-modal misalignment, computational bottlenecks, data quality issues, and the modality gap, along with recent efforts to address them. Applications ranging from social media analysis and medical imaging to emotion recognition and embodied AI are explored to illustrate the real-world impact of robust multimodal systems. The insights provided aim to guide future research toward optimizing multimodal learning systems for improved scalability, robustness, and generalizability across diverse domains.&lt;/p&gt;</content:encoded></item><item><title>Enabling Nearshore Cross-Modal Video Object Detector to Learn More Accurate Spatial and Temporal Information</title><link>https://doi.org/10.1016/j.knosys.2026.115426</link><guid>10.1016/j.knosys.2026.115426</guid><pubDate>Fri, 06 Feb 2026 16:30:27 +0000</pubDate><dc:creator>Yuanlin Zhao</dc:creator><dc:creator>Jiangang Ding</dc:creator><dc:creator>Yansong Wang</dc:creator><dc:creator>Yihui Shan</dc:creator><dc:creator>Lili Pei</dc:creator><dc:creator>Wei Li</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115426</prism:doi><description>Nearshore scenarios are frequently affected by fog and contain a variety of objects exhibiting distinct motion patterns. These inherent factors pose significant challenges for accurate object detection in nearshore scenarios. Common approach is to utilize Video Object Detection (VOD) to learn the spatial features and motion information of nearshore objects. However, this method becomes hindered in situations involving foggy conditions or when different objects share similar optical characteristics, thus impeding effective pipeline modeling. To address these challenges, we propose a nearshore Cross-modal Video Object Detector (CVODNet). By leveraging learnable feature interaction between Infrared (IR) and visible light videos, we reduce the obstacles in pipeline modeling caused by the transient loss of features from unimodal. Learning from correlated frames to obtain the optimal weights for moving objects. Finally, deformable convolution is employed to address the challenges of pixel-level misalignment in cross-modal data presented in video form. After end-to-end training, CVODNet achieves State-of-the-art (SOTA) performance in benchmark evaluations.
Published: 2026-02-06T16:30:27+00:00
Venue: Knowledge-Based Systems
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanlin Zhao; Jiangang Ding; Yansong Wang; Yihui Shan; Lili Pei; Wei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115426"&gt;10.1016/j.knosys.2026.115426&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Nearshore scenarios are frequently affected by fog and contain a variety of objects exhibiting distinct motion patterns. These inherent factors pose significant challenges for accurate object detection in nearshore scenarios. Common approach is to utilize Video Object Detection (VOD) to learn the spatial features and motion information of nearshore objects. However, this method becomes hindered in situations involving foggy conditions or when different objects share similar optical characteristics, thus impeding effective pipeline modeling. To address these challenges, we propose a nearshore Cross-modal Video Object Detector (CVODNet). By leveraging learnable feature interaction between Infrared (IR) and visible light videos, we reduce the obstacles in pipeline modeling caused by the transient loss of features from unimodal. Learning from correlated frames to obtain the optimal weights for moving objects. Finally, deformable convolution is employed to address the challenges of pixel-level misalignment in cross-modal data presented in video form. After end-to-end training, CVODNet achieves State-of-the-art (SOTA) performance in benchmark evaluations.&lt;/p&gt;</content:encoded></item><item><title>DMFusion: Degradation-Customized Mixture-of-Experts with Adaptive Discrimination for Multi-Modal Image Fusion</title><link>https://doi.org/10.1109/tcsvt.2026.3661209</link><guid>10.1109/tcsvt.2026.3661209</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Tao Chen</dc:creator><dc:creator>Chuang Wang</dc:creator><dc:creator>Yudong Zhang</dc:creator><dc:creator>Kaijian Xia</dc:creator><dc:creator>Pengjiang Qian</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3661209</prism:doi><description>Real-world multi-modal image fusion is hindered by mixed and unknown degradations such as low-light noise, blur, and exposure shifts. Prior fusion methods seldom estimate degradation explicitly at the modality level, which limits conditional fusion when the underlying degradation distribution shifts across scenes, sensors, and tasks. We introduce DMFusion, a two-stage, degradation-aware framework that couples degradation inference with conditional expert routing and pixel-level integration. A CLIP-LoRA discriminator estimates modality-specific degradation vectors, which condition a Degradation-Customized Mixture of Experts to select specialized fusion and restoration pathways. Guided by these selections, a FusionGate decoder performs pixel-level integration and reconstructs both a fused image and high-quality restored source images. Evaluated on LLVIP, RoadScene, and MSRS with 1,491 training pairs and 389 test pairs, DMFusion delivers state-of-the-art performance across standard fusion metrics and remains robust under severe degradations. On the M3FD detection task, the fused images reach mAP at IoU 0.50 of 0.879, while a fusion-only mode attains 125 milliseconds per image, and the multi-output mode remains efficient. These results show that explicit degradation inference, realized through learned conditioning of sparse experts and gated decoding, yields reliable fusion quality and practical benefits for downstream vision systems. The code is publicly available at https://github.com/CrisT777-JN/DMFusion.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Chen; Chuang Wang; Yudong Zhang; Kaijian Xia; Pengjiang Qian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3661209"&gt;10.1109/tcsvt.2026.3661209&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Real-world multi-modal image fusion is hindered by mixed and unknown degradations such as low-light noise, blur, and exposure shifts. Prior fusion methods seldom estimate degradation explicitly at the modality level, which limits conditional fusion when the underlying degradation distribution shifts across scenes, sensors, and tasks. We introduce DMFusion, a two-stage, degradation-aware framework that couples degradation inference with conditional expert routing and pixel-level integration. A CLIP-LoRA discriminator estimates modality-specific degradation vectors, which condition a Degradation-Customized Mixture of Experts to select specialized fusion and restoration pathways. Guided by these selections, a FusionGate decoder performs pixel-level integration and reconstructs both a fused image and high-quality restored source images. Evaluated on LLVIP, RoadScene, and MSRS with 1,491 training pairs and 389 test pairs, DMFusion delivers state-of-the-art performance across standard fusion metrics and remains robust under severe degradations. On the M3FD detection task, the fused images reach mAP at IoU 0.50 of 0.879, while a fusion-only mode attains 125 milliseconds per image, and the multi-output mode remains efficient. These results show that explicit degradation inference, realized through learned conditioning of sparse experts and gated decoding, yields reliable fusion quality and practical benefits for downstream vision systems. The code is publicly available at https://github.com/CrisT777-JN/DMFusion.&lt;/p&gt;</content:encoded></item><item><title>SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation</title><link>https://arxiv.org/abs/2602.04712v1</link><guid>http://arxiv.org/abs/2602.04712v1</guid><pubDate>Wed, 04 Feb 2026 16:23:16 +0000</pubDate><dc:creator>David F. Ramirez</dc:creator><dc:creator>Tim Overman</dc:creator><dc:creator>Kristen Jaskie</dc:creator><dc:creator>Joe Marvin</dc:creator><dc:creator>Andreas Spanias</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.
Published: 2026-02-04T16:23:16+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David F. Ramirez; Tim Overman; Kristen Jaskie; Joe Marvin; Andreas Spanias&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Subspace Disentangling for Light Field Spatial Super-Resolution</title><link>https://doi.org/10.1109/tcsvt.2026.3661516</link><guid>10.1109/tcsvt.2026.3661516</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Fengyuan Zhang</dc:creator><dc:creator>Yingqian Wang</dc:creator><dc:creator>Xueying Wang</dc:creator><dc:creator>Zhengyu Liang</dc:creator><dc:creator>Longguang Wang</dc:creator><dc:creator>Lvli Tian</dc:creator><dc:creator>Jungang Yang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3661516</prism:doi><description>Light field (LF) spatial super-resolution (SR) aims at reconstructing high-resolution LF images from low-resolution observations. Recently, subspace disentangling has been widely adopted in numerous methods. By decomposing high-dimensional LF data into spatial, angular and epipolar subspaces, the learning difficulties of deep networks can be significantly reduced. Although achieving continuously improved SR performance, several fundamental issues (e.g., the relative importance of each subspace) remain underexplored, leading to redundant network parameters and high model complexity. In this paper, we revisit this classical mechanism and conduct an empirical study to investigate these issues. Specifically, we first develop a simple, modular, and scalable LF spatial SR network, based on subspace disentangling. We then conduct extensive experiments to quantitatively evaluate the contributions of each subspace branch, the model scaling property, and the depth-width trade-off. Through comprehensive analyses, the inherent patterns are identified, based on which we derive optimal network designs under varying parameter budgets. Without bells and whistles, our method achieves state-of-the-art performance with reduced model size. Code and pre-trained models are available at https://github.com/fyzhang2024/SimSSR/.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fengyuan Zhang; Yingqian Wang; Xueying Wang; Zhengyu Liang; Longguang Wang; Lvli Tian; Jungang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3661516"&gt;10.1109/tcsvt.2026.3661516&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Light field (LF) spatial super-resolution (SR) aims at reconstructing high-resolution LF images from low-resolution observations. Recently, subspace disentangling has been widely adopted in numerous methods. By decomposing high-dimensional LF data into spatial, angular and epipolar subspaces, the learning difficulties of deep networks can be significantly reduced. Although achieving continuously improved SR performance, several fundamental issues (e.g., the relative importance of each subspace) remain underexplored, leading to redundant network parameters and high model complexity. In this paper, we revisit this classical mechanism and conduct an empirical study to investigate these issues. Specifically, we first develop a simple, modular, and scalable LF spatial SR network, based on subspace disentangling. We then conduct extensive experiments to quantitatively evaluate the contributions of each subspace branch, the model scaling property, and the depth-width trade-off. Through comprehensive analyses, the inherent patterns are identified, based on which we derive optimal network designs under varying parameter budgets. Without bells and whistles, our method achieves state-of-the-art performance with reduced model size. Code and pre-trained models are available at https://github.com/fyzhang2024/SimSSR/.&lt;/p&gt;</content:encoded></item><item><title>基于双分类头的遥感图像精细化目标检测方法</title><link>https://doi.org/10.11834/jrs.20254243</link><guid>10.11834/jrs.20254243</guid><pubDate>Fri, 06 Feb 2026 02:20:15 +0000</pubDate><dc:creator>ZHANG Feng</dc:creator><dc:creator>TENG Shuhua</dc:creator><dc:creator>HAN Xing</dc:creator><dc:creator>WANG Yingqian</dc:creator><dc:creator>WANG Xueying</dc:creator><prism:publicationName>National Remote Sensing Bulletin</prism:publicationName><prism:doi>10.11834/jrs.20254243</prism:doi><description>2026年1月30日湖南第一师范学院电子信息学院的张锋、滕书华团队在《遥感学报》发文，介绍了其在遥感图像目标精细化检测领域的研究进展，张锋、滕书华专家提出了一种基于双分类头的遥感图像精细化目标检测方法，为解决相似数据利用不充分、错误标签影响模型精度和相似类别难以区分等问题提供了有效解决方案。
Published: 2026-02-06T02:20:15+00:00
Venue: National Remote Sensing Bulletin
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; ZHANG Feng; TENG Shuhua; HAN Xing; WANG Yingqian; WANG Xueying&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; National Remote Sensing Bulletin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jrs.20254243"&gt;10.11834/jrs.20254243&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;2026年1月30日湖南第一师范学院电子信息学院的张锋、滕书华团队在《遥感学报》发文，介绍了其在遥感图像目标精细化检测领域的研究进展，张锋、滕书华专家提出了一种基于双分类头的遥感图像精细化目标检测方法，为解决相似数据利用不充分、错误标签影响模型精度和相似类别难以区分等问题提供了有效解决方案。&lt;/p&gt;</content:encoded></item><item><title>SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing</title><link>https://arxiv.org/abs/2602.05480v1</link><guid>http://arxiv.org/abs/2602.05480v1</guid><pubDate>Thu, 05 Feb 2026 09:39:49 +0000</pubDate><dc:creator>Peihao Wu</dc:creator><dc:creator>Yongxiang Yao</dc:creator><dc:creator>Yi Wan</dc:creator><dc:creator>Wenfei Zhang</dc:creator><dc:creator>Ruipeng Zhao</dc:creator><dc:creator>Jiayuan Li</dc:creator><dc:creator>Yongjun Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.
Published: 2026-02-05T09:39:49+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peihao Wu; Yongxiang Yao; Yi Wan; Wenfei Zhang; Ruipeng Zhao; Jiayuan Li; Yongjun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.&lt;/p&gt;</content:encoded></item><item><title>Transformer-Masked Autoencoder (MAE) for Robust Medical Image Classification: A Comprehensive Survey</title><link>https://doi.org/10.1016/j.eswa.2026.131462</link><guid>10.1016/j.eswa.2026.131462</guid><pubDate>Fri, 06 Feb 2026 00:31:36 +0000</pubDate><dc:creator>Ernest Asimeng</dc:creator><dc:creator>Jun Chen</dc:creator><dc:creator>Kai Han</dc:creator><dc:creator>Chongwen Lyu</dc:creator><dc:creator>Zhe Liu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131462</prism:doi><description>Medical image classification underpins screening, diagnosis, and treatment planning, but conventional CNN pipelines remain label-hungry and brittle under noise, occlusion, and domain shift. Transformer-based masked autoencoders (MAEs) offer a compelling alternative by exploiting large unlabeled archives to learn anatomy-aware representations. This survey systematically synthesizes various studies on MAE-based encoders for X-ray, MRI, CT, ultrasound, and histopathology. We propose a modality-aware taxonomy spanning ViT/Swin backbones, masking strategies, and training regimes, and harmonize reported results against strong CNN and contrastive baselines. Across modalities, MAE initialization consistently improves label efficiency, calibration, and cross-scanner transfer, especially when combined with parameter-efficient fine-tuning. We distill practical design heuristics (mask ratios, encoder &amp; decoder depth, PEFT rank), highlight recurrent pitfalls in evaluation and reporting, and outline future directions in domain-aware masking, scalable 3D pretraining, cross-modal co-masking, and clinically grounded interpretability. The codes for to generate the results can be found in this https://github.com/ekwadwo1/Medical-MAE-Survey .
Published: 2026-02-06T00:31:36+00:00
Venue: Expert Systems with Applications
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ernest Asimeng; Jun Chen; Kai Han; Chongwen Lyu; Zhe Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131462"&gt;10.1016/j.eswa.2026.131462&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Medical image classification underpins screening, diagnosis, and treatment planning, but conventional CNN pipelines remain label-hungry and brittle under noise, occlusion, and domain shift. Transformer-based masked autoencoders (MAEs) offer a compelling alternative by exploiting large unlabeled archives to learn anatomy-aware representations. This survey systematically synthesizes various studies on MAE-based encoders for X-ray, MRI, CT, ultrasound, and histopathology. We propose a modality-aware taxonomy spanning ViT/Swin backbones, masking strategies, and training regimes, and harmonize reported results against strong CNN and contrastive baselines. Across modalities, MAE initialization consistently improves label efficiency, calibration, and cross-scanner transfer, especially when combined with parameter-efficient fine-tuning. We distill practical design heuristics (mask ratios, encoder &amp;amp; decoder depth, PEFT rank), highlight recurrent pitfalls in evaluation and reporting, and outline future directions in domain-aware masking, scalable 3D pretraining, cross-modal co-masking, and clinically grounded interpretability. The codes for to generate the results can be found in this https://github.com/ekwadwo1/Medical-MAE-Survey .&lt;/p&gt;</content:encoded></item><item><title>LLFeat: Noise-Aware Feature Matching under Various Low-Light Conditions</title><link>https://doi.org/10.1109/tcsvt.2026.3662204</link><guid>10.1109/tcsvt.2026.3662204</guid><pubDate>Fri, 06 Feb 2026 20:52:22 +0000</pubDate><dc:creator>Longjian Zeng</dc:creator><dc:creator>Zunjie Zhu</dc:creator><dc:creator>Ming Lu</dc:creator><dc:creator>Bolun Zheng</dc:creator><dc:creator>Rongfeng Lu</dc:creator><dc:creator>Tingyu Wang</dc:creator><dc:creator>Zhongtian Zheng</dc:creator><dc:creator>Yaoqi Sun</dc:creator><dc:creator>Chenggang Yan</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662204</prism:doi><description>Simultaneous Localization and Mapping (SLAM) is a crucial technique in computer vision and has been widely used in robot navigation, virtual reality, and augmented reality. Although SLAM algorithms are relatively mature, their performance is severely degraded under low-light conditions. As a core module of SLAM algorithms, feature matching under low-light conditions faces huge challenges. Existing methods train a general feature matching model for all low-light conditions, ignoring the noise modeling for better accuracy. In this work, we introduce LLFeat, a noise-aware method for feature matching under various low-light conditions. To compress the models of various low-light conditions, we further introduce a Noise-aware Feature Modulation (NaFM) layer to the model structure. Therefore, the models can share the most of parameters and preserve only private NaFM layers for each low-light condition, boosting the accuracy of feature matching with negligible additional parameters. LLFeat achieves remarkable results in the highly challenging MID benchmark in both indoor and outdoor scenes, demonstrating the effectiveness of our method. The code will be released.
Published: 2026-02-06T20:52:22+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Longjian Zeng; Zunjie Zhu; Ming Lu; Bolun Zheng; Rongfeng Lu; Tingyu Wang; Zhongtian Zheng; Yaoqi Sun; Chenggang Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662204"&gt;10.1109/tcsvt.2026.3662204&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Simultaneous Localization and Mapping (SLAM) is a crucial technique in computer vision and has been widely used in robot navigation, virtual reality, and augmented reality. Although SLAM algorithms are relatively mature, their performance is severely degraded under low-light conditions. As a core module of SLAM algorithms, feature matching under low-light conditions faces huge challenges. Existing methods train a general feature matching model for all low-light conditions, ignoring the noise modeling for better accuracy. In this work, we introduce LLFeat, a noise-aware method for feature matching under various low-light conditions. To compress the models of various low-light conditions, we further introduce a Noise-aware Feature Modulation (NaFM) layer to the model structure. Therefore, the models can share the most of parameters and preserve only private NaFM layers for each low-light condition, boosting the accuracy of feature matching with negligible additional parameters. LLFeat achieves remarkable results in the highly challenging MID benchmark in both indoor and outdoor scenes, demonstrating the effectiveness of our method. The code will be released.&lt;/p&gt;</content:encoded></item><item><title>Tri-CoMamba: A Tri-Complementary Mamba Framework for Multisource Remote Sensing Image Classification</title><link>https://doi.org/10.1109/jstars.2026.3662146</link><guid>10.1109/jstars.2026.3662146</guid><pubDate>Fri, 06 Feb 2026 20:50:02 +0000</pubDate><dc:creator>Zhihui Geng</dc:creator><dc:creator>Jiangtao Wang</dc:creator><dc:creator>Rui Wang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3662146</prism:doi><description>The synergistic application of hyperspectral images (HSI) combined with Light Detection and Ranging (LiDAR) or Synthetic Aperture Radar (SAR) data is crucial for improving the accuracy in multi-source remote sensing joint classification. However, existing methods still suffer from limitations in long-range dependency modeling, cross-modal alignment, and the preservation of fine-grained spectral features. This study introduces the Tri-Complementary Mamba Modules (Tri-CoMamba) framework to resolve the aforementioned limitations. The proposed network architecture is founded upon the State Space Model (SSM) and employs selective scanning Mamba-S6 as its core structure, integrating three complementary modules: CoRe-Mamba, CF-SpecMamba, and MASM. Specifically, Complement-and-Rectify Mamba (CoRe-Mamba) mitigates feature mismatching through dual-level spatial and channel rectification, thereby enhancing semantic consistency and directional modeling capabilities. cross-frequency spectral Mamba (CF-SpecMamba) introduces bi-directional recurrence and cross-frequency interaction attention to balance low-frequency baselines with high-frequency details in spectral modeling, to achieve comprehensive spectral feature enhancement. Furthermore, Modality-Aware Spatial Modulation (MASM) utilizes modality-aware dynamic spatial modulation to highlight discriminative regions and suppress background interference, thereby optimizing the cross-modal fusion effect. The synergy of these three modules enables Tri-CoMamba to completely exploit the distinct yet supportive strengths of spatial, spectral, and modal features, all while preserving computational efficiency, which leads to the precise classification of multi-source data. The effectiveness of this approach was validated using the Berlin, Trento and Houston2018 datasets, with results demonstrating that Tri-CoMamba outperforms various representative methods, achieving Overall Accuracy (OA) of 78.51%, 99.80%, and 92.88%, respectively.
Published: 2026-02-06T20:50:02+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhihui Geng; Jiangtao Wang; Rui Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3662146"&gt;10.1109/jstars.2026.3662146&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;The synergistic application of hyperspectral images (HSI) combined with Light Detection and Ranging (LiDAR) or Synthetic Aperture Radar (SAR) data is crucial for improving the accuracy in multi-source remote sensing joint classification. However, existing methods still suffer from limitations in long-range dependency modeling, cross-modal alignment, and the preservation of fine-grained spectral features. This study introduces the Tri-Complementary Mamba Modules (Tri-CoMamba) framework to resolve the aforementioned limitations. The proposed network architecture is founded upon the State Space Model (SSM) and employs selective scanning Mamba-S6 as its core structure, integrating three complementary modules: CoRe-Mamba, CF-SpecMamba, and MASM. Specifically, Complement-and-Rectify Mamba (CoRe-Mamba) mitigates feature mismatching through dual-level spatial and channel rectification, thereby enhancing semantic consistency and directional modeling capabilities. cross-frequency spectral Mamba (CF-SpecMamba) introduces bi-directional recurrence and cross-frequency interaction attention to balance low-frequency baselines with high-frequency details in spectral modeling, to achieve comprehensive spectral feature enhancement. Furthermore, Modality-Aware Spatial Modulation (MASM) utilizes modality-aware dynamic spatial modulation to highlight discriminative regions and suppress background interference, thereby optimizing the cross-modal fusion effect. The synergy of these three modules enables Tri-CoMamba to completely exploit the distinct yet supportive strengths of spatial, spectral, and modal features, all while preserving computational efficiency, which leads to the precise classification of multi-source data. The effectiveness of this approach was validated using the Berlin, Trento and Houston2018 datasets, with results demonstrating that Tri-CoMamba outperforms various representative methods, achieving Overall Accuracy (OA) of 78.51%, 99.80%, and 92.88%, respectively.&lt;/p&gt;</content:encoded></item><item><title>Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification</title><link>https://arxiv.org/abs/2602.05218v1</link><guid>http://arxiv.org/abs/2602.05218v1</guid><pubDate>Thu, 05 Feb 2026 02:17:38 +0000</pubDate><dc:creator>Jiahao Nie</dc:creator><dc:creator>Yun Xing</dc:creator><dc:creator>Wenbin An</dc:creator><dc:creator>Qingsong Zhao</dc:creator><dc:creator>Jiawei Shao</dc:creator><dc:creator>Yap-Peng Tan</dc:creator><dc:creator>Alex C. Kot</dc:creator><dc:creator>Shijian Lu</dc:creator><dc:creator>Xuelong Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.
Published: 2026-02-05T02:17:38+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Nie; Yun Xing; Wenbin An; Qingsong Zhao; Jiawei Shao; Yap-Peng Tan; Alex C. Kot; Shijian Lu; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.&lt;/p&gt;</content:encoded></item><item><title>AdaMixCon: Difficulty-Aware Feature Enhancement for Few-Shot Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tgrs.2026.3661637</link><guid>10.1109/tgrs.2026.3661637</guid><pubDate>Fri, 06 Feb 2026 20:49:16 +0000</pubDate><dc:creator>Chenchen Wang</dc:creator><dc:creator>Jiamu Sheng</dc:creator><dc:creator>Jingyi Zhou</dc:creator><dc:creator>Zhende Song</dc:creator><dc:creator>Jiayuan Fan</dc:creator><dc:creator>Zhouchen Lin</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3661637</prism:doi><description>In recent years, few-shot learning (FSL) has been widely researched in hyperspectral image (HSI) classification, aiming to mitigate the challenge of insufficient labeled data due to the costly annotation process. However, existing methods primarily focus on intra-sample feature enhancement which treats each sample as an independent individual, overlooking the valuable inter-sample dependencies inherent in HSI data, resulting in suboptimal performance. Moreover, due to the lack of awareness and modeling of sample difficulty, current frameworks often face a trade-off between the optimal classification of hard samples and the risk of overfitting on easy ones, ultimately limiting the performance. To address the aforementioned challenges, we propose AdaMixCon, an adaptive difficulty-aware two-stage framework for few-shot hyperspectral image classification. First, we introduce HSI-MixCon, a spectral-spatial feature enhancement module that jointly exploits inter-sample dependencies and extracts intra-sample spectral-spatial information. Additionally, a novel Difficulty-Aware Sample Routing (DASR) module is designed to adaptively adjust the processing flow based on sample difficulty. The sample difficulty is estimated by performing internal assessment and mutual agreement between global and local predictions from the first stage, enabling early exit for easy samples and further refinement for hard ones. Experimental results demonstrate that our AdaMixCon outperforms state-of-the-art few-shot HSI classification methods on three public HSI datasets. The code is available at https://github.com/doctorlightt/AdaMixCon.
Published: 2026-02-06T20:49:16+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenchen Wang; Jiamu Sheng; Jingyi Zhou; Zhende Song; Jiayuan Fan; Zhouchen Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3661637"&gt;10.1109/tgrs.2026.3661637&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, few-shot learning (FSL) has been widely researched in hyperspectral image (HSI) classification, aiming to mitigate the challenge of insufficient labeled data due to the costly annotation process. However, existing methods primarily focus on intra-sample feature enhancement which treats each sample as an independent individual, overlooking the valuable inter-sample dependencies inherent in HSI data, resulting in suboptimal performance. Moreover, due to the lack of awareness and modeling of sample difficulty, current frameworks often face a trade-off between the optimal classification of hard samples and the risk of overfitting on easy ones, ultimately limiting the performance. To address the aforementioned challenges, we propose AdaMixCon, an adaptive difficulty-aware two-stage framework for few-shot hyperspectral image classification. First, we introduce HSI-MixCon, a spectral-spatial feature enhancement module that jointly exploits inter-sample dependencies and extracts intra-sample spectral-spatial information. Additionally, a novel Difficulty-Aware Sample Routing (DASR) module is designed to adaptively adjust the processing flow based on sample difficulty. The sample difficulty is estimated by performing internal assessment and mutual agreement between global and local predictions from the first stage, enabling early exit for easy samples and further refinement for hard ones. Experimental results demonstrate that our AdaMixCon outperforms state-of-the-art few-shot HSI classification methods on three public HSI datasets. The code is available at https://github.com/doctorlightt/AdaMixCon.&lt;/p&gt;</content:encoded></item></channel></rss>