<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 16 Jan 2026 02:50:07 +0000</lastBuildDate><item><title>EinsPT: Efficient Instance-Aware Pre-Training of Vision Foundation Models</title><link>https://doi.org/10.1109/tip.2026.3652371</link><guid>10.1109/tip.2026.3652371</guid><pubDate>Wed, 14 Jan 2026 20:41:45 +0000</pubDate><dc:creator>Zhaozhi Wang</dc:creator><dc:creator>Yunjie Tian</dc:creator><dc:creator>Lingxi Xie</dc:creator><dc:creator>Yaowei Wang</dc:creator><dc:creator>Qixiang Ye</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3652371</prism:doi><description>In this study, we introduce EinsPT, an efficient instance-aware pre-training paradigm designed to reduce the transfer gap between vision foundation models and downstream instance-level tasks. Unlike conventional image-level pre-training that relies solely on unlabeled images, EinsPT leverages both image reconstruction and instance annotations to learn representations that are spatially coherent and instance discriminative. To achieve this efficiently, we propose a proxy–foundation architecture that decouples high-resolution and low-resolution learning: the foundation model processes masked low-resolution images for global semantics, while a lightweight proxy model operates on complete high-resolution images to preserve fine-grained details. The two branches are jointly optimized through reconstruction and instance-level prediction losses on fused features. Extensive experiments demonstrate that EinsPT consistently enhances recognition accuracy across various downstream tasks with substantially reduced computational cost, while qualitative results further reveal improved instance perception and completeness in visual representations. Code is available at github.com/feufhd/EinsPT.
Published: 2026-01-14T20:41:45+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.838 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaozhi Wang; Yunjie Tian; Lingxi Xie; Yaowei Wang; Qixiang Ye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3652371"&gt;10.1109/tip.2026.3652371&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.838 (must_read)&lt;/p&gt;
&lt;p&gt;In this study, we introduce EinsPT, an efficient instance-aware pre-training paradigm designed to reduce the transfer gap between vision foundation models and downstream instance-level tasks. Unlike conventional image-level pre-training that relies solely on unlabeled images, EinsPT leverages both image reconstruction and instance annotations to learn representations that are spatially coherent and instance discriminative. To achieve this efficiently, we propose a proxy–foundation architecture that decouples high-resolution and low-resolution learning: the foundation model processes masked low-resolution images for global semantics, while a lightweight proxy model operates on complete high-resolution images to preserve fine-grained details. The two branches are jointly optimized through reconstruction and instance-level prediction losses on fused features. Extensive experiments demonstrate that EinsPT consistently enhances recognition accuracy across various downstream tasks with substantially reduced computational cost, while qualitative results further reveal improved instance perception and completeness in visual representations. Code is available at github.com/feufhd/EinsPT.&lt;/p&gt;</content:encoded></item><item><title>Multi-Modal Decouple and Recouple Network for Robust 3D Object Detection</title><link>https://doi.org/10.1109/tcsvt.2026.3654118</link><guid>10.1109/tcsvt.2026.3654118</guid><pubDate>Wed, 14 Jan 2026 20:41:30 +0000</pubDate><dc:creator>Rui Ding</dc:creator><dc:creator>Zhaonian Kuang</dc:creator><dc:creator>Yuzhe Ji</dc:creator><dc:creator>Meng Yang</dc:creator><dc:creator>Xinhu Zheng</dc:creator><dc:creator>Gang Hua</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3654118</prism:doi><description>Multi-modal 3D object detection with bird’s eye view (BEV) has achieved desired advances on benchmarks. Nonetheless, the accuracy may drop significantly in the real world due to data corruption such as sensor configurations for LiDAR and scene conditions for camera. One design bottleneck of previous models resides in the tightly coupling of multi-modal BEV features during fusion, which may degrade the overall system performance if one modality or both is corrupted. To mitigate, we propose a Multi-Modal Decouple and Recouple Network for robust 3D object detection under data corruption. Different modalities commonly share some high-level invariant features. We observe that these invariant features across modalities do not always fail simultaneously, because different types of data corruption affect each modality in distinct ways. These invariant features can be recovered across modalities for robust fusion under data corruption. To this end, we explicitly decouple Camera/LiDAR BEV features into modality-invariant and modality-specific parts. It allows invariant features to compensate each other while mitigates the negative impact of a corrupted modality on the other. We then recouple these features into three experts to handle different types of data corruption, respectively, i.e., LiDAR, camera, and both. For each expert, we use modality-invariant features as robust information, while modality-specific features serve as a complement. Finally, we adaptively fuse the three experts to exact robust features for 3D object detection. For validation, we collect a benchmark with a large quantity of data corruption for LiDAR, camera, and both based on nuScenes. Our model is trained on clean nuScenes and tested on all types of data corruption. Our model consistently achieves the best accuracy on both corrupted and clean data compared to recent models.
Published: 2026-01-14T20:41:30+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.837 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Ding; Zhaonian Kuang; Yuzhe Ji; Meng Yang; Xinhu Zheng; Gang Hua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3654118"&gt;10.1109/tcsvt.2026.3654118&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.837 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal 3D object detection with bird’s eye view (BEV) has achieved desired advances on benchmarks. Nonetheless, the accuracy may drop significantly in the real world due to data corruption such as sensor configurations for LiDAR and scene conditions for camera. One design bottleneck of previous models resides in the tightly coupling of multi-modal BEV features during fusion, which may degrade the overall system performance if one modality or both is corrupted. To mitigate, we propose a Multi-Modal Decouple and Recouple Network for robust 3D object detection under data corruption. Different modalities commonly share some high-level invariant features. We observe that these invariant features across modalities do not always fail simultaneously, because different types of data corruption affect each modality in distinct ways. These invariant features can be recovered across modalities for robust fusion under data corruption. To this end, we explicitly decouple Camera/LiDAR BEV features into modality-invariant and modality-specific parts. It allows invariant features to compensate each other while mitigates the negative impact of a corrupted modality on the other. We then recouple these features into three experts to handle different types of data corruption, respectively, i.e., LiDAR, camera, and both. For each expert, we use modality-invariant features as robust information, while modality-specific features serve as a complement. Finally, we adaptively fuse the three experts to exact robust features for 3D object detection. For validation, we collect a benchmark with a large quantity of data corruption for LiDAR, camera, and both based on nuScenes. Our model is trained on clean nuScenes and tested on all types of data corruption. Our model consistently achieves the best accuracy on both corrupted and clean data compared to recent models.&lt;/p&gt;</content:encoded></item><item><title>Spatial-Frequency Feature Learning for Infrared Small Target Detection</title><link>https://doi.org/10.1109/taes.2026.3653835</link><guid>10.1109/taes.2026.3653835</guid><pubDate>Wed, 14 Jan 2026 20:41:40 +0000</pubDate><dc:creator>Qiang Li</dc:creator><dc:creator>Zhigang Yang</dc:creator><dc:creator>Jiaxin Cheng</dc:creator><dc:creator>Qi Wang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2026.3653835</prism:doi><description>The precision of small target detection plays a pivotal role in infrared image analysis. Currently, various methods are proposed to solve the issue of insufficient small target features. However, many infrared target detection methods often fail to fully consider how to enhance detection accuracy of small targets through effective information guidance. Frequency domain analysis indicates that small targets typically exhibit significant and prominent differences in high-frequency regions. On the other hand, as a key manifestation of high-frequency content, edge information naturally connects the spatial and frequency domains. Inspired by these, this paper proposes an infrared small target detection method called Spatial-Frequency Feature Learning Network (SFLNet). It aims to solve the challenges in small target detection through collaborative modeling of the spatial and frequency domains. SFLNet adopts an edge-guided decoder structure that helps the network preserve the structural integrity and shape information of small targets during reconstruction. Moreover, this module introduces a spatial-frequency joint learning mechanism that enables complementary and enhanced information exchange between the spatial and frequency domains, which improves the perception and recognition of small target features. Experimental results show that SFLNet outperforms comparison methods on multiple datasets, particularly in precise localization of targets and preservation of target shapes.
Published: 2026-01-14T20:41:40+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiang Li; Zhigang Yang; Jiaxin Cheng; Qi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2026.3653835"&gt;10.1109/taes.2026.3653835&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;The precision of small target detection plays a pivotal role in infrared image analysis. Currently, various methods are proposed to solve the issue of insufficient small target features. However, many infrared target detection methods often fail to fully consider how to enhance detection accuracy of small targets through effective information guidance. Frequency domain analysis indicates that small targets typically exhibit significant and prominent differences in high-frequency regions. On the other hand, as a key manifestation of high-frequency content, edge information naturally connects the spatial and frequency domains. Inspired by these, this paper proposes an infrared small target detection method called Spatial-Frequency Feature Learning Network (SFLNet). It aims to solve the challenges in small target detection through collaborative modeling of the spatial and frequency domains. SFLNet adopts an edge-guided decoder structure that helps the network preserve the structural integrity and shape information of small targets during reconstruction. Moreover, this module introduces a spatial-frequency joint learning mechanism that enables complementary and enhanced information exchange between the spatial and frequency domains, which improves the perception and recognition of small target features. Experimental results show that SFLNet outperforms comparison methods on multiple datasets, particularly in precise localization of targets and preservation of target shapes.&lt;/p&gt;</content:encoded></item><item><title>DCTNet: Integrating Deformable Convolution and Transformer-Based Feature Fusion for High-Performance SAR Ship Detection</title><link>https://doi.org/10.1109/lgrs.2026.3654439</link><guid>10.1109/lgrs.2026.3654439</guid><pubDate>Wed, 14 Jan 2026 20:42:05 +0000</pubDate><dc:creator>Shufang Xu</dc:creator><dc:creator>Jinhui Lan</dc:creator><dc:creator>Yiliang Zeng</dc:creator><dc:creator>Wei Luo</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2026.3654439</prism:doi><description>Synthetic aperture radar (SAR)–based ship detection has become an indispensable technique for applications in maritime surveillance, oceanic defense systems, and marine resource assessment. However, factors such as speckle noise, sea clutter, and the diversity of ship scales severely restrict the robustness and accuracy of detection. To address these challenges, this paper proposes a one-stage detection network, termed DCTNet, which integrates a Deformable Convolution-based Multi-scale Fusion (DCMF) module and a Transformer-enhanced Feature Pyramid Aggregation Network (TFPAN). The DCMF module employs deformable convolution and channel partitioning to adaptively capture multi-scale features, thereby enhancing the representation of small-scale or low-contrast ships while effectively suppressing background interference. The TFPAN module incorporates Transformer attention mechanisms with progressive feature aggregation to strengthen multi-scale feature interaction and semantic consistency. In addition, a Triple Feature Fusion (TFF) strategy is designed to preserve local details and ensure global context transmission, effectively mitigating the loss of small target features during the fusion process. Experimental results on the SAR-Ship-Dataset and HRSID demonstrate that the proposed DCTNet achieves excellent performance and exhibits strong generalization ability.
Published: 2026-01-14T20:42:05+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shufang Xu; Jinhui Lan; Yiliang Zeng; Wei Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2026.3654439"&gt;10.1109/lgrs.2026.3654439&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR)–based ship detection has become an indispensable technique for applications in maritime surveillance, oceanic defense systems, and marine resource assessment. However, factors such as speckle noise, sea clutter, and the diversity of ship scales severely restrict the robustness and accuracy of detection. To address these challenges, this paper proposes a one-stage detection network, termed DCTNet, which integrates a Deformable Convolution-based Multi-scale Fusion (DCMF) module and a Transformer-enhanced Feature Pyramid Aggregation Network (TFPAN). The DCMF module employs deformable convolution and channel partitioning to adaptively capture multi-scale features, thereby enhancing the representation of small-scale or low-contrast ships while effectively suppressing background interference. The TFPAN module incorporates Transformer attention mechanisms with progressive feature aggregation to strengthen multi-scale feature interaction and semantic consistency. In addition, a Triple Feature Fusion (TFF) strategy is designed to preserve local details and ensure global context transmission, effectively mitigating the loss of small target features during the fusion process. Experimental results on the SAR-Ship-Dataset and HRSID demonstrate that the proposed DCTNet achieves excellent performance and exhibits strong generalization ability.&lt;/p&gt;</content:encoded></item><item><title>Consistency-Aware Spot-Guided Transformer for Accurate and Versatile Point Cloud Registration</title><link>https://doi.org/10.1109/tpami.2026.3653989</link><guid>10.1109/tpami.2026.3653989</guid><pubDate>Wed, 14 Jan 2026 20:40:13 +0000</pubDate><dc:creator>Renlang Huang</dc:creator><dc:creator>Li Chai</dc:creator><dc:creator>Yufan Tang</dc:creator><dc:creator>Zhoujian Li</dc:creator><dc:creator>Jiming Chen</dc:creator><dc:creator>Liang Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653989</prism:doi><description>Deep learning-based feature matching has showcased great superiority for point cloud registration. While coarse-to-fine matching architectures are prevalent, they typically perform sparse and geometrically inconsistent coarse matching. This forces the subsequent fine matching to rely on computationally expensive optimal transport and hypothesis-and-selection procedures to resolve inconsistencies, leading to inefficiency and poor scalability for large-scale real-time applications. In this paper, we design a consistency-aware spot-guided Transformer (CAST) to enhance the coarse matching by explicitly utilizing geometric consistency via two key sparse attention mechanisms. First, our consistency-aware self-attention selectively computes intra-point-cloud attention to a sparse subset of points with globally consistent correspondences, enabling other points to derive discriminative features through their relationships with these anchors while propagating global consistency for robust correspondence reasoning. Second, our spot-guided cross-attention restricts cross-point-cloud attention to dynamically defined “spots”—the union of correspondence neighborhoods of a query
Published: 2026-01-14T20:40:13+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Renlang Huang; Li Chai; Yufan Tang; Zhoujian Li; Jiming Chen; Liang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653989"&gt;10.1109/tpami.2026.3653989&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning-based feature matching has showcased great superiority for point cloud registration. While coarse-to-fine matching architectures are prevalent, they typically perform sparse and geometrically inconsistent coarse matching. This forces the subsequent fine matching to rely on computationally expensive optimal transport and hypothesis-and-selection procedures to resolve inconsistencies, leading to inefficiency and poor scalability for large-scale real-time applications. In this paper, we design a consistency-aware spot-guided Transformer (CAST) to enhance the coarse matching by explicitly utilizing geometric consistency via two key sparse attention mechanisms. First, our consistency-aware self-attention selectively computes intra-point-cloud attention to a sparse subset of points with globally consistent correspondences, enabling other points to derive discriminative features through their relationships with these anchors while propagating global consistency for robust correspondence reasoning. Second, our spot-guided cross-attention restricts cross-point-cloud attention to dynamically defined “spots”—the union of correspondence neighborhoods of a query&lt;/p&gt;</content:encoded></item><item><title>LSLFormer: A Lightweight Spectral-LiDAR Fusion Network for Remote Sensing Image Classification</title><link>https://doi.org/10.1109/tgrs.2026.3654154</link><guid>10.1109/tgrs.2026.3654154</guid><pubDate>Wed, 14 Jan 2026 20:40:24 +0000</pubDate><dc:creator>Dian Li</dc:creator><dc:creator>Siyuan Hao</dc:creator><dc:creator>Cheng Fang</dc:creator><dc:creator>Yuanxin Ye</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654154</prism:doi><description>The fusion of hyperspectral images (HSI) and LiDAR data provides rich complementary spectral and elevation information for land cover classification. However, existing fusion methods, particularly Transformer-based models, are often constrained by high computational costs and complex cross-modal interaction mechanisms. To address this challenge, we propose a Lightweight Spectral-LiDAR Fusion Network (LSLFormer), which aims to achieve efficient and accurate remote sensing image classification. We introduce three key modules into LSLFormer architecture: 1) A Hyperspectral-to-Multispectral (H2M) module to alleviate the computational burden of self-attention on high-dimensional spectral data. 2) A Multi-scale Channel Interaction Enhancement (MCIE) module to extract robust spatial-structural features from LiDAR data. 3) A Spectral-LiDAR Attention (SLA) module to achieve deep cross-modal interaction by dynamically fusing normalized spectral and structural affinity distributions. In addition, we also propose a novel Cross-Modal Structural Consistency (CMSC) loss. This mechanism aligns the geometric topology of the spectral features with the LiDAR structure via knowledge distillation, ensuring precise boundary delineation without compromising spectral semantics. Extensive experiments on three public benchmark datasets have demonstrated that LSLFormer consistently outperforms other state-of-the-art convolutional and Transformer-based methods in terms of classification accuracy and computational cost. The codes of this work will be available at https://github.com/DianLi2002/LSLFormer.
Published: 2026-01-14T20:40:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dian Li; Siyuan Hao; Cheng Fang; Yuanxin Ye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654154"&gt;10.1109/tgrs.2026.3654154&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;The fusion of hyperspectral images (HSI) and LiDAR data provides rich complementary spectral and elevation information for land cover classification. However, existing fusion methods, particularly Transformer-based models, are often constrained by high computational costs and complex cross-modal interaction mechanisms. To address this challenge, we propose a Lightweight Spectral-LiDAR Fusion Network (LSLFormer), which aims to achieve efficient and accurate remote sensing image classification. We introduce three key modules into LSLFormer architecture: 1) A Hyperspectral-to-Multispectral (H2M) module to alleviate the computational burden of self-attention on high-dimensional spectral data. 2) A Multi-scale Channel Interaction Enhancement (MCIE) module to extract robust spatial-structural features from LiDAR data. 3) A Spectral-LiDAR Attention (SLA) module to achieve deep cross-modal interaction by dynamically fusing normalized spectral and structural affinity distributions. In addition, we also propose a novel Cross-Modal Structural Consistency (CMSC) loss. This mechanism aligns the geometric topology of the spectral features with the LiDAR structure via knowledge distillation, ensuring precise boundary delineation without compromising spectral semantics. Extensive experiments on three public benchmark datasets have demonstrated that LSLFormer consistently outperforms other state-of-the-art convolutional and Transformer-based methods in terms of classification accuracy and computational cost. The codes of this work will be available at https://github.com/DianLi2002/LSLFormer.&lt;/p&gt;</content:encoded></item><item><title>BEV-CMHF: A Cross-Modality Hybrid Fusion Framework for BEV 3D Object Detection With Feature Interaction and Temporal Fusion</title><link>https://doi.org/10.1109/tits.2026.3651793</link><guid>10.1109/tits.2026.3651793</guid><pubDate>Wed, 14 Jan 2026 20:41:18 +0000</pubDate><dc:creator>Jiafeng Li</dc:creator><dc:creator>Jinquan Xu</dc:creator><dc:creator>MengXun Zhi</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Li Zhuo</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2026.3651793</prism:doi><description>Autonomous driving technology has garnered significant attention for its potential to reduce driver burden and enhance road safety. Modern autonomous driving systems rely on a variety of sensors to perceive complex driving environments. Many existing methods map heterogeneous data into the bird’s eye view (BEV) space for feature fusion. However, they often fail to fully exploit the cross-modal interactions between cameras and LiDAR, or incorporate temporal information, resulting in suboptimal performance. Furthermore, commonly used fusion strategies are often overly simplistic. This study proposes BEV-CMHF, a cross-modality hybrid fusion framework for BEV 3D object detection with feature interaction and temporal fusion. By introducing an interactive cross-attention module and a long-short-term temporal module, the proposed framework enhances the representational power of fused BEV features. Specifically, a feature-interaction attention module that facilitates effective interaction between the camera and LiDAR BEV features using deformable attention is designed, providing guidance and supervision for the camera BEV features. Subsequently, a historical feature temporal fusion module that integrates the long-short-term temporal module is introduced to incorporate additional critical temporal information into the BEV features. Moreover, a dynamic hybrid feature-fusion module is designed to fuse the BEV features of the camera and LiDAR effectively through a hybrid attention mechanism that combines coarse and fine attention. Extensive experiments conducted on the nuScenes benchmark validate the effectiveness of the proposed method, achieving 70.87% mAP and 74.00% NDS on the test set. Using a single NVIDIA GeForce RTX 4090, the method attained an inference speed of 5.79 images per second (5.79 img/s), corresponding to an inference time of 172.64 ms on the nuScenes dataset. The source code will be released at https://github.com/BJUTsipl/BEV-CMHF
Published: 2026-01-14T20:41:18+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiafeng Li; Jinquan Xu; MengXun Zhi; Jing Zhang; Li Zhuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2026.3651793"&gt;10.1109/tits.2026.3651793&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Autonomous driving technology has garnered significant attention for its potential to reduce driver burden and enhance road safety. Modern autonomous driving systems rely on a variety of sensors to perceive complex driving environments. Many existing methods map heterogeneous data into the bird’s eye view (BEV) space for feature fusion. However, they often fail to fully exploit the cross-modal interactions between cameras and LiDAR, or incorporate temporal information, resulting in suboptimal performance. Furthermore, commonly used fusion strategies are often overly simplistic. This study proposes BEV-CMHF, a cross-modality hybrid fusion framework for BEV 3D object detection with feature interaction and temporal fusion. By introducing an interactive cross-attention module and a long-short-term temporal module, the proposed framework enhances the representational power of fused BEV features. Specifically, a feature-interaction attention module that facilitates effective interaction between the camera and LiDAR BEV features using deformable attention is designed, providing guidance and supervision for the camera BEV features. Subsequently, a historical feature temporal fusion module that integrates the long-short-term temporal module is introduced to incorporate additional critical temporal information into the BEV features. Moreover, a dynamic hybrid feature-fusion module is designed to fuse the BEV features of the camera and LiDAR effectively through a hybrid attention mechanism that combines coarse and fine attention. Extensive experiments conducted on the nuScenes benchmark validate the effectiveness of the proposed method, achieving 70.87% mAP and 74.00% NDS on the test set. Using a single NVIDIA GeForce RTX 4090, the method attained an inference speed of 5.79 images per second (5.79 img/s), corresponding to an inference time of 172.64 ms on the nuScenes dataset. The source code will be released at https://github.com/BJUTsipl/BEV-CMHF&lt;/p&gt;</content:encoded></item><item><title>Towards Generative Understanding: Incremental Few-shot Semantic Segmentation with Diffusion Models</title><link>https://doi.org/10.1109/tip.2026.3652357</link><guid>10.1109/tip.2026.3652357</guid><pubDate>Wed, 14 Jan 2026 20:41:45 +0000</pubDate><dc:creator>Qun Li</dc:creator><dc:creator>Lu Huang</dc:creator><dc:creator>Fu Xiao</dc:creator><dc:creator>Na Zhao</dc:creator><dc:creator>Bir Bhanu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3652357</prism:doi><description>Incremental Few-shot Semantic Segmentation (iFSS) aims to learn novel classes with limited samples while preserving segmentation capability for base classes, addressing the challenge of continual learning of novel classes and catastrophic forgetting of previously seen classes. Existing methods mainly rely on techniques such as knowledge distillation and background learning, which, while partially effective, still suffer from issues such as feature drift and limited generalization to real-world novel classes, primarily due to a bidirectional coupling bottleneck between the learning of base classes and novel classes. To address these challenges, we propose, for the first time, a diffusion-based generative framework for iFSS. Specifically, we bridge the gap between generative and discriminative tasks through an innovative binary-to-RGB mask mapping mechanism, enabling pre-trained diffusion models to focus on target regions via class-specific semantic embedding optimization while sharpening foreground-background contrast with color embeddings. A lightweight post-processor then refines the generated images into high-quality binary masks. Crucially, by leveraging diffusion priors, our framework avoids complex training strategies. The optimization of class-specific semantic embeddings decouples the embedding spaces of base and novel classes, inherently preventing feature drift, mitigating catastrophic forgetting, and enabling rapid novel-class adaptation. Experimental results show that our method achieves state-of-the-art performance on the PASCAL-5i and COCO-20i datasets using much less data than other methods, and exhibiting competitive results in cross-domain few-shot segmentation tasks. Project page: https://ifss-diff.github.io/.
Published: 2026-01-14T20:41:45+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qun Li; Lu Huang; Fu Xiao; Na Zhao; Bir Bhanu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3652357"&gt;10.1109/tip.2026.3652357&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Incremental Few-shot Semantic Segmentation (iFSS) aims to learn novel classes with limited samples while preserving segmentation capability for base classes, addressing the challenge of continual learning of novel classes and catastrophic forgetting of previously seen classes. Existing methods mainly rely on techniques such as knowledge distillation and background learning, which, while partially effective, still suffer from issues such as feature drift and limited generalization to real-world novel classes, primarily due to a bidirectional coupling bottleneck between the learning of base classes and novel classes. To address these challenges, we propose, for the first time, a diffusion-based generative framework for iFSS. Specifically, we bridge the gap between generative and discriminative tasks through an innovative binary-to-RGB mask mapping mechanism, enabling pre-trained diffusion models to focus on target regions via class-specific semantic embedding optimization while sharpening foreground-background contrast with color embeddings. A lightweight post-processor then refines the generated images into high-quality binary masks. Crucially, by leveraging diffusion priors, our framework avoids complex training strategies. The optimization of class-specific semantic embeddings decouples the embedding spaces of base and novel classes, inherently preventing feature drift, mitigating catastrophic forgetting, and enabling rapid novel-class adaptation. Experimental results show that our method achieves state-of-the-art performance on the PASCAL-5i and COCO-20i datasets using much less data than other methods, and exhibiting competitive results in cross-domain few-shot segmentation tasks. Project page: https://ifss-diff.github.io/.&lt;/p&gt;</content:encoded></item><item><title>Seeing Clearly and Detecting Precisely: Perceptual Enhancement and Focus Calibration for Small-Object Detection</title><link>https://doi.org/10.1109/tnnls.2026.3651289</link><guid>10.1109/tnnls.2026.3651289</guid><pubDate>Wed, 14 Jan 2026 20:40:49 +0000</pubDate><dc:creator>Zhiqin Zhu</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Guanqiu Qi</dc:creator><dc:creator>Shuang Li</dc:creator><dc:creator>Huafeng Li</dc:creator><dc:creator>Yu Liu</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2026.3651289</prism:doi><description>Small-object detection remains challenging due to limited pixel information, blurred boundaries, and weak semantic cues. Although recent advances in multiscale fusion and attention mechanisms have led to improved performance, existing methods still struggle to preserve high-frequency structural details and achieve precise localization—particularly in dense, cluttered, or low-resolution scenarios. These limitations are primarily caused by the loss of fine-grained features during downsampling and the absence of region-aware focus mechanisms. Inspired by the human visual strategy of “see clearly and detect precisely,” we propose PEFC-Net, a novel framework that enhances both perceptual clarity and localization accuracy for small-object detection. To mitigate structural degradation, we introduce the hybrid structural perception (HSP) module, which jointly encodes spatial gradients and localized frequency components through wavelet-based decomposition and edge-aware refinement. To further improve region-level focus, we design the axis-aligned focus calibration (AAFC) module, which captures long-range directional context via axis-sensitive pooling and adaptively refines attention with shape-aware calibration. Extensive experiments on four challenging benchmarks—VisDrone-2019, TT100K, NWPU VHR-10, and DIOR—demonstrate that PEFC-Net consistently outperforms state-of-the-art methods, delivering robust performance under occlusion, dense distribution, and scale variation.
Published: 2026-01-14T20:40:49+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiqin Zhu; Yang Yang; Guanqiu Qi; Shuang Li; Huafeng Li; Yu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2026.3651289"&gt;10.1109/tnnls.2026.3651289&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Small-object detection remains challenging due to limited pixel information, blurred boundaries, and weak semantic cues. Although recent advances in multiscale fusion and attention mechanisms have led to improved performance, existing methods still struggle to preserve high-frequency structural details and achieve precise localization—particularly in dense, cluttered, or low-resolution scenarios. These limitations are primarily caused by the loss of fine-grained features during downsampling and the absence of region-aware focus mechanisms. Inspired by the human visual strategy of “see clearly and detect precisely,” we propose PEFC-Net, a novel framework that enhances both perceptual clarity and localization accuracy for small-object detection. To mitigate structural degradation, we introduce the hybrid structural perception (HSP) module, which jointly encodes spatial gradients and localized frequency components through wavelet-based decomposition and edge-aware refinement. To further improve region-level focus, we design the axis-aligned focus calibration (AAFC) module, which captures long-range directional context via axis-sensitive pooling and adaptively refines attention with shape-aware calibration. Extensive experiments on four challenging benchmarks—VisDrone-2019, TT100K, NWPU VHR-10, and DIOR—demonstrate that PEFC-Net consistently outperforms state-of-the-art methods, delivering robust performance under occlusion, dense distribution, and scale variation.&lt;/p&gt;</content:encoded></item><item><title>SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition</title><link>https://arxiv.org/abs/2601.10324v1</link><guid>http://arxiv.org/abs/2601.10324v1</guid><pubDate>Thu, 15 Jan 2026 12:09:49 +0000</pubDate><dc:creator>Yiming Zhang</dc:creator><dc:creator>Weibo Qin</dc:creator><dc:creator>Yuntian Liu</dc:creator><dc:creator>Feng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.
Published: 2026-01-15T12:09:49+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Zhang; Weibo Qin; Yuntian Liu; Feng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.&lt;/p&gt;</content:encoded></item><item><title>GeoCraft: A Diffusion Model-Based 3D Reconstruction Method Driven by Image and Point Cloud Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104149</link><guid>10.1016/j.inffus.2026.104149</guid><pubDate>Wed, 14 Jan 2026 00:29:37 +0000</pubDate><dc:creator>Weixuan Ma</dc:creator><dc:creator>Yamin Li</dc:creator><dc:creator>Chujin Liu</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Kansong Chen</dc:creator><dc:creator>Weixuan Gao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104149</prism:doi><description>With the rapid development of technologies like virtual reality (VR), autonomous driving, and digital twins, the demand for high-precision and realistic multimodal 3D reconstruction has surged. This technology has become a core research focus in computer vision and graphics due to its ability to integrate multi-source data, such as 2D images and point clouds. However, existing methods face challenges such as geometric inconsistency in single-view reconstruction, poor point cloud-to-mesh conversion, and insufficient multimodal feature fusion, limiting their practical application. To address these issues, this paper proposes GeoCraft, a multimodal 3D reconstruction method that generates high-precision 3D models from 2D images through three collaborative stages: Diff2DPoint, Point2DMesh, and Vision3DGen. Specifically, Diff2DPoint generates an initial point cloud with geometric alignment using a diffusion model and projection feature fusion; Point2DMesh converts the point cloud into a high-quality mesh using an autoregressive decoder-only Transformer and Direct Preference Optimization (DPO); Vision3DGen creates high-fidelity 3D objects through multimodal feature alignment. Experiments on the Google Scanned Objects (GSO) and Pix3D datasets show that GeoCraft excels in key metrics. On the GSO dataset, its CMMD is 2.810 and FID CLIP is 26.420; on Pix3D, CMMD is 3.020 and FID CLIP is 27.030. GeoCraft significantly outperforms existing 3D reconstruction methods and also demonstrates advantages in computational efficiency, effectively solving key challenges in 3D reconstruction.The code is available at https://github.com/weixuanma/GeoCraft .
Published: 2026-01-14T00:29:37+00:00
Venue: Information Fusion
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weixuan Ma; Yamin Li; Chujin Liu; Hao Zhang; Jie Li; Kansong Chen; Weixuan Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104149"&gt;10.1016/j.inffus.2026.104149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid development of technologies like virtual reality (VR), autonomous driving, and digital twins, the demand for high-precision and realistic multimodal 3D reconstruction has surged. This technology has become a core research focus in computer vision and graphics due to its ability to integrate multi-source data, such as 2D images and point clouds. However, existing methods face challenges such as geometric inconsistency in single-view reconstruction, poor point cloud-to-mesh conversion, and insufficient multimodal feature fusion, limiting their practical application. To address these issues, this paper proposes GeoCraft, a multimodal 3D reconstruction method that generates high-precision 3D models from 2D images through three collaborative stages: Diff2DPoint, Point2DMesh, and Vision3DGen. Specifically, Diff2DPoint generates an initial point cloud with geometric alignment using a diffusion model and projection feature fusion; Point2DMesh converts the point cloud into a high-quality mesh using an autoregressive decoder-only Transformer and Direct Preference Optimization (DPO); Vision3DGen creates high-fidelity 3D objects through multimodal feature alignment. Experiments on the Google Scanned Objects (GSO) and Pix3D datasets show that GeoCraft excels in key metrics. On the GSO dataset, its CMMD is 2.810 and FID CLIP is 26.420; on Pix3D, CMMD is 3.020 and FID CLIP is 27.030. GeoCraft significantly outperforms existing 3D reconstruction methods and also demonstrates advantages in computational efficiency, effectively solving key challenges in 3D reconstruction.The code is available at https://github.com/weixuanma/GeoCraft .&lt;/p&gt;</content:encoded></item><item><title>A Multi-Modal Approach for Robust Oriented Ship Detection: Dataset and Methodology</title><link>https://doi.org/10.3390/rs18020274</link><guid>10.3390/rs18020274</guid><pubDate>Wed, 14 Jan 2026 15:12:04 +0000</pubDate><dc:creator>Jianing You</dc:creator><dc:creator>Yixuan Lv</dc:creator><dc:creator>Shengyang Li</dc:creator><dc:creator>Silei Liu</dc:creator><dc:creator>Kailun Zhang</dc:creator><dc:creator>Yuxuan Liu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020274</prism:doi><description>Maritime ship detection is a critical task for security and traffic management. To advance research in this area, we constructed a new high-resolution, spatially aligned optical-SAR dataset, named MOS-Ship. Building on this, we propose MOS-DETR, a novel query-based framework. This model incorporates an innovative multi-modal Swin Transformer backbone to extract unified feature pyramids from both RGB and SAR images. This design allows the model to jointly exploit optical textures and SAR scattering signatures for precise, oriented bounding box prediction. We also introduce an adaptive probabilistic fusion mechanism. This post-processing module dynamically integrates the detection results generated by our model from the optical and SAR inputs, synergistically combining their complementary strengths. Experiments validate that MOS-DETR achieves highly competitive accuracy and significantly outperforms unimodal baselines, demonstrating superior robustness across diverse conditions. This work provides a robust framework and methodology for advancing multimodal maritime surveillance.
Published: 2026-01-14T15:12:04+00:00
Venue: Remote Sensing
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianing You; Yixuan Lv; Shengyang Li; Silei Liu; Kailun Zhang; Yuxuan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020274"&gt;10.3390/rs18020274&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Maritime ship detection is a critical task for security and traffic management. To advance research in this area, we constructed a new high-resolution, spatially aligned optical-SAR dataset, named MOS-Ship. Building on this, we propose MOS-DETR, a novel query-based framework. This model incorporates an innovative multi-modal Swin Transformer backbone to extract unified feature pyramids from both RGB and SAR images. This design allows the model to jointly exploit optical textures and SAR scattering signatures for precise, oriented bounding box prediction. We also introduce an adaptive probabilistic fusion mechanism. This post-processing module dynamically integrates the detection results generated by our model from the optical and SAR inputs, synergistically combining their complementary strengths. Experiments validate that MOS-DETR achieves highly competitive accuracy and significantly outperforms unimodal baselines, demonstrating superior robustness across diverse conditions. This work provides a robust framework and methodology for advancing multimodal maritime surveillance.&lt;/p&gt;</content:encoded></item><item><title>Unveiling the Unknown: A SAM Guided Open World Object Detection Method for Remote Sensing</title><link>https://doi.org/10.1109/tgrs.2026.3654387</link><guid>10.1109/tgrs.2026.3654387</guid><pubDate>Wed, 14 Jan 2026 20:40:24 +0000</pubDate><dc:creator>Mingtao Hu</dc:creator><dc:creator>Wenxin Yin</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Xin Gao</dc:creator><dc:creator>Xian Sun</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654387</prism:doi><description>Despite the remarkable success of remote sensing object detection, these methods primarily operate under a closed-world paradigm, as they often misclassify or ignore novel objects in real world scenarios. To address this limitation, Open World Object Detection (OWOD) has emerged, enabling the discovery and incremental learning of new categories. However, existing OWOD approaches typically distinguish between known and unknown categories based on feature distance, overlooking the inherent challenge in remote sensing: large intra-class variation versus small inter-class variation. To bridge the gap, we propose a novel framework for SAM guided OWOD tailored for remote sensing imagery. Our approach is designed to leverage SAM’s capabilities for discovering new categories while systematically handling the noisy labels produced by SAM. We introduce four key components: (I) a Multi-scale Feature Fusion Perception (MFFP) module to enhance the detection of unknown objects across various scales in remote sensing; (II) a Cross-layer Cascaded Decoupling Decoder (CCDD) to alleviate the optimization conflicts between objectness and classification tasks for similar known and unknown classes in remote sensing images; (III) a Label Mapping Alignment (LMA) mechanism to adaptively filter background noisy proposals from SAM. And (IV) an Active Learning (AL) strategy is proposed to intelligently select exemplars for robust incremental learning. Extensive experiments on benchmark remote sensing datasets, including DIOR, DOTA, and NWPU demonstrate that our method significantly improves the recall of unknown objects while maintaining robust detection performance for known classes, demonstrating the effectiveness and potential of the SAM guided OWOD paradigm.
Published: 2026-01-14T20:40:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingtao Hu; Wenxin Yin; Wenhui Diao; Xin Gao; Xian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654387"&gt;10.1109/tgrs.2026.3654387&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Despite the remarkable success of remote sensing object detection, these methods primarily operate under a closed-world paradigm, as they often misclassify or ignore novel objects in real world scenarios. To address this limitation, Open World Object Detection (OWOD) has emerged, enabling the discovery and incremental learning of new categories. However, existing OWOD approaches typically distinguish between known and unknown categories based on feature distance, overlooking the inherent challenge in remote sensing: large intra-class variation versus small inter-class variation. To bridge the gap, we propose a novel framework for SAM guided OWOD tailored for remote sensing imagery. Our approach is designed to leverage SAM’s capabilities for discovering new categories while systematically handling the noisy labels produced by SAM. We introduce four key components: (I) a Multi-scale Feature Fusion Perception (MFFP) module to enhance the detection of unknown objects across various scales in remote sensing; (II) a Cross-layer Cascaded Decoupling Decoder (CCDD) to alleviate the optimization conflicts between objectness and classification tasks for similar known and unknown classes in remote sensing images; (III) a Label Mapping Alignment (LMA) mechanism to adaptively filter background noisy proposals from SAM. And (IV) an Active Learning (AL) strategy is proposed to intelligently select exemplars for robust incremental learning. Extensive experiments on benchmark remote sensing datasets, including DIOR, DOTA, and NWPU demonstrate that our method significantly improves the recall of unknown objects while maintaining robust detection performance for known classes, demonstrating the effectiveness and potential of the SAM guided OWOD paradigm.&lt;/p&gt;</content:encoded></item><item><title>DVGBench: Implicit-to-explicit visual grounding benchmark in UAV imagery with large vision–language models</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.005</link><guid>10.1016/j.isprsjprs.2026.01.005</guid><pubDate>Wed, 14 Jan 2026 23:32:14 +0000</pubDate><dc:creator>Yue Zhou</dc:creator><dc:creator>Jue Chen</dc:creator><dc:creator>Zilun Zhang</dc:creator><dc:creator>Penghui Huang</dc:creator><dc:creator>Ran Ding</dc:creator><dc:creator>Zhentao Zou</dc:creator><dc:creator>PengFei Gao</dc:creator><dc:creator>Yuchen Wei</dc:creator><dc:creator>Ke Li</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Xue Jiang</dc:creator><dc:creator>Hongxin Yang</dc:creator><dc:creator>Jonathan Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.005</prism:doi><description>Remote sensing (RS) large vision–language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions – such as relative position, relative size, and color cues – thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench .
Published: 2026-01-14T23:32:14+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Zhou; Jue Chen; Zilun Zhang; Penghui Huang; Ran Ding; Zhentao Zou; PengFei Gao; Yuchen Wei; Ke Li; Xue Yang; Xue Jiang; Hongxin Yang; Jonathan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.005"&gt;10.1016/j.isprsjprs.2026.01.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing (RS) large vision–language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions – such as relative position, relative size, and color cues – thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench .&lt;/p&gt;</content:encoded></item><item><title>PQGNet: Perceptual Query Guided Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3654433</link><guid>10.1109/tgrs.2026.3654433</guid><pubDate>Wed, 14 Jan 2026 20:40:24 +0000</pubDate><dc:creator>Pingping Liu</dc:creator><dc:creator>Aohua Li</dc:creator><dc:creator>Yubing Lu</dc:creator><dc:creator>Tongshun Zhang</dc:creator><dc:creator>Ming Yang</dc:creator><dc:creator>Qiuzhan Zhou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654433</prism:doi><description>Infrared small target detection (IRSTD) holds critical importance for military security applications. Although U-shaped architectures have improved baseline performance, existing methods still suffer from two key limitations: 1) Insufficient spatial perception for tiny targets leads to target location loss.; 2) Edge degradation and semantic ambiguity in deep feature reconstruction. To address these challenges, we propose PQGNet with the following contributions: To enhance capability of spatial perception and improve feature fusion guidance, we introduce the Perceptual Query Supervision Mechanism (PQSM), which utilizes perceptual loss to constrain spatial feature learning of each encoder layer. The Perceptual Feature Construction Module (PFCM) constructs enhanced perceptual features to preserve target localization information, while the Perceptual Query Guidance Module (PQGM) adopts cross-attention to guide global and regional feature queries through skip connections, optimizing target feature extraction. To mitigate reconstruction degradation and semantic ambiguity, distinct from existing wavelet-based approaches that simply substitute pooling layers, we design a Max pooling-Wavelet Hybrid Layer (MWHL) and High-frequency Enhancement Wavelet Layer (HEWL) that exploit discrete wavelet transform properties to enhance deep semantic representations using shallow high-frequency details. Comprehensive experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate that PQGNet significantly surpasses state-of-the-art methods in detection performance, while maintaining a competitive balance between computational complexity and accuracy. Our code will be made public at https://github.com/PepperCS/PQGNet.
Published: 2026-01-14T20:40:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pingping Liu; Aohua Li; Yubing Lu; Tongshun Zhang; Ming Yang; Qiuzhan Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654433"&gt;10.1109/tgrs.2026.3654433&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) holds critical importance for military security applications. Although U-shaped architectures have improved baseline performance, existing methods still suffer from two key limitations: 1) Insufficient spatial perception for tiny targets leads to target location loss.; 2) Edge degradation and semantic ambiguity in deep feature reconstruction. To address these challenges, we propose PQGNet with the following contributions: To enhance capability of spatial perception and improve feature fusion guidance, we introduce the Perceptual Query Supervision Mechanism (PQSM), which utilizes perceptual loss to constrain spatial feature learning of each encoder layer. The Perceptual Feature Construction Module (PFCM) constructs enhanced perceptual features to preserve target localization information, while the Perceptual Query Guidance Module (PQGM) adopts cross-attention to guide global and regional feature queries through skip connections, optimizing target feature extraction. To mitigate reconstruction degradation and semantic ambiguity, distinct from existing wavelet-based approaches that simply substitute pooling layers, we design a Max pooling-Wavelet Hybrid Layer (MWHL) and High-frequency Enhancement Wavelet Layer (HEWL) that exploit discrete wavelet transform properties to enhance deep semantic representations using shallow high-frequency details. Comprehensive experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate that PQGNet significantly surpasses state-of-the-art methods in detection performance, while maintaining a competitive balance between computational complexity and accuracy. Our code will be made public at https://github.com/PepperCS/PQGNet.&lt;/p&gt;</content:encoded></item><item><title>Open Set Domain Adaptation via Known Joint Distribution Matching and Unknown Classification Risk Reformulation</title><link>https://doi.org/10.1109/tnnls.2025.3647483</link><guid>10.1109/tnnls.2025.3647483</guid><pubDate>Wed, 14 Jan 2026 20:40:49 +0000</pubDate><dc:creator>Sentao Chen</dc:creator><dc:creator>Ping Xuan</dc:creator><dc:creator>Lifang He</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3647483</prism:doi><description>Open set domain adaptation (OSDA) is an important problem in machine learning and computer vision. In OSDA, one is given a labeled dataset from a source domain (source joint distribution) and an unlabeled dataset from a target domain (target joint distribution), where the target domain contains not only the known classes presented in the source domain but also the unknown class. The goal of OSDA is to train a neural network with minimal target classification risk. From the statistical learning perspective, there are two fundamental challenges in this problem: (1) the source–target joint distribution difference regarding the known classes and (2) the target classification risk estimation regarding the unknown class. Although prior works have proposed various sophisticated solutions to the problem and achieved inspiring experimental results, they do not fully resolve these two challenges. In this article, we introduce a principled approach named known joint distribution matching and unknown classification risk reformulation (KMUR). KMUR tackles the first challenge by matching the source joint distribution to the target known joint distribution such that the distribution difference can be reduced and addresses the second challenge by reformulating the target unknown classification risk such that the reformulated risk can be estimated on the unlabeled target and source data. To be specific, we exploit cross entropy as the classification loss and triangular discrimination (TD) distance as the joint distribution matching loss. Since the TD distance needs to be estimated from data, we develop an innovative technique named least squares TD estimation (LSTDE), which casts the estimation into least squares classification. To achieve the OSDA goal, we train the network to minimize the estimations of target classification risk and TD distance. Experiments on benchmark and real-world datasets confirm the effectiveness of our approach. The introductory video and PyTorch code are ...
Published: 2026-01-14T20:40:49+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sentao Chen; Ping Xuan; Lifang He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3647483"&gt;10.1109/tnnls.2025.3647483&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Open set domain adaptation (OSDA) is an important problem in machine learning and computer vision. In OSDA, one is given a labeled dataset from a source domain (source joint distribution) and an unlabeled dataset from a target domain (target joint distribution), where the target domain contains not only the known classes presented in the source domain but also the unknown class. The goal of OSDA is to train a neural network with minimal target classification risk. From the statistical learning perspective, there are two fundamental challenges in this problem: (1) the source–target joint distribution difference regarding the known classes and (2) the target classification risk estimation regarding the unknown class. Although prior works have proposed various sophisticated solutions to the problem and achieved inspiring experimental results, they do not fully resolve these two challenges. In this article, we introduce a principled approach named known joint distribution matching and unknown classification risk reformulation (KMUR). KMUR tackles the first challenge by matching the source joint distribution to the target known joint distribution such that the distribution difference can be reduced and addresses the second challenge by reformulating the target unknown classification risk such that the reformulated risk can be estimated on the unlabeled target and source data. To be specific, we exploit cross entropy as the classification loss and triangular discrimination (TD) distance as the joint distribution matching loss. Since the TD distance needs to be estimated from data, we develop an innovative technique named least squares TD estimation (LSTDE), which casts the estimation into least squares classification. To achieve the OSDA goal, we train the network to minimize the estimations of target classification risk and TD distance. Experiments on benchmark and real-world datasets confirm the effectiveness of our approach. The introductory video and PyTorch code are ...&lt;/p&gt;</content:encoded></item><item><title>Reconstruction Guided Few-shot Network For Remote Sensing Image Classification</title><link>https://arxiv.org/abs/2601.07335v1</link><guid>http://arxiv.org/abs/2601.07335v1</guid><pubDate>Mon, 12 Jan 2026 09:02:30 +0000</pubDate><dc:creator>Mohit Jaiswal</dc:creator><dc:creator>Naman Jain</dc:creator><dc:creator>Shivani Pathak</dc:creator><dc:creator>Mainak Singha</dc:creator><dc:creator>Nikunja Bihari Kar</dc:creator><dc:creator>Ankit Jha</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.
Published: 2026-01-12T09:02:30+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohit Jaiswal; Naman Jain; Shivani Pathak; Mainak Singha; Nikunja Bihari Kar; Ankit Jha; Biplab Banerjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.&lt;/p&gt;</content:encoded></item><item><title>Disentangle Object and Non-object Infrared Features via Language Guidance</title><link>https://arxiv.org/abs/2601.09228v1</link><guid>http://arxiv.org/abs/2601.09228v1</guid><pubDate>Wed, 14 Jan 2026 06:59:54 +0000</pubDate><dc:creator>Fan Liu</dc:creator><dc:creator>Ting Wu</dc:creator><dc:creator>Chuanyi Zhang</dc:creator><dc:creator>Liang Yao</dc:creator><dc:creator>Xing Ma</dc:creator><dc:creator>Yuhui Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.
Published: 2026-01-14T06:59:54+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fan Liu; Ting Wu; Chuanyi Zhang; Liang Yao; Xing Ma; Yuhui Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.&lt;/p&gt;</content:encoded></item><item><title>Benchmarking large language models on safety risks in scientific laboratories</title><link>https://doi.org/10.1038/s42256-025-01152-1</link><guid>10.1038/s42256-025-01152-1</guid><pubDate>Wed, 14 Jan 2026 10:01:54 +0000</pubDate><dc:creator>Yujun Zhou</dc:creator><dc:creator>Jingdong Yang</dc:creator><dc:creator>Yue Huang</dc:creator><dc:creator>Kehan Guo</dc:creator><dc:creator>Zoe Emory</dc:creator><dc:creator>Bikram Ghosh</dc:creator><dc:creator>Amita Bedar</dc:creator><dc:creator>Sujay Shekar</dc:creator><dc:creator>Zhenwen Liang</dc:creator><dc:creator>Pin-Yu Chen</dc:creator><dc:creator>Tian Gao</dc:creator><dc:creator>Werner Geyer</dc:creator><dc:creator>Nuno Moniz</dc:creator><dc:creator>Nitesh V. Chawla</dc:creator><dc:creator>Xiangliang Zhang</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01152-1</prism:doi><description>Artificial intelligence is revolutionizing scientific research, yet its growing integration into laboratory environments presents critical safety challenges. Large language models and vision language models now assist in experiment design and procedural guidance, yet their ‘illusion of understanding’ may lead researchers to overtrust unsafe outputs. Here we show that current models remain far from meeting the reliability needed for safe laboratory operation. We introduce LabSafety Bench, a comprehensive benchmark that evaluates models on hazard identification, risk assessment and consequence prediction across 765 multiple-choice questions and 404 realistic laboratory scenarios, encompassing 3,128 open-ended tasks. Evaluations on 19 advanced large language models and vision language models show that no model evaluated on hazard identification surpasses 70% accuracy. While proprietary models perform well on structured assessments, they do not show a clear advantage in open-ended reasoning. These results underscore the urgent need for specialized safety evaluation frameworks before deploying artificial intelligence systems in real laboratory settings. Large language models are starting to be used in safety-critical tasks such as controlling robots. Zhou et al. present LabSafety Bench, a benchmark evaluating the ability of large language models to identify hazards and assess laboratory risks.
Published: 2026-01-14T10:01:54+00:00
Venue: Nature Machine Intelligence
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yujun Zhou; Jingdong Yang; Yue Huang; Kehan Guo; Zoe Emory; Bikram Ghosh; Amita Bedar; Sujay Shekar; Zhenwen Liang; Pin-Yu Chen; Tian Gao; Werner Geyer; Nuno Moniz; Nitesh V. Chawla; Xiangliang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01152-1"&gt;10.1038/s42256-025-01152-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Artificial intelligence is revolutionizing scientific research, yet its growing integration into laboratory environments presents critical safety challenges. Large language models and vision language models now assist in experiment design and procedural guidance, yet their ‘illusion of understanding’ may lead researchers to overtrust unsafe outputs. Here we show that current models remain far from meeting the reliability needed for safe laboratory operation. We introduce LabSafety Bench, a comprehensive benchmark that evaluates models on hazard identification, risk assessment and consequence prediction across 765 multiple-choice questions and 404 realistic laboratory scenarios, encompassing 3,128 open-ended tasks. Evaluations on 19 advanced large language models and vision language models show that no model evaluated on hazard identification surpasses 70% accuracy. While proprietary models perform well on structured assessments, they do not show a clear advantage in open-ended reasoning. These results underscore the urgent need for specialized safety evaluation frameworks before deploying artificial intelligence systems in real laboratory settings. Large language models are starting to be used in safety-critical tasks such as controlling robots. Zhou et al. present LabSafety Bench, a benchmark evaluating the ability of large language models to identify hazards and assess laboratory risks.&lt;/p&gt;</content:encoded></item><item><title>Positive Matching Benefits Fusion: A Novel Contrastive Learning Framework for Hyperspectral and LiDAR Data Classification</title><link>https://doi.org/10.1109/tgrs.2026.3654168</link><guid>10.1109/tgrs.2026.3654168</guid><pubDate>Wed, 14 Jan 2026 20:40:24 +0000</pubDate><dc:creator>Hui Liu</dc:creator><dc:creator>Chenjia Huang</dc:creator><dc:creator>Tao Xie</dc:creator><dc:creator>Wei Bao</dc:creator><dc:creator>Ning Chen</dc:creator><dc:creator>Jun Yue</dc:creator><dc:creator>Leyuan Fang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654168</prism:doi><description>Recent advancements in contrastive learning have led to significant progress in multi-modal remote sensing data classification, such as hyperspectral and LiDAR data, particularly in scenarios with limited labeled samples. However, these methods primarily rely on the contrast between positive and negative pairs, which makes them susceptible to interference from negative sample selection, thereby impairing effective multi-modal fusion and ultimately affecting classification accuracy. To address this issue, this paper introduces a novel framework for the joint classification of hyperspectral image (HSI) and LiDAR, termed the multi-Positive Matching-enhanced Contrastive Learning (mPMCL). The proposed framework facilitates contrastive training by matching multiple positive pairs across different modalities and hierarchical features of the same object, hence effectively enhancing multi-modal fusion and improving classification performance. Specifically, the HCIF module performs hierarchical and consistency-aware fusion by using a bidirectional cross-attention mechanism to integrate low-level, cross-modal interaction, and high-level semantic features from HSI and LiDAR data. This process gradually aligns heterogeneous representations across stages and captures complementary spectral–elevation cues, leading to more stable and discriminative multimodal features. Additionally, a multi-Positive Matching Strategy (mPMS) is develpoed to construct multiple positive pairs by matching high-level semantic features of the same object with low-level- and cross-modal fusion features from different modalities. By performing contrastive training on these positive pairs, the proposed method avoids the sensitivity of traditional approaches to negative sample selection, while further enhancing the effectiveness of multi-modal information fusion. The pivotal contribution of the proposed framework lies in demonstrating the importance of positive matching enhanced contrastive learning strategi...
Published: 2026-01-14T20:40:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hui Liu; Chenjia Huang; Tao Xie; Wei Bao; Ning Chen; Jun Yue; Leyuan Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654168"&gt;10.1109/tgrs.2026.3654168&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in contrastive learning have led to significant progress in multi-modal remote sensing data classification, such as hyperspectral and LiDAR data, particularly in scenarios with limited labeled samples. However, these methods primarily rely on the contrast between positive and negative pairs, which makes them susceptible to interference from negative sample selection, thereby impairing effective multi-modal fusion and ultimately affecting classification accuracy. To address this issue, this paper introduces a novel framework for the joint classification of hyperspectral image (HSI) and LiDAR, termed the multi-Positive Matching-enhanced Contrastive Learning (mPMCL). The proposed framework facilitates contrastive training by matching multiple positive pairs across different modalities and hierarchical features of the same object, hence effectively enhancing multi-modal fusion and improving classification performance. Specifically, the HCIF module performs hierarchical and consistency-aware fusion by using a bidirectional cross-attention mechanism to integrate low-level, cross-modal interaction, and high-level semantic features from HSI and LiDAR data. This process gradually aligns heterogeneous representations across stages and captures complementary spectral–elevation cues, leading to more stable and discriminative multimodal features. Additionally, a multi-Positive Matching Strategy (mPMS) is develpoed to construct multiple positive pairs by matching high-level semantic features of the same object with low-level- and cross-modal fusion features from different modalities. By performing contrastive training on these positive pairs, the proposed method avoids the sensitivity of traditional approaches to negative sample selection, while further enhancing the effectiveness of multi-modal information fusion. The pivotal contribution of the proposed framework lies in demonstrating the importance of positive matching enhanced contrastive learning strategi...&lt;/p&gt;</content:encoded></item><item><title>LiteEmbed: Adapting CLIP to Rare Classes</title><link>https://arxiv.org/abs/2601.09661v1</link><guid>http://arxiv.org/abs/2601.09661v1</guid><pubDate>Wed, 14 Jan 2026 17:53:11 +0000</pubDate><dc:creator>Aishwarya Agarwal</dc:creator><dc:creator>Srikrishna Karanam</dc:creator><dc:creator>Vineet Gandhi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.
Published: 2026-01-14T17:53:11+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aishwarya Agarwal; Srikrishna Karanam; Vineet Gandhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&amp;#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&amp;#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.&lt;/p&gt;</content:encoded></item><item><title>Advancing Multinational License Plate Recognition Through Synthetic and Real Data Fusion: A Comprehensive Evaluation</title><link>https://arxiv.org/abs/2601.07671v1</link><guid>http://arxiv.org/abs/2601.07671v1</guid><pubDate>Mon, 12 Jan 2026 15:52:52 +0000</pubDate><dc:creator>Rayson Laroca</dc:creator><dc:creator>Valter Estevam</dc:creator><dc:creator>Gladston J. P. Moreira</dc:creator><dc:creator>Rodrigo Minetto</dc:creator><dc:creator>David Menotti</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1049/itr2.70086</prism:doi><description>Automatic License Plate Recognition is a frequent research topic due to its wide-ranging practical applications. While recent studies use synthetic images to improve License Plate Recognition (LPR) results, there remain several limitations in these efforts. This work addresses these constraints by comprehensively exploring the integration of real and synthetic data to enhance LPR performance. We subject 16 Optical Character Recognition (OCR) models to a benchmarking process involving 12 public datasets acquired from various regions. Several key findings emerge from our investigation. Primarily, the massive incorporation of synthetic data substantially boosts model performance in both intra- and cross-dataset scenarios. We examine three distinct methodologies for generating synthetic data: template-based generation, character permutation, and utilizing a Generative Adversarial Network (GAN) model, each contributing significantly to performance enhancement. The combined use of these methodologies demonstrates a notable synergistic effect, leading to end-to-end results that surpass those reached by state-of-the-art methods and established commercial systems. Our experiments also underscore the efficacy of synthetic data in mitigating challenges posed by limited training data, enabling remarkable results to be achieved even with small fractions of the original training data. Finally, we investigate the trade-off between accuracy and speed among different models, identifying those that strike the optimal balance in each intra-dataset and cross-dataset settings.
Published: 2026-01-12T15:52:52+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rayson Laroca; Valter Estevam; Gladston J. P. Moreira; Rodrigo Minetto; David Menotti&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1049/itr2.70086"&gt;10.1049/itr2.70086&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Automatic License Plate Recognition is a frequent research topic due to its wide-ranging practical applications. While recent studies use synthetic images to improve License Plate Recognition (LPR) results, there remain several limitations in these efforts. This work addresses these constraints by comprehensively exploring the integration of real and synthetic data to enhance LPR performance. We subject 16 Optical Character Recognition (OCR) models to a benchmarking process involving 12 public datasets acquired from various regions. Several key findings emerge from our investigation. Primarily, the massive incorporation of synthetic data substantially boosts model performance in both intra- and cross-dataset scenarios. We examine three distinct methodologies for generating synthetic data: template-based generation, character permutation, and utilizing a Generative Adversarial Network (GAN) model, each contributing significantly to performance enhancement. The combined use of these methodologies demonstrates a notable synergistic effect, leading to end-to-end results that surpass those reached by state-of-the-art methods and established commercial systems. Our experiments also underscore the efficacy of synthetic data in mitigating challenges posed by limited training data, enabling remarkable results to be achieved even with small fractions of the original training data. Finally, we investigate the trade-off between accuracy and speed among different models, identifying those that strike the optimal balance in each intra-dataset and cross-dataset settings.&lt;/p&gt;</content:encoded></item><item><title>LLMDNet: An Aautonomous mining truck object detection network in low-light conditions</title><link>https://doi.org/10.1016/j.knosys.2026.115286</link><guid>10.1016/j.knosys.2026.115286</guid><pubDate>Wed, 14 Jan 2026 13:35:46 +0000</pubDate><dc:creator>Feixiang Xu</dc:creator><dc:creator>Rui Zhang</dc:creator><dc:creator>Yafei Wang</dc:creator><dc:creator>He Jiang</dc:creator><dc:creator>Deqiang Cheng</dc:creator><dc:creator>Jiansheng Qian</dc:creator><dc:creator>Fengqian Sun</dc:creator><dc:creator>Lige Xue</dc:creator><dc:creator>Chen Zhou</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115286</prism:doi><description>Accurate and reliable object detection is a crucial component of the perception system for autonomous mining trucks. However, low-light environment is a common working scenario in surface mines, where interference from low-light backgrounds and loss of object information pose significant challenges to object detection. Most existing end-to-end low-light object detection methods based on YOLO series are built upon hierarchical interactions for cross-layer fusion, in which information is often degraded during transmission, thereby hindering effective multi-scale feature integration. To this end, a Low-Light Modulation Detection Network (LLMDNet) is proposed to enhance object representation and detection robustness under such conditions. It consists of a robust feature fusion pathway, which is combination of Low-Light Modulation Network (LLMN) and Multi-level Feature Balancing Strategy (MFBS). Three key components are integrated in LLMN to enhance object representation in a progressive manner. Firstly, the Low-Light Information Filter (LLIF) conducts cross-scale differential operations to mitigate background interference and emphasize edge details. Following this, the Information Injection Module (IIM) is applied to facilitate dynamic fusion between deep and shallow features, enabling rich semantic interaction. Subsequently, the Directional Attention Mechanism (DAM) captures spatial structural cues along horizontal, vertical, and channel dimensions to enhance structural perception. To further refine the features processed by DAM, IIM is reintroduced to ensure deeper interaction across scales, boosting the representation capacity before detection. And to ensure effective detection, MFBS is utilized to integrate features across multiple scales in a coordinated manner before feeding them into the detection head. Finally, a custom dataset Low-light Auto-Mine (LAM) is constructed to realize object detection of autonomous mining trucks in low-light conditions. And extensive experiments are conducted on both LAM and Exdark datasets. LLMDNet achieves the mean Average Precision@50 (mAP 50 ) of 86.1% and 81.7% on the LAM and Exdark datasets, respectively. Compared to the state-of-the-art YOLA, the mAP 50 with the proposed LLMDNet is increased by 1.9% on LAM. Moreover, compared to YOLOv8, there is a significant improvement of 4.1% and a 3.3% increase in the mAP 50 , respectively. The results further demonstrate that our model can effectively improve detection accuracy.
Published: 2026-01-14T13:35:46+00:00
Venue: Knowledge-Based Systems
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feixiang Xu; Rui Zhang; Yafei Wang; He Jiang; Deqiang Cheng; Jiansheng Qian; Fengqian Sun; Lige Xue; Chen Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115286"&gt;10.1016/j.knosys.2026.115286&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate and reliable object detection is a crucial component of the perception system for autonomous mining trucks. However, low-light environment is a common working scenario in surface mines, where interference from low-light backgrounds and loss of object information pose significant challenges to object detection. Most existing end-to-end low-light object detection methods based on YOLO series are built upon hierarchical interactions for cross-layer fusion, in which information is often degraded during transmission, thereby hindering effective multi-scale feature integration. To this end, a Low-Light Modulation Detection Network (LLMDNet) is proposed to enhance object representation and detection robustness under such conditions. It consists of a robust feature fusion pathway, which is combination of Low-Light Modulation Network (LLMN) and Multi-level Feature Balancing Strategy (MFBS). Three key components are integrated in LLMN to enhance object representation in a progressive manner. Firstly, the Low-Light Information Filter (LLIF) conducts cross-scale differential operations to mitigate background interference and emphasize edge details. Following this, the Information Injection Module (IIM) is applied to facilitate dynamic fusion between deep and shallow features, enabling rich semantic interaction. Subsequently, the Directional Attention Mechanism (DAM) captures spatial structural cues along horizontal, vertical, and channel dimensions to enhance structural perception. To further refine the features processed by DAM, IIM is reintroduced to ensure deeper interaction across scales, boosting the representation capacity before detection. And to ensure effective detection, MFBS is utilized to integrate features across multiple scales in a coordinated manner before feeding them into the detection head. Finally, a custom dataset Low-light Auto-Mine (LAM) is constructed to realize object detection of autonomous mining trucks in low-light conditions. And extensive experiments are conducted on both LAM and Exdark datasets. LLMDNet achieves the mean Average Precision@50 (mAP 50 ) of 86.1% and 81.7% on the LAM and Exdark datasets, respectively. Compared to the state-of-the-art YOLA, the mAP 50 with the proposed LLMDNet is increased by 1.9% on LAM. Moreover, compared to YOLOv8, there is a significant improvement of 4.1% and a 3.3% increase in the mAP 50 , respectively. The results further demonstrate that our model can effectively improve detection accuracy.&lt;/p&gt;</content:encoded></item><item><title>Projection-Evidence Collaborative Optimization for Cross-Modal Few-Shot SAR Target Detection</title><link>https://doi.org/10.1109/jstars.2026.3654202</link><guid>10.1109/jstars.2026.3654202</guid><pubDate>Wed, 14 Jan 2026 20:40:41 +0000</pubDate><dc:creator>Zheng Zhou</dc:creator><dc:creator>Bohang Lin</dc:creator><dc:creator>Yijun Li</dc:creator><dc:creator>Zongyong Cui</dc:creator><dc:creator>Yiming Pi</dc:creator><dc:creator>Zongjie Cao</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3654202</prism:doi><description>Existing few-shot object detection (FSOD) methods face a dual challenge when applied to few-shot synthetic aperture radar target detection in cross-modal scenarios (optical ightarrow ightarrow SAR): (i) high-dimensional noisy features and nonlinear modality differences make traditional feature-alignment mechanisms ineffective for matching optical and SAR modalities; (ii) current probabilistic classification frameworks, which rely on point estimates under maximum likelihood estimation (MLE), cannot adequately model the epistemic uncertainty induced by sample scarcity, leading to over-confident detection errors. To address these issues, we propose a projection-evidence collaborative optimization (PECO) method for cross-modal few-shot SAR target detection. Specifically, we first design a projection distribution alignment (PDA) module, which constructs projected distributions and maps cross-modal data into a low-dimensional latent space, markedly reducing modality discrepancies and achieving effective cross-modal distribution alignment. Second, we introduce a dynamic uncertainty calibration (DUC) module that models class probabilities with a Dirichlet evidence distribution and jointly optimizes epistemic and aleatoric uncertainties through a dynamic-weighting and label-driven calibration mechanism, thereby mitigating over-confidence errors in scarce-sample settings. Experimental results on the cross-modal datasets DIOR2SSDD and FAIR1M2SAR-AIRcraft verify the effectiveness of the proposed approach: PECO surpasses existing state-of-the-art methods by 5.6% and 13.7%, respectively, in overall average detection performance, while also significantly improving model generalization. Code will be available at: https://github.com/Caltech-Z/PECO
Published: 2026-01-14T20:40:41+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheng Zhou; Bohang Lin; Yijun Li; Zongyong Cui; Yiming Pi; Zongjie Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3654202"&gt;10.1109/jstars.2026.3654202&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Existing few-shot object detection (FSOD) methods face a dual challenge when applied to few-shot synthetic aperture radar target detection in cross-modal scenarios (optical ightarrow ightarrow SAR): (i) high-dimensional noisy features and nonlinear modality differences make traditional feature-alignment mechanisms ineffective for matching optical and SAR modalities; (ii) current probabilistic classification frameworks, which rely on point estimates under maximum likelihood estimation (MLE), cannot adequately model the epistemic uncertainty induced by sample scarcity, leading to over-confident detection errors. To address these issues, we propose a projection-evidence collaborative optimization (PECO) method for cross-modal few-shot SAR target detection. Specifically, we first design a projection distribution alignment (PDA) module, which constructs projected distributions and maps cross-modal data into a low-dimensional latent space, markedly reducing modality discrepancies and achieving effective cross-modal distribution alignment. Second, we introduce a dynamic uncertainty calibration (DUC) module that models class probabilities with a Dirichlet evidence distribution and jointly optimizes epistemic and aleatoric uncertainties through a dynamic-weighting and label-driven calibration mechanism, thereby mitigating over-confidence errors in scarce-sample settings. Experimental results on the cross-modal datasets DIOR2SSDD and FAIR1M2SAR-AIRcraft verify the effectiveness of the proposed approach: PECO surpasses existing state-of-the-art methods by 5.6% and 13.7%, respectively, in overall average detection performance, while also significantly improving model generalization. Code will be available at: https://github.com/Caltech-Z/PECO&lt;/p&gt;</content:encoded></item><item><title>OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation</title><link>https://arxiv.org/abs/2601.07392v1</link><guid>http://arxiv.org/abs/2601.07392v1</guid><pubDate>Mon, 12 Jan 2026 10:20:43 +0000</pubDate><dc:creator>Alexandre Tuel</dc:creator><dc:creator>Thomas Kerdreux</dc:creator><dc:creator>Quentin Febvre</dc:creator><dc:creator>Alexis Mouche</dc:creator><dc:creator>Antoine Grouazel</dc:creator><dc:creator>Jean-Renaud Miadana</dc:creator><dc:creator>Antoine Audras</dc:creator><dc:creator>Chen Wang</dc:creator><dc:creator>Bertrand Chapron</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.
Published: 2026-01-12T10:20:43+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alexandre Tuel; Thomas Kerdreux; Quentin Febvre; Alexis Mouche; Antoine Grouazel; Jean-Renaud Miadana; Antoine Audras; Chen Wang; Bertrand Chapron&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.&lt;/p&gt;</content:encoded></item><item><title>Two-Stage Fine-Tuning of Large Vision-Language Models with Hierarchical Prompting for Few-Shot Object Detection in Remote Sensing Images</title><link>https://doi.org/10.3390/rs18020266</link><guid>10.3390/rs18020266</guid><pubDate>Wed, 14 Jan 2026 15:12:04 +0000</pubDate><dc:creator>Yongqi Shi</dc:creator><dc:creator>Ruopeng Yang</dc:creator><dc:creator>Changsheng Yin</dc:creator><dc:creator>Yiwei Lu</dc:creator><dc:creator>Bo Huang</dc:creator><dc:creator>Yu Tao</dc:creator><dc:creator>Yihao Zhong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020266</prism:doi><description>Few-shot object detection (FSOD) in high-resolution remote sensing (RS) imagery remains challenging due to scarce annotations, large intra-class variability, and high visual similarity between categories, which together limit the generalization ability of convolutional neural network (CNN)-based detectors. To address this issue, we explore leveraging large vision-language models (LVLMs) for FSOD in RS. We propose a two-stage, parameter-efficient fine-tuning framework with hierarchical prompting that adapts Qwen3-VL for object detection. In the first stage, low-rank adaptation (LoRA) modules are inserted into the vision and text encoders and trained jointly with a Detection Transformer (DETR)-style detection head on fully annotated base classes under three-level hierarchical prompts. In the second stage, the vision LoRA parameters are frozen, the text encoder is updated using K-shot novel-class samples, and the detection head is partially frozen, with selected components refined using the same three-level hierarchical prompting scheme. To preserve base-class performance and reduce class confusion, we further introduce knowledge distillation and semantic consistency losses. Experiments on the DIOR and NWPU VHR-10.v2 datasets show that the proposed method consistently improves novel-class performance while maintaining competitive base-class accuracy and surpasses existing baselines, demonstrating the effectiveness of integrating hierarchical semantic reasoning into LVLM-based FSOD for RS imagery.
Published: 2026-01-14T15:12:04+00:00
Venue: Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongqi Shi; Ruopeng Yang; Changsheng Yin; Yiwei Lu; Bo Huang; Yu Tao; Yihao Zhong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020266"&gt;10.3390/rs18020266&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) in high-resolution remote sensing (RS) imagery remains challenging due to scarce annotations, large intra-class variability, and high visual similarity between categories, which together limit the generalization ability of convolutional neural network (CNN)-based detectors. To address this issue, we explore leveraging large vision-language models (LVLMs) for FSOD in RS. We propose a two-stage, parameter-efficient fine-tuning framework with hierarchical prompting that adapts Qwen3-VL for object detection. In the first stage, low-rank adaptation (LoRA) modules are inserted into the vision and text encoders and trained jointly with a Detection Transformer (DETR)-style detection head on fully annotated base classes under three-level hierarchical prompts. In the second stage, the vision LoRA parameters are frozen, the text encoder is updated using K-shot novel-class samples, and the detection head is partially frozen, with selected components refined using the same three-level hierarchical prompting scheme. To preserve base-class performance and reduce class confusion, we further introduce knowledge distillation and semantic consistency losses. Experiments on the DIOR and NWPU VHR-10.v2 datasets show that the proposed method consistently improves novel-class performance while maintaining competitive base-class accuracy and surpasses existing baselines, demonstrating the effectiveness of integrating hierarchical semantic reasoning into LVLM-based FSOD for RS imagery.&lt;/p&gt;</content:encoded></item><item><title>Generative Compositional Zero-Shot Learning Using Learnable Primitive Disparity</title><link>https://doi.org/10.1016/j.knosys.2026.115278</link><guid>10.1016/j.knosys.2026.115278</guid><pubDate>Thu, 15 Jan 2026 07:17:30 +0000</pubDate><dc:creator>Minho Kim</dc:creator><dc:creator>Byeongkeun Kang</dc:creator><dc:creator>Yeejin Lee</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115278</prism:doi><description>Compositional zero-shot learning aims to recognize both object and attribute categories from images, including novel attribute–object combinations that are not observed during training. A key challenge is correctly identifying unseen compositions without supervision while avoiding reliance on inconsistent associations between class names and visual content. This issue mainly arises from the use of fixed text embeddings that are directly tied to class labels. To overcome these challenges, we propose a novel framework that learns primitive disparities without depending on textual labels. Our method integrates an embedding-based strategy with a generative framework, an approach that has received limited attention in compositional learning. Specifically, primitive classes are identified by comparing visual and textual representations in a shared embedding space. To improve visual feature quality, we introduce a region-specific feature aggregation strategy that effectively captures attribute-related information. In addition, to mitigate data scarcity in zero-shot learning scenarios, we design a generative module that synthesizes unseen features using metric-learning-based triplets and feature disparity modeling with learnable class features. This module enables feature synthesis in a unified visual space, reducing dependence on text-driven knowledge commonly used in existing methods. The synthesized features are then used to jointly refine both visual and textual representations, leading to improved generalization performance. Extensive experiments on four widely used benchmark datasets demonstrate that our method outperforms state-of-the-art approaches. The code will be released upon publication.
Published: 2026-01-15T07:17:30+00:00
Venue: Knowledge-Based Systems
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minho Kim; Byeongkeun Kang; Yeejin Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115278"&gt;10.1016/j.knosys.2026.115278&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Compositional zero-shot learning aims to recognize both object and attribute categories from images, including novel attribute–object combinations that are not observed during training. A key challenge is correctly identifying unseen compositions without supervision while avoiding reliance on inconsistent associations between class names and visual content. This issue mainly arises from the use of fixed text embeddings that are directly tied to class labels. To overcome these challenges, we propose a novel framework that learns primitive disparities without depending on textual labels. Our method integrates an embedding-based strategy with a generative framework, an approach that has received limited attention in compositional learning. Specifically, primitive classes are identified by comparing visual and textual representations in a shared embedding space. To improve visual feature quality, we introduce a region-specific feature aggregation strategy that effectively captures attribute-related information. In addition, to mitigate data scarcity in zero-shot learning scenarios, we design a generative module that synthesizes unseen features using metric-learning-based triplets and feature disparity modeling with learnable class features. This module enables feature synthesis in a unified visual space, reducing dependence on text-driven knowledge commonly used in existing methods. The synthesized features are then used to jointly refine both visual and textual representations, leading to improved generalization performance. Extensive experiments on four widely used benchmark datasets demonstrate that our method outperforms state-of-the-art approaches. The code will be released upon publication.&lt;/p&gt;</content:encoded></item><item><title>FLGF-Unet：融合局部-全局特征的光学遥感图像遥感建筑物提取网络</title><link>https://doi.org/10.11834/jrs.20264516</link><guid>10.11834/jrs.20264516</guid><pubDate>Thu, 15 Jan 2026 08:27:58 +0000</pubDate><dc:creator>LI Guoyan</dc:creator><dc:creator>LIU Tao</dc:creator><dc:creator>WANG Li</dc:creator><dc:creator>LIU Yi</dc:creator><prism:publicationName>National Remote Sensing Bulletin</prism:publicationName><prism:doi>10.11834/jrs.20264516</prism:doi><description>遥感图像的语义分割在城市变化检测、环境保护、地质灾害识别等领域具有重要作用。针对当前遥感建筑物提取中存在的漏检、误检、因树木遮挡或类似物体干扰导致提取不完整等问题，本文基于UNet网络提出一种改进的建筑物提取网络--融合局部-全局特征网络（Fusion of local global features network，FLGF-UNet）。FLGF-UNet的并行特征融合方式确保每个阶段的特征都包含细粒度的局部信息和全局依赖，使得网络在每一阶段的特征表示中同时具备局部和全局信息，有效克服Transformer在局部信息交换上的不足，同时在全局信息建模方面优于传统CNN。此外，为弥补编码器和解码器之间的语义鸿沟，编解码器之间加入交互融合（Interactive Fusion，IF）模块，增强空间细节、全局上下文和语义特征的融合效果。为验证FLGF-UNet的优越性和通用性，在WHU、Massachusetts数据集和中国典型城市建筑物实例数据集上，将所提网络与U2Net、Swin Transformer、MA-Net、HD-Net和RS-Mamba等网络进行对比。结果表明，FLGF-UNet在性能上优于其他SOTA网络，具有较高的实际应用价值。
Published: 2026-01-15T08:27:58+00:00
Venue: National Remote Sensing Bulletin
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; LI Guoyan; LIU Tao; WANG Li; LIU Yi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; National Remote Sensing Bulletin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jrs.20264516"&gt;10.11834/jrs.20264516&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;遥感图像的语义分割在城市变化检测、环境保护、地质灾害识别等领域具有重要作用。针对当前遥感建筑物提取中存在的漏检、误检、因树木遮挡或类似物体干扰导致提取不完整等问题，本文基于UNet网络提出一种改进的建筑物提取网络--融合局部-全局特征网络（Fusion of local global features network，FLGF-UNet）。FLGF-UNet的并行特征融合方式确保每个阶段的特征都包含细粒度的局部信息和全局依赖，使得网络在每一阶段的特征表示中同时具备局部和全局信息，有效克服Transformer在局部信息交换上的不足，同时在全局信息建模方面优于传统CNN。此外，为弥补编码器和解码器之间的语义鸿沟，编解码器之间加入交互融合（Interactive Fusion，IF）模块，增强空间细节、全局上下文和语义特征的融合效果。为验证FLGF-UNet的优越性和通用性，在WHU、Massachusetts数据集和中国典型城市建筑物实例数据集上，将所提网络与U2Net、Swin Transformer、MA-Net、HD-Net和RS-Mamba等网络进行对比。结果表明，FLGF-UNet在性能上优于其他SOTA网络，具有较高的实际应用价值。&lt;/p&gt;</content:encoded></item><item><title>GenDet: Painting Colored Bounding Boxes on Images via Diffusion Model for Object Detection</title><link>https://arxiv.org/abs/2601.07273v1</link><guid>http://arxiv.org/abs/2601.07273v1</guid><pubDate>Mon, 12 Jan 2026 07:29:59 +0000</pubDate><dc:creator>Chen Min</dc:creator><dc:creator>Chengyang Li</dc:creator><dc:creator>Fanjie Kong</dc:creator><dc:creator>Qi Zhu</dc:creator><dc:creator>Dawei Zhao</dc:creator><dc:creator>Liang Xiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper presents GenDet, a novel framework that redefines object detection as an image generation task. In contrast to traditional approaches, GenDet adopts a pioneering approach by leveraging generative modeling: it conditions on the input image and directly generates bounding boxes with semantic annotations in the original image space. GenDet establishes a conditional generation architecture built upon the large-scale pre-trained Stable Diffusion model, formulating the detection task as semantic constraints within the latent space. It enables precise control over bounding box positions and category attributes, while preserving the flexibility of the generative model. This novel methodology effectively bridges the gap between generative models and discriminative tasks, providing a fresh perspective for constructing unified visual understanding systems. Systematic experiments demonstrate that GenDet achieves competitive accuracy compared to discriminative detectors, while retaining the flexibility characteristic of generative methods.
Published: 2026-01-12T07:29:59+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Min; Chengyang Li; Fanjie Kong; Qi Zhu; Dawei Zhao; Liang Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;This paper presents GenDet, a novel framework that redefines object detection as an image generation task. In contrast to traditional approaches, GenDet adopts a pioneering approach by leveraging generative modeling: it conditions on the input image and directly generates bounding boxes with semantic annotations in the original image space. GenDet establishes a conditional generation architecture built upon the large-scale pre-trained Stable Diffusion model, formulating the detection task as semantic constraints within the latent space. It enables precise control over bounding box positions and category attributes, while preserving the flexibility of the generative model. This novel methodology effectively bridges the gap between generative models and discriminative tasks, providing a fresh perspective for constructing unified visual understanding systems. Systematic experiments demonstrate that GenDet achieves competitive accuracy compared to discriminative detectors, while retaining the flexibility characteristic of generative methods.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Saliency based Contextual Metric learning for Few-shot Open-set Recognition</title><link>https://doi.org/10.1016/j.patcog.2026.113096</link><guid>10.1016/j.patcog.2026.113096</guid><pubDate>Wed, 14 Jan 2026 16:51:29 +0000</pubDate><dc:creator>Ping Li</dc:creator><dc:creator>Jiajun Chen</dc:creator><dc:creator>Lijie Shang</dc:creator><dc:creator>Chenhao Ping</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113096</prism:doi><description>Few-Shot Open-set Recognition (FSOR) aims to recognize the samples from known classes while rejecting those from unknown (unseen) classes. It faces two primary challenges, including the dynamic changing of decision boundary for known classes due to varying episodes (tasks), and the discriminative ambiguity of visually-similar samples between known and unknown classes, which are not well addressed by previous methods. This inspires us to propose an Adaptive Saliency based Contextual Metric learning framework, termed ASCM . This framework consists of two main components, i.e., adaptive saliency fusion module, and contextual metric learning module. The former adaptively models the importance of spatial saliency features, which are indexed by most relevant spatial positions of feature map to the known classes. Also, the former adopts an adaptive saliency fusion strategy to dynamically calibrate class prototypes, by leveraging the global semantic similarity of different classes to adjust the spatial saliency feature by weighting. Meanwhile, the latter captures contextual similarity relation among neighbor embedding features by considering both shared and non-shared neighbors between query sample and class prototypes in terms of contextual metric. This alleviates the confusion problem of samples with similar appearance, because it also considers other dissimilar samples in the neighborhood. Extensive experiments on four benchmarks, i.e., mini-ImageNet, tiered-ImageNet, CIFAR-FS, and FC100, validate the advantage of the proposed approach. Our code is available at https://github.com/mlvccn/ASCM_FewshotOpenset .
Published: 2026-01-14T16:51:29+00:00
Venue: Pattern Recognition
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ping Li; Jiajun Chen; Lijie Shang; Chenhao Ping&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113096"&gt;10.1016/j.patcog.2026.113096&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Few-Shot Open-set Recognition (FSOR) aims to recognize the samples from known classes while rejecting those from unknown (unseen) classes. It faces two primary challenges, including the dynamic changing of decision boundary for known classes due to varying episodes (tasks), and the discriminative ambiguity of visually-similar samples between known and unknown classes, which are not well addressed by previous methods. This inspires us to propose an Adaptive Saliency based Contextual Metric learning framework, termed ASCM . This framework consists of two main components, i.e., adaptive saliency fusion module, and contextual metric learning module. The former adaptively models the importance of spatial saliency features, which are indexed by most relevant spatial positions of feature map to the known classes. Also, the former adopts an adaptive saliency fusion strategy to dynamically calibrate class prototypes, by leveraging the global semantic similarity of different classes to adjust the spatial saliency feature by weighting. Meanwhile, the latter captures contextual similarity relation among neighbor embedding features by considering both shared and non-shared neighbors between query sample and class prototypes in terms of contextual metric. This alleviates the confusion problem of samples with similar appearance, because it also considers other dissimilar samples in the neighborhood. Extensive experiments on four benchmarks, i.e., mini-ImageNet, tiered-ImageNet, CIFAR-FS, and FC100, validate the advantage of the proposed approach. Our code is available at https://github.com/mlvccn/ASCM_FewshotOpenset .&lt;/p&gt;</content:encoded></item></channel></rss>