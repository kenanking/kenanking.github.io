<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 25 Dec 2025 02:44:37 +0000</lastBuildDate><item><title>Toward Unified Expertise: Learning a Single Vision Model from Diverse Perception</title><link>https://doi.org/10.1109/tpami.2025.3647880</link><guid>10.1109/tpami.2025.3647880</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Zitian Chen</dc:creator><dc:creator>Mingyu Ding</dc:creator><dc:creator>Yikang Shen</dc:creator><dc:creator>Erik Learned-Miller</dc:creator><dc:creator>Chuang Gan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647880</prism:doi><description>Multi-task learning (MTL) presents greater optimization challenges than single-task learning (STL) due to conflicting gradients across tasks. While parameter sharing promotes cooperation among related tasks, many tasks require specialized representations. To balance cooperation and specialization, we propose Mod-Squad [1], a modular transformer-based model composed of a “squad” of experts. Each task activates a sparse subset of experts through a differentiable matching process, guided by a novel mutual information-based loss. This modular structure avoids full backbone sharing and scales effectively with the number of tasks and dataset size. In this extended version, we generalize Mod-Squad to support multi-dataset pre-training, enabling joint learning across disjoint, single-task datasets (e.g., ImageNet, COCO, ADE20K). This is achieved via a new formulation of the mutual information loss that unifies learning across heterogeneous sources. More importantly, while most prior work in large models has focused on efficiency, few have explored adjustable efficiency. In this study, we further evaluate the model's generalization to downstream tasks and introduce a set of efficient adaptation techniques that leverage Mod-Squad's modularity for flexible finetuning-enabling dynamic adjustment of model size, parameter count, and computational cost. Additionally, we present a hybrid adaptation scheme that combines these techniques to achieve favorable performance-efficiency trade-offs. In summary, Mod-Squad provides a robust foundation for sparse modular models that can learn from diverse supervision and datasets. Its emergent modularity enables strong generalization, decomposition into high-performing components, and rapid, resource-efficient adaptation for downstream applications.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.836 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zitian Chen; Mingyu Ding; Yikang Shen; Erik Learned-Miller; Chuang Gan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647880"&gt;10.1109/tpami.2025.3647880&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.836 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-task learning (MTL) presents greater optimization challenges than single-task learning (STL) due to conflicting gradients across tasks. While parameter sharing promotes cooperation among related tasks, many tasks require specialized representations. To balance cooperation and specialization, we propose Mod-Squad [1], a modular transformer-based model composed of a “squad” of experts. Each task activates a sparse subset of experts through a differentiable matching process, guided by a novel mutual information-based loss. This modular structure avoids full backbone sharing and scales effectively with the number of tasks and dataset size. In this extended version, we generalize Mod-Squad to support multi-dataset pre-training, enabling joint learning across disjoint, single-task datasets (e.g., ImageNet, COCO, ADE20K). This is achieved via a new formulation of the mutual information loss that unifies learning across heterogeneous sources. More importantly, while most prior work in large models has focused on efficiency, few have explored adjustable efficiency. In this study, we further evaluate the model&amp;#x27;s generalization to downstream tasks and introduce a set of efficient adaptation techniques that leverage Mod-Squad&amp;#x27;s modularity for flexible finetuning-enabling dynamic adjustment of model size, parameter count, and computational cost. Additionally, we present a hybrid adaptation scheme that combines these techniques to achieve favorable performance-efficiency trade-offs. In summary, Mod-Squad provides a robust foundation for sparse modular models that can learn from diverse supervision and datasets. Its emergent modularity enables strong generalization, decomposition into high-performing components, and rapid, resource-efficient adaptation for downstream applications.&lt;/p&gt;</content:encoded></item><item><title>You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts</title><link>https://doi.org/10.1109/tpami.2025.3647857</link><guid>10.1109/tpami.2025.3647857</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Hongkun Dou</dc:creator><dc:creator>Zeyu Li</dc:creator><dc:creator>Xingyu Jiang</dc:creator><dc:creator>Hongjue Li</dc:creator><dc:creator>Lijun Yang</dc:creator><dc:creator>Wen Yao</dc:creator><dc:creator>Yue Deng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647857</prism:doi><description>Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by \sim 90\% \sim 90\% while maintaining superior performance.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.832 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongkun Dou; Zeyu Li; Xingyu Jiang; Hongjue Li; Lijun Yang; Wen Yao; Yue Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647857"&gt;10.1109/tpami.2025.3647857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.832 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by \sim 90\% \sim 90\% while maintaining superior performance.&lt;/p&gt;</content:encoded></item><item><title>Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking</title><link>https://doi.org/10.1109/tpami.2025.3648020</link><guid>10.1109/tpami.2025.3648020</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Hanzheng Wang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Xiang-Gen Xia</dc:creator><dc:creator>Qian Du</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648020</prism:doi><description>Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model's sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanzheng Wang; Wei Li; Xiang-Gen Xia; Qian Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648020"&gt;10.1109/tpami.2025.3648020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model&amp;#x27;s sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.&lt;/p&gt;</content:encoded></item><item><title>Vision-Language Models for Person Re-identification: A Survey and Outlook</title><link>https://doi.org/10.1016/j.inffus.2025.104095</link><guid>10.1016/j.inffus.2025.104095</guid><pubDate>Wed, 24 Dec 2025 16:39:57 +0000</pubDate><dc:creator>Guorong Lin</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:creator>Zuoyong Li</dc:creator><dc:creator>Yao Lu</dc:creator><dc:creator>Xiaowen Ma</dc:creator><dc:creator>Zhenhua Huang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104095</prism:doi><description>Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.
Published: 2025-12-24T16:39:57+00:00
Venue: Information Fusion
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guorong Lin; Wei-Shi Zheng; Zuoyong Li; Yao Lu; Xiaowen Ma; Zhenhua Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104095"&gt;10.1016/j.inffus.2025.104095&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.&lt;/p&gt;</content:encoded></item><item><title>E
                    &lt;sup&gt;2&lt;/sup&gt;
                    MPL: An Enduring and Efficient Meta Prompt Learning Framework for Few-shot Unsupervised Domain Adaptation</title><link>https://doi.org/10.1109/tip.2025.3645560</link><guid>10.1109/tip.2025.3645560</guid><pubDate>Tue, 23 Dec 2025 18:32:49 +0000</pubDate><dc:creator>Wanqi Yang</dc:creator><dc:creator>Haoran Wang</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Lei Wang</dc:creator><dc:creator>Ge Song</dc:creator><dc:creator>Ming Yang</dc:creator><dc:creator>Yang Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3645560</prism:doi><description>Few-shot unsupervised domain adaptation (FS-UDA) leverages a limited amount of labeled data from a source domain to enable accurate classification in an unlabeled target domain. Despite recent advancements, current approaches of FS-UDA continue to confront a major challenge: models often demonstrate instability when adapted to new FS-UDA tasks and necessitate considerable time investment. To address these challenges, we put forward a novel framework called Enduring and Efficient Meta-Prompt Learning (E2MPL) for FS-UDA. Within this framework, we utilize the pre-trained CLIP model as the backbone of feature learning. Firstly, we design domain-shared prompts, consisting of virtual tokens, which primarily capture meta-knowledge from a wide range of meta-tasks to mitigate the domain gaps. Secondly, we develop a task prompt learning network that adaptively learns task-specific prompts with the goal of achieving fast and stable task generalization. Thirdly, we formulate the meta-prompt learning process as a bilevel optimization problem, consisting of (outer) meta-prompt learner and (inner) task-specific classifier and domain adapter. Also, the inner objective of each meta-task has the closed-form solution, which enables efficient prompt learning and adaptation to new tasks in a single step. Extensive experimental studies demonstrate the promising performance of our framework in a domain adaptation benchmark dataset DomainNet. Compared with state-of-the-art methods, our approach has improved the average accuracy by at least 15 percentage points and reduces the average time by 64.67% in the 5-way 1-shot task; in the 5-way 5-shot task, it achieves at least a 9-percentage-point improvement in average accuracy and reduces the average time by 63.18%. Moreover, our method exhibits more enduring and stable performance than the other methods, i.e., reducing the average IQR value by over 40.80% and 25.35% in the 5-way 1-shot and 5-shot task, respectively.
Published: 2025-12-23T18:32:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wanqi Yang; Haoran Wang; Wei Wang; Lei Wang; Ge Song; Ming Yang; Yang Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3645560"&gt;10.1109/tip.2025.3645560&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot unsupervised domain adaptation (FS-UDA) leverages a limited amount of labeled data from a source domain to enable accurate classification in an unlabeled target domain. Despite recent advancements, current approaches of FS-UDA continue to confront a major challenge: models often demonstrate instability when adapted to new FS-UDA tasks and necessitate considerable time investment. To address these challenges, we put forward a novel framework called Enduring and Efficient Meta-Prompt Learning (E2MPL) for FS-UDA. Within this framework, we utilize the pre-trained CLIP model as the backbone of feature learning. Firstly, we design domain-shared prompts, consisting of virtual tokens, which primarily capture meta-knowledge from a wide range of meta-tasks to mitigate the domain gaps. Secondly, we develop a task prompt learning network that adaptively learns task-specific prompts with the goal of achieving fast and stable task generalization. Thirdly, we formulate the meta-prompt learning process as a bilevel optimization problem, consisting of (outer) meta-prompt learner and (inner) task-specific classifier and domain adapter. Also, the inner objective of each meta-task has the closed-form solution, which enables efficient prompt learning and adaptation to new tasks in a single step. Extensive experimental studies demonstrate the promising performance of our framework in a domain adaptation benchmark dataset DomainNet. Compared with state-of-the-art methods, our approach has improved the average accuracy by at least 15 percentage points and reduces the average time by 64.67% in the 5-way 1-shot task; in the 5-way 5-shot task, it achieves at least a 9-percentage-point improvement in average accuracy and reduces the average time by 63.18%. Moreover, our method exhibits more enduring and stable performance than the other methods, i.e., reducing the average IQR value by over 40.80% and 25.35% in the 5-way 1-shot and 5-shot task, respectively.&lt;/p&gt;</content:encoded></item><item><title>Edge-Semantic Synergy Network with Edge-Aware Attention for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3648149</link><guid>10.1109/tgrs.2025.3648149</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Maoyong Li</dc:creator><dc:creator>Yingying Gao</dc:creator><dc:creator>Xuedong Guo</dc:creator><dc:creator>Zhixiang Chen</dc:creator><dc:creator>Lei Deng</dc:creator><dc:creator>Mingli Dong</dc:creator><dc:creator>Lianqing Zhu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648149</prism:doi><description>Infrared small target detection (IRSTD) plays a crucial role in applications such as autonomous driving, environmental monitoring, and industrial inspection. However, the small size of targets, their blurred edges, and complex backgrounds often result in significant limitations of existing methods in terms of feature synergy and edge information utilization. To address these challenges, this paper proposes an Edge-Semantic Synergy Network with Edge-Aware Attention (ESSNet) for Infrared Small Target Detection. ESSNet substantially enhances detection performance by explicitly reinforcing edge information and optimizing multi-level feature interactions. Specifically, the Edge-Semantic Synergy Module (ESSM) leverages edge details at the lowest level and semantic information at the highest level to achieve long-range level modulation, thereby enhancing the synergy between edges and semantics. Additionally, ESSM integrates a Multi-Scale Edge-Aware Attention (MSEA), which embeds edge features into the attention mechanism through explicit edge supervision, effectively improving the accuracy of boundary detection. Furthermore, the Multi-Level Feature Fusion (MLFF) module is introduced to mitigate semantic loss during the decoding process via a layer-wise guidance mechanism, preserving the structural integrity of detected targets. Experiments conducted on the SIRST, NUDT-SIRST, and IRSTD-1K datasets demonstrate that ESSNet significantly outperforms existing methods on key metrics, achieving state-of-the-art performance.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maoyong Li; Yingying Gao; Xuedong Guo; Zhixiang Chen; Lei Deng; Mingli Dong; Lianqing Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648149"&gt;10.1109/tgrs.2025.3648149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) plays a crucial role in applications such as autonomous driving, environmental monitoring, and industrial inspection. However, the small size of targets, their blurred edges, and complex backgrounds often result in significant limitations of existing methods in terms of feature synergy and edge information utilization. To address these challenges, this paper proposes an Edge-Semantic Synergy Network with Edge-Aware Attention (ESSNet) for Infrared Small Target Detection. ESSNet substantially enhances detection performance by explicitly reinforcing edge information and optimizing multi-level feature interactions. Specifically, the Edge-Semantic Synergy Module (ESSM) leverages edge details at the lowest level and semantic information at the highest level to achieve long-range level modulation, thereby enhancing the synergy between edges and semantics. Additionally, ESSM integrates a Multi-Scale Edge-Aware Attention (MSEA), which embeds edge features into the attention mechanism through explicit edge supervision, effectively improving the accuracy of boundary detection. Furthermore, the Multi-Level Feature Fusion (MLFF) module is introduced to mitigate semantic loss during the decoding process via a layer-wise guidance mechanism, preserving the structural integrity of detected targets. Experiments conducted on the SIRST, NUDT-SIRST, and IRSTD-1K datasets demonstrate that ESSNet significantly outperforms existing methods on key metrics, achieving state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Forget Me Not: Fighting Local Overfitting With Knowledge Fusion and Distillation</title><link>https://doi.org/10.1109/tpami.2025.3647862</link><guid>10.1109/tpami.2025.3647862</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Uri Stern</dc:creator><dc:creator>Eli Corn</dc:creator><dc:creator>Daphna Weinshall</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647862</prism:doi><description>Overfitting in deep neural networks occurs less frequently than expected. This is a puzzling observation, as theory predicts that greater model capacity should eventually lead to overfitting – yet this is rarely seen in practice. But what if overfitting does occur, not globally, but in specific sub-regions of the data space? In this work, we introduce a novel score that measures the forgetting rate of deep models on validation data, capturing what we term local overfitting: a performance degradation confined to certain regions of the input space. We demonstrate that local overfitting can arise even without conventional overfitting, and is closely linked to the double descent phenomenon. Building on these insights, we introduce a two-stage approach that leverages the training history of a single model to recover and retain forgotten knowledge: first, by aggregating checkpoints into an ensemble, and then by distilling it into a single model of the original size, thus enhancing performance without added inference cost. Extensive experiments across multiple datasets, modern architectures, and training regimes validate the effectiveness of our approach. Notably, in the presence of label noise, our method – Knowledge Fusion followed by Knowledge Distillation – outperforms both the original model and independently trained ensembles, achieving a rare win-win scenario: reduced training and inference complexity.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Uri Stern; Eli Corn; Daphna Weinshall&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647862"&gt;10.1109/tpami.2025.3647862&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Overfitting in deep neural networks occurs less frequently than expected. This is a puzzling observation, as theory predicts that greater model capacity should eventually lead to overfitting – yet this is rarely seen in practice. But what if overfitting does occur, not globally, but in specific sub-regions of the data space? In this work, we introduce a novel score that measures the forgetting rate of deep models on validation data, capturing what we term local overfitting: a performance degradation confined to certain regions of the input space. We demonstrate that local overfitting can arise even without conventional overfitting, and is closely linked to the double descent phenomenon. Building on these insights, we introduce a two-stage approach that leverages the training history of a single model to recover and retain forgotten knowledge: first, by aggregating checkpoints into an ensemble, and then by distilling it into a single model of the original size, thus enhancing performance without added inference cost. Extensive experiments across multiple datasets, modern architectures, and training regimes validate the effectiveness of our approach. Notably, in the presence of label noise, our method – Knowledge Fusion followed by Knowledge Distillation – outperforms both the original model and independently trained ensembles, achieving a rare win-win scenario: reduced training and inference complexity.&lt;/p&gt;</content:encoded></item><item><title>Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation</title><link>https://doi.org/10.1109/tpami.2025.3647855</link><guid>10.1109/tpami.2025.3647855</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Jingtao Sun</dc:creator><dc:creator>Yaonan Wang</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Chao Ding</dc:creator><dc:creator>Mike Zheng Shou</dc:creator><dc:creator>Ajmal Mian</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647855</prism:doi><description>Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingtao Sun; Yaonan Wang; Mingtao Feng; Chao Ding; Mike Zheng Shou; Ajmal Mian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647855"&gt;10.1109/tpami.2025.3647855&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.&lt;/p&gt;</content:encoded></item><item><title>FRFSL: Feature Reconstruction based Cross-Domain Few-Shot Learning for Coastal Wetland Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tip.2025.3646073</link><guid>10.1109/tip.2025.3646073</guid><pubDate>Wed, 24 Dec 2025 18:48:18 +0000</pubDate><dc:creator>Qixing Yu</dc:creator><dc:creator>Zhongwei Li</dc:creator><dc:creator>Ziqi Xin</dc:creator><dc:creator>Fangming Guo</dc:creator><dc:creator>Guangbo Ren</dc:creator><dc:creator>Jianbu Wang</dc:creator><dc:creator>Zhenggang Bi</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646073</prism:doi><description>Hyperspectral image classification (HSIC) is a valuable method for identifying coastal wetland vegetation, but challenges like environmental complexity and difficulty in distinguishing land cover types make large-scale labeling difficult. Cross-domain few-shot learning (CDFSL) offers a potential solution to limited labeling. Existing CDFSL HSIC methods have made significant progress, but still face challenges like prototype deviation, covariate shifts, and rely on complex domain alignment (DA) methods. To address these issues, a feature reconstruction-based CDFSL (FRFSL) algorithm is proposed. Within FRFSL, a Prototype Calibration Module (PCM) is designed for the prototype deviation, which employs a Bayesian inference-enhanced Gaussian Mixture Model to select reliable query features for prototype reconstruction, aligning the prototypes more closely with the actual distribution. Additionally, a ridge regression closed-form solution is incorporated into the Distance Metric Module (DMM), employing a projection matrix for prototype reconstruction to mitigate covariate shifts between the support and query sets. Features from both source and target domains are reconstructed into dynamic graphs, transforming DA into a graph matching problem guided by optimal transport theory. A novel shared transport matrix implementation algorithm is developed to achieve lightweight and interpretable alignment. Extensive experiments on three self-constructed coastal wetland datasets and one public dataset show that FRFSL outperforms eleven state-of-the-art algorithms. The code will be available at https://github.com/Yqx-ACE/TIP_2025_FRFSL.
Published: 2025-12-24T18:48:18+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qixing Yu; Zhongwei Li; Ziqi Xin; Fangming Guo; Guangbo Ren; Jianbu Wang; Zhenggang Bi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646073"&gt;10.1109/tip.2025.3646073&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral image classification (HSIC) is a valuable method for identifying coastal wetland vegetation, but challenges like environmental complexity and difficulty in distinguishing land cover types make large-scale labeling difficult. Cross-domain few-shot learning (CDFSL) offers a potential solution to limited labeling. Existing CDFSL HSIC methods have made significant progress, but still face challenges like prototype deviation, covariate shifts, and rely on complex domain alignment (DA) methods. To address these issues, a feature reconstruction-based CDFSL (FRFSL) algorithm is proposed. Within FRFSL, a Prototype Calibration Module (PCM) is designed for the prototype deviation, which employs a Bayesian inference-enhanced Gaussian Mixture Model to select reliable query features for prototype reconstruction, aligning the prototypes more closely with the actual distribution. Additionally, a ridge regression closed-form solution is incorporated into the Distance Metric Module (DMM), employing a projection matrix for prototype reconstruction to mitigate covariate shifts between the support and query sets. Features from both source and target domains are reconstructed into dynamic graphs, transforming DA into a graph matching problem guided by optimal transport theory. A novel shared transport matrix implementation algorithm is developed to achieve lightweight and interpretable alignment. Extensive experiments on three self-constructed coastal wetland datasets and one public dataset show that FRFSL outperforms eleven state-of-the-art algorithms. The code will be available at https://github.com/Yqx-ACE/TIP_2025_FRFSL.&lt;/p&gt;</content:encoded></item><item><title>LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</title><link>https://arxiv.org/abs/2512.20217v1</link><guid>http://arxiv.org/abs/2512.20217v1</guid><pubDate>Tue, 23 Dec 2025 10:16:33 +0000</pubDate><dc:creator>Xiangxuan Ren</dc:creator><dc:creator>Zhongdao Wang</dc:creator><dc:creator>Pin Tang</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Jilai Zheng</dc:creator><dc:creator>Chao Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.
Published: 2025-12-23T10:16:33+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangxuan Ren; Zhongdao Wang; Pin Tang; Guoqing Wang; Jilai Zheng; Chao Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.&lt;/p&gt;</content:encoded></item><item><title>Elaborate Feature Decoupling for Weakly Supervised Fine-Grained Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3647662</link><guid>10.1109/tgrs.2025.3647662</guid><pubDate>Tue, 23 Dec 2025 18:31:04 +0000</pubDate><dc:creator>Xi Yang</dc:creator><dc:creator>Zhongyuan Zhou</dc:creator><dc:creator>Dong Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647662</prism:doi><description>Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.
Published: 2025-12-23T18:31:04+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xi Yang; Zhongyuan Zhou; Dong Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647662"&gt;10.1109/tgrs.2025.3647662&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.&lt;/p&gt;</content:encoded></item><item><title>Unlocking Cross-Domain Synergies for Domain Adaptive Semantic Segmentation</title><link>https://doi.org/10.1109/tip.2025.3645599</link><guid>10.1109/tip.2025.3645599</guid><pubDate>Tue, 23 Dec 2025 18:32:49 +0000</pubDate><dc:creator>Qin Xu</dc:creator><dc:creator>Qihang Wu</dc:creator><dc:creator>Bo Jiang</dc:creator><dc:creator>Jiahui Wang</dc:creator><dc:creator>Yuan Chen</dc:creator><dc:creator>Jinhui Tang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3645599</prism:doi><description>Unsupervised domain adaptation semantic segmentation (UDASS) aims to perform dense prediction on the unlabeled target domain by training the model on a labeled source domain. In this field, self-training approaches have demonstrated strong competitiveness and advantages. However, existing methods often rely on additional training data (such as reference datasets or depth maps) to rectify the unreliable pseudo-labels, ignoring the cross-domain interaction between the target and source domains. To address this issue, in this paper, we propose a novel method for unsupervised domain adaptation semantic segmentation, termed Unlocking Cross-Domain Synergies (UCDS). Specifically, in the UCDS network, we design a new Dynamic Self-Correction (DSC) module that effectively transfers source domain knowledge and generates high-confidence pseudolabels without additional training resources. Unlike the existing methods, DSC proposes a Dynamic Noisy Label Detection method for the target domain. To correct the noisy pseudo-labels, we design a Dual Bank mechanism that explores the reliable and unreliable predictions of the source domain, and conducts cross-domain synergy through Weighted Reassignment Self-Correction and Negative Correction Prevention strategies. To enhance the discriminative ability of features and amplify the dissimilarity of different categories, we propose Discrepancy-based Contrastive Learning (DCL). The DCL selects positive and negative samples in the source and target domains based on the semantic discrepancies among different categories, effectively avoiding the numerous false negative samples found in existing methods. Extensive experimental results on three commonly used datasets demonstrate the superiority of the proposed UCDS in comparison with the state-of-the-art methods. The project and code are available at https://github.com/wqh011128/UCDS.
Published: 2025-12-23T18:32:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qin Xu; Qihang Wu; Bo Jiang; Jiahui Wang; Yuan Chen; Jinhui Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3645599"&gt;10.1109/tip.2025.3645599&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation semantic segmentation (UDASS) aims to perform dense prediction on the unlabeled target domain by training the model on a labeled source domain. In this field, self-training approaches have demonstrated strong competitiveness and advantages. However, existing methods often rely on additional training data (such as reference datasets or depth maps) to rectify the unreliable pseudo-labels, ignoring the cross-domain interaction between the target and source domains. To address this issue, in this paper, we propose a novel method for unsupervised domain adaptation semantic segmentation, termed Unlocking Cross-Domain Synergies (UCDS). Specifically, in the UCDS network, we design a new Dynamic Self-Correction (DSC) module that effectively transfers source domain knowledge and generates high-confidence pseudolabels without additional training resources. Unlike the existing methods, DSC proposes a Dynamic Noisy Label Detection method for the target domain. To correct the noisy pseudo-labels, we design a Dual Bank mechanism that explores the reliable and unreliable predictions of the source domain, and conducts cross-domain synergy through Weighted Reassignment Self-Correction and Negative Correction Prevention strategies. To enhance the discriminative ability of features and amplify the dissimilarity of different categories, we propose Discrepancy-based Contrastive Learning (DCL). The DCL selects positive and negative samples in the source and target domains based on the semantic discrepancies among different categories, effectively avoiding the numerous false negative samples found in existing methods. Extensive experimental results on three commonly used datasets demonstrate the superiority of the proposed UCDS in comparison with the state-of-the-art methods. The project and code are available at https://github.com/wqh011128/UCDS.&lt;/p&gt;</content:encoded></item><item><title>TFST: Two-Frame Ship Tracking for SAR Using YOLOv12 and Feature-Based Matching</title><link>https://doi.org/10.1109/jstars.2025.3647680</link><guid>10.1109/jstars.2025.3647680</guid><pubDate>Wed, 24 Dec 2025 18:46:08 +0000</pubDate><dc:creator>Muhammad Yasir</dc:creator><dc:creator>Shanwei Liu</dc:creator><dc:creator>Mingming Xu</dc:creator><dc:creator>Fernando J. Aguilar</dc:creator><dc:creator>Jianhua Wan</dc:creator><dc:creator>Shiqing Wei</dc:creator><dc:creator>Saied Pirasteh</dc:creator><dc:creator>Hong Fan</dc:creator><dc:creator>Qamar Ul Islam</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647680</prism:doi><description>Tracking objects in SAR imagery is critical for maritime surveillance, traffic monitoring, and security applications, but remains a major challenge due to speckle noise, sea clutter, and limited temporal continuity. Most existing tracking-by-detection methods process frames independently, often resulting in weak associations and frequent identity switches. To overcome these limitations, we propose TFST, a two-frame SAR ship tracking framework that integrates detection, feature encoding, and optimal assignment. In this way, the goal of this work is to address the current gaps in SAR ship tracking by strengthening cross-frame partnerships and reducing identity switches through an integrated two-frame tracking framework. In our approach, a deep detector first processes consecutive frames to generate candidate bounding boxes. A lightweight feature extractor encodes both appearance and structural cues, while a matching module constructs a cost matrix that combines feature similarity and positional consistency. Gating is applied to remove infeasible associations, and the Hungarian algorithm is employed to achieve a globally optimal assignment. Quantitative evaluations performed on three widely known and publicly available SAR-Ship datasets (SSTD, SSDD, and SAR-Ship) further highlight the advantages of TFST. In terms of ship detection performance, TFST achieved an average mAP@50 improvement of 2.2% over the YOLOv12 baseline model on all three tested datasets. Regarding tracking results, the superiority of TFST over state-of-the-art multi-object trackers becomes even more evident. In fact, the proposed model achieved the highest MOTA accuracy (86.9%) and the best IDF1 score (82.7%), thus outperforming strong baselines such as Siam-SORT (82.1% MOTA, 79.8% IDF1) and TrackFormer (80.7% MOTA, 78.7% IDF1). In conclusion, TFST demonstrated improved robustness, fewer ID switches, and higher tracking accuracy compared to baseline methods, underscoring its effectiveness in complex m...
Published: 2025-12-24T18:46:08+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Muhammad Yasir; Shanwei Liu; Mingming Xu; Fernando J. Aguilar; Jianhua Wan; Shiqing Wei; Saied Pirasteh; Hong Fan; Qamar Ul Islam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647680"&gt;10.1109/jstars.2025.3647680&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Tracking objects in SAR imagery is critical for maritime surveillance, traffic monitoring, and security applications, but remains a major challenge due to speckle noise, sea clutter, and limited temporal continuity. Most existing tracking-by-detection methods process frames independently, often resulting in weak associations and frequent identity switches. To overcome these limitations, we propose TFST, a two-frame SAR ship tracking framework that integrates detection, feature encoding, and optimal assignment. In this way, the goal of this work is to address the current gaps in SAR ship tracking by strengthening cross-frame partnerships and reducing identity switches through an integrated two-frame tracking framework. In our approach, a deep detector first processes consecutive frames to generate candidate bounding boxes. A lightweight feature extractor encodes both appearance and structural cues, while a matching module constructs a cost matrix that combines feature similarity and positional consistency. Gating is applied to remove infeasible associations, and the Hungarian algorithm is employed to achieve a globally optimal assignment. Quantitative evaluations performed on three widely known and publicly available SAR-Ship datasets (SSTD, SSDD, and SAR-Ship) further highlight the advantages of TFST. In terms of ship detection performance, TFST achieved an average mAP@50 improvement of 2.2% over the YOLOv12 baseline model on all three tested datasets. Regarding tracking results, the superiority of TFST over state-of-the-art multi-object trackers becomes even more evident. In fact, the proposed model achieved the highest MOTA accuracy (86.9%) and the best IDF1 score (82.7%), thus outperforming strong baselines such as Siam-SORT (82.1% MOTA, 79.8% IDF1) and TrackFormer (80.7% MOTA, 78.7% IDF1). In conclusion, TFST demonstrated improved robustness, fewer ID switches, and higher tracking accuracy compared to baseline methods, underscoring its effectiveness in complex m...&lt;/p&gt;</content:encoded></item><item><title>HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving</title><link>https://doi.org/10.1109/tpami.2025.3647952</link><guid>10.1109/tpami.2025.3647952</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Hongyu Zhou</dc:creator><dc:creator>Longzhong Lin</dc:creator><dc:creator>Jiabao Wang</dc:creator><dc:creator>Yichong Lu</dc:creator><dc:creator>Dongfeng Bai</dc:creator><dc:creator>Bingbing Liu</dc:creator><dc:creator>Yue Wang</dc:creator><dc:creator>Andreas Geiger</dc:creator><dc:creator>Yiyi Liao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647952</prism:doi><description>In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, we tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongyu Zhou; Longzhong Lin; Jiabao Wang; Yichong Lu; Dongfeng Bai; Bingbing Liu; Yue Wang; Andreas Geiger; Yiyi Liao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647952"&gt;10.1109/tpami.2025.3647952&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, we tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.&lt;/p&gt;</content:encoded></item><item><title>RSS-Net: A Mamba-Based Network for SAR Image Denoising</title><link>https://doi.org/10.1109/jstars.2025.3647971</link><guid>10.1109/jstars.2025.3647971</guid><pubDate>Wed, 24 Dec 2025 18:46:08 +0000</pubDate><dc:creator>Min Huang</dc:creator><dc:creator>Yunzhao Yang</dc:creator><dc:creator>Qiuhong Sun</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647971</prism:doi><description>Synthetic Aperture Radar (SAR), with its outstanding features, has developed into a vital technical method for the global detection of maritime and terrestrial targets. However, the unique imaging mechanism intrinsic to SAR inherently produces coherent speckle noise. This noise significantly deteriorates the quality of the image, thereby adversely affecting downstream applications like target recognition and classification. Therefore, research on SAR image denoising holds significant practical and theoretical value. However, due to the limitations of the local receptive field of CNN, it is difficult for it to capture global spatial features. Although Transformer can achieve global spatial modeling, its quadratic complexity results in a large amount of computational overhead. To tackle these issues, this paper introduces RSS-Net, a new denoising network for SAR images, built on an encoder-decoder architecture. RSS-Net demonstrates significant improvements in SAR image denoising performance, due to its ability to extract multi-scale feature information. The network incorporates the Residue State-Space Block (RSSB), which fuses Mamba's Vision State Space Module (VSSM) and a CNN-based Channel Attention Block (CAB). VSSM leverages 2D-Selective Scan (SS2D) better acquires spatial information features in SAR images by scanning in four directions. RSSB efficiently merges Mamba's ability to model long-range dependencies and conventional convolution's strengths in extracting local features. This integration effectively alleviates the common detail blurring problem in existing SAR denoising methods and strengthens the restoration of high-frequency image details. The novel application of the Mamba to SAR image denoising enables the proposed method to attain both long-range contextual dependency modeling and linear computational cost, thus resolving the balance between global modeling capacity and efficiency in computation, while also pointing to new avenues for future research....
Published: 2025-12-24T18:46:08+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Min Huang; Yunzhao Yang; Qiuhong Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647971"&gt;10.1109/jstars.2025.3647971&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR), with its outstanding features, has developed into a vital technical method for the global detection of maritime and terrestrial targets. However, the unique imaging mechanism intrinsic to SAR inherently produces coherent speckle noise. This noise significantly deteriorates the quality of the image, thereby adversely affecting downstream applications like target recognition and classification. Therefore, research on SAR image denoising holds significant practical and theoretical value. However, due to the limitations of the local receptive field of CNN, it is difficult for it to capture global spatial features. Although Transformer can achieve global spatial modeling, its quadratic complexity results in a large amount of computational overhead. To tackle these issues, this paper introduces RSS-Net, a new denoising network for SAR images, built on an encoder-decoder architecture. RSS-Net demonstrates significant improvements in SAR image denoising performance, due to its ability to extract multi-scale feature information. The network incorporates the Residue State-Space Block (RSSB), which fuses Mamba&amp;#x27;s Vision State Space Module (VSSM) and a CNN-based Channel Attention Block (CAB). VSSM leverages 2D-Selective Scan (SS2D) better acquires spatial information features in SAR images by scanning in four directions. RSSB efficiently merges Mamba&amp;#x27;s ability to model long-range dependencies and conventional convolution&amp;#x27;s strengths in extracting local features. This integration effectively alleviates the common detail blurring problem in existing SAR denoising methods and strengthens the restoration of high-frequency image details. The novel application of the Mamba to SAR image denoising enables the proposed method to attain both long-range contextual dependency modeling and linear computational cost, thus resolving the balance between global modeling capacity and efficiency in computation, while also pointing to new avenues for future research....&lt;/p&gt;</content:encoded></item><item><title>BEVTrack: Multi-View Multi-Human Registration and Tracking in the Bird's Eye View</title><link>https://doi.org/10.1109/tpami.2025.3647707</link><guid>10.1109/tpami.2025.3647707</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Zekun Qian</dc:creator><dc:creator>Wei Feng</dc:creator><dc:creator>Feifan Wang</dc:creator><dc:creator>Ruize Han</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647707</prism:doi><description>We handle a new problem of multi-view multi-human tracking in the bird's eye view (BEV). Different from previous works, we require neither the calibration among the multi-view cameras nor the actually captured BEV video. This makes the studied problem closer to real-world applications, however, more challenging. For this purpose, in this work, we propose a novel BEVTrack scheme. Specifically, given multi-view videos, we first use a virtual BEV transform module to obtain the BEV for each view. Then, we propose a unified BEV alignment module to fuse the respectively generated BEVs, in which we specifically design the self-supervised losses by considering both the spatial consistency and the temporal continuity. During the inference, we design the camera-subject collaborative registration and tracking strategy to make use of the mutual dependence between the multi-view cameras and the multiple targets, to achieve the desired BEV tracking. We also build a new benchmark for training and evaluation, the experimental results on which have verified the rationality of the problem and the effectiveness of our method.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zekun Qian; Wei Feng; Feifan Wang; Ruize Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647707"&gt;10.1109/tpami.2025.3647707&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;We handle a new problem of multi-view multi-human tracking in the bird&amp;#x27;s eye view (BEV). Different from previous works, we require neither the calibration among the multi-view cameras nor the actually captured BEV video. This makes the studied problem closer to real-world applications, however, more challenging. For this purpose, in this work, we propose a novel BEVTrack scheme. Specifically, given multi-view videos, we first use a virtual BEV transform module to obtain the BEV for each view. Then, we propose a unified BEV alignment module to fuse the respectively generated BEVs, in which we specifically design the self-supervised losses by considering both the spatial consistency and the temporal continuity. During the inference, we design the camera-subject collaborative registration and tracking strategy to make use of the mutual dependence between the multi-view cameras and the multiple targets, to achieve the desired BEV tracking. We also build a new benchmark for training and evaluation, the experimental results on which have verified the rationality of the problem and the effectiveness of our method.&lt;/p&gt;</content:encoded></item><item><title>Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion</title><link>https://doi.org/10.1109/tip.2025.3635475</link><guid>10.1109/tip.2025.3635475</guid><pubDate>Wed, 24 Dec 2025 18:48:18 +0000</pubDate><dc:creator>Haihong Xiao</dc:creator><dc:creator>Wenxiong Kang</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Ying He</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3635475</prism:doi><description>Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.
Published: 2025-12-24T18:48:18+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haihong Xiao; Wenxiong Kang; Yulan Guo; Hao Liu; Ying He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3635475"&gt;10.1109/tip.2025.3635475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Metaformer-like Convolutional Neural Networks and Learnable Decision Fusion for SAR Ship Classification</title><link>https://doi.org/10.3390/rs18010053</link><guid>10.3390/rs18010053</guid><pubDate>Wed, 24 Dec 2025 14:27:51 +0000</pubDate><dc:creator>Shanhong Guo</dc:creator><dc:creator>Hairui Zhu</dc:creator><dc:creator>Ji Zhu</dc:creator><dc:creator>Weixing Sheng</dc:creator><dc:creator>Jiachen Tan</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010053</prism:doi><description>With the increasing number of the ocean ships, the demand for synthetic aperture radar (SAR) image ship classification has been much increased. With the development of deep learning, many neural network-based ship classification methods have been presented. However, these networks show unsatisfactory performance on low-quality SAR ship datasets. In this paper, we propose a SAR ship classification method based on dual Metaformer-like networks and learnable decision fusion, which we call LDF-D-MLCNNs. First, we design a Metaformer-like convolutional block to improve learning performance. Secondly, we implement two networks with different kernel sizes and propose the learnable decision fusion module to obtain the final prediction. Kernels of different sizes exhibit diverse extraction capabilities. Experimental results show that the accuracy of the proposed method outperforms many existing SAR ship classification networks.
Published: 2025-12-24T14:27:51+00:00
Venue: Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shanhong Guo; Hairui Zhu; Ji Zhu; Weixing Sheng; Jiachen Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010053"&gt;10.3390/rs18010053&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;With the increasing number of the ocean ships, the demand for synthetic aperture radar (SAR) image ship classification has been much increased. With the development of deep learning, many neural network-based ship classification methods have been presented. However, these networks show unsatisfactory performance on low-quality SAR ship datasets. In this paper, we propose a SAR ship classification method based on dual Metaformer-like networks and learnable decision fusion, which we call LDF-D-MLCNNs. First, we design a Metaformer-like convolutional block to improve learning performance. Secondly, we implement two networks with different kernel sizes and propose the learnable decision fusion module to obtain the final prediction. Kernels of different sizes exhibit diverse extraction capabilities. Experimental results show that the accuracy of the proposed method outperforms many existing SAR ship classification networks.&lt;/p&gt;</content:encoded></item><item><title>Learning to Reason in LLMs by Expectation Maximization</title><link>https://arxiv.org/abs/2512.20169v1</link><guid>http://arxiv.org/abs/2512.20169v1</guid><pubDate>Tue, 23 Dec 2025 08:56:49 +0000</pubDate><dc:creator>Junghyun Lee</dc:creator><dc:creator>Branislav Kveton</dc:creator><dc:creator>Sunav Choudhary</dc:creator><dc:creator>Subhojyoti Mukherjee</dc:creator><dc:creator>Anup Rao</dc:creator><dc:creator>Ryan A. Rossi</dc:creator><dc:creator>Alexa Siu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.
Published: 2025-12-23T08:56:49+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junghyun Lee; Branislav Kveton; Sunav Choudhary; Subhojyoti Mukherjee; Anup Rao; Ryan A. Rossi; Alexa Siu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.&lt;/p&gt;</content:encoded></item><item><title>MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models</title><link>https://doi.org/10.1007/s11263-025-02586-1</link><guid>10.1007/s11263-025-02586-1</guid><pubDate>Wed, 24 Dec 2025 08:52:53 +0000</pubDate><dc:creator>Yuncheng Guo</dc:creator><dc:creator>Xiaodong Gu</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02586-1</prism:doi><description>Large-scale pre-trained Vision-Language Models (VLMs) have significantly advanced transfer learning across a wide range of tasks. However, adapting these models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks. To address this challenge, we propose a novel framework, Multi-Modal Representation Learning (MMRL), which introduces a shared, learnable, and modality-agnostic representation space. Specifically, MMRL generates a set of space tokens, which are projected into both the text and image encoders as representation tokens, facilitating more effective cross-modal interactions. Unlike prior methods that primarily optimize class token features, MMRL integrates representation tokens into the higher layers of the encoders–where task-specific features are more prominent–while preserving general knowledge in the lower layers. During training, both class and representation features are jointly optimized: a trainable projection layer is applied to representation tokens for task adaptation, while the projection layer for class token remains frozen to retain pre-trained knowledge. To further promote generalization, we introduce a regularization term that aligns class and text features with the frozen VLM’s zero-shot features. During inference, we employ a decoupling strategy: both class and representation features are used for base tasks, while only class features, which are more generalizable, are utilized for novel tasks. Building upon this, we propose MMRL++, a parameter-efficient and interaction-aware extension that significantly reduces the number of trainable parameters and enhances intra-modal interactions–particularly across the layers of representation tokens–allowing gradient sharing and instance-specific information to propagate more effectively through the network. Extensive experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.
Published: 2025-12-24T08:52:53+00:00
Venue: International Journal of Computer Vision
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuncheng Guo; Xiaodong Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02586-1"&gt;10.1007/s11263-025-02586-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale pre-trained Vision-Language Models (VLMs) have significantly advanced transfer learning across a wide range of tasks. However, adapting these models with limited few-shot data often leads to overfitting, undermining their ability to generalize to new tasks. To address this challenge, we propose a novel framework, Multi-Modal Representation Learning (MMRL), which introduces a shared, learnable, and modality-agnostic representation space. Specifically, MMRL generates a set of space tokens, which are projected into both the text and image encoders as representation tokens, facilitating more effective cross-modal interactions. Unlike prior methods that primarily optimize class token features, MMRL integrates representation tokens into the higher layers of the encoders–where task-specific features are more prominent–while preserving general knowledge in the lower layers. During training, both class and representation features are jointly optimized: a trainable projection layer is applied to representation tokens for task adaptation, while the projection layer for class token remains frozen to retain pre-trained knowledge. To further promote generalization, we introduce a regularization term that aligns class and text features with the frozen VLM’s zero-shot features. During inference, we employ a decoupling strategy: both class and representation features are used for base tasks, while only class features, which are more generalizable, are utilized for novel tasks. Building upon this, we propose MMRL++, a parameter-efficient and interaction-aware extension that significantly reduces the number of trainable parameters and enhances intra-modal interactions–particularly across the layers of representation tokens–allowing gradient sharing and instance-specific information to propagate more effectively through the network. Extensive experiments on 15 datasets demonstrate that MMRL and MMRL++ consistently outperform state-of-the-art methods, achieving a strong balance between task-specific adaptation and generalization.&lt;/p&gt;</content:encoded></item><item><title>Fast SAM2 with Text-Driven Token Pruning</title><link>https://arxiv.org/abs/2512.21333v1</link><guid>http://arxiv.org/abs/2512.21333v1</guid><pubDate>Wed, 24 Dec 2025 18:59:05 +0000</pubDate><dc:creator>Avilasha Mandal</dc:creator><dc:creator>Chaoning Zhang</dc:creator><dc:creator>Fachrina Dewi Puspitasari</dc:creator><dc:creator>Xudong Wang</dc:creator><dc:creator>Jiaquan Zhang</dc:creator><dc:creator>Caiyan Qin</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Heng Tao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.
Published: 2025-12-24T18:59:05+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Avilasha Mandal; Chaoning Zhang; Fachrina Dewi Puspitasari; Xudong Wang; Jiaquan Zhang; Caiyan Qin; Guoqing Wang; Yang Yang; Heng Tao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.&lt;/p&gt;</content:encoded></item><item><title>DirMixE: Harnessing Test Agnostic Long-tail Recognition with Hierarchical Label Vartiations</title><link>https://doi.org/10.1109/tpami.2025.3647124</link><guid>10.1109/tpami.2025.3647124</guid><pubDate>Tue, 23 Dec 2025 18:31:03 +0000</pubDate><dc:creator>Zhiyong Yang</dc:creator><dc:creator>Qianqian Xu</dc:creator><dc:creator>Sicong Li</dc:creator><dc:creator>Zitai Wang</dc:creator><dc:creator>Xiaochun Cao</dc:creator><dc:creator>Qingming Huang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647124</prism:doi><description>This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced. We argue that the variation in these distributions can be broken down hierarchically into global and local levels. The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor. Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations. However, the local variations are left unconsidered. To address this issue, we propose a new MoE strategy, \mathsf {DirMixE} \mathsf {DirMixE} , which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations. Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations. This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes. Building on this idea, we develop a general Latent Skill Finetuning (LSF) framework for parameter-efficient finetuning of foundation models. We provide implementations based on LoRA and Adapter. Theoretically, we derive upper bounds on the generalization error for both standard learning and PEFT. Under mild assumptions, we show that the variance-based regularization helps tighten these bounds. Furthermore, we prove that the covering number of the PEFT hypothesis class scales with the number of trainable parameters. Finally, extensive experiments on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist validate the effectiveness of \mathsf {DirMixE} \mathsf {DirMixE} .
Published: 2025-12-23T18:31:03+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiyong Yang; Qianqian Xu; Sicong Li; Zitai Wang; Xiaochun Cao; Qingming Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647124"&gt;10.1109/tpami.2025.3647124&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced. We argue that the variation in these distributions can be broken down hierarchically into global and local levels. The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor. Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations. However, the local variations are left unconsidered. To address this issue, we propose a new MoE strategy, \mathsf {DirMixE} \mathsf {DirMixE} , which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations. Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations. This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes. Building on this idea, we develop a general Latent Skill Finetuning (LSF) framework for parameter-efficient finetuning of foundation models. We provide implementations based on LoRA and Adapter. Theoretically, we derive upper bounds on the generalization error for both standard learning and PEFT. Under mild assumptions, we show that the variance-based regularization helps tighten these bounds. Furthermore, we prove that the covering number of the PEFT hypothesis class scales with the number of trainable parameters. Finally, extensive experiments on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist validate the effectiveness of \mathsf {DirMixE} \mathsf {DirMixE} .&lt;/p&gt;</content:encoded></item><item><title>TransAUAV: A Transformer-Enhanced RGB-Infrared Fusion Network for Anti-UAV Detection</title><link>https://doi.org/10.1109/taes.2025.3646996</link><guid>10.1109/taes.2025.3646996</guid><pubDate>Tue, 23 Dec 2025 18:32:43 +0000</pubDate><dc:creator>Chenyang Li</dc:creator><dc:creator>Suiping Zhou</dc:creator><dc:creator>Zhiheng Liu</dc:creator><dc:creator>Wenjie Zhang</dc:creator><dc:creator>Ting Wu</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3646996</prism:doi><description>To improve the accuracy and robustness of anti-UAV detection, this study introduces TransAUAV, a Transformer-based RGB- infrared image fusion detection network. This approach strengthens object feature representation by leveraging multi-modal data fusion and self-attention mechanisms. Specifically, 1) we propose a multi-modal attention fusion module, which enhances the complementarity of RGB and infrared image features through a dual-path attention mechanism; 2) we propose a cross-layer multi-scale Transformer module, which improves detection performance by extracting multi-scale features and facilitating cross-modal information interaction; 3) we propose a texture information focus module, which enhances the representation of local texture details. Additionally, this paper designs a hybrid loss function to improve feature discrimination capability and training efficiency. Experimental results on anti-UAV and Drone-detection datasets show that TransAUAV outperforms state-of-the-art methods on all evaluation metrics.
Published: 2025-12-23T18:32:43+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenyang Li; Suiping Zhou; Zhiheng Liu; Wenjie Zhang; Ting Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3646996"&gt;10.1109/taes.2025.3646996&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;To improve the accuracy and robustness of anti-UAV detection, this study introduces TransAUAV, a Transformer-based RGB- infrared image fusion detection network. This approach strengthens object feature representation by leveraging multi-modal data fusion and self-attention mechanisms. Specifically, 1) we propose a multi-modal attention fusion module, which enhances the complementarity of RGB and infrared image features through a dual-path attention mechanism; 2) we propose a cross-layer multi-scale Transformer module, which improves detection performance by extracting multi-scale features and facilitating cross-modal information interaction; 3) we propose a texture information focus module, which enhances the representation of local texture details. Additionally, this paper designs a hybrid loss function to improve feature discrimination capability and training efficiency. Experimental results on anti-UAV and Drone-detection datasets show that TransAUAV outperforms state-of-the-art methods on all evaluation metrics.&lt;/p&gt;</content:encoded></item><item><title>SAR-to-Optical Remote Sensing Image Translation Method Based on InternImage and Cascaded Multi-Head Attention</title><link>https://doi.org/10.3390/rs18010055</link><guid>10.3390/rs18010055</guid><pubDate>Wed, 24 Dec 2025 14:27:51 +0000</pubDate><dc:creator>Cheng Xu</dc:creator><dc:creator>Yingying Kong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010055</prism:doi><description>Synthetic aperture radar (SAR), with its all-weather and all-day observation capabilities, plays a significant role in the field of remote sensing. However, due to the unique imaging mechanism of SAR, its interpretation is challenging. Translating SAR images into optical remote sensing images has become a research hotspot in recent years to enhance the interpretability of SAR images. This paper proposes a deep learning-based method for SAR-to-optical remote sensing image translation. The network comprises three parts: a global representor, a generator with cascaded multi-head attention, and a multi-scale discriminator. The global representor, built upon InternImage with deformable convolution v3 (DCNv3) as its core operator, leverages its global receptive field and adaptive spatial aggregation capabilities to extract global semantic features from SAR images. The generator follows the classic “encoder-bottleneck-decoder” structure, where the encoder focuses on extracting local detail features from SAR images. The cascaded multi-head attention module within the bottleneck layer optimizes local detail features and facilitates feature interaction between global semantics and local details. The discriminator adopts a multi-scale structure based on the local receptive field PatchGAN, enabling joint global and local discrimination. Furthermore, for the first time in SAR image translation tasks, structural similarity index metric (SSIM) loss is combined with adversarial loss, perceptual loss, and feature matching loss as the loss function. A series of experiments demonstrate the effectiveness and reliability of the proposed method. Compared to mainstream image translation methods, our method ultimately generates higher-quality optical remote sensing images that are semantically consistent, texturally authentic, clearly detailed, and visually reasonable appearances.
Published: 2025-12-24T14:27:51+00:00
Venue: Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheng Xu; Yingying Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010055"&gt;10.3390/rs18010055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR), with its all-weather and all-day observation capabilities, plays a significant role in the field of remote sensing. However, due to the unique imaging mechanism of SAR, its interpretation is challenging. Translating SAR images into optical remote sensing images has become a research hotspot in recent years to enhance the interpretability of SAR images. This paper proposes a deep learning-based method for SAR-to-optical remote sensing image translation. The network comprises three parts: a global representor, a generator with cascaded multi-head attention, and a multi-scale discriminator. The global representor, built upon InternImage with deformable convolution v3 (DCNv3) as its core operator, leverages its global receptive field and adaptive spatial aggregation capabilities to extract global semantic features from SAR images. The generator follows the classic “encoder-bottleneck-decoder” structure, where the encoder focuses on extracting local detail features from SAR images. The cascaded multi-head attention module within the bottleneck layer optimizes local detail features and facilitates feature interaction between global semantics and local details. The discriminator adopts a multi-scale structure based on the local receptive field PatchGAN, enabling joint global and local discrimination. Furthermore, for the first time in SAR image translation tasks, structural similarity index metric (SSIM) loss is combined with adversarial loss, perceptual loss, and feature matching loss as the loss function. A series of experiments demonstrate the effectiveness and reliability of the proposed method. Compared to mainstream image translation methods, our method ultimately generates higher-quality optical remote sensing images that are semantically consistent, texturally authentic, clearly detailed, and visually reasonable appearances.&lt;/p&gt;</content:encoded></item><item><title>SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction</title><link>https://doi.org/10.1016/j.inffus.2025.104074</link><guid>10.1016/j.inffus.2025.104074</guid><pubDate>Tue, 23 Dec 2025 16:57:18 +0000</pubDate><dc:creator>Yihao Ding</dc:creator><dc:creator>Soyeon Caren Han</dc:creator><dc:creator>Zechuan Li</dc:creator><dc:creator>Hyunsuk Chung</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104074</prism:doi><description>Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.
Published: 2025-12-23T16:57:18+00:00
Venue: Information Fusion
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihao Ding; Soyeon Caren Han; Zechuan Li; Hyunsuk Chung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104074"&gt;10.1016/j.inffus.2025.104074&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.&lt;/p&gt;</content:encoded></item><item><title>Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning</title><link>https://doi.org/10.1109/tgrs.2025.3648057</link><guid>10.1109/tgrs.2025.3648057</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Lanxiao Wang</dc:creator><dc:creator>Heqian Qiu</dc:creator><dc:creator>Minjian Zhang</dc:creator><dc:creator>Fanman Meng</dc:creator><dc:creator>Qingbo Wu</dc:creator><dc:creator>Hongliang Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648057</prism:doi><description>Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lanxiao Wang; Heqian Qiu; Minjian Zhang; Fanman Meng; Qingbo Wu; Hongliang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648057"&gt;10.1109/tgrs.2025.3648057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.&lt;/p&gt;</content:encoded></item><item><title>Image-free Multi-label Image Recognition via LLM-powered Hierarchical Prompt Tuning</title><link>https://doi.org/10.1016/j.patcog.2025.112986</link><guid>10.1016/j.patcog.2025.112986</guid><pubDate>Wed, 24 Dec 2025 15:58:29 +0000</pubDate><dc:creator>Shuo Yang</dc:creator><dc:creator>Zirui Shang</dc:creator><dc:creator>Yongqi Wang</dc:creator><dc:creator>Derong Deng</dc:creator><dc:creator>Hongwei Chen</dc:creator><dc:creator>Xinxiao Wu</dc:creator><dc:creator>Qiyuan Cheng</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112986</prism:doi><description>This paper proposes a novel framework for multi-label image recognition without any training images, namely image-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt a pre-trained Vision-Language Model (VLM) like Contrastive Language–Image Pre-training (CLIP) to multi-label classification. Through asking LLM well-designed questions, we acquire comprehensive knowledge about the characteristics and contexts of objects, which provides valuable text descriptions for learning prompts. Then, we propose a hierarchical prompt learning method by taking the multi-label dependency into consideration, wherein a subset of category-specific prompt tokens is shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical prompts learned from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets, i.e. , Microsoft Common Objects in Context (MS-COCO), Visual Object Classes 2007 (VOC2007), and National University of Singapore Web Image Database (NUS-WIDE), demonstrate that our method achieves better results than the state-of-the-art methods.
Published: 2025-12-24T15:58:29+00:00
Venue: Pattern Recognition
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuo Yang; Zirui Shang; Yongqi Wang; Derong Deng; Hongwei Chen; Xinxiao Wu; Qiyuan Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112986"&gt;10.1016/j.patcog.2025.112986&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;This paper proposes a novel framework for multi-label image recognition without any training images, namely image-free framework, which uses knowledge of pre-trained Large Language Model (LLM) to learn prompts to adapt a pre-trained Vision-Language Model (VLM) like Contrastive Language–Image Pre-training (CLIP) to multi-label classification. Through asking LLM well-designed questions, we acquire comprehensive knowledge about the characteristics and contexts of objects, which provides valuable text descriptions for learning prompts. Then, we propose a hierarchical prompt learning method by taking the multi-label dependency into consideration, wherein a subset of category-specific prompt tokens is shared when the corresponding objects exhibit similar attributes or are more likely to co-occur. Benefiting from the remarkable alignment between visual and linguistic semantics of CLIP, the hierarchical prompts learned from text descriptions are applied to perform classification of images during inference. Our framework presents a new way to explore the synergies between multiple pre-trained models for novel category recognition. Extensive experiments on three public datasets, i.e. , Microsoft Common Objects in Context (MS-COCO), Visual Object Classes 2007 (VOC2007), and National University of Singapore Web Image Database (NUS-WIDE), demonstrate that our method achieves better results than the state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>AgentMath: Empowering Mathematical Reasoning for Large Language Models via Tool-Augmented Agent</title><link>https://arxiv.org/abs/2512.20745v1</link><guid>http://arxiv.org/abs/2512.20745v1</guid><pubDate>Tue, 23 Dec 2025 19:57:49 +0000</pubDate><dc:creator>Haipeng Luo</dc:creator><dc:creator>Huawen Feng</dc:creator><dc:creator>Qingfeng Sun</dc:creator><dc:creator>Can Xu</dc:creator><dc:creator>Kai Zheng</dc:creator><dc:creator>Yufei Wang</dc:creator><dc:creator>Tao Yang</dc:creator><dc:creator>Han Hu</dc:creator><dc:creator>Yansong Tang</dc:creator><dc:creator>Di Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models' reasoning capabilities with code interpreters' computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.
Published: 2025-12-23T19:57:49+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haipeng Luo; Huawen Feng; Qingfeng Sun; Can Xu; Kai Zheng; Yufei Wang; Tao Yang; Han Hu; Yansong Tang; Di Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) like o3 and DeepSeek-R1 have achieved remarkable progress in natural language reasoning with long chain-of-thought. However, they remain computationally inefficient and struggle with accuracy when solving problems requiring complex mathematical operations. In this work, we present AgentMath, an agent framework that seamlessly integrates language models&amp;#x27; reasoning capabilities with code interpreters&amp;#x27; computational precision to efficiently tackle complex mathematical problems. Our approach introduces three key innovations: (1) An automated method that converts natural language chain-of-thought into structured tool-augmented trajectories, generating high-quality supervised fine-tuning (SFT) data to alleviate data scarcity; (2) A novel agentic reinforcement learning (RL) paradigm that dynamically interleaves natural language generation with real-time code execution. This enables models to autonomously learn optimal tool-use strategies through multi-round interactive feedback, while fostering emergent capabilities in code refinement and error correction; (3) An efficient training system incorporating innovative techniques, including request-level asynchronous rollout scheduling, agentic partial rollout, and prefix-aware weighted load balancing, achieving 4-5x speedup and making efficient RL training feasible on ultra-long sequences with scenarios with massive tool calls.Extensive evaluations show that AgentMath achieves state-of-the-art performance on challenging mathematical competition benchmarks including AIME24, AIME25, and HMMT25. Specifically, AgentMath-30B-A3B attains 90.6%, 86.4%, and 73.8% accuracy respectively, achieving advanced capabilities.These results validate the effectiveness of our approach and pave the way for building more sophisticated and scalable mathematical reasoning agents.&lt;/p&gt;</content:encoded></item><item><title>Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation</title><link>https://doi.org/10.1109/tpami.2025.3647829</link><guid>10.1109/tpami.2025.3647829</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Yifei Shi</dc:creator><dc:creator>Boyan Wan</dc:creator><dc:creator>Xin Xu</dc:creator><dc:creator>Kai Xu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647829</prism:doi><description>Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifei Shi; Boyan Wan; Xin Xu; Kai Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647829"&gt;10.1109/tpami.2025.3647829&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object&amp;#x27;s canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model&amp;#x27;s generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network&amp;#x27;s accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...&lt;/p&gt;</content:encoded></item><item><title>Latent Implicit Visual Reasoning</title><link>https://arxiv.org/abs/2512.21218v1</link><guid>http://arxiv.org/abs/2512.21218v1</guid><pubDate>Wed, 24 Dec 2025 14:59:49 +0000</pubDate><dc:creator>Kelvin Li</dc:creator><dc:creator>Chuyi Shang</dc:creator><dc:creator>Leonid Karlinsky</dc:creator><dc:creator>Rogerio Feris</dc:creator><dc:creator>Trevor Darrell</dc:creator><dc:creator>Roei Herzig</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.
Published: 2025-12-24T14:59:49+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kelvin Li; Chuyi Shang; Leonid Karlinsky; Rogerio Feris; Trevor Darrell; Roei Herzig&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what &amp;quot;useful&amp;quot; visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.&lt;/p&gt;</content:encoded></item></channel></rss>