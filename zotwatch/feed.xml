<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 12 Jan 2026 02:44:48 +0000</lastBuildDate><item><title>DNN-aided Low-rank and Sparse Decomposition Model for Infrared Small Target Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113070</link><guid>10.1016/j.patcog.2026.113070</guid><pubDate>Sun, 11 Jan 2026 07:02:27 +0000</pubDate><dc:creator>Jia-Jie Yin</dc:creator><dc:creator>Heng-Chao Li</dc:creator><dc:creator>Yu-Bang Zheng</dc:creator><dc:creator>Xiong-Fei Geng</dc:creator><dc:creator>Jie Pan</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113070</prism:doi><description>Recently, deep neural network-aided low-rank and sparse decomposition (DNN-aided LRSD) models have received increasing attention for infrared small target detection. The main idea of these methods is to utilize DNNs to learn a dataset-free deep prior as an implicit regularization for the background. In this work, we propose a novel DNN-aided LRSD model, which leverages DNNs to enhance the model’s ability to reconstruct low-rank background and detect sparse small targets. First, to efficiently and accurately reconstruct low-rank background, we propose a hierarchical tensor-ring-based background module (HTR) that captures the underlying low-rank structure of the background with compact nonlinear representation. In this module, nonlinear transforms using multilayer perceptrons (MLPs) and parameterized factor tensors are learned from data in an unsupervised manner. Second, to address the limitation of the l 1 norm in accurately describing sparse small targets in complex scenes, we specifically design an attention-guided sparse target module (SpAttention). It can progressively focus on the target region during the iterative process, thus improving target saliency and suppressing background structures. Comprehensive experiments on multiple real-world sequences validate the superior performance of our method in target detection and background suppression, surpassing state-of-the-art approaches. Code is available at: https://github.com/Yiniaie/DNN-aided-LRSD .
Published: 2026-01-11T07:02:27+00:00
Venue: Pattern Recognition
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jia-Jie Yin; Heng-Chao Li; Yu-Bang Zheng; Xiong-Fei Geng; Jie Pan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113070"&gt;10.1016/j.patcog.2026.113070&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, deep neural network-aided low-rank and sparse decomposition (DNN-aided LRSD) models have received increasing attention for infrared small target detection. The main idea of these methods is to utilize DNNs to learn a dataset-free deep prior as an implicit regularization for the background. In this work, we propose a novel DNN-aided LRSD model, which leverages DNNs to enhance the model’s ability to reconstruct low-rank background and detect sparse small targets. First, to efficiently and accurately reconstruct low-rank background, we propose a hierarchical tensor-ring-based background module (HTR) that captures the underlying low-rank structure of the background with compact nonlinear representation. In this module, nonlinear transforms using multilayer perceptrons (MLPs) and parameterized factor tensors are learned from data in an unsupervised manner. Second, to address the limitation of the l 1 norm in accurately describing sparse small targets in complex scenes, we specifically design an attention-guided sparse target module (SpAttention). It can progressively focus on the target region during the iterative process, thus improving target saliency and suppressing background structures. Comprehensive experiments on multiple real-world sequences validate the superior performance of our method in target detection and background suppression, surpassing state-of-the-art approaches. Code is available at: https://github.com/Yiniaie/DNN-aided-LRSD .&lt;/p&gt;</content:encoded></item><item><title>TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2601.05446v1</link><guid>http://arxiv.org/abs/2601.05446v1</guid><pubDate>Fri, 09 Jan 2026 00:27:18 +0000</pubDate><dc:creator>Hongyang Xie</dc:creator><dc:creator>Hongyang He</dc:creator><dc:creator>Victor Sanchez</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.
Published: 2026-01-09T00:27:18+00:00
Venue: arXiv
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongyang Xie; Hongyang He; Victor Sanchez&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack a mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space, which is an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose the Trajectory-Aware Mamba Propagation Network (TAPM-Net), which explicitly models the spatial diffusion behavior of target-induced feature disturbances. TAPM-Net is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The PGM constructs perturbation energy fields from multi-level features and extracts gradient-following feature trajectories that reflect the directionality of local responses. The resulting feature trajectories are fed into the TASB, a Mamba-based state-space unit that models dynamic propagation along each trajectory while incorporating velocity-constrained diffusion and semantically aligned feature fusion from word-level and sentence-level embeddings. Unlike existing attention-based methods, TAPM-Net enables anisotropic, context-sensitive state transitions along spatial trajectories while maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAPM-Net achieves state-of-the-art performance in ISTD.&lt;/p&gt;</content:encoded></item><item><title>Cross-domain Few-shot Classification via Invariant-content Feature Reconstruction</title><link>https://doi.org/10.1007/s11263-025-02601-5</link><guid>10.1007/s11263-025-02601-5</guid><pubDate>Sun, 11 Jan 2026 14:52:44 +0000</pubDate><dc:creator>Hongduan Tian</dc:creator><dc:creator>Feng Liu</dc:creator><dc:creator>Ka Chun Cheung</dc:creator><dc:creator>Zhen Fang</dc:creator><dc:creator>Simon See</dc:creator><dc:creator>Tongliang Liu</dc:creator><dc:creator>Bo Han</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02601-5</prism:doi><description>Abstract In cross-domain few-shot classification (CFC), mainstream studies aim to train a simple module (e.g. a linear transformation head) to select or transform features (a.k.a., the high-level semantic features) for previously unseen domains with a few labeled training data available on top of a powerful pre-trained model. These studies usually assume that high-level semantic features are shared across these domains, and just simple feature selection or transformations are enough to adapt features to previously unseen domains. However, in this paper, we find that the simply transformed features are too general to fully cover the key content features regarding each class. Thus, we propose an effective method, invariant-content feature reconstruction (IFR), to train a simple module that simultaneously considers both high-level and fine-grained invariant-content features for the previously unseen domains. Specifically, the fine-grained invariant-content features are considered as a set of informative and discriminative features learned from a few labeled training data of tasks sampled from unseen domains and are extracted by retrieving features that are invariant to style modifications from a set of content-preserving augmented data in pixel level with an attention module. Extensive experiments on the Meta-Dataset benchmark show that IFR achieves good generalization performance on unseen domains, which demonstrates the effectiveness of the fusion of the high-level features and the fine-grained invariant-content features. Specifically, IFR improves the average accuracy on unseen domains by 1.6% and 6.5% respectively under two different cross-domain few-shot classification settings.
Published: 2026-01-11T14:52:44+00:00
Venue: International Journal of Computer Vision
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongduan Tian; Feng Liu; Ka Chun Cheung; Zhen Fang; Simon See; Tongliang Liu; Bo Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02601-5"&gt;10.1007/s11263-025-02601-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract In cross-domain few-shot classification (CFC), mainstream studies aim to train a simple module (e.g. a linear transformation head) to select or transform features (a.k.a., the high-level semantic features) for previously unseen domains with a few labeled training data available on top of a powerful pre-trained model. These studies usually assume that high-level semantic features are shared across these domains, and just simple feature selection or transformations are enough to adapt features to previously unseen domains. However, in this paper, we find that the simply transformed features are too general to fully cover the key content features regarding each class. Thus, we propose an effective method, invariant-content feature reconstruction (IFR), to train a simple module that simultaneously considers both high-level and fine-grained invariant-content features for the previously unseen domains. Specifically, the fine-grained invariant-content features are considered as a set of informative and discriminative features learned from a few labeled training data of tasks sampled from unseen domains and are extracted by retrieving features that are invariant to style modifications from a set of content-preserving augmented data in pixel level with an attention module. Extensive experiments on the Meta-Dataset benchmark show that IFR achieves good generalization performance on unseen domains, which demonstrates the effectiveness of the fusion of the high-level features and the fine-grained invariant-content features. Specifically, IFR improves the average accuracy on unseen domains by 1.6% and 6.5% respectively under two different cross-domain few-shot classification settings.&lt;/p&gt;</content:encoded></item><item><title>MCIVA: A Multi-View Pedestrian Detection Framework with Central Inverse Nearest Neighbor Map and View Adaptive Module</title><link>https://doi.org/10.1016/j.inffus.2026.104142</link><guid>10.1016/j.inffus.2026.104142</guid><pubDate>Sun, 11 Jan 2026 15:13:21 +0000</pubDate><dc:creator>He Li</dc:creator><dc:creator>Taiyu Liao</dc:creator><dc:creator>Weihang Kong</dc:creator><dc:creator>Xingchen Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104142</prism:doi><description>Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.
Published: 2026-01-11T15:13:21+00:00
Venue: Information Fusion
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; He Li; Taiyu Liao; Weihang Kong; Xingchen Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104142"&gt;10.1016/j.inffus.2026.104142&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>MambaFPN: A SSM-based Feature Pyramid Network for Object Detection</title><link>https://doi.org/10.1016/j.neunet.2026.108544</link><guid>10.1016/j.neunet.2026.108544</guid><pubDate>Sat, 10 Jan 2026 00:23:34 +0000</pubDate><dc:creator>Le Liang</dc:creator><dc:creator>Cheng Wang</dc:creator><dc:creator>Lefei Zhang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108544</prism:doi><description>Object detection is a fundamental task in computer vision, aiming to localize and classify objects within images. Feature pyramid networks (FPNs) play a crucial role in modern object detectors by constructing hierarchical multi-scale feature maps to effectively handle objects of varying sizes. However, most existing advanced FPN methods rely heavily on convolutional neural networks (CNNs), which struggle to capture global context information. To address this limitation, we propose leveraging vision mamba blocks to enhance global modeling capabilities. The vanilla vision mamba block, through its state space mechanism, enables global context modeling for every spatial pixel within a single feature map. Building on this, we first use vision mamba blocks to extract global information from individual feature maps in the hierarchy. Subsequently, additional vision mamba blocks facilitate inter-scale information exchange among multi-scale feature maps, ensuring comprehensive global context integration. The proposed method, termed MambaFPN, significantly enhances object detector performance. For instance, it improves the Average Precision (AP) of vanilla FPN from 38.6 to 39.4, with fewer parameters. This demonstrates the effectiveness and efficiency of MambaFPN in advancing object detection.
Published: 2026-01-10T00:23:34+00:00
Venue: Neural Networks
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Le Liang; Cheng Wang; Lefei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108544"&gt;10.1016/j.neunet.2026.108544&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection is a fundamental task in computer vision, aiming to localize and classify objects within images. Feature pyramid networks (FPNs) play a crucial role in modern object detectors by constructing hierarchical multi-scale feature maps to effectively handle objects of varying sizes. However, most existing advanced FPN methods rely heavily on convolutional neural networks (CNNs), which struggle to capture global context information. To address this limitation, we propose leveraging vision mamba blocks to enhance global modeling capabilities. The vanilla vision mamba block, through its state space mechanism, enables global context modeling for every spatial pixel within a single feature map. Building on this, we first use vision mamba blocks to extract global information from individual feature maps in the hierarchy. Subsequently, additional vision mamba blocks facilitate inter-scale information exchange among multi-scale feature maps, ensuring comprehensive global context integration. The proposed method, termed MambaFPN, significantly enhances object detector performance. For instance, it improves the Average Precision (AP) of vanilla FPN from 38.6 to 39.4, with fewer parameters. This demonstrates the effectiveness and efficiency of MambaFPN in advancing object detection.&lt;/p&gt;</content:encoded></item><item><title>Unraveling Domain Styles for Enhanced Cross-Domain Generalization</title><link>https://doi.org/10.1016/j.knosys.2026.115302</link><guid>10.1016/j.knosys.2026.115302</guid><pubDate>Sat, 10 Jan 2026 16:17:21 +0000</pubDate><dc:creator>Zhonghua Yao</dc:creator><dc:creator>Juncheng Lian</dc:creator><dc:creator>Qiang Zhang</dc:creator><dc:creator>Yanming Guo</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115302</prism:doi><description>Domain generalization is aimed at enabling deep neural networks trained on multiple source domains to generalize effectively to unseen target domains. In this study, we propose a unified and novel framework for domain generalization in image classification. Unlike existing approaches that only adapt domain-invariant features or augment styles, our method implicitly separates style and semantic representations via three novel modules: (1) a Dynamic Mapping Module that implicitly preserves style information while maintaining semantic consistency, (2) a Spatial Regrouping Weighting Module that selectively emphasizes domain-adaptive spatial semantics via attention-guided regrouping, and (3) a Distribution Metrics Alignment Module that aligns high-order feature distributions across domains to reduce domain shift. Unlike existing works that passively align domains or suppress domain-specific cues, our framework actively leverages these for robust generalization. Extensive experiments across three standard domain generalization(DG) benchmarks, namely, Digits-DG, PACS, and Office-Home, demonstrate that our method achieves state-of-the-art performance.
Published: 2026-01-10T16:17:21+00:00
Venue: Knowledge-Based Systems
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhonghua Yao; Juncheng Lian; Qiang Zhang; Yanming Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115302"&gt;10.1016/j.knosys.2026.115302&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Domain generalization is aimed at enabling deep neural networks trained on multiple source domains to generalize effectively to unseen target domains. In this study, we propose a unified and novel framework for domain generalization in image classification. Unlike existing approaches that only adapt domain-invariant features or augment styles, our method implicitly separates style and semantic representations via three novel modules: (1) a Dynamic Mapping Module that implicitly preserves style information while maintaining semantic consistency, (2) a Spatial Regrouping Weighting Module that selectively emphasizes domain-adaptive spatial semantics via attention-guided regrouping, and (3) a Distribution Metrics Alignment Module that aligns high-order feature distributions across domains to reduce domain shift. Unlike existing works that passively align domains or suppress domain-specific cues, our framework actively leverages these for robust generalization. Extensive experiments across three standard domain generalization(DG) benchmarks, namely, Digits-DG, PACS, and Office-Home, demonstrate that our method achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Complex convolutional sparse coding InSAR phase filtering Incorporating directional gradients and second-order difference regularization</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.016</link><guid>10.1016/j.isprsjprs.2025.12.016</guid><pubDate>Sat, 10 Jan 2026 20:22:59 +0000</pubDate><dc:creator>Pengcheng Hu</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Junhuan Peng</dc:creator><dc:creator>Xu Ma</dc:creator><dc:creator>Yuhan Su</dc:creator><dc:creator>Xiaoman Qi</dc:creator><dc:creator>Xinwei Jiang</dc:creator><dc:creator>Wenwen Wang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.016</prism:doi><description>Interferometric Synthetic Aperture Radar (InSAR) is a technology that can effectively obtain ground information, conduct large-scale topography mapping, and monitor surface deformation. However, InSAR data is interfered by speckle noise caused by radar echo signal fading, ground background clutter, and decoherence, which affects the InSAR interferometric phase quality and thus reduces the accuracy of InSAR results. The existing Complex Convolutional Sparse Coding Gradient Regularization (ComCSC-GR) method incorporates gradient regularization by considering the sparse coefficient matrix’s gradients in both row (azimuth) and column (range) directions. It is an advanced and effective interferogram phase filtering method that can improve the interferogram quality. However, this method does not take into account the variation characteristics of the diagonal gradient and the second-order difference information (caused by edge mutations). As a result, the interferogram still exhibits problems such as staircase artifacts in high-noise and low-coherence areas, uneven interferograms (caused by a large number of residual points), and unclear phase edge structure. This article introduces multiple directional gradients and second-order differential Laplacian operator information, and construct two models: “Complex Convolutional Sparse Coding Model with L 2 -norm Regularization of Directional Gradients and Laplacian Operator (ComCSC-RCDL) ” and “Complex Convolutional Sparse Coding Model Coupled with L 1 -norm Total Variation Regularization (ComCSC-RCDL-TV)”. These methods enhance the fidelity of phase texture and edge structure, and improve the quality of InSAR interferogram filtering phase in low-coherence scenarios. Comparative experiments were conducted using simulated data, real data from Sentinel-1 and LuTan-1 (LT-1), and advanced methods including ComCSC-GR and InSAR-BM3D (real data experiments included comparison experiments before and after removing the interferogram orbit error). The results show that the proposed model method performs better than the comparative model, verifying the effectiveness of the proposed model.
Published: 2026-01-10T20:22:59+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengcheng Hu; Xu Li; Junhuan Peng; Xu Ma; Yuhan Su; Xiaoman Qi; Xinwei Jiang; Wenwen Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.016"&gt;10.1016/j.isprsjprs.2025.12.016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Interferometric Synthetic Aperture Radar (InSAR) is a technology that can effectively obtain ground information, conduct large-scale topography mapping, and monitor surface deformation. However, InSAR data is interfered by speckle noise caused by radar echo signal fading, ground background clutter, and decoherence, which affects the InSAR interferometric phase quality and thus reduces the accuracy of InSAR results. The existing Complex Convolutional Sparse Coding Gradient Regularization (ComCSC-GR) method incorporates gradient regularization by considering the sparse coefficient matrix’s gradients in both row (azimuth) and column (range) directions. It is an advanced and effective interferogram phase filtering method that can improve the interferogram quality. However, this method does not take into account the variation characteristics of the diagonal gradient and the second-order difference information (caused by edge mutations). As a result, the interferogram still exhibits problems such as staircase artifacts in high-noise and low-coherence areas, uneven interferograms (caused by a large number of residual points), and unclear phase edge structure. This article introduces multiple directional gradients and second-order differential Laplacian operator information, and construct two models: “Complex Convolutional Sparse Coding Model with L 2 -norm Regularization of Directional Gradients and Laplacian Operator (ComCSC-RCDL) ” and “Complex Convolutional Sparse Coding Model Coupled with L 1 -norm Total Variation Regularization (ComCSC-RCDL-TV)”. These methods enhance the fidelity of phase texture and edge structure, and improve the quality of InSAR interferogram filtering phase in low-coherence scenarios. Comparative experiments were conducted using simulated data, real data from Sentinel-1 and LuTan-1 (LT-1), and advanced methods including ComCSC-GR and InSAR-BM3D (real data experiments included comparison experiments before and after removing the interferogram orbit error). The results show that the proposed model method performs better than the comparative model, verifying the effectiveness of the proposed model.&lt;/p&gt;</content:encoded></item><item><title>OMD: Optimal Transport-guided Multimodal Disentangled Learning for Leptomeningeal Metastasis Diagnosis</title><link>https://doi.org/10.1016/j.inffus.2025.104121</link><guid>10.1016/j.inffus.2025.104121</guid><pubDate>Sun, 11 Jan 2026 15:13:09 +0000</pubDate><dc:creator>Shengjia Chen</dc:creator><dc:creator>Huihua Hu</dc:creator><dc:creator>Hongfu Zeng</dc:creator><dc:creator>Chenxin Li</dc:creator><dc:creator>Qing Xu</dc:creator><dc:creator>Longfeng Zhang</dc:creator><dc:creator>Haipeng Xu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104121</prism:doi><description>Leptomeningeal metastasis (LM) diagnosis represents a significant clinical challenge. Existing diagnostic approaches are often limited by their reliance on single-modality data and the inherent difficulties in effectively integrating heterogeneous information from imaging and genomics. To address these challenges, we propose OMD, an O ptimal Transport-guided M ultimodal D isentangled Learning framework that integrates MRI data with genomic information for enhanced diagnostic accuracy. Our method combines optimal transport-based cross-modal attention to robustly align heterogeneous features, information bottleneck compression to mitigate noise and redundancy, and feature disentanglement to explicitly model shared and modality-specific representations, integrated with hierarchical attention for MRI processing and graph-based cross-modal reasoning. Experimental results show that OMD achieves superior diagnostic accuracy, sensitivity, and specificity on our clinical dataset, substantially outperforming current state-of-the-art methods across all evaluation metrics. The model also provides interpretable insights into the cross-modal biomarkers associated with LM. The proposed OMD framework establishes a new paradigm for multimodal medical diagnosis that effectively addresses the complementary strengths of imaging and genomic data. Beyond its immediate application to LM diagnosis, our approach offers a generalizable methodology for integrating heterogeneous medical data sources while providing clinically relevant interpretability. This work represents an important step toward personalized medicine approaches that combine multiple data modalities for improved diagnostic accuracy and treatment planning.
Published: 2026-01-11T15:13:09+00:00
Venue: Information Fusion
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengjia Chen; Huihua Hu; Hongfu Zeng; Chenxin Li; Qing Xu; Longfeng Zhang; Haipeng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104121"&gt;10.1016/j.inffus.2025.104121&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Leptomeningeal metastasis (LM) diagnosis represents a significant clinical challenge. Existing diagnostic approaches are often limited by their reliance on single-modality data and the inherent difficulties in effectively integrating heterogeneous information from imaging and genomics. To address these challenges, we propose OMD, an O ptimal Transport-guided M ultimodal D isentangled Learning framework that integrates MRI data with genomic information for enhanced diagnostic accuracy. Our method combines optimal transport-based cross-modal attention to robustly align heterogeneous features, information bottleneck compression to mitigate noise and redundancy, and feature disentanglement to explicitly model shared and modality-specific representations, integrated with hierarchical attention for MRI processing and graph-based cross-modal reasoning. Experimental results show that OMD achieves superior diagnostic accuracy, sensitivity, and specificity on our clinical dataset, substantially outperforming current state-of-the-art methods across all evaluation metrics. The model also provides interpretable insights into the cross-modal biomarkers associated with LM. The proposed OMD framework establishes a new paradigm for multimodal medical diagnosis that effectively addresses the complementary strengths of imaging and genomic data. Beyond its immediate application to LM diagnosis, our approach offers a generalizable methodology for integrating heterogeneous medical data sources while providing clinically relevant interpretability. This work represents an important step toward personalized medicine approaches that combine multiple data modalities for improved diagnostic accuracy and treatment planning.&lt;/p&gt;</content:encoded></item><item><title>Infrared-Assisted Single-Stage Framework for Joint Restoration and Fusion of Visible and Infrared Images under Hazy Conditions</title><link>https://doi.org/10.1016/j.patcog.2026.113074</link><guid>10.1016/j.patcog.2026.113074</guid><pubDate>Sat, 10 Jan 2026 23:28:00 +0000</pubDate><dc:creator>Huafeng li</dc:creator><dc:creator>Jiaqi fang</dc:creator><dc:creator>Yafei Zhang</dc:creator><dc:creator>Yu Liu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113074</prism:doi><description>Infrared and visible (IR-VIS) image fusion has gained significant attention for its broad application value. However, existing methods often neglect the complementary role of infrared image in restoring visible image features under hazy conditions. To address this, we propose a joint learning framework that utilizes infrared image for the restoration and fusion of hazy IR-VIS images. To mitigate the adverse effects of feature diversity between IR-VIS images, we introduce a prompt generation mechanism that regulates modality-specific feature incompatibility. This creates a prompt selection matrix from non-shared image information, followed by prompt embeddings generated from a prompt pool. These embeddings help generate candidate features for dehazing. We further design an infrared-assisted feature restoration mechanism that selects candidate features based on haze density, enabling simultaneous restoration and fusion within a single-stage framework. To enhance fusion quality, we construct a multi-stage prompt embedding fusion module that leverages feature supplementation from the prompt generation module. Our method effectively fuses IR-VIS images while removing haze, yielding clear, haze-free fusion results. In contrast to two-stage methods that dehaze and then fuse, our approach enables collaborative training in a single-stage framework, making the model relatively lightweight and suitable for practical deployment. Experimental results validate its effectiveness and demonstrate advantages over existing methods. The source code of the paper is available at .
Published: 2026-01-10T23:28:00+00:00
Venue: Pattern Recognition
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huafeng li; Jiaqi fang; Yafei Zhang; Yu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113074"&gt;10.1016/j.patcog.2026.113074&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared and visible (IR-VIS) image fusion has gained significant attention for its broad application value. However, existing methods often neglect the complementary role of infrared image in restoring visible image features under hazy conditions. To address this, we propose a joint learning framework that utilizes infrared image for the restoration and fusion of hazy IR-VIS images. To mitigate the adverse effects of feature diversity between IR-VIS images, we introduce a prompt generation mechanism that regulates modality-specific feature incompatibility. This creates a prompt selection matrix from non-shared image information, followed by prompt embeddings generated from a prompt pool. These embeddings help generate candidate features for dehazing. We further design an infrared-assisted feature restoration mechanism that selects candidate features based on haze density, enabling simultaneous restoration and fusion within a single-stage framework. To enhance fusion quality, we construct a multi-stage prompt embedding fusion module that leverages feature supplementation from the prompt generation module. Our method effectively fuses IR-VIS images while removing haze, yielding clear, haze-free fusion results. In contrast to two-stage methods that dehaze and then fuse, our approach enables collaborative training in a single-stage framework, making the model relatively lightweight and suitable for practical deployment. Experimental results validate its effectiveness and demonstrate advantages over existing methods. The source code of the paper is available at .&lt;/p&gt;</content:encoded></item><item><title>GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras</title><link>https://arxiv.org/abs/2601.05839v1</link><guid>http://arxiv.org/abs/2601.05839v1</guid><pubDate>Fri, 09 Jan 2026 15:13:28 +0000</pubDate><dc:creator>Weimin Liu</dc:creator><dc:creator>Wenjun Wang</dc:creator><dc:creator>Joshua H. Meng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.
Published: 2026-01-09T15:13:28+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weimin Liu; Wenjun Wang; Joshua H. Meng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.&lt;/p&gt;</content:encoded></item><item><title>YOFOR: You Only Focus on Object Regions for Tiny Object Detection in Aerial Images</title><link>https://doi.org/10.1016/j.neunet.2026.108571</link><guid>10.1016/j.neunet.2026.108571</guid><pubDate>Sat, 10 Jan 2026 16:16:44 +0000</pubDate><dc:creator>Heng Hu</dc:creator><dc:creator>Hao-Zhe Wang</dc:creator><dc:creator>Si-Bao Chen</dc:creator><dc:creator>Jin Tang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108571</prism:doi><description>With development of deep learning methods, performance of object detection has been greatly improved. However, the high resolution of remotely sensed images, the complexity of the background, the uneven distribution of objects, and the uneven number of objects among them lead to unsatisfactory detection results of existing detectors. Facing these challenges, we propose YOFOR (You Only Focus on Object Regions), an adaptive local sensing enhancement network. It contains three components: adaptive local sensing module, fuzzy enhancement module and class balance module. Among them, adaptive local sensing module can adaptively localize dense object regions and dynamically crop dense object regions on view, which effectively solves problem of uneven distribution of objects. Fuzzy enhancement module further enhances object region by weakening the background interference, thus improving detection performance. Class balancing module, which analyzes dataset to obtain distribution of long-tailed classes, takes into account direction of tailed classes and distance around object, and operates on tailed classes within a certain range to alleviate long-tailed class problem and further improve detection performance. All three components are unsupervised and can be easily inserted into existing networks. Extensive experiments on the VisDrone, DOTA, and AI-TOD datasets demonstrate the effectiveness and adaptability of the method.
Published: 2026-01-10T16:16:44+00:00
Venue: Neural Networks
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Heng Hu; Hao-Zhe Wang; Si-Bao Chen; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108571"&gt;10.1016/j.neunet.2026.108571&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;With development of deep learning methods, performance of object detection has been greatly improved. However, the high resolution of remotely sensed images, the complexity of the background, the uneven distribution of objects, and the uneven number of objects among them lead to unsatisfactory detection results of existing detectors. Facing these challenges, we propose YOFOR (You Only Focus on Object Regions), an adaptive local sensing enhancement network. It contains three components: adaptive local sensing module, fuzzy enhancement module and class balance module. Among them, adaptive local sensing module can adaptively localize dense object regions and dynamically crop dense object regions on view, which effectively solves problem of uneven distribution of objects. Fuzzy enhancement module further enhances object region by weakening the background interference, thus improving detection performance. Class balancing module, which analyzes dataset to obtain distribution of long-tailed classes, takes into account direction of tailed classes and distance around object, and operates on tailed classes within a certain range to alleviate long-tailed class problem and further improve detection performance. All three components are unsupervised and can be easily inserted into existing networks. Extensive experiments on the VisDrone, DOTA, and AI-TOD datasets demonstrate the effectiveness and adaptability of the method.&lt;/p&gt;</content:encoded></item><item><title>Frequency-Aware and Lifting-Based Efficient Transformer for Person Search</title><link>https://doi.org/10.1016/j.eswa.2026.131090</link><guid>10.1016/j.eswa.2026.131090</guid><pubDate>Sat, 10 Jan 2026 00:24:32 +0000</pubDate><dc:creator>Qilin Shu</dc:creator><dc:creator>Qixian Zhang</dc:creator><dc:creator>Duoqian Miao</dc:creator><dc:creator>Qi Zhang</dc:creator><dc:creator>Hongyun Zhang</dc:creator><dc:creator>Cairong Zhao</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131090</prism:doi><description>The person search task aims to locate a target person within a set of scene images. In recent years, transformer-based models in this field have made some progress. However, they still face two primary challenges: 1) the self-attention mechanism tends to suppress high-frequency components in the features, which severely impacts model performance; 2) the computational cost of transformers is relatively high. To address these issues, we propose a novel Frequency-Aware and Lifting-Based Efficient Transformer (FLET) method for person search. FLET is designed to enhance the discriminative feature extraction capabilities of transformers while reducing computational overhead and improving efficiency. Specifically, we develop a three-stage framework that progressively optimizes both detection and re-identification performance. Our model enhances the perception of high-frequency features by learning from augmented inputs. The augmented inputs are generated via High-Pass Filtering (HPF) and contain additional high-frequency components. Furthermore, we replace the self-attention layers in the transformer with a Learnable Lifting Block (LLB) to capture multiscale features. LLB not only lowers the computational complexity but also alleviates the suppression of high-frequency features and enhances the ability to exploit multiscale information. Extensive experiments demonstrate that FLET achieves state-of-the-art performance on both the CUHK-SYSU and PRW datasets.
Published: 2026-01-10T00:24:32+00:00
Venue: Expert Systems with Applications
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qilin Shu; Qixian Zhang; Duoqian Miao; Qi Zhang; Hongyun Zhang; Cairong Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131090"&gt;10.1016/j.eswa.2026.131090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;The person search task aims to locate a target person within a set of scene images. In recent years, transformer-based models in this field have made some progress. However, they still face two primary challenges: 1) the self-attention mechanism tends to suppress high-frequency components in the features, which severely impacts model performance; 2) the computational cost of transformers is relatively high. To address these issues, we propose a novel Frequency-Aware and Lifting-Based Efficient Transformer (FLET) method for person search. FLET is designed to enhance the discriminative feature extraction capabilities of transformers while reducing computational overhead and improving efficiency. Specifically, we develop a three-stage framework that progressively optimizes both detection and re-identification performance. Our model enhances the perception of high-frequency features by learning from augmented inputs. The augmented inputs are generated via High-Pass Filtering (HPF) and contain additional high-frequency components. Furthermore, we replace the self-attention layers in the transformer with a Learnable Lifting Block (LLB) to capture multiscale features. LLB not only lowers the computational complexity but also alleviates the suppression of high-frequency features and enhances the ability to exploit multiscale information. Extensive experiments demonstrate that FLET achieves state-of-the-art performance on both the CUHK-SYSU and PRW datasets.&lt;/p&gt;</content:encoded></item><item><title>SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection</title><link>https://arxiv.org/abs/2601.04968v1</link><guid>http://arxiv.org/abs/2601.04968v1</guid><pubDate>Thu, 08 Jan 2026 14:16:11 +0000</pubDate><dc:creator>Maximilian Pittner</dc:creator><dc:creator>Joel Janai</dc:creator><dc:creator>Mario Faigle</dc:creator><dc:creator>Alexandru Paul Condurache</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.
Published: 2026-01-08T14:16:11+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maximilian Pittner; Joel Janai; Mario Faigle; Alexandru Paul Condurache&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.&lt;/p&gt;</content:encoded></item><item><title>Compressing image encoders via latent distillation</title><link>https://arxiv.org/abs/2601.05639v1</link><guid>http://arxiv.org/abs/2601.05639v1</guid><pubDate>Fri, 09 Jan 2026 08:50:38 +0000</pubDate><dc:creator>Caroline Mazini Rodrigues</dc:creator><dc:creator>Nicolas Keriven</dc:creator><dc:creator>Thomas Maugey</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.
Published: 2026-01-09T08:50:38+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Caroline Mazini Rodrigues; Nicolas Keriven; Thomas Maugey&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment</title><link>https://arxiv.org/abs/2601.04571v1</link><guid>http://arxiv.org/abs/2601.04571v1</guid><pubDate>Thu, 08 Jan 2026 04:02:49 +0000</pubDate><dc:creator>Delong Zeng</dc:creator><dc:creator>Yuexiang Xie</dc:creator><dc:creator>Yaliang Li</dc:creator><dc:creator>Ying Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.
Published: 2026-01-08T04:02:49+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Delong Zeng; Yuexiang Xie; Yaliang Li; Ying Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.&lt;/p&gt;</content:encoded></item><item><title>AdaAlign: A Unified Solution for Traditional and Modern Zero-Shot Sketch-Based Image Retrieval</title><link>https://doi.org/10.1016/j.neunet.2026.108586</link><guid>10.1016/j.neunet.2026.108586</guid><pubDate>Sat, 10 Jan 2026 23:33:38 +0000</pubDate><dc:creator>Mingrui Zhu</dc:creator><dc:creator>Fangzhou Wang</dc:creator><dc:creator>Xin Wei</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108586</prism:doi><description>Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen classes. With the rapid advancements in modern large vision-language models (VLMs), traditional approaches that relied solely on small vision encoders have gradually been supplanted. However, both the traditional vision encoder-based methods and the modern VLMs-based methods have their limitations, and no unified approach effectively addresses both. In this paper, we present an effective “Adaptation and Alignment (AdaAlign)” approach to address the key challenges. Specifically, we insert lightweight Adapter or LoRA to learn new abstract concepts of the sketches and improve cross-domain representation capabilities, which helps alleviate domain heterogeneity. Then, we propose to directly align the learned image embedding with the more semantically rich text embedding within a distillation framework to bridge the semantic gap. This enables the model to learn more generalizable visual representations from linguistic semantic cues. We integrate our key innovations into both traditional small models ( e.g. , ResNet50 or DINO-S) and modern VLMs ( e.g. , SigLIP), resulting in state-of-the-art performance. Extensive experiments on three benchmark datasets demonstrate the superiority of our method in terms of retrieval accuracy and flexibility.
Published: 2026-01-10T23:33:38+00:00
Venue: Neural Networks
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingrui Zhu; Fangzhou Wang; Xin Wei; Nannan Wang; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108586"&gt;10.1016/j.neunet.2026.108586&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen classes. With the rapid advancements in modern large vision-language models (VLMs), traditional approaches that relied solely on small vision encoders have gradually been supplanted. However, both the traditional vision encoder-based methods and the modern VLMs-based methods have their limitations, and no unified approach effectively addresses both. In this paper, we present an effective “Adaptation and Alignment (AdaAlign)” approach to address the key challenges. Specifically, we insert lightweight Adapter or LoRA to learn new abstract concepts of the sketches and improve cross-domain representation capabilities, which helps alleviate domain heterogeneity. Then, we propose to directly align the learned image embedding with the more semantically rich text embedding within a distillation framework to bridge the semantic gap. This enables the model to learn more generalizable visual representations from linguistic semantic cues. We integrate our key innovations into both traditional small models ( e.g. , ResNet50 or DINO-S) and modern VLMs ( e.g. , SigLIP), resulting in state-of-the-art performance. Extensive experiments on three benchmark datasets demonstrate the superiority of our method in terms of retrieval accuracy and flexibility.&lt;/p&gt;</content:encoded></item><item><title>Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens</title><link>https://arxiv.org/abs/2601.05927v1</link><guid>http://arxiv.org/abs/2601.05927v1</guid><pubDate>Fri, 09 Jan 2026 16:41:08 +0000</pubDate><dc:creator>Yohann Perron</dc:creator><dc:creator>Vladyslav Sydorov</dc:creator><dc:creator>Christophe Pottier</dc:creator><dc:creator>Loic Landrieu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .
Published: 2026-01-09T16:41:08+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yohann Perron; Vladyslav Sydorov; Christophe Pottier; Loic Landrieu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .&lt;/p&gt;</content:encoded></item><item><title>DMDNet:Dual-branch Multi-modal Deep Fusion Network for V-D-T Salient Object Detection</title><link>https://doi.org/10.1016/j.neunet.2026.108579</link><guid>10.1016/j.neunet.2026.108579</guid><pubDate>Sun, 11 Jan 2026 15:10:56 +0000</pubDate><dc:creator>Yaoqi Sun</dc:creator><dc:creator>Bin Wan</dc:creator><dc:creator>Haibing Yin</dc:creator><dc:creator>Yahong Chen</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108579</prism:doi><description>In the multi-modal salient object detection task, depth or thermal features are often directly fused with visible feature during the encoding stage, which directly results in the fused encoder features containing a large amount of noise information and reducing the accuracy of detection. To address this challenge, in this paper, we propose a novel dual-branch multi-modal deep fusion network (DMDNet) where visible image serves as one branch, and depth and thermal images serve as another branch to achieve multi-modal feature fusion in the decoder phase. In the encoder phase, we apply two types of backbone networks to three modalities to ensure sufficient information extraction and design the modal interaction (MI) module to dig the complementarity between depth and thermal features. In the decoder phase, we propose the multi-scale feature perception (MFP) module and region optimization (RO) module in succession to mine and optimize the saliency region. After that, we introduce the dual-branch fusion (DF) module to integrate multi-modal feature in the bottom-to-top manner for generating final saliency map. DMDNet achieves superior performance on the VDT-2048 dataset, as verified by comprehensive experimental results.
Published: 2026-01-11T15:10:56+00:00
Venue: Neural Networks
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaoqi Sun; Bin Wan; Haibing Yin; Yahong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108579"&gt;10.1016/j.neunet.2026.108579&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;In the multi-modal salient object detection task, depth or thermal features are often directly fused with visible feature during the encoding stage, which directly results in the fused encoder features containing a large amount of noise information and reducing the accuracy of detection. To address this challenge, in this paper, we propose a novel dual-branch multi-modal deep fusion network (DMDNet) where visible image serves as one branch, and depth and thermal images serve as another branch to achieve multi-modal feature fusion in the decoder phase. In the encoder phase, we apply two types of backbone networks to three modalities to ensure sufficient information extraction and design the modal interaction (MI) module to dig the complementarity between depth and thermal features. In the decoder phase, we propose the multi-scale feature perception (MFP) module and region optimization (RO) module in succession to mine and optimize the saliency region. After that, we introduce the dual-branch fusion (DF) module to integrate multi-modal feature in the bottom-to-top manner for generating final saliency map. DMDNet achieves superior performance on the VDT-2048 dataset, as verified by comprehensive experimental results.&lt;/p&gt;</content:encoded></item><item><title>Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification</title><link>https://arxiv.org/abs/2601.05498v1</link><guid>http://arxiv.org/abs/2601.05498v1</guid><pubDate>Fri, 09 Jan 2026 03:02:41 +0000</pubDate><dc:creator>Samuel E. Johnny</dc:creator><dc:creator>Bernes L. Atabonfack</dc:creator><dc:creator>Israel Alagbe</dc:creator><dc:creator>Assane Gueye</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.
Published: 2026-01-09T03:02:41+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Samuel E. Johnny; Bernes L. Atabonfack; Israel Alagbe; Assane Gueye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.&lt;/p&gt;</content:encoded></item><item><title>DIFF-MF: A Difference-Driven Channel-Spatial State Space Model for Multi-Modal Image Fusion</title><link>https://arxiv.org/abs/2601.05538v1</link><guid>http://arxiv.org/abs/2601.05538v1</guid><pubDate>Fri, 09 Jan 2026 05:26:54 +0000</pubDate><dc:creator>Yiming Sun</dc:creator><dc:creator>Zifan Ye</dc:creator><dc:creator>Qinghua Hu</dc:creator><dc:creator>Pengfei Zhu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.
Published: 2026-01-09T05:26:54+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Sun; Zifan Ye; Qinghua Hu; Pengfei Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal image fusion aims to integrate complementary information from multiple source images to produce high-quality fused images with enriched content. Although existing approaches based on state space model have achieved satisfied performance with high computational efficiency, they tend to either over-prioritize infrared intensity at the cost of visible details, or conversely, preserve visible structure while diminishing thermal target salience. To overcome these challenges, we propose DIFF-MF, a novel difference-driven channel-spatial state space model for multi-modal image fusion. Our approach leverages feature discrepancy maps between modalities to guide feature extraction, followed by a fusion process across both channel and spatial dimensions. In the channel dimension, a channel-exchange module enhances channel-wise interaction through cross-attention dual state space modeling, enabling adaptive feature reweighting. In the spatial dimension, a spatial-exchange module employs cross-modal state space scanning to achieve comprehensive spatial fusion. By efficiently capturing global dependencies while maintaining linear computational complexity, DIFF-MF effectively integrates complementary multi-modal features. Experimental results on the driving scenarios and low-altitude UAV datasets demonstrate that our method outperforms existing approaches in both visual quality and quantitative evaluation.&lt;/p&gt;</content:encoded></item><item><title>From Rays to Projections: Better Inputs for Feed-Forward View Synthesis</title><link>https://arxiv.org/abs/2601.05116v1</link><guid>http://arxiv.org/abs/2601.05116v1</guid><pubDate>Thu, 08 Jan 2026 17:03:44 +0000</pubDate><dc:creator>Zirui Wu</dc:creator><dc:creator>Zeren Jiang</dc:creator><dc:creator>Martin R. Oswald</dc:creator><dc:creator>Jie Song</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.
Published: 2026-01-08T17:03:44+00:00
Venue: arXiv
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zirui Wu; Zeren Jiang; Martin R. Oswald; Jie Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.&lt;/p&gt;</content:encoded></item><item><title>High-Level Adaptive Feature Enhancement and Attention Mask-Guided Aggregation for Visual Place Recognition</title><link>https://doi.org/10.1016/j.knosys.2026.115285</link><guid>10.1016/j.knosys.2026.115285</guid><pubDate>Sun, 11 Jan 2026 15:11:40 +0000</pubDate><dc:creator>Longhao Wang</dc:creator><dc:creator>Chaozhen Lan</dc:creator><dc:creator>Beibei Wu</dc:creator><dc:creator>Fushan Yao</dc:creator><dc:creator>Zijun Wei</dc:creator><dc:creator>Tian Gao</dc:creator><dc:creator>Hanyang Yu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115285</prism:doi><description>Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .
Published: 2026-01-11T15:11:40+00:00
Venue: Knowledge-Based Systems
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Longhao Wang; Chaozhen Lan; Beibei Wu; Fushan Yao; Zijun Wei; Tian Gao; Hanyang Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115285"&gt;10.1016/j.knosys.2026.115285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .&lt;/p&gt;</content:encoded></item><item><title>A Lightweight Dual-View Network for Sand-Dust Degraded Image Enhancement</title><link>https://doi.org/10.1016/j.knosys.2026.115308</link><guid>10.1016/j.knosys.2026.115308</guid><pubDate>Sat, 10 Jan 2026 00:24:28 +0000</pubDate><dc:creator>Guxue Gao</dc:creator><dc:creator>Yang Xiao</dc:creator><dc:creator>Xiaopeng Wen</dc:creator><dc:creator>Chunyun Sun</dc:creator><dc:creator>Yuanyuan Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115308</prism:doi><description>To address the issue that current supervised sand-dust image enhancement networks require large parameters and consume substantial computational resources and storage space, we propose a lightweight dual-view sand-dust image network. The proposed dual-view sharpening encoder and the original encoder are designed to provide complementary feature information, thereby maximizing the diversity of extracted features. At the encoder stage, a parameter-free feature modulation module is introduced and selectively embedded into the encoder branches to enhance feature extraction capability. In the decoding stage, a contextual attention integration module is designed to improve image contrast and enhance regional details by adaptively leveraging variance-based weighting and long-range pixel dependencies. These modules collectively strengthen feature representation and network reconstruction capacity while significantly reducing parameter overhead. Experimental results demonstrate that the proposed network can effectively enhance sand-dust images with fewer network parameters while ensuring performance. Additionally, the proposed algorithm generalizes well to haze and turbid underwater image enhancement. The processed images also improve the detection accuracy of targets such as vehicles and pedestrians, indicating its strong application potential.
Published: 2026-01-10T00:24:28+00:00
Venue: Knowledge-Based Systems
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guxue Gao; Yang Xiao; Xiaopeng Wen; Chunyun Sun; Yuanyuan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115308"&gt;10.1016/j.knosys.2026.115308&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;To address the issue that current supervised sand-dust image enhancement networks require large parameters and consume substantial computational resources and storage space, we propose a lightweight dual-view sand-dust image network. The proposed dual-view sharpening encoder and the original encoder are designed to provide complementary feature information, thereby maximizing the diversity of extracted features. At the encoder stage, a parameter-free feature modulation module is introduced and selectively embedded into the encoder branches to enhance feature extraction capability. In the decoding stage, a contextual attention integration module is designed to improve image contrast and enhance regional details by adaptively leveraging variance-based weighting and long-range pixel dependencies. These modules collectively strengthen feature representation and network reconstruction capacity while significantly reducing parameter overhead. Experimental results demonstrate that the proposed network can effectively enhance sand-dust images with fewer network parameters while ensuring performance. Additionally, the proposed algorithm generalizes well to haze and turbid underwater image enhancement. The processed images also improve the detection accuracy of targets such as vehicles and pedestrians, indicating its strong application potential.&lt;/p&gt;</content:encoded></item><item><title>Enhancing UAV small target detection: A balanced accuracy-efficiency algorithm with tiered feature focus</title><link>https://doi.org/10.1016/j.imavis.2026.105897</link><guid>10.1016/j.imavis.2026.105897</guid><pubDate>Sat, 10 Jan 2026 07:39:26 +0000</pubDate><dc:creator>Hanwei Guo</dc:creator><dc:creator>Shugang Liu</dc:creator><prism:publicationName>Image and Vision Computing</prism:publicationName><prism:doi>10.1016/j.imavis.2026.105897</prism:doi><description>Small target detection in unmanned aerial vehicle (UAV) imagery is crucial for both military and civilian applications. However, achieving a balance between detection performance, efficiency, and lightweight architecture remains challenging. This paper introduces TF-DEIM-DFINE, a tiered focused small target detection model designed specifically for UAV tasks.We propose the Convolutional Gated-Visual Mamba (CG-VIM) module to enhance global dependency capture and local detail extraction through long sequence modeling, along with the Half-Channel Single-Head Attention (HCSA) module for global modeling, which improves fine-grained representation while reducing computational redundancy. Additionally, our Tiered Focus-Feature Pyramid Networks (TF-FPN) improve the representational capability of high-frequency information in multi-scale features without significantly increasing computational overhead. Experimental results on the VisDrone dataset demonstrate a 4.7% improvement in AP M " role="presentation"&gt; M M and a 5.8% improvement in AP metrics, with a 37% reduction in parameter count and only a 6% increase in GFLOPs, maintaining unchanged FPS. These results highlight TF-DEIM-DFINE’s ability to improve detection accuracy while preserving a lightweight and efficient structure
Published: 2026-01-10T07:39:26+00:00
Venue: Image and Vision Computing
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanwei Guo; Shugang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Image and Vision Computing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.imavis.2026.105897"&gt;10.1016/j.imavis.2026.105897&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Small target detection in unmanned aerial vehicle (UAV) imagery is crucial for both military and civilian applications. However, achieving a balance between detection performance, efficiency, and lightweight architecture remains challenging. This paper introduces TF-DEIM-DFINE, a tiered focused small target detection model designed specifically for UAV tasks.We propose the Convolutional Gated-Visual Mamba (CG-VIM) module to enhance global dependency capture and local detail extraction through long sequence modeling, along with the Half-Channel Single-Head Attention (HCSA) module for global modeling, which improves fine-grained representation while reducing computational redundancy. Additionally, our Tiered Focus-Feature Pyramid Networks (TF-FPN) improve the representational capability of high-frequency information in multi-scale features without significantly increasing computational overhead. Experimental results on the VisDrone dataset demonstrate a 4.7% improvement in AP M &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; M M and a 5.8% improvement in AP metrics, with a 37% reduction in parameter count and only a 6% increase in GFLOPs, maintaining unchanged FPS. These results highlight TF-DEIM-DFINE’s ability to improve detection accuracy while preserving a lightweight and efficient structure&lt;/p&gt;</content:encoded></item><item><title>Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data</title><link>https://arxiv.org/abs/2601.04518v1</link><guid>http://arxiv.org/abs/2601.04518v1</guid><pubDate>Thu, 08 Jan 2026 02:32:12 +0000</pubDate><dc:creator>Shogo Nakayama</dc:creator><dc:creator>Masahiro Okuda</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/ITC-CSCC66376.2025.11137694</prism:doi><description>The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.
Published: 2026-01-08T02:32:12+00:00
Venue: arXiv
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shogo Nakayama; Masahiro Okuda&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/ITC-CSCC66376.2025.11137694"&gt;10.1109/ITC-CSCC66376.2025.11137694&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.&lt;/p&gt;</content:encoded></item><item><title>DDR-YOLO: An efficient and accurate object detection algorithm for distracted driving behaviors</title><link>https://doi.org/10.1016/j.eswa.2026.131170</link><guid>10.1016/j.eswa.2026.131170</guid><pubDate>Sun, 11 Jan 2026 15:12:00 +0000</pubDate><dc:creator>Qian Shen</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Yan Zhang</dc:creator><dc:creator>Yuxiang Zhang</dc:creator><dc:creator>Shihao Liu</dc:creator><dc:creator>Yi Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131170</prism:doi><description>In recent years, researchers have employed image classification and object detection methods to recognize distracted driving behaviors (DDB). Nevertheless, a comprehensive comparative analysis of these two methods within the realm of distracted driving behavior recognition (DDR) remains underexplored, resulting in most existing algorithms struggling to balance efficiency and accuracy. Therefore, based on a comparative analysis of these two methods, this paper proposes a novel DDR algorithm named DDR-YOLO inspired by YOLO11. Initially, this paper explores the method that performs better in DDR using 250,000 manually labeled images from the 100-Drivers dataset. Furthermore, the lightweight DDR-YOLO algorithm that achieves high accuracy while improving efficiency is introduced. To accurately capture both the local details and overall postural features of DDB, an innovative Neck structure called MHMS is designed along with a new feature extraction module referred to as SGHCB. To further optimize model efficiency, this paper presents an efficient spatial-reorganization upsampling (ESU) module and a novel Shared Convolution Detection head (SCDetection). ESU restructures feature information across channel and spatial dimensions through channel shuffle and spatial shift, with a significant reduction in computational complexity and loss of feature information. By introducing a dedicated detection head branch for huge targets and sharing convolutional parameters across all four branches, SCDetection achieves enhanced detection capability for oversized objects and greater computational efficiency. Additionally, an adaptive dynamic label assignment strategy is developed to enhance the discriminative ability of both high-confidence class predictions and precisely regressed bounding box coordinates, thereby improving recognition accuracy. Moreover, a novel channel pruning method termed DG-LAMP is proposed to significantly reduce the computational cost of the model. Then knowledge distillation is implemented to compensate for the accuracy loss. Experimental results reveal that on the 100-Drivers dataset, most existing lightweight classification algorithms underperform, achieving classification accuracies of only 70% to 80%, and fail to classify multiple DDB occurring at the same time. The DDR-YOLO achieves accuracies of 91.6% and 88.8% on RGB and near-infrared modalities with a computational cost of 1.2 GFLOPs, a parameter count of 0.45M and approximately 2000 FPS. In addition, generalization experiments conducted on the StateFarm dataset and our self-collected dataset achieve accuracies of 44.3% and 87.6%, respectively. Furthermore, the proposed algorithm is deployed on an NVIDIA Jetson Orin Nano 8GB platform for practical validation. In high-power mode, DDR-YOLO runs stably for extended periods with the FPS remaining at around 29, and the operating temperature stays within a normal range. These results confirm that the proposed algorithm shows outstanding performance in terms of model size and real-time capability while maintaining high accuracy.
Published: 2026-01-11T15:12:00+00:00
Venue: Expert Systems with Applications
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Shen; Lei Zhang; Yan Zhang; Yuxiang Zhang; Shihao Liu; Yi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131170"&gt;10.1016/j.eswa.2026.131170&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;In recent years, researchers have employed image classification and object detection methods to recognize distracted driving behaviors (DDB). Nevertheless, a comprehensive comparative analysis of these two methods within the realm of distracted driving behavior recognition (DDR) remains underexplored, resulting in most existing algorithms struggling to balance efficiency and accuracy. Therefore, based on a comparative analysis of these two methods, this paper proposes a novel DDR algorithm named DDR-YOLO inspired by YOLO11. Initially, this paper explores the method that performs better in DDR using 250,000 manually labeled images from the 100-Drivers dataset. Furthermore, the lightweight DDR-YOLO algorithm that achieves high accuracy while improving efficiency is introduced. To accurately capture both the local details and overall postural features of DDB, an innovative Neck structure called MHMS is designed along with a new feature extraction module referred to as SGHCB. To further optimize model efficiency, this paper presents an efficient spatial-reorganization upsampling (ESU) module and a novel Shared Convolution Detection head (SCDetection). ESU restructures feature information across channel and spatial dimensions through channel shuffle and spatial shift, with a significant reduction in computational complexity and loss of feature information. By introducing a dedicated detection head branch for huge targets and sharing convolutional parameters across all four branches, SCDetection achieves enhanced detection capability for oversized objects and greater computational efficiency. Additionally, an adaptive dynamic label assignment strategy is developed to enhance the discriminative ability of both high-confidence class predictions and precisely regressed bounding box coordinates, thereby improving recognition accuracy. Moreover, a novel channel pruning method termed DG-LAMP is proposed to significantly reduce the computational cost of the model. Then knowledge distillation is implemented to compensate for the accuracy loss. Experimental results reveal that on the 100-Drivers dataset, most existing lightweight classification algorithms underperform, achieving classification accuracies of only 70% to 80%, and fail to classify multiple DDB occurring at the same time. The DDR-YOLO achieves accuracies of 91.6% and 88.8% on RGB and near-infrared modalities with a computational cost of 1.2 GFLOPs, a parameter count of 0.45M and approximately 2000 FPS. In addition, generalization experiments conducted on the StateFarm dataset and our self-collected dataset achieve accuracies of 44.3% and 87.6%, respectively. Furthermore, the proposed algorithm is deployed on an NVIDIA Jetson Orin Nano 8GB platform for practical validation. In high-power mode, DDR-YOLO runs stably for extended periods with the FPS remaining at around 29, and the operating temperature stays within a normal range. These results confirm that the proposed algorithm shows outstanding performance in terms of model size and real-time capability while maintaining high accuracy.&lt;/p&gt;</content:encoded></item><item><title>STResNet &amp; STYOLO : A New Family of Compact Classification and Object Detection Models for MCUs</title><link>https://arxiv.org/abs/2601.05364v1</link><guid>http://arxiv.org/abs/2601.05364v1</guid><pubDate>Thu, 08 Jan 2026 20:39:50 +0000</pubDate><dc:creator>Sudhakar Sah</dc:creator><dc:creator>Ravish Kumar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.
Published: 2026-01-08T20:39:50+00:00
Venue: arXiv
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sudhakar Sah; Ravish Kumar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in lightweight neural networks have significantly improved the efficiency of deploying deep learning models on edge hardware. However, most existing architectures still trade accuracy for latency, which limits their applicability on microcontroller and neural processing unit based devices. In this work, we introduce two new model families, STResNet for image classification and STYOLO for object detection, jointly optimized for accuracy, efficiency, and memory footprint on resource constrained platforms. The proposed STResNet series, ranging from Nano to Tiny variants, achieves competitive ImageNet 1K accuracy within a four million parameter budget. Specifically, STResNetMilli attains 70.0 percent Top 1 accuracy with only three million parameters, outperforming MobileNetV1 and ShuffleNetV2 at comparable computational complexity. For object detection, STYOLOMicro and STYOLOMilli achieve 30.5 percent and 33.6 percent mean average precision, respectively, on the MS COCO dataset, surpassing YOLOv5n and YOLOX Nano in both accuracy and efficiency. Furthermore, when STResNetMilli is used as a backbone with the Ultralytics training environment.&lt;/p&gt;</content:encoded></item><item><title>Training-free and Zero-shot Regeneration for Hallucination Mitigation in MLLMs: Representation Understanding Perspective</title><link>https://doi.org/10.1016/j.eswa.2026.131102</link><guid>10.1016/j.eswa.2026.131102</guid><pubDate>Sat, 10 Jan 2026 00:24:28 +0000</pubDate><dc:creator>Dong Zhang</dc:creator><dc:creator>Yuansheng Ma</dc:creator><dc:creator>Linqin Li</dc:creator><dc:creator>Shoushan Li</dc:creator><dc:creator>Erik Cambria</dc:creator><dc:creator>Guodong Zhou</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131102</prism:doi><description>Hallucinations in multimodal large language models (MLLMs) are urgent problems to be solved in the new era of artificial general intelligence (AGI). Compared with traditional large language models (LLMs), besides handling language understanding and modeling, we also need to consider the detection and position determination of objects in vision. Therefore, to tackle the hallucination issues, the existing studies attempt to employ few-shot learning on the following perspectives: 1) limit the length of the generated response, 2) iteratively generate multiple candidates or select from multiple candidates via beam search, 3) locally edit the possible parts of primary response, and 4) leverage external knowledge to augment the generation capability. To address the above potential weaknesses, this paper proposes a multimodal training-free and zero-shot regeneration approach by obtain various multimodal evidences and globally improving the raw response to alleviate hallucinations in MLLMs ( Mtzr ). Specifically, we first extract the entity-level evidences by object-based pre-trained models with in-context learning. Then, we mine the attribute-level evidences inside each entity and cross different entities with heterogeneous in-context learning based on both uni- and multimodal pre-trained models. Finally, towards the obtained multimodal evidences, we regenerate the response with augmented context by residually connecting both the input text and image. For better understanding, we provide theoretical explanations with universal approximation to support why our approach can bring about smaller hallucination. Detailed experimental results and extensive analysis demonstrate that our approach is very suitable for mitigating hallucination in MLLMs.
Published: 2026-01-10T00:24:28+00:00
Venue: Expert Systems with Applications
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dong Zhang; Yuansheng Ma; Linqin Li; Shoushan Li; Erik Cambria; Guodong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131102"&gt;10.1016/j.eswa.2026.131102&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Hallucinations in multimodal large language models (MLLMs) are urgent problems to be solved in the new era of artificial general intelligence (AGI). Compared with traditional large language models (LLMs), besides handling language understanding and modeling, we also need to consider the detection and position determination of objects in vision. Therefore, to tackle the hallucination issues, the existing studies attempt to employ few-shot learning on the following perspectives: 1) limit the length of the generated response, 2) iteratively generate multiple candidates or select from multiple candidates via beam search, 3) locally edit the possible parts of primary response, and 4) leverage external knowledge to augment the generation capability. To address the above potential weaknesses, this paper proposes a multimodal training-free and zero-shot regeneration approach by obtain various multimodal evidences and globally improving the raw response to alleviate hallucinations in MLLMs ( Mtzr ). Specifically, we first extract the entity-level evidences by object-based pre-trained models with in-context learning. Then, we mine the attribute-level evidences inside each entity and cross different entities with heterogeneous in-context learning based on both uni- and multimodal pre-trained models. Finally, towards the obtained multimodal evidences, we regenerate the response with augmented context by residually connecting both the input text and image. For better understanding, we provide theoretical explanations with universal approximation to support why our approach can bring about smaller hallucination. Detailed experimental results and extensive analysis demonstrate that our approach is very suitable for mitigating hallucination in MLLMs.&lt;/p&gt;</content:encoded></item><item><title>Detector-Augmented SAMURAI for Long-Duration Drone Tracking</title><link>https://arxiv.org/abs/2601.04798v1</link><guid>http://arxiv.org/abs/2601.04798v1</guid><pubDate>Thu, 08 Jan 2026 10:27:05 +0000</pubDate><dc:creator>Tamara R. Lenhard</dc:creator><dc:creator>Andreas Weinmann</dc:creator><dc:creator>Hichem Snoussi</dc:creator><dc:creator>Tobias Koch</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.
Published: 2026-01-08T10:27:05+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tamara R. Lenhard; Andreas Weinmann; Hichem Snoussi; Tobias Koch&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI&amp;#x27;s potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI&amp;#x27;s zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.&lt;/p&gt;</content:encoded></item><item><title>One Language-Free Foundation Model Is Enough for Universal Vision Anomaly Detection</title><link>https://arxiv.org/abs/2601.05552v1</link><guid>http://arxiv.org/abs/2601.05552v1</guid><pubDate>Fri, 09 Jan 2026 06:05:18 +0000</pubDate><dc:creator>Bin-Bin Gao</dc:creator><dc:creator>Chengjie Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.
Published: 2026-01-09T06:05:18+00:00
Venue: arXiv
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bin-Bin Gao; Chengjie Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Universal visual anomaly detection (AD) aims to identify anomaly images and segment anomaly regions towards open and dynamic scenarios, following zero- and few-shot paradigms without any dataset-specific fine-tuning. We have witnessed significant progress in widely use of visual-language foundational models in recent approaches. However, current methods often struggle with complex prompt engineering, elaborate adaptation modules, and challenging training strategies, ultimately limiting their flexibility and generality. To address these issues, this paper rethinks the fundamental mechanism behind visual-language models for AD and presents an embarrassingly simple, general, and effective framework for Universal vision Anomaly Detection (UniADet). Specifically, we first find language encoder is used to derive decision weights for anomaly classification and segmentation, and then demonstrate that it is unnecessary for universal AD. Second, we propose an embarrassingly simple method to completely decouple classification and segmentation, and decouple cross-level features, i.e., learning independent weights for different tasks and hierarchical features. UniADet is highly simple (learning only decoupled weights), parameter-efficient (only 0.002M learnable parameters), general (adapting a variety of foundation models), and effective (surpassing state-of-the-art zero-/few-shot by a large margin and even full-shot AD methods for the first time) on 14 real-world AD benchmarks covering both industrial and medical domains. We will make the code and model of UniADet available at https://github.com/gaobb/UniADet.&lt;/p&gt;</content:encoded></item></channel></rss>