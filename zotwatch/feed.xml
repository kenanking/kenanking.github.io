<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 19 Dec 2025 02:43:43 +0000</lastBuildDate><item><title>Boosting Multi-Modal Large Language Model With Enhanced Visual Features</title><link>https://doi.org/10.1109/tpami.2025.3644851</link><guid>10.1109/tpami.2025.3644851</guid><pubDate>Wed, 17 Dec 2025 18:46:02 +0000</pubDate><dc:creator>Yiwei Ma</dc:creator><dc:creator>Weihuang Lin</dc:creator><dc:creator>Zhibin Wang</dc:creator><dc:creator>Jiayi Ji</dc:creator><dc:creator>Xiaoshuai Sun</dc:creator><dc:creator>Chia-Wen Lin</dc:creator><dc:creator>Rongrong Ji</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644851</prism:doi><description>Recent advancements in computer vision (CV) and large language models (LLMs) have spurred significant interest in multi-modal large language models (MLLMs), which aim to integrate visual and textual modalities for enhanced understanding and generation tasks. While much of the existing research focuses on optimizing projectors and LLMs to improve MLLM performance, a critical question remains underexplored: Has the full potential of visual features in MLLMs been realized? To address this question, we identify two key limitations in current MLLM architectures and propose vMLLM, a vision-enhanced MLLM designed to fully leverage the capabilities of visual features. vMLLM introduces two novel components: the Multi-level Aggregation Module (MAM) and the Intra- and inter-modal Enhancement Module (IEM). The MAM aggregates multi-layer features from the vision encoder, capturing both high-level semantic information and low-level spatial details, thereby enriching the visual representation. The IEM enhances visual features through intra- and inter-modal interactions, effectively suppressing irrelevant information while amplifying task-relevant features, leading to more robust multimodal understanding. We conduct extensive experiments on multiple benchmarks, evaluating vMLLM across diverse settings, including different vision encoders, training dataset scales, and varying sizes of LLMs. Our results demonstrate that vMLLM consistently achieves significant performance improvements, validating its effectiveness in harnessing the potential of visual features. These findings highlight the importance of optimizing visual feature extraction and interaction mechanisms in MLLMs, paving the way for more advanced multimodal AI systems. To promote reproducibility and further research, we have made the code and pre-trained models publicly available on GitHub: https://github.com/xmu-xiaoma666/vMLLM.
Published: 2025-12-17T18:46:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.841 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiwei Ma; Weihuang Lin; Zhibin Wang; Jiayi Ji; Xiaoshuai Sun; Chia-Wen Lin; Rongrong Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644851"&gt;10.1109/tpami.2025.3644851&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.841 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in computer vision (CV) and large language models (LLMs) have spurred significant interest in multi-modal large language models (MLLMs), which aim to integrate visual and textual modalities for enhanced understanding and generation tasks. While much of the existing research focuses on optimizing projectors and LLMs to improve MLLM performance, a critical question remains underexplored: Has the full potential of visual features in MLLMs been realized? To address this question, we identify two key limitations in current MLLM architectures and propose vMLLM, a vision-enhanced MLLM designed to fully leverage the capabilities of visual features. vMLLM introduces two novel components: the Multi-level Aggregation Module (MAM) and the Intra- and inter-modal Enhancement Module (IEM). The MAM aggregates multi-layer features from the vision encoder, capturing both high-level semantic information and low-level spatial details, thereby enriching the visual representation. The IEM enhances visual features through intra- and inter-modal interactions, effectively suppressing irrelevant information while amplifying task-relevant features, leading to more robust multimodal understanding. We conduct extensive experiments on multiple benchmarks, evaluating vMLLM across diverse settings, including different vision encoders, training dataset scales, and varying sizes of LLMs. Our results demonstrate that vMLLM consistently achieves significant performance improvements, validating its effectiveness in harnessing the potential of visual features. These findings highlight the importance of optimizing visual feature extraction and interaction mechanisms in MLLMs, paving the way for more advanced multimodal AI systems. To promote reproducibility and further research, we have made the code and pre-trained models publicly available on GitHub: https://github.com/xmu-xiaoma666/vMLLM.&lt;/p&gt;</content:encoded></item><item><title>SARMAE: Masked Autoencoder for SAR Representation Learning</title><link>https://arxiv.org/abs/2512.16635v1</link><guid>http://arxiv.org/abs/2512.16635v1</guid><pubDate>Thu, 18 Dec 2025 15:10:19 +0000</pubDate><dc:creator>Danxu Liu</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Hebaixu Wang</dc:creator><dc:creator>Haoyang Chen</dc:creator><dc:creator>Wentao Jiang</dc:creator><dc:creator>Yilin Cheng</dc:creator><dc:creator>Haonan Guo</dc:creator><dc:creator>Wei Cui</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.
Published: 2025-12-18T15:10:19+00:00
Venue: arXiv
Score: 0.834 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Danxu Liu; Di Wang; Hebaixu Wang; Haoyang Chen; Wentao Jiang; Yilin Cheng; Haonan Guo; Wei Cui; Jing Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.834 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.&lt;/p&gt;</content:encoded></item><item><title>$\ell _{0}$-Regularized Sparse Coding-based Interpretable Network for Multi-Modal Image Fusion</title><link>https://doi.org/10.1109/tpami.2025.3643898</link><guid>10.1109/tpami.2025.3643898</guid><pubDate>Wed, 17 Dec 2025 18:46:02 +0000</pubDate><dc:creator>Gargi Panda</dc:creator><dc:creator>Soumitra Kundu</dc:creator><dc:creator>Saumik Bhattacharya</dc:creator><dc:creator>Aurobinda Routray</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643898</prism:doi><description>Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an \ell _{0} \ell _{0} -regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the \ell _{0} \ell _{0} -regularized CSC problem, we design a learnable \ell _{0} \ell _{0} -regularized sparse coding (LZSC) block in a principled manner through deep unfolding. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an \ell _{0} \ell _{0} -regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet's training. Extensive experiments show that FNet achieves high-quality fusion results across eight different MMIF datasets. Furthermore, we show that FNet enhances downstream object detection 0, 0, 0 and semantic segmentation in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. Link for code and models: https://github.com/ggpp132/code.
Published: 2025-12-17T18:46:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gargi Panda; Soumitra Kundu; Saumik Bhattacharya; Aurobinda Routray&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643898"&gt;10.1109/tpami.2025.3643898&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal image fusion (MMIF) enhances the information content of the fused image by combining the unique as well as common features obtained from different modality sensor images, improving visualization, object detection, and many more tasks. In this work, we introduce an interpretable network for the MMIF task, named FNet, based on an \ell _{0} \ell _{0} -regularized multi-modal convolutional sparse coding (MCSC) model. Specifically, for solving the \ell _{0} \ell _{0} -regularized CSC problem, we design a learnable \ell _{0} \ell _{0} -regularized sparse coding (LZSC) block in a principled manner through deep unfolding. Given different modality source images, FNet first separates the unique and common features from them using the LZSC block and then these features are combined to generate the final fused image. Additionally, we propose an \ell _{0} \ell _{0} -regularized MCSC model for the inverse fusion process. Based on this model, we introduce an interpretable inverse fusion network named IFNet, which is utilized during FNet&amp;#x27;s training. Extensive experiments show that FNet achieves high-quality fusion results across eight different MMIF datasets. Furthermore, we show that FNet enhances downstream object detection 0, 0, 0 and semantic segmentation in visible-thermal image pairs. We have also visualized the intermediate results of FNet, which demonstrates the good interpretability of our network. Link for code and models: https://github.com/ggpp132/code.&lt;/p&gt;</content:encoded></item><item><title>Two-Stage SAR Image Generation Based on Attribute Feature Decoupling</title><link>https://doi.org/10.1109/lgrs.2025.3645620</link><guid>10.1109/lgrs.2025.3645620</guid><pubDate>Thu, 18 Dec 2025 18:36:05 +0000</pubDate><dc:creator>Rubo Jin</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Jianda Cheng</dc:creator><dc:creator>Hui Fan</dc:creator><dc:creator>Jiyuan Liu</dc:creator><dc:creator>Hongqi Fan</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3645620</prism:doi><description>Training of synthetic aperture radar (SAR) target detection and recognition methods based on deep learning heavily relies on a large amount of data. As one of the significant approaches to address the scarcity of SAR data, SAR image intelligent generation methods have witnessed rapid development. However, these methods often require many data samples for learning and are prone to deviating from the physical scattering characteristics. To address these issues, this paper proposes a two-stage SAR image generation method based on attribute feature decoupling within a generative adversarial network (GAN) architecture. In the first stage, the original SAR target image undergoes feature extraction and reconstruction, yielding generated images highly similar to real images. The attribute features decoupled during this process correlate with the scattering characteristics of SAR target, providing guiding information for generating target images in the second stage. In the second stage, by applying perturbations to specific dimensions of the decoupled features, we can reconstruct target images with altered attributes, achieving diverse data augmentation. Multi-task discrimination based on pixel intensity, authenticity, and feature distance differences enhances the quality of generated images across multiple levels. The decoupled representation-driven generation paradigm simplifies the network’s mapping learning task through task decomposition, diminishing the dependency on the volume of data. The experimental results demonstrate that the generated images possess higher quality and superior application performance, with an improvement of 5.23% in recognition accuracy.
Published: 2025-12-18T18:36:05+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rubo Jin; Wei Wang; Jianda Cheng; Hui Fan; Jiyuan Liu; Hongqi Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3645620"&gt;10.1109/lgrs.2025.3645620&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Training of synthetic aperture radar (SAR) target detection and recognition methods based on deep learning heavily relies on a large amount of data. As one of the significant approaches to address the scarcity of SAR data, SAR image intelligent generation methods have witnessed rapid development. However, these methods often require many data samples for learning and are prone to deviating from the physical scattering characteristics. To address these issues, this paper proposes a two-stage SAR image generation method based on attribute feature decoupling within a generative adversarial network (GAN) architecture. In the first stage, the original SAR target image undergoes feature extraction and reconstruction, yielding generated images highly similar to real images. The attribute features decoupled during this process correlate with the scattering characteristics of SAR target, providing guiding information for generating target images in the second stage. In the second stage, by applying perturbations to specific dimensions of the decoupled features, we can reconstruct target images with altered attributes, achieving diverse data augmentation. Multi-task discrimination based on pixel intensity, authenticity, and feature distance differences enhances the quality of generated images across multiple levels. The decoupled representation-driven generation paradigm simplifies the network’s mapping learning task through task decomposition, diminishing the dependency on the volume of data. The experimental results demonstrate that the generated images possess higher quality and superior application performance, with an improvement of 5.23% in recognition accuracy.&lt;/p&gt;</content:encoded></item><item><title>SCG-FSOD: Semantic Correlation-Guided Few-shot Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3645045</link><guid>10.1109/tgrs.2025.3645045</guid><pubDate>Wed, 17 Dec 2025 18:46:07 +0000</pubDate><dc:creator>Hengchao Hu</dc:creator><dc:creator>Aobo Li</dc:creator><dc:creator>Jinjian Wu</dc:creator><dc:creator>Jie Feng</dc:creator><dc:creator>Yaoqiang Jia</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3645045</prism:doi><description>Few-shot object detection (FSOD) in remote sensing imagery aims to achieve accurate detection of novel object categories with limited training samples. However, current mainstream transfer learning based methods are frequently constrained by two major challenges. Firstly, due to the scarcity of novel class samples and the bias of pre-trained models towards base classes, novel classes may rely overly on base class knowledge to construct feature representations, causing novel class features to be easily confused with similar base classes. Secondly, the inter-class similarity and intra-class diversity in remote sensing images can further exacerbate classification confusion. To tackle the above problems, we propose a semantic correlation-guided method for FSOD (SCG-FSOD) in remote sensing images. Specifically, we design an inter-class semantic correlation transfer (ISCT) module to fully explore the semantic correlations between base and novel classes, employing knowledge distillation for cross-class correlation transfer. This module effectively mitigates base-class bias while enhancing the discriminative power of novel class features. Furthermore, we propose a semantic correlation-driven supervised contrastive learning (SSCL) module, which employs semantic correlation priors to weight negative sample pairs in supervised contrastive learning. By imposing stronger separation constraints on negative pairs with high inter-class similarity, this module significantly alleviates feature confusion in remote sensing images. Extensive experiments conducted on two public benchmark datasets (DIOR and NWPU VHR-10.v2) demonstrate the effectiveness of our proposed method, which achieves competitive performance compared with several state-of-the-art approaches.
Published: 2025-12-17T18:46:07+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengchao Hu; Aobo Li; Jinjian Wu; Jie Feng; Yaoqiang Jia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3645045"&gt;10.1109/tgrs.2025.3645045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) in remote sensing imagery aims to achieve accurate detection of novel object categories with limited training samples. However, current mainstream transfer learning based methods are frequently constrained by two major challenges. Firstly, due to the scarcity of novel class samples and the bias of pre-trained models towards base classes, novel classes may rely overly on base class knowledge to construct feature representations, causing novel class features to be easily confused with similar base classes. Secondly, the inter-class similarity and intra-class diversity in remote sensing images can further exacerbate classification confusion. To tackle the above problems, we propose a semantic correlation-guided method for FSOD (SCG-FSOD) in remote sensing images. Specifically, we design an inter-class semantic correlation transfer (ISCT) module to fully explore the semantic correlations between base and novel classes, employing knowledge distillation for cross-class correlation transfer. This module effectively mitigates base-class bias while enhancing the discriminative power of novel class features. Furthermore, we propose a semantic correlation-driven supervised contrastive learning (SSCL) module, which employs semantic correlation priors to weight negative sample pairs in supervised contrastive learning. By imposing stronger separation constraints on negative pairs with high inter-class similarity, this module significantly alleviates feature confusion in remote sensing images. Extensive experiments conducted on two public benchmark datasets (DIOR and NWPU VHR-10.v2) demonstrate the effectiveness of our proposed method, which achieves competitive performance compared with several state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>Handwritten Text Recognition: A Survey</title><link>https://doi.org/10.1109/tpami.2025.3646002</link><guid>10.1109/tpami.2025.3646002</guid><pubDate>Thu, 18 Dec 2025 18:33:13 +0000</pubDate><dc:creator>Carlos Garrido-Munoz</dc:creator><dc:creator>Antonio Rios-Vila</dc:creator><dc:creator>Jorge Calvo-Zaragoza</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646002</prism:doi><description>Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) up to line-level, encompassing word and line recognition, and (2) beyond line-level, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.
Published: 2025-12-18T18:33:13+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Carlos Garrido-Munoz; Antonio Rios-Vila; Jorge Calvo-Zaragoza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646002"&gt;10.1109/tpami.2025.3646002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) up to line-level, encompassing word and line recognition, and (2) beyond line-level, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.&lt;/p&gt;</content:encoded></item><item><title>A psychometric framework for evaluating and shaping personality traits in large language models</title><link>https://doi.org/10.1038/s42256-025-01115-6</link><guid>10.1038/s42256-025-01115-6</guid><pubDate>Thu, 18 Dec 2025 10:02:35 +0000</pubDate><dc:creator>Gregory Serapio-García</dc:creator><dc:creator>Mustafa Safdari</dc:creator><dc:creator>Clément Crepy</dc:creator><dc:creator>Luning Sun</dc:creator><dc:creator>Stephen Fitz</dc:creator><dc:creator>Peter Romero</dc:creator><dc:creator>Marwa Abdulhai</dc:creator><dc:creator>Aleksandra Faust</dc:creator><dc:creator>Maja Matarić</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01115-6</prism:doi><description>The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public worldwide, the synthetic personality traits embedded in these models by virtue of training on large amounts of human data are becoming increasingly important to evaluate. The style in which LLMs respond can mimic different human personality traits. Here, as these patterns can be a key factor determining the effectiveness of communication, we present a comprehensive psychometric methodology for administering and validating personality tests on widely used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found that: personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction-fine-tuned models; and personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible artificial intelligence. Serapio-García, Safdari and colleagues develop a method based on psychometric tests to measure and validate personality-like traits in LLMs. Large, instruction-tuned models give reliable personality measurement results, and specific personality profiles can be mimicked in downstream tasks.
Published: 2025-12-18T10:02:35+00:00
Venue: Nature Machine Intelligence
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gregory Serapio-García; Mustafa Safdari; Clément Crepy; Luning Sun; Stephen Fitz; Peter Romero; Marwa Abdulhai; Aleksandra Faust; Maja Matarić&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01115-6"&gt;10.1038/s42256-025-01115-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public worldwide, the synthetic personality traits embedded in these models by virtue of training on large amounts of human data are becoming increasingly important to evaluate. The style in which LLMs respond can mimic different human personality traits. Here, as these patterns can be a key factor determining the effectiveness of communication, we present a comprehensive psychometric methodology for administering and validating personality tests on widely used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method to 18 LLMs, we found that: personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction-fine-tuned models; and personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss the application and ethical implications of the measurement and shaping method, in particular regarding responsible artificial intelligence. Serapio-García, Safdari and colleagues develop a method based on psychometric tests to measure and validate personality-like traits in LLMs. Large, instruction-tuned models give reliable personality measurement results, and specific personality profiles can be mimicked in downstream tasks.&lt;/p&gt;</content:encoded></item><item><title>Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution</title><link>https://doi.org/10.1109/tip.2025.3643146</link><guid>10.1109/tip.2025.3643146</guid><pubDate>Thu, 18 Dec 2025 18:35:49 +0000</pubDate><dc:creator>Junbo Qiao</dc:creator><dc:creator>Jincheng Liao</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Yulun Zhang</dc:creator><dc:creator>Yong Guo</dc:creator><dc:creator>Jiao Xie</dc:creator><dc:creator>Jie Hu</dc:creator><dc:creator>Shaohui Lin</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643146</prism:doi><description>Despite Transformers have achieved significant success in low-level vision tasks, they are constrained by computing self-attention with a quadratic complexity and limited-size windows. This limitation results in a lack of global receptive field across the entire image. Recently, State Space Models (SSMs) have gained widespread attention due to their global receptive field and linear complexity with respect to input length. However, integrating SSMs into low-level vision tasks presents two major challenges: (1) Relationship degradation of long-range tokens with a long-range forgetting problem by encoding pixel-by-pixel high-resolution images. (2) Significant redundancy in the existing multi-direction scanning strategy. To this end, we propose Hi-Mamba for image super-resolution (SR) to address these challenges, which unfolds the image with only a single scan. Specifically, the Global Hierarchical Mamba Block (GHMB) enables token interactions across the entire image, providing a global receptive field while leveraging a multi-scale structure to facilitate long-range dependency learning. Additionally, the Direction Alternation Module (DAM) adjusts the scanning patterns of GHMB across different layers to enhance spatial relationship modeling. Extensive experiments demonstrate that our Hi-Mamba achieves 0.2-0.27dB PSNR gains on the Urban100 dataset across different scaling factors compared to the state-of-the-art MambaIRv2 for SR. Moreover, our lightweight Hi-Mamba also outperforms lightweight SRFormer by 0.39dB PSNR for ×2 SR.
Published: 2025-12-18T18:35:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junbo Qiao; Jincheng Liao; Wei Li; Yulun Zhang; Yong Guo; Jiao Xie; Jie Hu; Shaohui Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643146"&gt;10.1109/tip.2025.3643146&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Despite Transformers have achieved significant success in low-level vision tasks, they are constrained by computing self-attention with a quadratic complexity and limited-size windows. This limitation results in a lack of global receptive field across the entire image. Recently, State Space Models (SSMs) have gained widespread attention due to their global receptive field and linear complexity with respect to input length. However, integrating SSMs into low-level vision tasks presents two major challenges: (1) Relationship degradation of long-range tokens with a long-range forgetting problem by encoding pixel-by-pixel high-resolution images. (2) Significant redundancy in the existing multi-direction scanning strategy. To this end, we propose Hi-Mamba for image super-resolution (SR) to address these challenges, which unfolds the image with only a single scan. Specifically, the Global Hierarchical Mamba Block (GHMB) enables token interactions across the entire image, providing a global receptive field while leveraging a multi-scale structure to facilitate long-range dependency learning. Additionally, the Direction Alternation Module (DAM) adjusts the scanning patterns of GHMB across different layers to enhance spatial relationship modeling. Extensive experiments demonstrate that our Hi-Mamba achieves 0.2-0.27dB PSNR gains on the Urban100 dataset across different scaling factors compared to the state-of-the-art MambaIRv2 for SR. Moreover, our lightweight Hi-Mamba also outperforms lightweight SRFormer by 0.39dB PSNR for ×2 SR.&lt;/p&gt;</content:encoded></item><item><title>Incomplete Modalities Restoration via Hierarchical Adaptation for Robust Multimodal Segmentation</title><link>https://doi.org/10.1109/tip.2025.3642612</link><guid>10.1109/tip.2025.3642612</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Yujia Sun</dc:creator><dc:creator>Weisheng Dong</dc:creator><dc:creator>Peng Wu</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Tao Huang</dc:creator><dc:creator>Xin Li</dc:creator><dc:creator>Guangming Shi</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3642612</prism:doi><description>Multimodal semantic segmentation has significantly advanced the field of semantic segmentation by integrating data from multiple sources. However, this task often encounters missing modality scenarios due to challenges such as sensor failures or data transmission errors, which can result in substantial performance degradation. Existing approaches to addressing missing modalities predominantly involve training separate models tailored to specific missing scenarios, typically requiring considerable computational resources. In this paper, we propose a Hierarchical Adaptation framework to Restore Missing Modalities for Multimodal segmentation (HARM3), which enables frozen pretrained multimodal models to be directly applied to missing-modality semantic segmentation tasks with minimal parameter updates. Central to HARM3 is a text-instructed missing modality prompt module, which learns multimodal semantic knowledge by utilizing available modalities and textual instructions to generate prompts for the missing modalities. By incorporating a small set of trainable parameters, this module effectively facilitates knowledge transfer between high-resource domains and low-resource domains where missing modalities are more prevalent. Besides, to further enhance the model’s robustness and adaptability, we introduce adaptive perturbation training and an affine modality adapter. Extensive experimental results demonstrate the effectiveness and robustness of HARM3 across a variety of missing modality scenarios.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yujia Sun; Weisheng Dong; Peng Wu; Mingtao Feng; Tao Huang; Xin Li; Guangming Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3642612"&gt;10.1109/tip.2025.3642612&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal semantic segmentation has significantly advanced the field of semantic segmentation by integrating data from multiple sources. However, this task often encounters missing modality scenarios due to challenges such as sensor failures or data transmission errors, which can result in substantial performance degradation. Existing approaches to addressing missing modalities predominantly involve training separate models tailored to specific missing scenarios, typically requiring considerable computational resources. In this paper, we propose a Hierarchical Adaptation framework to Restore Missing Modalities for Multimodal segmentation (HARM3), which enables frozen pretrained multimodal models to be directly applied to missing-modality semantic segmentation tasks with minimal parameter updates. Central to HARM3 is a text-instructed missing modality prompt module, which learns multimodal semantic knowledge by utilizing available modalities and textual instructions to generate prompts for the missing modalities. By incorporating a small set of trainable parameters, this module effectively facilitates knowledge transfer between high-resource domains and low-resource domains where missing modalities are more prevalent. Besides, to further enhance the model’s robustness and adaptability, we introduce adaptive perturbation training and an affine modality adapter. Extensive experimental results demonstrate the effectiveness and robustness of HARM3 across a variety of missing modality scenarios.&lt;/p&gt;</content:encoded></item><item><title>UniqueSplat: View-conditioned 3D Gaussian Splatting for Generalizable 3D Reconstruction</title><link>https://doi.org/10.1109/tip.2025.3642574</link><guid>10.1109/tip.2025.3642574</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Haixu Song</dc:creator><dc:creator>Xiaoke Yang</dc:creator><dc:creator>Shengjun Zhang</dc:creator><dc:creator>Jiwen Lu</dc:creator><dc:creator>Yueqi Duan</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3642574</prism:doi><description>In this paper, we propose UniqueSplat, a view-conditioned feed-forward 3D Gaussian Splatting model to reconstruct customized 3D radiance fields for each view query. Existing feed-forward methods such as pixelSplat and MVS-plat aim to generate fixed Gaussians across all views of each scene by minimizing the error between rendered views and ground-truth images. However, such fixed Gaussians generally render images from all views and lack the ability to adapt to specific viewpoints, as they do not incorporate target view information when predicting Gaussians. To address this, our UniqueSplat learns the view-conditioned information as a prior and incorporates this knowledge into network parameters, so that Gaussians are dynamically adjusted in accordance with different views. Specifically, we propose a two-branch view-conditioned hyperNetwork to simultaneously learn view-agnostic embeddings and view-specific knowledge, which not only explores the shareable knowledge from various views, but also adapts the model to specific views at test time. Extensive experiments on widely-used datasets including RealEstate10K, ACID and DTU demonstrate the superiority of UniqueSplat over the state-of-the-art methods. Moreover, UniqueSplat encouragingly outperforms existing methods in cross-dataset evaluation, showing its notable generalization ability.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haixu Song; Xiaoke Yang; Shengjun Zhang; Jiwen Lu; Yueqi Duan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3642574"&gt;10.1109/tip.2025.3642574&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we propose UniqueSplat, a view-conditioned feed-forward 3D Gaussian Splatting model to reconstruct customized 3D radiance fields for each view query. Existing feed-forward methods such as pixelSplat and MVS-plat aim to generate fixed Gaussians across all views of each scene by minimizing the error between rendered views and ground-truth images. However, such fixed Gaussians generally render images from all views and lack the ability to adapt to specific viewpoints, as they do not incorporate target view information when predicting Gaussians. To address this, our UniqueSplat learns the view-conditioned information as a prior and incorporates this knowledge into network parameters, so that Gaussians are dynamically adjusted in accordance with different views. Specifically, we propose a two-branch view-conditioned hyperNetwork to simultaneously learn view-agnostic embeddings and view-specific knowledge, which not only explores the shareable knowledge from various views, but also adapts the model to specific views at test time. Extensive experiments on widely-used datasets including RealEstate10K, ACID and DTU demonstrate the superiority of UniqueSplat over the state-of-the-art methods. Moreover, UniqueSplat encouragingly outperforms existing methods in cross-dataset evaluation, showing its notable generalization ability.&lt;/p&gt;</content:encoded></item><item><title>AFS-Net: An Anchor-Free Domain Adaptation Network for SAR Ship Detection</title><link>https://doi.org/10.1109/lgrs.2025.3646070</link><guid>10.1109/lgrs.2025.3646070</guid><pubDate>Thu, 18 Dec 2025 18:36:05 +0000</pubDate><dc:creator>Mengze Wang</dc:creator><dc:creator>Bin Pan</dc:creator><dc:creator>Haiyang Xu</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3646070</prism:doi><description>Object detection in synthetic aperture radar (SAR) imagery is crucial for maritime surveillance, yet it is often limited by the scarcity of large-scale annotated data. To address this challenge, this work proposes AFS-Net, a novel anchor-free domain adaptation framework for SAR ship detection. AFS-Net effectively transfers knowledge from easily annotated optical source-domain images to unlabeled SAR target-domain images, mitigating the need for expensive SAR data annotation. Unlike conventional anchor-based detectors that struggle with the diverse scales and shapes of ships, our anchor-free approach, built upon CenterNet, provides a more streamlined and accurate localization paradigm. The core of AFS-Net consists of two innovative alignment modules designed to counteract domain shifts: the Keypoint Alignment Module (KAM) and the Box Alignment Module (BAM). Specifically, KAM aligns the structural distribution of heatmap responses across domains, forcing the target domain features to mimic the ideal Gaussian-like keypoint structures from the source domain, thereby enhancing keypoint localization robustness. Concurrently, BAM aligns the center offset and size regressions to ensure the geometric consistency of the predicted bounding boxes. Experimental results show that the proposed AFS-Net outperforms existing domain adaptation object detection frameworks on SAR ship detection tasks, achieving mAP improvements of 12.10% to 33.37% over the baseline on cross-domain settings.
Published: 2025-12-18T18:36:05+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengze Wang; Bin Pan; Haiyang Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3646070"&gt;10.1109/lgrs.2025.3646070&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection in synthetic aperture radar (SAR) imagery is crucial for maritime surveillance, yet it is often limited by the scarcity of large-scale annotated data. To address this challenge, this work proposes AFS-Net, a novel anchor-free domain adaptation framework for SAR ship detection. AFS-Net effectively transfers knowledge from easily annotated optical source-domain images to unlabeled SAR target-domain images, mitigating the need for expensive SAR data annotation. Unlike conventional anchor-based detectors that struggle with the diverse scales and shapes of ships, our anchor-free approach, built upon CenterNet, provides a more streamlined and accurate localization paradigm. The core of AFS-Net consists of two innovative alignment modules designed to counteract domain shifts: the Keypoint Alignment Module (KAM) and the Box Alignment Module (BAM). Specifically, KAM aligns the structural distribution of heatmap responses across domains, forcing the target domain features to mimic the ideal Gaussian-like keypoint structures from the source domain, thereby enhancing keypoint localization robustness. Concurrently, BAM aligns the center offset and size regressions to ensure the geometric consistency of the predicted bounding boxes. Experimental results show that the proposed AFS-Net outperforms existing domain adaptation object detection frameworks on SAR ship detection tasks, achieving mAP improvements of 12.10% to 33.37% over the baseline on cross-domain settings.&lt;/p&gt;</content:encoded></item><item><title>CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification</title><link>https://doi.org/10.1109/tpami.2025.3644853</link><guid>10.1109/tpami.2025.3644853</guid><pubDate>Wed, 17 Dec 2025 18:46:02 +0000</pubDate><dc:creator>Yuan Gong</dc:creator><dc:creator>Sameer Khurana</dc:creator><dc:creator>Andrew Rouditchenko</dc:creator><dc:creator>James Glass</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644853</prism:doi><description>Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.
Published: 2025-12-17T18:46:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuan Gong; Sameer Khurana; Andrew Rouditchenko; James Glass&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644853"&gt;10.1109/tpami.2025.3644853&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Audio classification is an active research area with a wide range of applications. Over the past decade, convolutional neural networks (CNNs) have been the de-facto standard building block for end-to-end audio classification models. Recently, neural networks based solely on self-attention mechanisms such as the Audio Spectrogram Transformer (AST) have been shown to outperform CNNs. In this paper, we find an intriguing interaction between the two very different models - CNN and AST models are good teachers for each other. When we use either of them as the teacher and train the other model as the student via knowledge distillation (KD), the performance of the student model noticeably improves, and in many cases, is better than the teacher model. In our experiments with this CNN/Transformer Cross-Model Knowledge Distillation (CMKD) method we achieve new state-of-the-art performance on FSD50K, AudioSet, and ESC-50.&lt;/p&gt;</content:encoded></item><item><title>Towards High spatial resolution and fine-grained fidelity depth reconstruction of single-photon LiDAR with context-aware spatiotemporal modeling</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.034</link><guid>10.1016/j.isprsjprs.2025.11.034</guid><pubDate>Thu, 18 Dec 2025 16:01:21 +0000</pubDate><dc:creator>Zhenyu Zhang</dc:creator><dc:creator>Yuan Li</dc:creator><dc:creator>Feihu Zhu</dc:creator><dc:creator>Yuechao Ma</dc:creator><dc:creator>Junying Lv</dc:creator><dc:creator>Qian Sun</dc:creator><dc:creator>Lin Li</dc:creator><dc:creator>Wuming Zhang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.034</prism:doi><description>While single-photon LiDAR promises revolutionary depth sensing capabilities, existing deep learning frameworks fundamentally fail to overcome the challenge of high spatial resolution (SR) data processing. To address the amplification of fine geometric details and complex spatiotemporal dependencies in high-SR single-photon data, we adopt a U-Net++ backbone with dense skip connections to preserve high-frequency features. Our encoder cascades two novel modules, integrating attention-driven modulation and convolution to adaptively model intricate patterns without sacrificing detail. We propose a 3D triple local-attention fusion module (3D-TriLAF) to suppress incoherent responses across temporal, spatial, and channel axes. In parallel, an opposite continuous dilation spatial–temporal convolution module (OCDSConv) is designed to extract structured context while preserving transient cues. To alleviate the misalignment and semantic drift between low and high-level features—problems exacerbated by increased resolution—we design a multi-scale fusion mechanism that facilitates consistent geometric modeling across scales. Finally, we propose a hybrid loss combining ordinal regression (OR) loss, structural similarity index measure (SSIM) loss, and bilateral total variation (BTV) loss to jointly enhances peak localization, structural fidelity, and edge-aware smoothness. Extensive experiments on two 128 × 128 SR simulated datasets show that, compared with the best baseline, our framework reduces RMSE and Abs Rel by up to 60.00 % and 31.58 %. On two (200 + )×(200 + ) SR real-world datasets, RMSE and Abs Rel drop by 42.31 % and 39.44 %. These quantitative gains and visual improvements in geometric continuity under complex lighting confirm its suitability for fine-grained high-SR single-photon depth reconstruction.
Published: 2025-12-18T16:01:21+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyu Zhang; Yuan Li; Feihu Zhu; Yuechao Ma; Junying Lv; Qian Sun; Lin Li; Wuming Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.034"&gt;10.1016/j.isprsjprs.2025.11.034&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;While single-photon LiDAR promises revolutionary depth sensing capabilities, existing deep learning frameworks fundamentally fail to overcome the challenge of high spatial resolution (SR) data processing. To address the amplification of fine geometric details and complex spatiotemporal dependencies in high-SR single-photon data, we adopt a U-Net++ backbone with dense skip connections to preserve high-frequency features. Our encoder cascades two novel modules, integrating attention-driven modulation and convolution to adaptively model intricate patterns without sacrificing detail. We propose a 3D triple local-attention fusion module (3D-TriLAF) to suppress incoherent responses across temporal, spatial, and channel axes. In parallel, an opposite continuous dilation spatial–temporal convolution module (OCDSConv) is designed to extract structured context while preserving transient cues. To alleviate the misalignment and semantic drift between low and high-level features—problems exacerbated by increased resolution—we design a multi-scale fusion mechanism that facilitates consistent geometric modeling across scales. Finally, we propose a hybrid loss combining ordinal regression (OR) loss, structural similarity index measure (SSIM) loss, and bilateral total variation (BTV) loss to jointly enhances peak localization, structural fidelity, and edge-aware smoothness. Extensive experiments on two 128 × 128 SR simulated datasets show that, compared with the best baseline, our framework reduces RMSE and Abs Rel by up to 60.00 % and 31.58 %. On two (200 + )×(200 + ) SR real-world datasets, RMSE and Abs Rel drop by 42.31 % and 39.44 %. These quantitative gains and visual improvements in geometric continuity under complex lighting confirm its suitability for fine-grained high-SR single-photon depth reconstruction.&lt;/p&gt;</content:encoded></item><item><title>Solving finite element methods with spiking networks</title><link>https://doi.org/10.1038/s42256-025-01158-9</link><guid>10.1038/s42256-025-01158-9</guid><pubDate>Wed, 17 Dec 2025 10:02:54 +0000</pubDate><dc:creator>Wenhao Song</dc:creator><dc:creator>Zixu Wang</dc:creator><dc:creator>J. Joshua Yang</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01158-9</prism:doi><description>Brain-inspired computing can enhance the finite element method, a cornerstone of scientific modelling, by reducing energy costs and reframing numerical simulation through neural dynamics.
Published: 2025-12-17T10:02:54+00:00
Venue: Nature Machine Intelligence
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhao Song; Zixu Wang; J. Joshua Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01158-9"&gt;10.1038/s42256-025-01158-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Brain-inspired computing can enhance the finite element method, a cornerstone of scientific modelling, by reducing energy costs and reframing numerical simulation through neural dynamics.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Visual Classification via Adaptive Attention Quantization Transformer</title><link>https://doi.org/10.1109/tnnls.2025.3643809</link><guid>10.1109/tnnls.2025.3643809</guid><pubDate>Wed, 17 Dec 2025 18:47:14 +0000</pubDate><dc:creator>Shishi Qiao</dc:creator><dc:creator>Shixian Li</dc:creator><dc:creator>Haiyong Zheng</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3643809</prism:doi><description>Vision transformer (ViT) has recently demonstrated remarkable performance in fine-grained visual classification (FGVC). However, most existing ViT-based methods often overlook the varied focus of different attention heads, in which heads that attend to nondiscriminative regions would dilute the discriminative signal crucial for FGVC. To address such issues, we propose a novel adaptive attention quantization transformer (A2QTrans) for FGVC to select the key discriminative features by analyzing the heads’ attention, which comprises three key modules: the adaptive quantization selection (AQS) module, the background elimination (BE) module, and the dynamic hybrid optimization (DHO) module. Specifically, the AQS module dynamically selects the most discriminative features in a data-driven manner by quantizing the attention scores across multiple attention heads with a global, learnable threshold. This process effectively filters out generally irrelevant information from nondiscriminative tokens, thus concentrating attention on important regions. To address the nondifferentiability inherent in updating this threshold during binarization, our AQS module employs a straight-through estimator (STE) for discrete optimization, enabling end-to-end gradient backpropagation. In addition, we utilize the prior that background regions usually do not contain meaningful information, and design the BE module to further calibrate the focus of the attention heads to the main objects in images. Finally, the DHO module adaptively optimizes and integrates the attentive results of the AQS and BE modules to achieve optimal classification performance. Extensive experiments conducted on four challenging FGVC benchmark datasets and three ViT variants demonstrate A2QTrans’s superior performance, achieving state-of-the-art (SOTA) results. The source code is available at https://github.com/Lishixian0817/A2QTrans
Published: 2025-12-17T18:47:14+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shishi Qiao; Shixian Li; Haiyong Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3643809"&gt;10.1109/tnnls.2025.3643809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Vision transformer (ViT) has recently demonstrated remarkable performance in fine-grained visual classification (FGVC). However, most existing ViT-based methods often overlook the varied focus of different attention heads, in which heads that attend to nondiscriminative regions would dilute the discriminative signal crucial for FGVC. To address such issues, we propose a novel adaptive attention quantization transformer (A2QTrans) for FGVC to select the key discriminative features by analyzing the heads’ attention, which comprises three key modules: the adaptive quantization selection (AQS) module, the background elimination (BE) module, and the dynamic hybrid optimization (DHO) module. Specifically, the AQS module dynamically selects the most discriminative features in a data-driven manner by quantizing the attention scores across multiple attention heads with a global, learnable threshold. This process effectively filters out generally irrelevant information from nondiscriminative tokens, thus concentrating attention on important regions. To address the nondifferentiability inherent in updating this threshold during binarization, our AQS module employs a straight-through estimator (STE) for discrete optimization, enabling end-to-end gradient backpropagation. In addition, we utilize the prior that background regions usually do not contain meaningful information, and design the BE module to further calibrate the focus of the attention heads to the main objects in images. Finally, the DHO module adaptively optimizes and integrates the attentive results of the AQS and BE modules to achieve optimal classification performance. Extensive experiments conducted on four challenging FGVC benchmark datasets and three ViT variants demonstrate A2QTrans’s superior performance, achieving state-of-the-art (SOTA) results. The source code is available at https://github.com/Lishixian0817/A2QTrans&lt;/p&gt;</content:encoded></item><item><title>Improving the Stability and Efficiency of Diffusion Models for Content Consistent Super-Resolution</title><link>https://doi.org/10.1109/tip.2025.3640863</link><guid>10.1109/tip.2025.3640863</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Lingchen Sun</dc:creator><dc:creator>Rongyuan Wu</dc:creator><dc:creator>Jie Liang</dc:creator><dc:creator>Zhengqiang Zhang</dc:creator><dc:creator>Hongwei Yong</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3640863</prism:doi><description>The generative priors of pre-trained latent diffusion models (DMs) have demonstrated great potential to enhance the visual quality of image super-resolution (SR) results. However, the noise sampling process in DMs introduces randomness in the SR outputs, and the generated contents can differ a lot with different noise samples. The multi-step diffusion process can be accelerated by distilling methods, but the generative capacity is difficult to control. To address these issues, we analyze the respective advantages of DMs and generative adversarial networks (GANs) and propose to partition the generative SR process into two stages, where the DM is employed for reconstructing image structures and the GAN is employed for improving fine-grained details. Specifically, we propose a non-uniform timestep sampling strategy in the first stage. A single timestep sampling is first applied to extract the coarse information from the input image, then a few reverse steps are used to reconstruct the main structures. In the second stage, we finetune the decoder of the pre-trained variational auto-encoder by adversarial GAN training for deterministic detail enhancement. Once trained, our proposed method, namely content consistent super-resolution (CCSR), allows flexible use of different diffusion steps in the inference stage without re-training. Extensive experiments show that with 2 or even 1 diffusion step, CCSR can significantly improve the content consistency of SR outputs while keeping high perceptual quality. Codes and models can be found at https://github.com/csslc/CCSR.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lingchen Sun; Rongyuan Wu; Jie Liang; Zhengqiang Zhang; Hongwei Yong; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3640863"&gt;10.1109/tip.2025.3640863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;The generative priors of pre-trained latent diffusion models (DMs) have demonstrated great potential to enhance the visual quality of image super-resolution (SR) results. However, the noise sampling process in DMs introduces randomness in the SR outputs, and the generated contents can differ a lot with different noise samples. The multi-step diffusion process can be accelerated by distilling methods, but the generative capacity is difficult to control. To address these issues, we analyze the respective advantages of DMs and generative adversarial networks (GANs) and propose to partition the generative SR process into two stages, where the DM is employed for reconstructing image structures and the GAN is employed for improving fine-grained details. Specifically, we propose a non-uniform timestep sampling strategy in the first stage. A single timestep sampling is first applied to extract the coarse information from the input image, then a few reverse steps are used to reconstruct the main structures. In the second stage, we finetune the decoder of the pre-trained variational auto-encoder by adversarial GAN training for deterministic detail enhancement. Once trained, our proposed method, namely content consistent super-resolution (CCSR), allows flexible use of different diffusion steps in the inference stage without re-training. Extensive experiments show that with 2 or even 1 diffusion step, CCSR can significantly improve the content consistency of SR outputs while keeping high perceptual quality. Codes and models can be found at https://github.com/csslc/CCSR.&lt;/p&gt;</content:encoded></item><item><title>Local Constraints Convolutional Neural Network for SAR Image Denoising and Target Configuration Recognition</title><link>https://doi.org/10.1109/taes.2025.3645154</link><guid>10.1109/taes.2025.3645154</guid><pubDate>Wed, 17 Dec 2025 18:49:24 +0000</pubDate><dc:creator>Ming Liu</dc:creator><dc:creator>Zhenning Dong</dc:creator><dc:creator>Shichao Chen</dc:creator><dc:creator>Mingliang Tao</dc:creator><dc:creator>Mengdao Xing</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3645154</prism:doi><description>Synthetic aperture radar (SAR) target recognition is an important branch of SAR image processing. To overcome the influences of inevitable speckle noise, especially for similar configurations recognition, we propose a local constraints convolutional neural network (LC-CNN) for joint SAR image denoising and target configurations recognition. The proposed LC-CNN enhances recognition performance through a collaboratively designed of multi-task loss function. In the denoising stage, a speckle suppression loss is designed to smooth background noise whereas retaining target details. In the recognition stage, a local structure maintenance loss is designed to enhance discrimination of similar configurations by maintaining local geometric relationships. And a feature invariance loss is established to ensure core target features remain stable after denoising. Experimental results demonstrate LC-CNN's robustness under varying speckle noise levels and excellent performance in similar SAR target configurations recognition.
Published: 2025-12-17T18:49:24+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Liu; Zhenning Dong; Shichao Chen; Mingliang Tao; Mengdao Xing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3645154"&gt;10.1109/taes.2025.3645154&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR) target recognition is an important branch of SAR image processing. To overcome the influences of inevitable speckle noise, especially for similar configurations recognition, we propose a local constraints convolutional neural network (LC-CNN) for joint SAR image denoising and target configurations recognition. The proposed LC-CNN enhances recognition performance through a collaboratively designed of multi-task loss function. In the denoising stage, a speckle suppression loss is designed to smooth background noise whereas retaining target details. In the recognition stage, a local structure maintenance loss is designed to enhance discrimination of similar configurations by maintaining local geometric relationships. And a feature invariance loss is established to ensure core target features remain stable after denoising. Experimental results demonstrate LC-CNN&amp;#x27;s robustness under varying speckle noise levels and excellent performance in similar SAR target configurations recognition.&lt;/p&gt;</content:encoded></item><item><title>Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</title><link>https://arxiv.org/abs/2512.15687v1</link><guid>http://arxiv.org/abs/2512.15687v1</guid><pubDate>Wed, 17 Dec 2025 18:44:45 +0000</pubDate><dc:creator>Zhenwen Liang</dc:creator><dc:creator>Sidi Lu</dc:creator><dc:creator>Wenhao Yu</dc:creator><dc:creator>Kishan Panaganti</dc:creator><dc:creator>Yujun Zhou</dc:creator><dc:creator>Haitao Mi</dc:creator><dc:creator>Dong Yu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.
Published: 2025-12-17T18:44:45+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenwen Liang; Sidi Lu; Wenhao Yu; Kishan Panaganti; Yujun Zhou; Haitao Mi; Dong Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.&lt;/p&gt;</content:encoded></item><item><title>Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models</title><link>https://arxiv.org/abs/2512.15089v1</link><guid>http://arxiv.org/abs/2512.15089v1</guid><pubDate>Wed, 17 Dec 2025 05:11:58 +0000</pubDate><dc:creator>Jinwu Hu</dc:creator><dc:creator>Dongjin Yang</dc:creator><dc:creator>Langyu Bian</dc:creator><dc:creator>Zhiquan Wen</dc:creator><dc:creator>Yufeng Wang</dc:creator><dc:creator>Yaofo Chen</dc:creator><dc:creator>Bin Xiao</dc:creator><dc:creator>Yuanqing Li</dc:creator><dc:creator>Mingkui Tan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.
Published: 2025-12-17T05:11:58+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinwu Hu; Dongjin Yang; Langyu Bian; Zhiquan Wen; Yufeng Wang; Yaofo Chen; Bin Xiao; Yuanqing Li; Mingkui Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state-of-the-art Test-Time scaling methods, achieving at least a 13% relative improvement in average exact match on In-Domain tasks and an 8% relative gain on Out-of-Domain tasks.&lt;/p&gt;</content:encoded></item><item><title>ViV-ReID: Bidirectional Structural-Aware Spatial-Temporal Graph Networks on Large-Scale Video-Based Vessel Re-Identification Dataset</title><link>https://doi.org/10.1109/tip.2025.3643156</link><guid>10.1109/tip.2025.3643156</guid><pubDate>Thu, 18 Dec 2025 18:35:49 +0000</pubDate><dc:creator>Mingxin Zhang</dc:creator><dc:creator>Fuxiang Feng</dc:creator><dc:creator>Xing Fang</dc:creator><dc:creator>Lin Zhang</dc:creator><dc:creator>Youmei Zhang</dc:creator><dc:creator>Xiaolei Li</dc:creator><dc:creator>Wei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643156</prism:doi><description>Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.
Published: 2025-12-18T18:35:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingxin Zhang; Fuxiang Feng; Xing Fang; Lin Zhang; Youmei Zhang; Xiaolei Li; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643156"&gt;10.1109/tip.2025.3643156&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.&lt;/p&gt;</content:encoded></item><item><title>Arithmetic-Intensity-Aware Quantization</title><link>https://arxiv.org/abs/2512.14090v2</link><guid>http://arxiv.org/abs/2512.14090v2</guid><pubDate>Tue, 16 Dec 2025 04:59:08 +0000</pubDate><dc:creator>Taig Singh</dc:creator><dc:creator>Shreshth Rajan</dc:creator><dc:creator>Nikhil Jain</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.
Published: 2025-12-16T04:59:08+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Taig Singh; Shreshth Rajan; Nikhil Jain&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.&lt;/p&gt;</content:encoded></item><item><title>NiCI-Pruning: Enhancing Diffusion Model Pruning via Noise in Clean Image Guidance</title><link>https://doi.org/10.1109/tip.2025.3643138</link><guid>10.1109/tip.2025.3643138</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Junzhu Mao</dc:creator><dc:creator>Zeren Sun</dc:creator><dc:creator>Yazhou Yao</dc:creator><dc:creator>Tianfei Zhou</dc:creator><dc:creator>Liqiang Nie</dc:creator><dc:creator>Xiansheng Hua</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643138</prism:doi><description>The substantial successes achieved by diffusion probabilistic models have prompted the study of their employment in resource-limited scenarios. Pruning methods have been proven effective in compressing discriminative models relying on the correlation between training losses and model performances. However, diffusion models employ an iterative process for generating high-quality images, leading to a breakdown of such connections. To address this challenge, we propose a simple yet effective method, named NiCI-Pruning (Noise in Clean Image Pruning), for the compression of diffusion models. NiCI-Pruning capitalizes the noise predicted by the model based on clean image inputs, favoring it as a feature for establishing reconstruction losses. Accordingly, Taylor expansion is employed for the proposed reconstruction loss to evaluate the parameter importance effectively. Moreover, we propose an interval sampling strategy that incorporates a timestep-weighted schema, alleviating the risk of misleading information obtained at later timesteps. We provide comprehensive experimental results to affirm the superiority of our proposed approach. Notably, our method achieves a remarkable average reduction of 30.4% in FID score increase across five different datasets compared to the state-of-the-art diffusion pruning method at equivalent pruning rates. Our code and models have been made available at https://github.com/ NUST-Machine-Intelligence-Laboratory/NiCI-Pruning.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junzhu Mao; Zeren Sun; Yazhou Yao; Tianfei Zhou; Liqiang Nie; Xiansheng Hua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643138"&gt;10.1109/tip.2025.3643138&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;The substantial successes achieved by diffusion probabilistic models have prompted the study of their employment in resource-limited scenarios. Pruning methods have been proven effective in compressing discriminative models relying on the correlation between training losses and model performances. However, diffusion models employ an iterative process for generating high-quality images, leading to a breakdown of such connections. To address this challenge, we propose a simple yet effective method, named NiCI-Pruning (Noise in Clean Image Pruning), for the compression of diffusion models. NiCI-Pruning capitalizes the noise predicted by the model based on clean image inputs, favoring it as a feature for establishing reconstruction losses. Accordingly, Taylor expansion is employed for the proposed reconstruction loss to evaluate the parameter importance effectively. Moreover, we propose an interval sampling strategy that incorporates a timestep-weighted schema, alleviating the risk of misleading information obtained at later timesteps. We provide comprehensive experimental results to affirm the superiority of our proposed approach. Notably, our method achieves a remarkable average reduction of 30.4% in FID score increase across five different datasets compared to the state-of-the-art diffusion pruning method at equivalent pruning rates. Our code and models have been made available at https://github.com/ NUST-Machine-Intelligence-Laboratory/NiCI-Pruning.&lt;/p&gt;</content:encoded></item><item><title>Learning Quaternion Convolutional Neural Networks for PolSAR Target Recognition</title><link>https://doi.org/10.1109/taes.2025.3645163</link><guid>10.1109/taes.2025.3645163</guid><pubDate>Wed, 17 Dec 2025 18:49:24 +0000</pubDate><dc:creator>Huiping Lin</dc:creator><dc:creator>Junjun Yin</dc:creator><dc:creator>Jian Yang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3645163</prism:doi><description>Polarization offers rich information for enhancing target recognition in synthetic aperture radar (SAR) imagery. However, most existing SAR target recognition methods rely on single-channel data, and the potential of multi-channel polarimetric images remains underexplored. In this paper, we propose an end to-end target recognition framework for polarimetric SAR (Pol SAR) images based on a quaternion convolutional neural network (QCNN) operating in the Poincare sphere parameter domain. The QCNN is constructed with a sequence of quaternion operation layers and incorporates a specialized loss function designed for quaternion-valued representations. To address the mismatch problem in the quaternion field, we introduce quaternion maximum pooling (QuatMaxPool) and quaternion average pooling (QuatAvgPool) operations. To the best of our knowledge, this is the first QCNN developed for PolSAR target recognition. Experiments on both simulated and real datasets demonstrate that the proposed QCNN achieves recognition performance comparable to state-of-the-art real- and complex-valued models while requiring significantly fewer parameters and offering enhanced physical interpretability, thereby validating the effectiveness and superiority of the proposed approach
Published: 2025-12-17T18:49:24+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huiping Lin; Junjun Yin; Jian Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3645163"&gt;10.1109/taes.2025.3645163&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Polarization offers rich information for enhancing target recognition in synthetic aperture radar (SAR) imagery. However, most existing SAR target recognition methods rely on single-channel data, and the potential of multi-channel polarimetric images remains underexplored. In this paper, we propose an end to-end target recognition framework for polarimetric SAR (Pol SAR) images based on a quaternion convolutional neural network (QCNN) operating in the Poincare sphere parameter domain. The QCNN is constructed with a sequence of quaternion operation layers and incorporates a specialized loss function designed for quaternion-valued representations. To address the mismatch problem in the quaternion field, we introduce quaternion maximum pooling (QuatMaxPool) and quaternion average pooling (QuatAvgPool) operations. To the best of our knowledge, this is the first QCNN developed for PolSAR target recognition. Experiments on both simulated and real datasets demonstrate that the proposed QCNN achieves recognition performance comparable to state-of-the-art real- and complex-valued models while requiring significantly fewer parameters and offering enhanced physical interpretability, thereby validating the effectiveness and superiority of the proposed approach&lt;/p&gt;</content:encoded></item><item><title>4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</title><link>https://arxiv.org/abs/2512.14235v1</link><guid>http://arxiv.org/abs/2512.14235v1</guid><pubDate>Tue, 16 Dec 2025 09:43:05 +0000</pubDate><dc:creator>Jimmie Kwok</dc:creator><dc:creator>Holger Caesar</dc:creator><dc:creator>Andras Palffy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.
Published: 2025-12-16T09:43:05+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jimmie Kwok; Holger Caesar; Andras Palffy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.&lt;/p&gt;</content:encoded></item><item><title>Hiding Local Manipulations on SAR Images: a Counter-Forensic Attack</title><link>https://doi.org/10.1109/tip.2025.3643154</link><guid>10.1109/tip.2025.3643154</guid><pubDate>Wed, 17 Dec 2025 18:49:42 +0000</pubDate><dc:creator>Sara Mandelli</dc:creator><dc:creator>Edoardo Daniele Cannas</dc:creator><dc:creator>Paolo Bestagini</dc:creator><dc:creator>Stefano Tebaldini</dc:creator><dc:creator>Stefano Tubaro</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643154</prism:doi><description>The vast accessibility of Synthetic Aperture Radar (SAR) images through online portals has propelled the research across various fields. This widespread use and easy availability have unfortunately made SAR data susceptible to malicious alterations, such as local editing applied to the images for inserting or covering the presence of sensitive targets. To contrast malicious manipulations, in the last years the forensic community has begun to dig into the SAR manipulation issue, proposing detectors that effectively localize the tampering traces in amplitude images. Nonetheless, in this paper we demonstrate that an expert practitioner can exploit the complex nature of SAR data to obscure any signs of manipulation within a locally altered amplitude image. We refer to this approach as a counter-forensic attack. To achieve the concealment of manipulation traces, the attacker can simulate a re-acquisition of the manipulated scene by the SAR system that initially generated the pristine image. In doing so, the attacker can obscure any evidence of manipulation, making it appear as if the image was legitimately produced by the system. This attack has unique features that make it both highly generalizable and relatively easy to apply. First, it is a black-box attack, meaning it is not designed to deceive a specific forensic detector. Furthermore, it does not require a training phase and is not based on adversarial operations. We assess the effectiveness of the proposed counter-forensic approach across diverse scenarios, examining various manipulation operations. The obtained results indicate that our devised attack successfully eliminates traces of manipulation, deceiving even the most advanced forensic detectors.
Published: 2025-12-17T18:49:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sara Mandelli; Edoardo Daniele Cannas; Paolo Bestagini; Stefano Tebaldini; Stefano Tubaro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643154"&gt;10.1109/tip.2025.3643154&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;The vast accessibility of Synthetic Aperture Radar (SAR) images through online portals has propelled the research across various fields. This widespread use and easy availability have unfortunately made SAR data susceptible to malicious alterations, such as local editing applied to the images for inserting or covering the presence of sensitive targets. To contrast malicious manipulations, in the last years the forensic community has begun to dig into the SAR manipulation issue, proposing detectors that effectively localize the tampering traces in amplitude images. Nonetheless, in this paper we demonstrate that an expert practitioner can exploit the complex nature of SAR data to obscure any signs of manipulation within a locally altered amplitude image. We refer to this approach as a counter-forensic attack. To achieve the concealment of manipulation traces, the attacker can simulate a re-acquisition of the manipulated scene by the SAR system that initially generated the pristine image. In doing so, the attacker can obscure any evidence of manipulation, making it appear as if the image was legitimately produced by the system. This attack has unique features that make it both highly generalizable and relatively easy to apply. First, it is a black-box attack, meaning it is not designed to deceive a specific forensic detector. Furthermore, it does not require a training phase and is not based on adversarial operations. We assess the effectiveness of the proposed counter-forensic approach across diverse scenarios, examining various manipulation operations. The obtained results indicate that our devised attack successfully eliminates traces of manipulation, deceiving even the most advanced forensic detectors.&lt;/p&gt;</content:encoded></item><item><title>Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs</title><link>https://arxiv.org/abs/2512.13898v1</link><guid>http://arxiv.org/abs/2512.13898v1</guid><pubDate>Mon, 15 Dec 2025 21:01:37 +0000</pubDate><dc:creator>Rachit Bansal</dc:creator><dc:creator>Aston Zhang</dc:creator><dc:creator>Rishabh Tiwari</dc:creator><dc:creator>Lovish Madaan</dc:creator><dc:creator>Sai Surya Duvvuri</dc:creator><dc:creator>Devvrit Khatri</dc:creator><dc:creator>David Brandfonbrener</dc:creator><dc:creator>David Alvarez-Melis</dc:creator><dc:creator>Prajjwal Bhargava</dc:creator><dc:creator>Mihir Sanjay Kale</dc:creator><dc:creator>Samy Jelassi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.
Published: 2025-12-15T21:01:37+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rachit Bansal; Aston Zhang; Rishabh Tiwari; Lovish Madaan; Sai Surya Duvvuri; Devvrit Khatri; David Brandfonbrener; David Alvarez-Melis; Prajjwal Bhargava; Mihir Sanjay Kale; Samy Jelassi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.&lt;/p&gt;</content:encoded></item><item><title>Light CNN-Transformer Dual-Branch Network for Real-Time Semantic Segmentation</title><link>https://doi.org/10.1109/tmm.2025.3645624</link><guid>10.1109/tmm.2025.3645624</guid><pubDate>Thu, 18 Dec 2025 18:34:14 +0000</pubDate><dc:creator>Yongsheng Dong</dc:creator><dc:creator>Siming Jia</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3645624</prism:doi><description>Convolutional Neural Networks (CNN) have widely used in semantic segmentation, and can effectively extract local hierarchical information while being unsatisfactory in extracting global information. By contrast, Transformer is good at extracting long-distance dependencies in semantics while it is time-consuming. In this work, we propose a Light CNN-Transformer Dual-Branch Network (LCTDBNet) for real-time semantic segmentation. It consists of a longer CNN branch to extract local hierarchical information and a shorter Transformer branch to extract global contextual information. The CNN branch uses a lightweight encoder-decoder structure to further extract more local hierarchical information. We propose a Deep Strip Aggregation Pyramid Pooling Module (DSAPPM) to extract contextual and strip information. We further propose a Feature Pooling Refinement Module (FPRM) to optimise the feature representation at different stages. Finally, we propose a CNN-Transformer Fusion Module (CTFM) to fuse the features of two branches. Experimental results demonstrate that our proposed LCTDBNet is effective and achieves satisfactory results. Specifically, the base version of LCTDBNet achieves 80.3% mean intersection over union (mIoU) at 78.6 frames per second (FPS) on Cityscapes, 80.0% mIoU at 137.5 FPS on CamVid and 40.9% mIoU at 253.7 FPS on ADE20 K.
Published: 2025-12-18T18:34:14+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongsheng Dong; Siming Jia; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3645624"&gt;10.1109/tmm.2025.3645624&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNN) have widely used in semantic segmentation, and can effectively extract local hierarchical information while being unsatisfactory in extracting global information. By contrast, Transformer is good at extracting long-distance dependencies in semantics while it is time-consuming. In this work, we propose a Light CNN-Transformer Dual-Branch Network (LCTDBNet) for real-time semantic segmentation. It consists of a longer CNN branch to extract local hierarchical information and a shorter Transformer branch to extract global contextual information. The CNN branch uses a lightweight encoder-decoder structure to further extract more local hierarchical information. We propose a Deep Strip Aggregation Pyramid Pooling Module (DSAPPM) to extract contextual and strip information. We further propose a Feature Pooling Refinement Module (FPRM) to optimise the feature representation at different stages. Finally, we propose a CNN-Transformer Fusion Module (CTFM) to fuse the features of two branches. Experimental results demonstrate that our proposed LCTDBNet is effective and achieves satisfactory results. Specifically, the base version of LCTDBNet achieves 80.3% mean intersection over union (mIoU) at 78.6 frames per second (FPS) on Cityscapes, 80.0% mIoU at 137.5 FPS on CamVid and 40.9% mIoU at 253.7 FPS on ADE20 K.&lt;/p&gt;</content:encoded></item><item><title>MDAFNet: Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/lgrs.2025.3645669</link><guid>10.1109/lgrs.2025.3645669</guid><pubDate>Wed, 17 Dec 2025 18:50:17 +0000</pubDate><dc:creator>Shuying Li</dc:creator><dc:creator>Qiang Ma</dc:creator><dc:creator>San Zhang</dc:creator><dc:creator>Wuwei Wang</dc:creator><dc:creator>Chuang Yang</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3645669</prism:doi><description>Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network’s capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.
Published: 2025-12-17T18:50:17+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuying Li; Qiang Ma; San Zhang; Wuwei Wang; Chuang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3645669"&gt;10.1109/lgrs.2025.3645669&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) plays a crucial role in numerous military and civilian applications. However, existing methods often face the gradual degradation of target edge pixels as the number of network layers increases, and traditional convolution struggles to differentiate between frequency components during feature extraction, leading to low-frequency backgrounds interfering with high-frequency targets and high-frequency noise triggering false detections. To address these limitations, we propose MDAFNet (Multi-scale Differential Edge and Adaptive Frequency Guided Network for Infrared Small Target Detection), which integrates the Multi-Scale Differential Edge (MSDE) module and Dual-Domain Adaptive Feature Enhancement (DAFE) module. The MSDE module, through a multi-scale edge extraction and enhancement mechanism, effectively compensates for the cumulative loss of target edge information during downsampling. The DAFE module combines frequency domain processing mechanisms with simulated frequency decomposition and fusion mechanisms in the spatial domain to effectively improve the network’s capability to adaptively enhance high-frequency targets and selectively suppress high-frequency noise. Experimental results on multiple datasets demonstrate the superior detection performance of MDAFNet.&lt;/p&gt;</content:encoded></item><item><title>Online Class-Incremental SAR Target Recognition with Interference-Aware Replay</title><link>https://doi.org/10.1109/lgrs.2025.3645699</link><guid>10.1109/lgrs.2025.3645699</guid><pubDate>Wed, 17 Dec 2025 18:50:17 +0000</pubDate><dc:creator>Yuchao Ma</dc:creator><dc:creator>Gong Zhang</dc:creator><dc:creator>Yansen He</dc:creator><dc:creator>Biao Xue</dc:creator><dc:creator>Henry Leung</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3645699</prism:doi><description>To address the challenge of catastrophic forgetting in synthetic aperture radar (SAR) image recognition caused by viewpoint-sensitive, high-interference samples encountered in dynamic environments, we propose a lightweight and efficient online class-incremental learning (OCI) framework named Interference-Aware Replay with Dynamic Review for SAR Target Recognition (IAR-DR). Based on the experience replay (ER) mechanism, a Maximally Interfered Retrieval (MIR) strategy is designed to prioritize the replay of high-interference samples by measuring loss changes before and after model updates, thereby preserving decision boundaries under viewpoint variation. A Review Trick (RT) mechanism is further introduced to periodically revisit all buffered samples with a low learning rate, which complements MIR by reinforcing global feature retention and enhancing long-term memory stability. The combination of MIR and RT achieves a synergistic balance between local discrimination and global generalization, mitigating the forgetting effect while maintaining the efficiency. Extensive experiments conducted on the MSTAR and Bistatic MiniSAR datasets demonstrate that the proposed IAR-DR framework maintains high recognition accuracy while achieving a forgetting rate as low as 6.92% in ablation studies, and improving retention by 4.7% over recent SAR class-incremental methods.
Published: 2025-12-17T18:50:17+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchao Ma; Gong Zhang; Yansen He; Biao Xue; Henry Leung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3645699"&gt;10.1109/lgrs.2025.3645699&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;To address the challenge of catastrophic forgetting in synthetic aperture radar (SAR) image recognition caused by viewpoint-sensitive, high-interference samples encountered in dynamic environments, we propose a lightweight and efficient online class-incremental learning (OCI) framework named Interference-Aware Replay with Dynamic Review for SAR Target Recognition (IAR-DR). Based on the experience replay (ER) mechanism, a Maximally Interfered Retrieval (MIR) strategy is designed to prioritize the replay of high-interference samples by measuring loss changes before and after model updates, thereby preserving decision boundaries under viewpoint variation. A Review Trick (RT) mechanism is further introduced to periodically revisit all buffered samples with a low learning rate, which complements MIR by reinforcing global feature retention and enhancing long-term memory stability. The combination of MIR and RT achieves a synergistic balance between local discrimination and global generalization, mitigating the forgetting effect while maintaining the efficiency. Extensive experiments conducted on the MSTAR and Bistatic MiniSAR datasets demonstrate that the proposed IAR-DR framework maintains high recognition accuracy while achieving a forgetting rate as low as 6.92% in ablation studies, and improving retention by 4.7% over recent SAR class-incremental methods.&lt;/p&gt;</content:encoded></item><item><title>A Novel Approach to GNN Explainability: Distilling Knowledge with Inter-Layer Alignment</title><link>https://doi.org/10.1109/tpami.2025.3645279</link><guid>10.1109/tpami.2025.3645279</guid><pubDate>Wed, 17 Dec 2025 18:46:02 +0000</pubDate><dc:creator>Xiaoxia Zhang</dc:creator><dc:creator>Xingyu Liu</dc:creator><dc:creator>Guoyin Wang</dc:creator><dc:creator>Yongduan Song</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3645279</prism:doi><description>Graph Neural Networks (GNNs) have made significant strides in the analysis and modeling of complex network data, particularly excelling in graph and node classification tasks. However, the ”black-box” nature of GNNs impedes user understanding and trust, thereby restricting their broader application. This challenge has spurred a growing focus on demystifying GNNs to make their decision-making processes more transparent. Traditional methods for explaining GNNs often rely on selecting subgraphs and employing combinatorial optimization to generate understandable outputs. However, these methods are closely linked to the inherent complexity of GNNs, leading to higher explanation costs. To address this issue, we introduce a lower-complexity proxy model to explain GNNs. Our approach leverages knowledge distillation with inter-layer alignment, specifically targeting the challenge of over-smoothing and its detrimental impact on model explanation. Initially, we distill critical insights from complex GNN models into a more manageable proxy model. We then apply an inter-layer alignment-based distillation technique to ensure alignment between the proxy and the original model, facilitating the extraction of node or edge-level explanations within the proxy framework. We theoretically prove that the explanations derived from the proxy model are faithful to both the proxy and the original model. Additionally, we show that the upper bound of unfaithfulness between the proxy and the original model remains consistent when the distillation error is infinitesimal. This inter-layer alignment knowledge distillation technique enables the proxy model to retain the knowledge learning and topological representation capabilities of the original model to the greatest extent. Experimental evaluations on numerous real-world datasets confirm the effectiveness of our method, demonstrating robust performance.
Published: 2025-12-17T18:46:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxia Zhang; Xingyu Liu; Guoyin Wang; Yongduan Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3645279"&gt;10.1109/tpami.2025.3645279&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Graph Neural Networks (GNNs) have made significant strides in the analysis and modeling of complex network data, particularly excelling in graph and node classification tasks. However, the ”black-box” nature of GNNs impedes user understanding and trust, thereby restricting their broader application. This challenge has spurred a growing focus on demystifying GNNs to make their decision-making processes more transparent. Traditional methods for explaining GNNs often rely on selecting subgraphs and employing combinatorial optimization to generate understandable outputs. However, these methods are closely linked to the inherent complexity of GNNs, leading to higher explanation costs. To address this issue, we introduce a lower-complexity proxy model to explain GNNs. Our approach leverages knowledge distillation with inter-layer alignment, specifically targeting the challenge of over-smoothing and its detrimental impact on model explanation. Initially, we distill critical insights from complex GNN models into a more manageable proxy model. We then apply an inter-layer alignment-based distillation technique to ensure alignment between the proxy and the original model, facilitating the extraction of node or edge-level explanations within the proxy framework. We theoretically prove that the explanations derived from the proxy model are faithful to both the proxy and the original model. Additionally, we show that the upper bound of unfaithfulness between the proxy and the original model remains consistent when the distillation error is infinitesimal. This inter-layer alignment knowledge distillation technique enables the proxy model to retain the knowledge learning and topological representation capabilities of the original model to the greatest extent. Experimental evaluations on numerous real-world datasets confirm the effectiveness of our method, demonstrating robust performance.&lt;/p&gt;</content:encoded></item></channel></rss>