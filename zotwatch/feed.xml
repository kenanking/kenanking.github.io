<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 24 Jan 2026 02:51:40 +0000</lastBuildDate><item><title>Bridging optical and SAR images via semantic prompt-guided progressive alignment for rotated cross-domain ship detection</title><link>https://doi.org/10.1016/j.jag.2026.105119</link><guid>10.1016/j.jag.2026.105119</guid><pubDate>Fri, 23 Jan 2026 10:14:42 +0000</pubDate><dc:creator>Longli Ran</dc:creator><dc:creator>Jiaming Li</dc:creator><dc:creator>Haodong Wu</dc:creator><dc:creator>Anqi Wu</dc:creator><dc:creator>Yi He</dc:creator><dc:creator>Qingfeng Guan</dc:creator><dc:creator>Qiqi Zhu</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105119</prism:doi><description>Ship detection in remote sensing imagery is essential for diverse maritime-related tasks, including ocean surveillance, fisheries management, and environmental assessment. In operational scenarios, optical imagery provides rich texture cues under clear conditions, whereas synthetic aperture radar (SAR) enables reliable observation in nighttime and cloudy weather. However, cross-domain ship detection across optical and SAR modalities is still challenging due to discrepancies in imaging mechanisms, speckle noise, and background clutter, particularly in near-shore scenarios with similar reflection characteristics, together with the arbitrariness of ship orientation. To address these issues, we propose RotCD-Ship, a rotated cross-domain ship detection framework that bridges the domain gap between optical and SAR images while enabling accurate detection of arbitrarily oriented ships. Specifically, a domain knowledge-guided semantic prompt (DKSP) strategy based on SAR physical priors is introduced to suppress background clutter such as ship wakes and coastal interference. To handle modal divergence, we design a progressive feature alignment scheme that combines multi-scale local feature alignment (MSL-align) and global feature alignment (GF-align), enabling transfer of both fine-grained textures and high-level semantics across domains. Furthermore, a coarse-to-fine rotated region of interest (CF-RRoI) generator is developed to enhance localization precision of strip-like ships in SAR images by progressively refining orientation-aware proposals. Extensive evaluations on five public ship detection datasets show that RotCD-Ship significantly outperforms state-of-the-art methods in both accuracy and robustness, achieving an average mAP improvement of 7.5% in the horizontal ship detection task and 5.5% in the oriented ship detection task compared to the best existing methods. In addition, large-scale tests on Gaofen-3 SAR images further verify the strong generalization in dense-ship and complex coastal environments, highlighting the practical applicability of our framework for all-weather maritime monitoring.
Published: 2026-01-23T10:14:42+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.842 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Longli Ran; Jiaming Li; Haodong Wu; Anqi Wu; Yi He; Qingfeng Guan; Qiqi Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105119"&gt;10.1016/j.jag.2026.105119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.842 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection in remote sensing imagery is essential for diverse maritime-related tasks, including ocean surveillance, fisheries management, and environmental assessment. In operational scenarios, optical imagery provides rich texture cues under clear conditions, whereas synthetic aperture radar (SAR) enables reliable observation in nighttime and cloudy weather. However, cross-domain ship detection across optical and SAR modalities is still challenging due to discrepancies in imaging mechanisms, speckle noise, and background clutter, particularly in near-shore scenarios with similar reflection characteristics, together with the arbitrariness of ship orientation. To address these issues, we propose RotCD-Ship, a rotated cross-domain ship detection framework that bridges the domain gap between optical and SAR images while enabling accurate detection of arbitrarily oriented ships. Specifically, a domain knowledge-guided semantic prompt (DKSP) strategy based on SAR physical priors is introduced to suppress background clutter such as ship wakes and coastal interference. To handle modal divergence, we design a progressive feature alignment scheme that combines multi-scale local feature alignment (MSL-align) and global feature alignment (GF-align), enabling transfer of both fine-grained textures and high-level semantics across domains. Furthermore, a coarse-to-fine rotated region of interest (CF-RRoI) generator is developed to enhance localization precision of strip-like ships in SAR images by progressively refining orientation-aware proposals. Extensive evaluations on five public ship detection datasets show that RotCD-Ship significantly outperforms state-of-the-art methods in both accuracy and robustness, achieving an average mAP improvement of 7.5% in the horizontal ship detection task and 5.5% in the oriented ship detection task compared to the best existing methods. In addition, large-scale tests on Gaofen-3 SAR images further verify the strong generalization in dense-ship and complex coastal environments, highlighting the practical applicability of our framework for all-weather maritime monitoring.&lt;/p&gt;</content:encoded></item><item><title>PromptMix: LLM-Aided Prompt Learning for Generalizing Vision-Language Models</title><link>https://doi.org/10.1016/j.inffus.2026.104186</link><guid>10.1016/j.inffus.2026.104186</guid><pubDate>Fri, 23 Jan 2026 16:17:45 +0000</pubDate><dc:creator>Yongcai Chen</dc:creator><dc:creator>Qinghua Zhang</dc:creator><dc:creator>Xinfa Shi</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104186</prism:doi><description>Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.
Published: 2026-01-23T16:17:45+00:00
Venue: Information Fusion
Score: 0.836 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongcai Chen; Qinghua Zhang; Xinfa Shi; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104186"&gt;10.1016/j.inffus.2026.104186&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.836 (must_read)&lt;/p&gt;
&lt;p&gt;Intelligent engineering tasks step into real application with the development of deep learning techniques. However, performance in real conditions often falls into decline caused by scarce data, or subtle, easily confused patterns. Although vision-language models with prompt learning provide a new way for learning without retraining the backbone, these approaches still suffer from problems of overfitting under low-data regimes or poor expressive ability of prompts. To address these challenges, we propose a novel framework PromptMix that jointly considers semantic prompt learning, multimodal information fusion, and the alignment between pre-trained and domain-specific data. Specifically, PromptMix integrates three key components: (1) a Modality-Agnostic Shared Representation module to construct a shared latent space that mitigates the distribution discrepancies between pre-trained and target data, (2) a LLM-Aided Prompt Evolution mechanism to semantically enrich and iteratively refine learnable context prompts, and (3) a Cross-Attentive Adapter to enhance multimodal information fusion and robustness under low-sample conditions. Experiments on seven datasets, including six public benchmarks and one custom industrial dataset, demonstrate that PromptMix effectively enhances vision-language model adaptability, improves semantic representations, and achieves robust generalization under both base-to-novel and few-shot learning scenarios, delivering superior performance in engineering applications with limited labeled data.&lt;/p&gt;</content:encoded></item><item><title>UniPerception: Towards Unification of Perception Using Multi-Stage Training Pipeline in Adverse Weather Conditions</title><link>https://doi.org/10.1109/tiv.2026.3656901</link><guid>10.1109/tiv.2026.3656901</guid><pubDate>Thu, 22 Jan 2026 21:03:41 +0000</pubDate><dc:creator>Jianping Li</dc:creator><dc:creator>Qifan Tan</dc:creator><dc:creator>Songchao Tan</dc:creator><dc:creator>Xiao Ke</dc:creator><dc:creator>Zhiwei Li</dc:creator><dc:creator>Tianyu Shen</dc:creator><dc:creator>Guozhen Tan</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Vehicles</prism:publicationName><prism:doi>10.1109/tiv.2026.3656901</prism:doi><description>Panoptic perception forms the foundation for decision-making in autonomous vehicles. This comprehensive perception includes essential functions such as lane line detection, drivable area recognition and vehicle detection. However, existing methods mainly focus on normal weather conditions, resulting in a significant degradation of Panoptic perception performance under inclement weather conditions including snow rain and haze. To improve the accuracy and robustness of panoramic perception under inclement Weather conditions, a multi-task network is proposed termed UniPerception, which uses a hybrid architecture of Transformer and CNN and a multi-stage learning strategy for parameter updating. Due to the lack of a dataset for severe weather, we developed the BDD100 K dataset using image enhancement techniques. Experimental results indicate that the UniPerception model consistently outperforms advanced multitasking and single-tasking networks in a variety of tasks under inclement weather conditions.
Published: 2026-01-22T21:03:41+00:00
Venue: IEEE Transactions on Intelligent Vehicles
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianping Li; Qifan Tan; Songchao Tan; Xiao Ke; Zhiwei Li; Tianyu Shen; Guozhen Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Vehicles&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tiv.2026.3656901"&gt;10.1109/tiv.2026.3656901&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Panoptic perception forms the foundation for decision-making in autonomous vehicles. This comprehensive perception includes essential functions such as lane line detection, drivable area recognition and vehicle detection. However, existing methods mainly focus on normal weather conditions, resulting in a significant degradation of Panoptic perception performance under inclement weather conditions including snow rain and haze. To improve the accuracy and robustness of panoramic perception under inclement Weather conditions, a multi-task network is proposed termed UniPerception, which uses a hybrid architecture of Transformer and CNN and a multi-stage learning strategy for parameter updating. Due to the lack of a dataset for severe weather, we developed the BDD100 K dataset using image enhancement techniques. Experimental results indicate that the UniPerception model consistently outperforms advanced multitasking and single-tasking networks in a variety of tasks under inclement weather conditions.&lt;/p&gt;</content:encoded></item><item><title>PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2601.14716v1</link><guid>http://arxiv.org/abs/2601.14716v1</guid><pubDate>Wed, 21 Jan 2026 07:11:40 +0000</pubDate><dc:creator>Yao Lu</dc:creator><dc:creator>Dengdong Fan</dc:creator><dc:creator>Jianzheng Nie</dc:creator><dc:creator>Fan Xu</dc:creator><dc:creator>Jie Chen</dc:creator><dc:creator>Bin Zhou</dc:creator><dc:creator>Yonghong Tian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.
Published: 2026-01-21T07:11:40+00:00
Venue: arXiv
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yao Lu; Dengdong Fan; Jianzheng Nie; Fan Xu; Jie Chen; Bin Zhou; Yonghong Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.&lt;/p&gt;</content:encoded></item><item><title>CO
                    &lt;sup&gt;+&lt;/sup&gt;
                    &lt;sub&gt;3&lt;/sub&gt;
                    : Improved Collaborative Consortium of Foundation Models for Open-World Few-Shot Learning</title><link>https://doi.org/10.1109/tcsvt.2026.3656950</link><guid>10.1109/tcsvt.2026.3656950</guid><pubDate>Fri, 23 Jan 2026 21:01:10 +0000</pubDate><dc:creator>Shuai Shao</dc:creator><dc:creator>Rui Xu</dc:creator><dc:creator>Bingfeng Zhang</dc:creator><dc:creator>Baodi Liu</dc:creator><dc:creator>Weifeng Liu</dc:creator><dc:creator>Yicong Zhou</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3656950</prism:doi><description>Open-World Few-Shot Learning (OFSL) is a critical research domain focused on accurately identifying target samples under conditions where data is scarce and labels are unreliable. This field is highly relevant to real-world scenarios, holding significant practical implications. Currently, the field has only a few solutions, primarily relying on conventional methods such as metric learning and feature aggregation. However, these methods often struggle in more complex scenarios. Recent breakthroughs in foundation models such as CLIP and DINO have demonstrated their strong representational capabilities, even in resource-limited environments. These advancements have led to a shift from “training model from scratch” towards “exploiting the extensive capabilities and expertise of these pre-trained foundation models for OFSL”. Inspired by this shift, we introduce the Improved Collaborative Consortium of Foundation Models (CO+3), an extension of CO3, first presented in AAAI 2024. CO+3 significantly improves the accuracy of OFSL by integrating the strengths of four foundational models. It includes three decoupled blocks: (1) The Label Correction Block (LC-Block) rectifies unreliable labels, (2) the Data Augmentation Block (DA-Block) enriches the available data, and (3) the Text-guided Fusion Adapter (TeFu-Adapter) merges various features and reduces the impact of noisy labels through semantic constraints. We evaluate CO+3 across eleven benchmark datasets, comparing it against recent state-of-the-art methods. Our thorough evaluations demonstrate that the proposed CO+3 consistently surpasses existing methods by a substantial margin, particularly in high-noise scenarios.
Published: 2026-01-23T21:01:10+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Shao; Rui Xu; Bingfeng Zhang; Baodi Liu; Weifeng Liu; Yicong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3656950"&gt;10.1109/tcsvt.2026.3656950&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Open-World Few-Shot Learning (OFSL) is a critical research domain focused on accurately identifying target samples under conditions where data is scarce and labels are unreliable. This field is highly relevant to real-world scenarios, holding significant practical implications. Currently, the field has only a few solutions, primarily relying on conventional methods such as metric learning and feature aggregation. However, these methods often struggle in more complex scenarios. Recent breakthroughs in foundation models such as CLIP and DINO have demonstrated their strong representational capabilities, even in resource-limited environments. These advancements have led to a shift from “training model from scratch” towards “exploiting the extensive capabilities and expertise of these pre-trained foundation models for OFSL”. Inspired by this shift, we introduce the Improved Collaborative Consortium of Foundation Models (CO+3), an extension of CO3, first presented in AAAI 2024. CO+3 significantly improves the accuracy of OFSL by integrating the strengths of four foundational models. It includes three decoupled blocks: (1) The Label Correction Block (LC-Block) rectifies unreliable labels, (2) the Data Augmentation Block (DA-Block) enriches the available data, and (3) the Text-guided Fusion Adapter (TeFu-Adapter) merges various features and reduces the impact of noisy labels through semantic constraints. We evaluate CO+3 across eleven benchmark datasets, comparing it against recent state-of-the-art methods. Our thorough evaluations demonstrate that the proposed CO+3 consistently surpasses existing methods by a substantial margin, particularly in high-noise scenarios.&lt;/p&gt;</content:encoded></item><item><title>Unc-SOD: An Uncertainty Learning Framework for Small Object Detection</title><link>https://doi.org/10.1109/tip.2026.3654892</link><guid>10.1109/tip.2026.3654892</guid><pubDate>Thu, 22 Jan 2026 21:04:43 +0000</pubDate><dc:creator>Xiang Yuan</dc:creator><dc:creator>Gong Cheng</dc:creator><dc:creator>Jiacheng Cheng</dc:creator><dc:creator>Ruixiang Yao</dc:creator><dc:creator>Junwei Han</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654892</prism:doi><description>Small object detection (SOD) constitutes a notable yet immensely arduous task, stemming from the restricted informative regions inherent in size-limited instances, which further sparks off heightened uncertainty beyond the capacity of current two-stage detectors. Specifically, the intrinsic ambiguity in small objects undermines the prevailing sampling paradigms and may mislead the model to devote futile effort to those unrecognizable targets, while the inconsistency of features utilized for the detection at two stages further exposes the hierarchical uncertainty. In this paper, we develop an Uncertainty learning framework for Small Object Detection, dubbed as Unc-SOD. By incorporating an auxiliary uncertainty branch to conventional Region Proposal Network (RPN), we model the indeterminacy at instance-level which later on serves as a surrogate criterion for sampling, thereby unearthing adequate candidates dynamically based on the varying degrees of uncertainty and facilitating the learning of proposal networks. In parallel, a Perception-and-Interaction strategy is devised to capture rich and discriminative representations, through optimizing the intrinsic properties from the regional features at the original pyramid and the assigned one, in which the perceptual process unfolds in a mutual paradigm. As the seminal attempt to model uncertainty in SOD task, our Unc-SOD yields state-of-the-art performance on two large-scale small object detection benchmarks, SODA-D and SODA-A, and the results on several SOD-oriented datasets including COCO, VisDrone, and Tsinghua-Tencent 100K also exhibit the promotion to baseline detector. This underscores the efficacy of our approach and its superiority over prevailing detectors when dealing with small instances.
Published: 2026-01-22T21:04:43+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Yuan; Gong Cheng; Jiacheng Cheng; Ruixiang Yao; Junwei Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654892"&gt;10.1109/tip.2026.3654892&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Small object detection (SOD) constitutes a notable yet immensely arduous task, stemming from the restricted informative regions inherent in size-limited instances, which further sparks off heightened uncertainty beyond the capacity of current two-stage detectors. Specifically, the intrinsic ambiguity in small objects undermines the prevailing sampling paradigms and may mislead the model to devote futile effort to those unrecognizable targets, while the inconsistency of features utilized for the detection at two stages further exposes the hierarchical uncertainty. In this paper, we develop an Uncertainty learning framework for Small Object Detection, dubbed as Unc-SOD. By incorporating an auxiliary uncertainty branch to conventional Region Proposal Network (RPN), we model the indeterminacy at instance-level which later on serves as a surrogate criterion for sampling, thereby unearthing adequate candidates dynamically based on the varying degrees of uncertainty and facilitating the learning of proposal networks. In parallel, a Perception-and-Interaction strategy is devised to capture rich and discriminative representations, through optimizing the intrinsic properties from the regional features at the original pyramid and the assigned one, in which the perceptual process unfolds in a mutual paradigm. As the seminal attempt to model uncertainty in SOD task, our Unc-SOD yields state-of-the-art performance on two large-scale small object detection benchmarks, SODA-D and SODA-A, and the results on several SOD-oriented datasets including COCO, VisDrone, and Tsinghua-Tencent 100K also exhibit the promotion to baseline detector. This underscores the efficacy of our approach and its superiority over prevailing detectors when dealing with small instances.&lt;/p&gt;</content:encoded></item><item><title>LaCon: Late-Constraint Controllable Visual Generation</title><link>https://doi.org/10.1109/tip.2026.3654412</link><guid>10.1109/tip.2026.3654412</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Chang Liu</dc:creator><dc:creator>Rui Li</dc:creator><dc:creator>Kaidong Zhang</dc:creator><dc:creator>Yunwei Lan</dc:creator><dc:creator>Xin Luo</dc:creator><dc:creator>Dong Liu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654412</prism:doi><description>Diffusion models have demonstrated impressive abilities in generating photo-realistic and creative images. To offer more controllability for the generation process of diffusion models, previous studies normally adopt extra modules to integrate condition signals by manipulating the intermediate features of the noise predictors, where they often fail in conditions not seen in the training. Although subsequent studies are motivated to handle multi-condition control, they are mostly resource-consuming to implement, where more generalizable and efficient solutions are expected for controllable visual generation. In this paper, we present a late-constraint controllable visual generation method, namely LaCon, which enables generalization across various modalities and granularities for each single-condition control. LaCon establishes an alignment between the external condition and specific diffusion timesteps, and guides diffusion models to produce conditional results based on this built alignment. Experimental results on prevailing benchmark datasets illustrate the promising performance and generalization capability of LaCon under various conditions and settings. Ablation studies analyze different components in LaCon, illustrating its great potential to offer flexible condition controls for different backbones.
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chang Liu; Rui Li; Kaidong Zhang; Yunwei Lan; Xin Luo; Dong Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654412"&gt;10.1109/tip.2026.3654412&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models have demonstrated impressive abilities in generating photo-realistic and creative images. To offer more controllability for the generation process of diffusion models, previous studies normally adopt extra modules to integrate condition signals by manipulating the intermediate features of the noise predictors, where they often fail in conditions not seen in the training. Although subsequent studies are motivated to handle multi-condition control, they are mostly resource-consuming to implement, where more generalizable and efficient solutions are expected for controllable visual generation. In this paper, we present a late-constraint controllable visual generation method, namely LaCon, which enables generalization across various modalities and granularities for each single-condition control. LaCon establishes an alignment between the external condition and specific diffusion timesteps, and guides diffusion models to produce conditional results based on this built alignment. Experimental results on prevailing benchmark datasets illustrate the promising performance and generalization capability of LaCon under various conditions and settings. Ablation studies analyze different components in LaCon, illustrating its great potential to offer flexible condition controls for different backbones.&lt;/p&gt;</content:encoded></item><item><title>HR-SemNet: A High-Resolution Network for Enhanced Small Object Detection With Local Contextual Semantics</title><link>https://doi.org/10.1109/tip.2026.3654770</link><guid>10.1109/tip.2026.3654770</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Can Peng</dc:creator><dc:creator>Manxin Chao</dc:creator><dc:creator>Ruoyu Li</dc:creator><dc:creator>Zaiqing Chen</dc:creator><dc:creator>Lijun Yun</dc:creator><dc:creator>Yuelong Xia</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654770</prism:doi><description>Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Can Peng; Manxin Chao; Ruoyu Li; Zaiqing Chen; Lijun Yun; Yuelong Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654770"&gt;10.1109/tip.2026.3654770&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Using higher-resolution feature maps in the network is an effective approach for detecting small objects. However, high-resolution feature maps face the challenge of lacking semantic information. This has led previous methods to rely on downsampling feature maps, applying large-kernel convolution layers, and then upsampling the feature maps to obtain semantic information. However, these methods have certain limitations: first, large kernel convolutions in deeper layers typically provide significant global semantic information, but our experiments reveal that such prominent semantic information introduces background smear, which in turn leads to overfitting. Second, deep features often contain substantial redundant information, and the features of small objects are either minimal or have disappeared, which causes a degradation in detection performance when directly relying on deep features. To address these issues, we propose a high-resolution network based on local contextual semantics (HR-SemNet). The network is built on the proposed high-resolution backbone (HRB), which replaces the traditional backbone-FPN architecture by focusing all computational resources of large kernel convolutions on highresolution feature layers to capture clearer features of small objects. Additionally, a local context semantic module (LCSM) is employed to extract semantic information from the background, confining the semantic extraction to a local window to avoid interference from large-scale backgrounds and objects. HRSemNet decouples small object semantics from contextual semantics, with HRB and LCSM independently extracting these features. Extensive experiments and comprehensive evaluations on the VisDrone, AI-TOD, and TinyPerson datasets validate the effectiveness of the method. On the VisDrone dataset, which contains a large number of small objects, HR-SemNet improves the mean average precision (mAP) by 4.6%, reduces the computational cost (GFLOPs) by 49.9%, and decreases the param...&lt;/p&gt;</content:encoded></item><item><title>AITQE: An Adaptive Image-Text Quality Enhancer for Scalable MLLM Pretraining</title><link>https://doi.org/10.1109/tcsvt.2026.3657433</link><guid>10.1109/tcsvt.2026.3657433</guid><pubDate>Fri, 23 Jan 2026 21:01:10 +0000</pubDate><dc:creator>Han Huang</dc:creator><dc:creator>Yuqi Huo</dc:creator><dc:creator>Zijia Zhao</dc:creator><dc:creator>Haoyu Lu</dc:creator><dc:creator>Shu Wu</dc:creator><dc:creator>Bingning Wang</dc:creator><dc:creator>Qiang Liu</dc:creator><dc:creator>Weipeng Chen</dc:creator><dc:creator>Liang Wang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657433</prism:doi><description>Multimodal large language models (MLLMs) have made significant strides by integrating visual and textual modalities. A critical factor in training MLLMs is the quality of image-text pairs within multimodal pretraining datasets. However, in the process of high-quality data curation, filter-based paradigms often discard a substantial portion of high-quality images due to inadequate semantic alignment between images and texts, leading to inefficiency in data utilization and scalability. In this paper, we propose the Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically assesses and enhances the quality of image-text pairs. AITQE employs a text rewriting mechanism for low-quality pairs and incorporates a negative sample learning strategy to improve evaluative capabilities by integrating deliberately generated low-quality samples during training. Unlike prior approaches that significantly alter text distributions, our method minimally adjusts text to preserve data volume while enhancing quality. Experimental results demonstrate that AITQE surpasses existing methods on various benchmarks, effectively leveraging raw data and scaling with increasing data volumes. Codes and model are available at https://github.com/hanhuang22/AITQE.
Published: 2026-01-23T21:01:10+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Huang; Yuqi Huo; Zijia Zhao; Haoyu Lu; Shu Wu; Bingning Wang; Qiang Liu; Weipeng Chen; Liang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657433"&gt;10.1109/tcsvt.2026.3657433&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have made significant strides by integrating visual and textual modalities. A critical factor in training MLLMs is the quality of image-text pairs within multimodal pretraining datasets. However, in the process of high-quality data curation, filter-based paradigms often discard a substantial portion of high-quality images due to inadequate semantic alignment between images and texts, leading to inefficiency in data utilization and scalability. In this paper, we propose the Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically assesses and enhances the quality of image-text pairs. AITQE employs a text rewriting mechanism for low-quality pairs and incorporates a negative sample learning strategy to improve evaluative capabilities by integrating deliberately generated low-quality samples during training. Unlike prior approaches that significantly alter text distributions, our method minimally adjusts text to preserve data volume while enhancing quality. Experimental results demonstrate that AITQE surpasses existing methods on various benchmarks, effectively leveraging raw data and scaling with increasing data volumes. Codes and model are available at https://github.com/hanhuang22/AITQE.&lt;/p&gt;</content:encoded></item><item><title>CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation</title><link>https://arxiv.org/abs/2601.14695v1</link><guid>http://arxiv.org/abs/2601.14695v1</guid><pubDate>Wed, 21 Jan 2026 06:17:52 +0000</pubDate><dc:creator>Yutong Chen</dc:creator><dc:creator>Jiandong Gao</dc:creator><dc:creator>Ji Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.
Published: 2026-01-21T06:17:52+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yutong Chen; Jiandong Gao; Ji Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&amp;#x27;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&amp;#x27;s reasoning ability.&lt;/p&gt;</content:encoded></item><item><title>A Cross-Modality Feature Adaptive Interaction Approach for RGB-Infrared Object Detection in Aerial Imagery</title><link>https://doi.org/10.1109/tgrs.2026.3657379</link><guid>10.1109/tgrs.2026.3657379</guid><pubDate>Fri, 23 Jan 2026 20:58:37 +0000</pubDate><dc:creator>Chushi Yu</dc:creator><dc:creator>Yoan Shin</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657379</prism:doi><description>Object detection in aerial imagery, particularly from unmanned aerial vehicles (UAVs) and remote sensing platforms, is crucial but faces significant challenges such as modality misalignment, feature fusion degradation, and high computational complexity. To address these issues, this paper introduces CMFADet (Cross-Modality Feature Adaptive Detection), a novel framework for robust RGB-infrared object detection across diverse aerial scenarios. CMFADet improves feature learning through its innovative spatial-frequency feature enhancement module (SFEM) and infrared adaptive feature aggregation block (IR-AFAB). It also integrates a channel interaction fusion (CIF) module for dynamic weight allocation, ensuring truly complementary information integration and avoiding mutual interference. This allocation is governed by the specific characteristics of the target and the inherent strengths of each modality. Detection accuracy is further refined via an adaptive task-aware alignment head (ATAH) that learns the joint features. Extensive experiments on the DroneVehicle, VEDAI and OGSOD-1.0 datasets demonstrate CMFADet’s superior performance, consistently surpassing state-of- the-art algorithms, and effectively addressing the aforementioned challenges. The source code for this work is publicly available at https://github.com/Yooyoo95/CMFADet.
Published: 2026-01-23T20:58:37+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chushi Yu; Yoan Shin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657379"&gt;10.1109/tgrs.2026.3657379&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection in aerial imagery, particularly from unmanned aerial vehicles (UAVs) and remote sensing platforms, is crucial but faces significant challenges such as modality misalignment, feature fusion degradation, and high computational complexity. To address these issues, this paper introduces CMFADet (Cross-Modality Feature Adaptive Detection), a novel framework for robust RGB-infrared object detection across diverse aerial scenarios. CMFADet improves feature learning through its innovative spatial-frequency feature enhancement module (SFEM) and infrared adaptive feature aggregation block (IR-AFAB). It also integrates a channel interaction fusion (CIF) module for dynamic weight allocation, ensuring truly complementary information integration and avoiding mutual interference. This allocation is governed by the specific characteristics of the target and the inherent strengths of each modality. Detection accuracy is further refined via an adaptive task-aware alignment head (ATAH) that learns the joint features. Extensive experiments on the DroneVehicle, VEDAI and OGSOD-1.0 datasets demonstrate CMFADet’s superior performance, consistently surpassing state-of- the-art algorithms, and effectively addressing the aforementioned challenges. The source code for this work is publicly available at https://github.com/Yooyoo95/CMFADet.&lt;/p&gt;</content:encoded></item><item><title>All-weather Multi-Modality Image Fusion: Unified Framework and 100k Benchmark</title><link>https://doi.org/10.1016/j.inffus.2026.104130</link><guid>10.1016/j.inffus.2026.104130</guid><pubDate>Fri, 23 Jan 2026 16:54:15 +0000</pubDate><dc:creator>Xilai Li</dc:creator><dc:creator>Wuyang Liu</dc:creator><dc:creator>Xiaosong Li</dc:creator><dc:creator>Fuqiang Zhou</dc:creator><dc:creator>Huafeng Li</dc:creator><dc:creator>Feiping Nie</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104130</prism:doi><description>Multi-modality image fusion (MMIF) combines complementary information from different image modalities to provide a comprehensive and objective interpretation of scenes. However, existing fusion methods cannot resist different weather interferences in real-world scenes, limiting their practical applicability. To bridge this gap, we propose an end-to-end, unified all-weather MMIF model. Rather than focusing solely on pixel-level recovery, our method emphasizes maximizing the representation of key scene information through joint feature fusion and restoration. Specifically, we first decompose images into low-rank and sparse components, enabling effective feature separation for enhanced multi-modality perception. During feature recovery, we introduce a physically-aware clear feature prediction module, inferring variations in light transmission via illumination and reflectance. Clear features generated by the network are used to enhance salient information representation. We also construct a large-scale MMIF dataset with 100,000 image pairs comprehensively across rain, haze, and snow conditions, as well as covering various degradation levels and diverse scenes. Experimental results in both real-world and synthetic scenes demonstrate that the proposed method excels in image fusion and downstream tasks such as object detection, semantic segmentation, and depth estimation. The source code is available at https://github.com/ixilai/AWFusion .
Published: 2026-01-23T16:54:15+00:00
Venue: Information Fusion
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xilai Li; Wuyang Liu; Xiaosong Li; Fuqiang Zhou; Huafeng Li; Feiping Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104130"&gt;10.1016/j.inffus.2026.104130&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modality image fusion (MMIF) combines complementary information from different image modalities to provide a comprehensive and objective interpretation of scenes. However, existing fusion methods cannot resist different weather interferences in real-world scenes, limiting their practical applicability. To bridge this gap, we propose an end-to-end, unified all-weather MMIF model. Rather than focusing solely on pixel-level recovery, our method emphasizes maximizing the representation of key scene information through joint feature fusion and restoration. Specifically, we first decompose images into low-rank and sparse components, enabling effective feature separation for enhanced multi-modality perception. During feature recovery, we introduce a physically-aware clear feature prediction module, inferring variations in light transmission via illumination and reflectance. Clear features generated by the network are used to enhance salient information representation. We also construct a large-scale MMIF dataset with 100,000 image pairs comprehensively across rain, haze, and snow conditions, as well as covering various degradation levels and diverse scenes. Experimental results in both real-world and synthetic scenes demonstrate that the proposed method excels in image fusion and downstream tasks such as object detection, semantic segmentation, and depth estimation. The source code is available at https://github.com/ixilai/AWFusion .&lt;/p&gt;</content:encoded></item><item><title>Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models</title><link>https://arxiv.org/abs/2601.14327v1</link><guid>http://arxiv.org/abs/2601.14327v1</guid><pubDate>Tue, 20 Jan 2026 08:39:04 +0000</pubDate><dc:creator>YuanLab. ai</dc:creator><dc:creator>Shawn Wu</dc:creator><dc:creator>Jiangang Luo</dc:creator><dc:creator>Tong Yu</dc:creator><dc:creator>Darcy Chen</dc:creator><dc:creator>Sean Wang</dc:creator><dc:creator>Xudong Zhao</dc:creator><dc:creator>Louie Li</dc:creator><dc:creator>Claire Wang</dc:creator><dc:creator>Hunter He</dc:creator><dc:creator>Carol Wang</dc:creator><dc:creator>Allen Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.
Published: 2026-01-20T08:39:04+00:00
Venue: arXiv
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; YuanLab. ai; Shawn Wu; Jiangang Luo; Tong Yu; Darcy Chen; Sean Wang; Xudong Zhao; Louie Li; Claire Wang; Hunter He; Carol Wang; Allen Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.&lt;/p&gt;</content:encoded></item><item><title>Deep Learning-Based Object Pose Estimation: A Comprehensive Survey</title><link>https://doi.org/10.1007/s11263-025-02646-6</link><guid>10.1007/s11263-025-02646-6</guid><pubDate>Thu, 22 Jan 2026 05:23:16 +0000</pubDate><dc:creator>Jian Liu</dc:creator><dc:creator>Wei Sun</dc:creator><dc:creator>Hui Yang</dc:creator><dc:creator>Zhiwen Zeng</dc:creator><dc:creator>Chongpei Liu</dc:creator><dc:creator>Jin Zheng</dc:creator><dc:creator>Xingyu Liu</dc:creator><dc:creator>Hossein Rahmani</dc:creator><dc:creator>Nicu Sebe</dc:creator><dc:creator>Ajmal Mian</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02646-6</prism:doi><description>Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, i.e., instance-level, category-level, and unseen (including both instance-unseen and category-unseen cases) object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We cover the literature up to our submission date and will continue to follow the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation .
Published: 2026-01-22T05:23:16+00:00
Venue: International Journal of Computer Vision
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Liu; Wei Sun; Hui Yang; Zhiwen Zeng; Chongpei Liu; Jin Zheng; Xingyu Liu; Hossein Rahmani; Nicu Sebe; Ajmal Mian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02646-6"&gt;10.1007/s11263-025-02646-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, i.e., instance-level, category-level, and unseen (including both instance-unseen and category-unseen cases) object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We cover the literature up to our submission date and will continue to follow the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation .&lt;/p&gt;</content:encoded></item><item><title>Positive Data Augmentation Based on Manifold Heuristic Optimization for Image Classification</title><link>https://doi.org/10.1109/tpami.2026.3657249</link><guid>10.1109/tpami.2026.3657249</guid><pubDate>Fri, 23 Jan 2026 20:58:34 +0000</pubDate><dc:creator>Fangqing Liu</dc:creator><dc:creator>Han Huang</dc:creator><dc:creator>Fujian Feng</dc:creator><dc:creator>Xueming Yan</dc:creator><dc:creator>Zhifeng Hao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3657249</prism:doi><description>Data augmentation is crucial for addressing insufficient training data, especially for augmenting positive samples. However, existing methods mostly rely on neural network-based feedback for data augmentation and often overlook the optimization of feature distribution. In this study, we present a practical, distribution-preserving data augmentation pipeline that augments positive samples by optimizing a feature indicator (e.g., two-dimensional entropy), aiming to maintain alignment with the original data distribution. Inspired by the manifold hypothesis, we propose a Manifold Heuristic Optimization Algorithm (MHOA), which augments positive samples by exploring the low-dimensional Euclidean space around object contour pixels instead of the entire decision space. Guided by a “distribution-preservation-first” perspective, our approach explicitly optimizes fidelity to the original data manifold and only retains augmented samples whose feature statistics (e.g., mean, variance) align with the source class. It significantly improves image classification accuracy across neural networks, outperforming state-of-the-art data augmentation methods—especially when the dataset's feature indicator follows a Gaussian distribution. The algorithm's search space, focused on neighborhoods of key feature pixels, is the core driver of its superior performance.
Published: 2026-01-23T20:58:34+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fangqing Liu; Han Huang; Fujian Feng; Xueming Yan; Zhifeng Hao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3657249"&gt;10.1109/tpami.2026.3657249&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Data augmentation is crucial for addressing insufficient training data, especially for augmenting positive samples. However, existing methods mostly rely on neural network-based feedback for data augmentation and often overlook the optimization of feature distribution. In this study, we present a practical, distribution-preserving data augmentation pipeline that augments positive samples by optimizing a feature indicator (e.g., two-dimensional entropy), aiming to maintain alignment with the original data distribution. Inspired by the manifold hypothesis, we propose a Manifold Heuristic Optimization Algorithm (MHOA), which augments positive samples by exploring the low-dimensional Euclidean space around object contour pixels instead of the entire decision space. Guided by a “distribution-preservation-first” perspective, our approach explicitly optimizes fidelity to the original data manifold and only retains augmented samples whose feature statistics (e.g., mean, variance) align with the source class. It significantly improves image classification accuracy across neural networks, outperforming state-of-the-art data augmentation methods—especially when the dataset&amp;#x27;s feature indicator follows a Gaussian distribution. The algorithm&amp;#x27;s search space, focused on neighborhoods of key feature pixels, is the core driver of its superior performance.&lt;/p&gt;</content:encoded></item><item><title>Millimeter-Wave Radar Dataset for Automotive SAR Imaging and Interpretation</title><link>https://doi.org/10.1109/tits.2026.3651565</link><guid>10.1109/tits.2026.3651565</guid><pubDate>Thu, 22 Jan 2026 21:03:32 +0000</pubDate><dc:creator>Cuiqi Si</dc:creator><dc:creator>Bo Zhao</dc:creator><dc:creator>Lei Huang</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2026.3651565</prism:doi><description>The operational safety and efficiency of Intelligent Transportation Systems (ITS) critically depend on reliable environmental perception, which is severely compromised in adverse weather conditions where conventional vision sensors fail. Synthetic aperture radar (SAR) technology, which can deliver high-resolution images under severe weather and poor-light conditions, is being increasingly integrated into automotive systems. However, the challenges like dynamic trajectory estimation and real-time SAR imaging remain significant obstacles. This paper proposes a novel automotive SAR system designed to function as an all-weather perception enabler for autonomous driving. By employing sub-aperture scheme for SAR system, the approach eliminates the need for complicated and time-consuming range cell migration and motion correction. Due to the short coherent accumulation, the instant range doppler algorithm enables high-efficient SAR imagery generation while maintaining two-dimensional (2D) high-resolution performance, which allows precise target detection. The theoretical analysis and experimental results confirm the effectiveness of the proposed scheme. Then, a benchmark is firstly established, to advance the data-driven radar perception in ITS. To validate the radar data, diverse targets in SAR images are annotated, providing a robust database support for the automotive SAR target recognition. Furthermore, the value of this dataset for ITS is demonstrated by evaluating various mainstream deep learning methods on object detection tasks, confirming its potential to enhance the robustness of perception modules in intelligent vehicles.
Published: 2026-01-22T21:03:32+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cuiqi Si; Bo Zhao; Lei Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2026.3651565"&gt;10.1109/tits.2026.3651565&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;The operational safety and efficiency of Intelligent Transportation Systems (ITS) critically depend on reliable environmental perception, which is severely compromised in adverse weather conditions where conventional vision sensors fail. Synthetic aperture radar (SAR) technology, which can deliver high-resolution images under severe weather and poor-light conditions, is being increasingly integrated into automotive systems. However, the challenges like dynamic trajectory estimation and real-time SAR imaging remain significant obstacles. This paper proposes a novel automotive SAR system designed to function as an all-weather perception enabler for autonomous driving. By employing sub-aperture scheme for SAR system, the approach eliminates the need for complicated and time-consuming range cell migration and motion correction. Due to the short coherent accumulation, the instant range doppler algorithm enables high-efficient SAR imagery generation while maintaining two-dimensional (2D) high-resolution performance, which allows precise target detection. The theoretical analysis and experimental results confirm the effectiveness of the proposed scheme. Then, a benchmark is firstly established, to advance the data-driven radar perception in ITS. To validate the radar data, diverse targets in SAR images are annotated, providing a robust database support for the automotive SAR target recognition. Furthermore, the value of this dataset for ITS is demonstrated by evaluating various mainstream deep learning methods on object detection tasks, confirming its potential to enhance the robustness of perception modules in intelligent vehicles.&lt;/p&gt;</content:encoded></item><item><title>FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2601.14690v1</link><guid>http://arxiv.org/abs/2601.14690v1</guid><pubDate>Wed, 21 Jan 2026 06:06:36 +0000</pubDate><dc:creator>Yian Huang</dc:creator><dc:creator>Qing Qin</dc:creator><dc:creator>Aji Mao</dc:creator><dc:creator>Xiangyu Qiu</dc:creator><dc:creator>Liang Xu</dc:creator><dc:creator>Xian Zhang</dc:creator><dc:creator>Zhenming Peng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.
Published: 2026-01-21T06:06:36+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yian Huang; Qing Qin; Aji Mao; Xiangyu Qiu; Liang Xu; Xian Zhang; Zhenming Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.&lt;/p&gt;</content:encoded></item><item><title>DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment</title><link>https://doi.org/10.1109/tcsvt.2026.3657415</link><guid>10.1109/tcsvt.2026.3657415</guid><pubDate>Fri, 23 Jan 2026 21:01:10 +0000</pubDate><dc:creator>Li Yu</dc:creator><dc:creator>Situo Wang</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Moncef Gabbouj</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657415</prism:doi><description>Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.
Published: 2026-01-23T21:01:10+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Yu; Situo Wang; Wei Zhou; Moncef Gabbouj&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657415"&gt;10.1109/tcsvt.2026.3657415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Inspired by the dual-stream (dorsal and ventral streams) theory of the human visual system (HVS), recent Video Quality Assessment (VQA) methods have integrated Contrastive Language-Image Pretraining (CLIP) to enhance semantic understanding. However, as CLIP is originally designed for images, it lacks the ability to adequately capture the temporal dynamics and motion perception (dorsal stream) inherent in videos. To address this limitation, we propose DVLTA-VQA (Decoupled Vision-Language Modeling with Text-Guided Adaptation), which decouples CLIP’s visual and textual components to better align with the NR-VQA pipeline. Specifically, we introduce a Video-Based Temporal CLIP module and a Temporal Context Module to explicitly model motion dynamics, effectively enhancing the dorsal stream representation. Complementing this, a Basic Visual Feature Extraction Module is employed to strengthen spatial detail analysis in the ventral stream. Furthermore, we propose a text-guided adaptive fusion strategy that leverages textual semantics to dynamically weight visual features, facilitating effective spatiotemporal integration. Extensive experiments on multiple public datasets demonstrate that the proposed method achieves state-of-the-art performance, significantly improving prediction accuracy and generalization capability.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Multi-Task Visual Representation Learning</title><link>https://arxiv.org/abs/2601.13886v1</link><guid>http://arxiv.org/abs/2601.13886v1</guid><pubDate>Tue, 20 Jan 2026 11:59:19 +0000</pubDate><dc:creator>Shangzhe Di</dc:creator><dc:creator>Zhonghua Zhai</dc:creator><dc:creator>Weidi Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.
Published: 2026-01-20T11:59:19+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shangzhe Di; Zhonghua Zhai; Weidi Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &amp;quot;expert&amp;quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &amp;quot;best-of-both-worlds&amp;quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.&lt;/p&gt;</content:encoded></item><item><title>Knowledge distillation with spatial semantic enhancement for remote sensing object detection</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.017</link><guid>10.1016/j.isprsjprs.2026.01.017</guid><pubDate>Thu, 22 Jan 2026 17:36:36 +0000</pubDate><dc:creator>Kai Hu</dc:creator><dc:creator>Jiaxin Li</dc:creator><dc:creator>Nan Ji</dc:creator><dc:creator>Xueshang Xiang</dc:creator><dc:creator>Kai Jiang</dc:creator><dc:creator>Xieping Gao</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.017</prism:doi><description>Knowledge distillation is extensively utilized in remote sensing object detection within resource-constrained environments. Among knowledge distillation methods, prediction imitation has garnered significant attention due to its ease of deployment. However, prevailing prediction imitation paradigms, which rely on an isolated, point-wise alignment of prediction scores, neglect the crucial spatial semantic information. This oversight is particularly detrimental in remote sensing images due to the abundance of objects with weak feature responses. To this end, we propose a novel Spatial Semantic Enhanced Knowledge Distillation framework, called S 2 " role="presentation"&gt; S 2 S 2 EKD , for remote sensing object detection. Through two complementary modules, S 2 " role="presentation"&gt; S 2 S 2 EKD shifts the focus of prediction imitation from matching isolated values to learning structured spatial semantic information. First, for classification distillation, we introduce a Weak-feature Response Enhancement Module, which models the structured spatial relationships between objects and their background to establish an initial perception of objects with weak feature responses. Second, to further capture more refined spatial information, we propose a Teacher Boundary Refinement Module for localization distillation. It provides robust boundary guidance by constructing a regression target enriched with more comprehensive spatial information. Furthermore, we introduce a Feature Mapping mechanism to ensure this spatial semantic knowledge is effectively utilized. Through extensive experiments on the DIOR and DOTA-v1.0 datasets, our method’s superiority is consistently demonstrated across diverse architectures, including both single-stage and two-stage detectors. The results show that our S 2 " role="presentation"&gt; S 2 S 2 EKD achieves state-of-the-art results and, in some cases, even surpasses the performance of its teacher model. The code will be available soon.
Published: 2026-01-22T17:36:36+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Hu; Jiaxin Li; Nan Ji; Xueshang Xiang; Kai Jiang; Xieping Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.017"&gt;10.1016/j.isprsjprs.2026.01.017&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation is extensively utilized in remote sensing object detection within resource-constrained environments. Among knowledge distillation methods, prediction imitation has garnered significant attention due to its ease of deployment. However, prevailing prediction imitation paradigms, which rely on an isolated, point-wise alignment of prediction scores, neglect the crucial spatial semantic information. This oversight is particularly detrimental in remote sensing images due to the abundance of objects with weak feature responses. To this end, we propose a novel Spatial Semantic Enhanced Knowledge Distillation framework, called S 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; S 2 S 2 EKD , for remote sensing object detection. Through two complementary modules, S 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; S 2 S 2 EKD shifts the focus of prediction imitation from matching isolated values to learning structured spatial semantic information. First, for classification distillation, we introduce a Weak-feature Response Enhancement Module, which models the structured spatial relationships between objects and their background to establish an initial perception of objects with weak feature responses. Second, to further capture more refined spatial information, we propose a Teacher Boundary Refinement Module for localization distillation. It provides robust boundary guidance by constructing a regression target enriched with more comprehensive spatial information. Furthermore, we introduce a Feature Mapping mechanism to ensure this spatial semantic knowledge is effectively utilized. Through extensive experiments on the DIOR and DOTA-v1.0 datasets, our method’s superiority is consistently demonstrated across diverse architectures, including both single-stage and two-stage detectors. The results show that our S 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; S 2 S 2 EKD achieves state-of-the-art results and, in some cases, even surpasses the performance of its teacher model. The code will be available soon.&lt;/p&gt;</content:encoded></item><item><title>Towards Understanding Best Practices for Quantization of Vision-Language Models</title><link>https://arxiv.org/abs/2601.15287v1</link><guid>http://arxiv.org/abs/2601.15287v1</guid><pubDate>Wed, 21 Jan 2026 18:59:51 +0000</pubDate><dc:creator>Gautom Das</dc:creator><dc:creator>Vincent La</dc:creator><dc:creator>Ethan Lau</dc:creator><dc:creator>Abhinav Shrivastava</dc:creator><dc:creator>Matthew Gwilliam</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.
Published: 2026-01-21T18:59:51+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gautom Das; Vincent La; Ethan Lau; Abhinav Shrivastava; Matthew Gwilliam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.&lt;/p&gt;</content:encoded></item><item><title>Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning</title><link>https://arxiv.org/abs/2601.15160v1</link><guid>http://arxiv.org/abs/2601.15160v1</guid><pubDate>Wed, 21 Jan 2026 16:38:59 +0000</pubDate><dc:creator>Yuval Kansal</dc:creator><dc:creator>Niraj K. Jha</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.
Published: 2026-01-21T16:38:59+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuval Kansal; Niraj K. Jha&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &amp;quot;compositional bridge&amp;quot;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.&lt;/p&gt;</content:encoded></item><item><title>Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception</title><link>https://doi.org/10.1109/tcsvt.2026.3657111</link><guid>10.1109/tcsvt.2026.3657111</guid><pubDate>Thu, 22 Jan 2026 21:03:59 +0000</pubDate><dc:creator>Lianqing Zheng</dc:creator><dc:creator>Jianan Liu</dc:creator><dc:creator>Runwei Guan</dc:creator><dc:creator>Long Yang</dc:creator><dc:creator>Shouyi Lu</dc:creator><dc:creator>Yuanzhe Li</dc:creator><dc:creator>Xiaokai Bai</dc:creator><dc:creator>Jie Bai</dc:creator><dc:creator>Zhixiong Ma</dc:creator><dc:creator>Hui-Liang Shen</dc:creator><dc:creator>Xichan Zhu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657111</prism:doi><description>3D object detection and occupancy prediction are critical tasks in autonomous driving, attracting significant attention. Despite the potential of recent vision-based methods, they encounter challenges under adverse conditions. Thus, integrating cameras with next-generation 4D imaging radar to achieve unified multi-task perception is highly significant, though research in this domain remains limited. In this paper, we propose Doracamom, the first framework that fuses multi-view cameras and 4D radar for joint 3D object detection and semantic occupancy prediction, enabling comprehensive environmental perception. Specifically, we introduce a novel Coarse Voxel Queries Generator that integrates geometric priors from 4D radar with semantic features from images to initialize voxel queries, establishing a robust foundation for subsequent Transformer-based refinement. To leverage temporal information, we design a Dual-Branch Temporal Encoder that processes multi-modal temporal features in parallel across BEV and voxel spaces, enabling comprehensive spatio-temporal representation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusion module that adaptively fuses complementary features through attention mechanisms while employing auxiliary tasks to enhance feature quality. Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSet datasets demonstrate that Doracamom achieves state-of-the-art performance in both tasks, establishing new benchmarks for multi-modal 3D perception. Code and models can be available at https: //github.com/TJRadarLab/Doracamom.
Published: 2026-01-22T21:03:59+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lianqing Zheng; Jianan Liu; Runwei Guan; Long Yang; Shouyi Lu; Yuanzhe Li; Xiaokai Bai; Jie Bai; Zhixiong Ma; Hui-Liang Shen; Xichan Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657111"&gt;10.1109/tcsvt.2026.3657111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detection and occupancy prediction are critical tasks in autonomous driving, attracting significant attention. Despite the potential of recent vision-based methods, they encounter challenges under adverse conditions. Thus, integrating cameras with next-generation 4D imaging radar to achieve unified multi-task perception is highly significant, though research in this domain remains limited. In this paper, we propose Doracamom, the first framework that fuses multi-view cameras and 4D radar for joint 3D object detection and semantic occupancy prediction, enabling comprehensive environmental perception. Specifically, we introduce a novel Coarse Voxel Queries Generator that integrates geometric priors from 4D radar with semantic features from images to initialize voxel queries, establishing a robust foundation for subsequent Transformer-based refinement. To leverage temporal information, we design a Dual-Branch Temporal Encoder that processes multi-modal temporal features in parallel across BEV and voxel spaces, enabling comprehensive spatio-temporal representation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusion module that adaptively fuses complementary features through attention mechanisms while employing auxiliary tasks to enhance feature quality. Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSet datasets demonstrate that Doracamom achieves state-of-the-art performance in both tasks, establishing new benchmarks for multi-modal 3D perception. Code and models can be available at https: //github.com/TJRadarLab/Doracamom.&lt;/p&gt;</content:encoded></item><item><title>Weak supervision makes strong details: fine-grained object recognition in remote sensing images via regional diffusion with VLM</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.024</link><guid>10.1016/j.isprsjprs.2026.01.024</guid><pubDate>Fri, 23 Jan 2026 14:36:28 +0000</pubDate><dc:creator>Liuqian Wang</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Guangming Mi</dc:creator><dc:creator>Li Zhuo</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.024</prism:doi><description>Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .
Published: 2026-01-23T14:36:28+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liuqian Wang; Jing Zhang; Guangming Mi; Li Zhuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.024"&gt;10.1016/j.isprsjprs.2026.01.024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-grained object recognition (FGOR) is gaining increasing attention in automated remote sensing analysis and interpretation (RSAI). However, the full potential of FGOR in remote sensing images (RSIs) is still constrained by several key issues: the reliance on high-quality labeled data, the difficulty of reconstructing fine details in low-resolution images, and the limited robustness of FGOR model for distinguishing similar object categories. In response, we propose an automatic fine-grained object recognition network (AutoFGOR) that follows a hierarchical dual-pipeline architecture for object analysis at global and regional levels. Specifically, Pipeline I: region detection network, which leverages geometric invariance module for weakly-supervised learning to improve the detection accuracy of sparsely labeled RSIs and extract category-free regions; and on top of that, Pipeline II: regional diffusion with vision language model (RD-VLM), which pioneers the combination of stable diffusion XL (SDXL) and large language and vision assistant (LLaVA) through a specially designed adaptive resolution adaptor (ARA) for object region super-resolution reconstruction, fundamentally solving the difficulties of feature extraction from low-quality regions and fine-grained feature mining. In addition, we introduce a winner-takes-all (WTA) strategy that utilizes a voting mechanism to enhance the reliability of fine-grained classification in complex scenes. Experimental results on FAIR1M-v2.0, VEDAI, and HRSC2016 datasets demonstrate our AutoFGOR achieving 31.72%, 80.25%, and 88.05% mAP, respectively, with highly competitive performance. In addition, the × 4 reconstruction results achieve scores of 0.5275 and 0.8173 on the MANIQA and CLIP-IQA indicators, respectively. The code will be available on GitHub: https://github.com/BJUT-AIVBD/AutoFGOR .&lt;/p&gt;</content:encoded></item><item><title>What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study</title><link>https://arxiv.org/abs/2601.14888v1</link><guid>http://arxiv.org/abs/2601.14888v1</guid><pubDate>Wed, 21 Jan 2026 11:22:29 +0000</pubDate><dc:creator>Keyu Lv</dc:creator><dc:creator>Manyi Zhang</dc:creator><dc:creator>Xiaobo Xia</dc:creator><dc:creator>Jingchen Ni</dc:creator><dc:creator>Shannan Yan</dc:creator><dc:creator>Xianzhi Yu</dc:creator><dc:creator>Lu Hou</dc:creator><dc:creator>Chun Yuan</dc:creator><dc:creator>Haoli Bai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.
Published: 2026-01-21T11:22:29+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keyu Lv; Manyi Zhang; Xiaobo Xia; Jingchen Ni; Shannan Yan; Xianzhi Yu; Lu Hou; Chun Yuan; Haoli Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.&lt;/p&gt;</content:encoded></item><item><title>Unleashing Mamba’s Expressive Power: A Non-tradeoff Approach to Spatio-Temporal Forecasting</title><link>https://doi.org/10.1016/j.inffus.2026.104172</link><guid>10.1016/j.inffus.2026.104172</guid><pubDate>Fri, 23 Jan 2026 00:27:32 +0000</pubDate><dc:creator>Zhiqi Shao</dc:creator><dc:creator>Ze Wang</dc:creator><dc:creator>Haoning Xi</dc:creator><dc:creator>Michael G H Bell</dc:creator><dc:creator>Xusheng Yao</dc:creator><dc:creator>D. Glenn Geers</dc:creator><dc:creator>Junbin Gao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104172</prism:doi><description>Real-time spatiotemporal forecasting, particularly in traffic systems, requires balancing computational cost and predictive accuracy—a challenge that conventional methods struggle to address effectively. In this work, we propose a non-trade-off framework called Spatial-Temporal Selective State Space (ST-Mamba), which leverages two key components to achieve both efficiency and accuracy concurrently. The Spatial-Temporal Mixer (ST-Mixer) dynamically fuses spatial and temporal features to capture complex dependencies, and the STF-Mamba layer incorporates Mamba’s selective state-space formulation to capture long-range dynamics efficiently. Beyond empirical improvements, we address a critical gap in the literature by presenting a theoretical analysis of ST-Mamba’s expressive power. Specifically, we establish its ability to approximate a broad class of Transformer and formally demonstrate its equivalence to at least two consecutive attention layers within the same framework. This result highlights ST-Mamba’s capacity to capture long-range dependencies while reducing computational overhead efficiently, reinforcing its theoretical and practical advantages over conventional transformer-based models. Through extensive evaluations of real-world traffic datasets, ST-Mamba demonstrates a 61.11% reduction in runtime alongside a 0.67% improvement in predictive performance compared to leading approaches, underscoring its potential to set a new benchmark for real-time spatiotemporal forecasting.
Published: 2026-01-23T00:27:32+00:00
Venue: Information Fusion
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiqi Shao; Ze Wang; Haoning Xi; Michael G H Bell; Xusheng Yao; D. Glenn Geers; Junbin Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104172"&gt;10.1016/j.inffus.2026.104172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time spatiotemporal forecasting, particularly in traffic systems, requires balancing computational cost and predictive accuracy—a challenge that conventional methods struggle to address effectively. In this work, we propose a non-trade-off framework called Spatial-Temporal Selective State Space (ST-Mamba), which leverages two key components to achieve both efficiency and accuracy concurrently. The Spatial-Temporal Mixer (ST-Mixer) dynamically fuses spatial and temporal features to capture complex dependencies, and the STF-Mamba layer incorporates Mamba’s selective state-space formulation to capture long-range dynamics efficiently. Beyond empirical improvements, we address a critical gap in the literature by presenting a theoretical analysis of ST-Mamba’s expressive power. Specifically, we establish its ability to approximate a broad class of Transformer and formally demonstrate its equivalence to at least two consecutive attention layers within the same framework. This result highlights ST-Mamba’s capacity to capture long-range dependencies while reducing computational overhead efficiently, reinforcing its theoretical and practical advantages over conventional transformer-based models. Through extensive evaluations of real-world traffic datasets, ST-Mamba demonstrates a 61.11% reduction in runtime alongside a 0.67% improvement in predictive performance compared to leading approaches, underscoring its potential to set a new benchmark for real-time spatiotemporal forecasting.&lt;/p&gt;</content:encoded></item><item><title>A Data Fusion Approach to Synthesize Microwave Imagery of Tropical Cyclones from Infrared Data using Vision Transformers</title><link>https://doi.org/10.1016/j.inffus.2026.104167</link><guid>10.1016/j.inffus.2026.104167</guid><pubDate>Fri, 23 Jan 2026 00:27:27 +0000</pubDate><dc:creator>Fan Meng</dc:creator><dc:creator>Tao Song</dc:creator><dc:creator>Xianxuan Lin</dc:creator><dc:creator>Kunlin Yang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104167</prism:doi><description>Microwave images with high spatiotemporal resolution are essential for observing and predicting tropical cyclones (TCs), including TC positioning, intensity estimation, and detection of concentric eyewall. Nevertheless, the temporal resolution of tropical cyclone microwave (TCMW) images is limited due to satellite quantity and orbit constraints, presenting a challenging problem for TC disaster forecasting. This research suggests a multi-sensor data fusion approach, using high-temporal-resolution tropical cyclone infrared (TCIR) images to generate synthetic TCMW images, offering a solution to this data scarcity problem. In particular, we introduce a deep learning network based on the Vision Transformer (TCA-ViT) to translate TCIR images into TCMW images. This can be viewed as a form of synthetic data generation, enhancing the available information for decision-making. We integrate a phase-based physical guidance mechanism into the training process. Furthermore, we have developed a dataset of TC infrared-to-microwave image conversions (TCIR2MW) for training and testing the model. Experimental results demonstrate the method’s capability in rapidly and accurately extracting key features of TCs. Leveraging techniques like Mask and Transfer Learning, it addresses the absence of TCMW images by generating MW images from IR images, thereby aiding downstream tasks like TC intensity and precipitation forecasting. This study introduces a novel approach to the field of TC image research, with the potential to advance deep learning in this direction and provide vital insights for real-time observation and prediction of global TCs. Our source code and data are publicly available online at https://github.com/kleenY/TCIR2MW .
Published: 2026-01-23T00:27:27+00:00
Venue: Information Fusion
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fan Meng; Tao Song; Xianxuan Lin; Kunlin Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104167"&gt;10.1016/j.inffus.2026.104167&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Microwave images with high spatiotemporal resolution are essential for observing and predicting tropical cyclones (TCs), including TC positioning, intensity estimation, and detection of concentric eyewall. Nevertheless, the temporal resolution of tropical cyclone microwave (TCMW) images is limited due to satellite quantity and orbit constraints, presenting a challenging problem for TC disaster forecasting. This research suggests a multi-sensor data fusion approach, using high-temporal-resolution tropical cyclone infrared (TCIR) images to generate synthetic TCMW images, offering a solution to this data scarcity problem. In particular, we introduce a deep learning network based on the Vision Transformer (TCA-ViT) to translate TCIR images into TCMW images. This can be viewed as a form of synthetic data generation, enhancing the available information for decision-making. We integrate a phase-based physical guidance mechanism into the training process. Furthermore, we have developed a dataset of TC infrared-to-microwave image conversions (TCIR2MW) for training and testing the model. Experimental results demonstrate the method’s capability in rapidly and accurately extracting key features of TCs. Leveraging techniques like Mask and Transfer Learning, it addresses the absence of TCMW images by generating MW images from IR images, thereby aiding downstream tasks like TC intensity and precipitation forecasting. This study introduces a novel approach to the field of TC image research, with the potential to advance deep learning in this direction and provide vital insights for real-time observation and prediction of global TCs. Our source code and data are publicly available online at https://github.com/kleenY/TCIR2MW .&lt;/p&gt;</content:encoded></item><item><title>Topology-Guided Semantic Face Center Estimation for Rotation-Invariant Face Detection</title><link>https://doi.org/10.1109/tip.2026.3654422</link><guid>10.1109/tip.2026.3654422</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Hathai Kaewkorn</dc:creator><dc:creator>Lifang Zhou</dc:creator><dc:creator>Weisheng Li</dc:creator><dc:creator>Chengjiang Long</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654422</prism:doi><description>Face detection accuracy significantly decreases under rotational variations, including in-plane (RIP) and out-of-plane (ROP) rotations. ROP is particularly problematic due to its impact on landmark distortion, which leads to inaccurate face center localization. Meanwhile, many existing rotation-invariant models are primarily designed to handle RIP, they often fail under ROP because they lack the ability to capture semantic and topological relationships. Moreover, existing datasets frequently suffer from unreliable landmark annotations caused by imperfect ground truth labeling, the absence of precise center annotations, and imbalanced data across different rotation angles. To address these challenges, we propose a topology-guided semantic face center estimation method that leverages graph-based landmark relationships to preserve structural integrity under both RIP and ROP. Additionally, we construct a rotation-aware face dataset with accurate face center annotations and balanced rotational diversity to support training under extreme pose conditions. Next, we introduce a Hybrid-ViT model that fuses CNN spatial features with transformer-based global context and employ a center-guided module for robust landmark localization under extreme rotations. In order to evaluate center quality, we further design a hybrid metric that combines topological geometry with semantic perception for a more comprehensive evaluation of face center accuracy. Finally, experimental results demonstrate that our method outperforms state-of-the-art models in cross-dataset evaluations. Code: https://github.com/Catster111/TCE_RIFD.
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hathai Kaewkorn; Lifang Zhou; Weisheng Li; Chengjiang Long&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654422"&gt;10.1109/tip.2026.3654422&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Face detection accuracy significantly decreases under rotational variations, including in-plane (RIP) and out-of-plane (ROP) rotations. ROP is particularly problematic due to its impact on landmark distortion, which leads to inaccurate face center localization. Meanwhile, many existing rotation-invariant models are primarily designed to handle RIP, they often fail under ROP because they lack the ability to capture semantic and topological relationships. Moreover, existing datasets frequently suffer from unreliable landmark annotations caused by imperfect ground truth labeling, the absence of precise center annotations, and imbalanced data across different rotation angles. To address these challenges, we propose a topology-guided semantic face center estimation method that leverages graph-based landmark relationships to preserve structural integrity under both RIP and ROP. Additionally, we construct a rotation-aware face dataset with accurate face center annotations and balanced rotational diversity to support training under extreme pose conditions. Next, we introduce a Hybrid-ViT model that fuses CNN spatial features with transformer-based global context and employ a center-guided module for robust landmark localization under extreme rotations. In order to evaluate center quality, we further design a hybrid metric that combines topological geometry with semantic perception for a more comprehensive evaluation of face center accuracy. Finally, experimental results demonstrate that our method outperforms state-of-the-art models in cross-dataset evaluations. Code: https://github.com/Catster111/TCE_RIFD.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Multi-scale Lagrange Dynamics Spatial-Temporal Network for 3D Skeleton-based Human Motion Prediction</title><link>https://doi.org/10.1109/tcsvt.2026.3657489</link><guid>10.1109/tcsvt.2026.3657489</guid><pubDate>Fri, 23 Jan 2026 21:01:10 +0000</pubDate><dc:creator>Hanghang Zhou</dc:creator><dc:creator>Yumei Zhang</dc:creator><dc:creator>Xiangying Guo</dc:creator><dc:creator>Keying Zhao</dc:creator><dc:creator>Honghong Yang</dc:creator><dc:creator>Xiaojun Wu</dc:creator><dc:creator>Zexing Du</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657489</prism:doi><description>Human body dynamics, as a temporal variation pattern of pose sequences in 3D skeleton-based human motion prediction, has been extensively studied in spatial-temporal dependent modeling of deep learning. However, designing an effective modeling approach that fully harnesses physical principles to enhance algorithmic performance remains a challenge. Existing approaches prioritize displacement information, processing deterministic physical parameters via standard neural networks while modeling rotation motion through simplified angular constraints. Such physical approximation methods neglect the high-dimensional and dynamic characteristics of Dynamics variables, undermining the integrity and diversity of human motion feature representations. To alleviate these limitations, we propose an Adaptive Multi-scale Lagrange Dynamics Spatial-Temporal Network (AMLD-STNet), which directly embeds learnable neural network modules within physical equations to activate multi-scale dynamic physical feature modeling of human motion. Specifically, A Lagrange Dynamics Network (LD-Net) is constructed, which designs a set of joint force adjacency matrices to analyze the mechanical correlation between the velocity and acceleration of each joint motion through the Lagrange Dynamics equation. Subsequently, the Lagrange Dynamic Spatial-Temporal Network (LD-STNet) is established, which utilizes LD-Net to extract multi-perspective high-dimensional features of human displacement and rotational motion represented by Dynamics pose variables. To capture the mechanical correlation of joint node groups, we design a multi-scale streams LD-STNet, which can realize adaptive scale transformation according to the joint force adjacency. Additionally, Euler angle loss is employed to enforce rotational consistency constraints, thereby enhancing physical realism during network training. Finally, extensive experiments are conducted on three popular benchmarks, such as Human 3.6M, AMASS, and 3DPW, among which AM...
Published: 2026-01-23T21:01:10+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanghang Zhou; Yumei Zhang; Xiangying Guo; Keying Zhao; Honghong Yang; Xiaojun Wu; Zexing Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657489"&gt;10.1109/tcsvt.2026.3657489&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Human body dynamics, as a temporal variation pattern of pose sequences in 3D skeleton-based human motion prediction, has been extensively studied in spatial-temporal dependent modeling of deep learning. However, designing an effective modeling approach that fully harnesses physical principles to enhance algorithmic performance remains a challenge. Existing approaches prioritize displacement information, processing deterministic physical parameters via standard neural networks while modeling rotation motion through simplified angular constraints. Such physical approximation methods neglect the high-dimensional and dynamic characteristics of Dynamics variables, undermining the integrity and diversity of human motion feature representations. To alleviate these limitations, we propose an Adaptive Multi-scale Lagrange Dynamics Spatial-Temporal Network (AMLD-STNet), which directly embeds learnable neural network modules within physical equations to activate multi-scale dynamic physical feature modeling of human motion. Specifically, A Lagrange Dynamics Network (LD-Net) is constructed, which designs a set of joint force adjacency matrices to analyze the mechanical correlation between the velocity and acceleration of each joint motion through the Lagrange Dynamics equation. Subsequently, the Lagrange Dynamic Spatial-Temporal Network (LD-STNet) is established, which utilizes LD-Net to extract multi-perspective high-dimensional features of human displacement and rotational motion represented by Dynamics pose variables. To capture the mechanical correlation of joint node groups, we design a multi-scale streams LD-STNet, which can realize adaptive scale transformation according to the joint force adjacency. Additionally, Euler angle loss is employed to enforce rotational consistency constraints, thereby enhancing physical realism during network training. Finally, extensive experiments are conducted on three popular benchmarks, such as Human 3.6M, AMASS, and 3DPW, among which AM...&lt;/p&gt;</content:encoded></item><item><title>RAM-VQA: Restoration Assisted Multi-modality Video Quality Assessment</title><link>https://doi.org/10.1109/tip.2026.3655117</link><guid>10.1109/tip.2026.3655117</guid><pubDate>Fri, 23 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Pengfei Chen</dc:creator><dc:creator>Jiebin Yan</dc:creator><dc:creator>Rajiv Soundararajan</dc:creator><dc:creator>Giuseppe Valenzise</dc:creator><dc:creator>Cai Li</dc:creator><dc:creator>Leida Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3655117</prism:doi><description>Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.
Published: 2026-01-23T21:01:47+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Chen; Jiebin Yan; Rajiv Soundararajan; Giuseppe Valenzise; Cai Li; Leida Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3655117"&gt;10.1109/tip.2026.3655117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Video Quality Assessment (VQA) strives to computationally emulate human perceptual judgments and has garnered significant attention given its widespread applicability. However, existing methodologies face two primary impediments: (1) limited proficiency in evaluating samples at quality extremes (e.g., severely degraded or near-perfect videos), and (2) insufficient sensitivity to nuanced quality variations arising from a misalignment with human perceptual mechanisms. Although vision-language models offer promising semantic understanding, their reliance on visual encoders pre-trained for high-level tasks often compromises their sensitivity to low-level distortions. To surmount these challenges, we propose the Restoration-Assisted Multi-modality VQA (RAM-VQA) framework. Uniquely, our approach leverages video restoration as a proxy to explicitly model distortion-sensitive features. The framework operates through two synergistic stages: a prompt learning stage that constructs a quality-aware textual space using triple-level references (degraded, restored, and pristine) derived from the restoration process, and a dual-branch evaluation stage that integrates semantic cues with technical quality indicators via spatio-temporal differential analysis. Extensive experiments demonstrate that RAM-VQA achieves state-of-the-art performance across diverse benchmarks, exhibiting superior capability in handling extreme-quality content while ensuring robust generalization.&lt;/p&gt;</content:encoded></item></channel></rss>