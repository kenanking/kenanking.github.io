<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 11 Jan 2026 02:54:08 +0000</lastBuildDate><item><title>Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.004</link><guid>10.1016/j.isprsjprs.2026.01.004</guid><pubDate>Fri, 09 Jan 2026 09:38:18 +0000</pubDate><dc:creator>Gui Gao</dc:creator><dc:creator>Caiyi Li</dc:creator><dc:creator>Xi Zhang</dc:creator><dc:creator>Bingxiu Yao</dc:creator><dc:creator>Zhen Chen</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.004</prism:doi><description>Ship detection is crucial for both military and civilian applications and is a key use of polarimetric SAR (PolSAR). While convolutional neural networks (CNNs) enhance PolSAR ship detection with powerful feature extraction, existing approaches still face challenges in discriminating targets from clutter, detecting multi-scale objects in complex scenes, and achieving real-time detection. To address these issues, we propose a Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering. First, at the input feature level, a Dual-perception feature of Doppler and Scattering (DDS) is introduced, effectively differentiating ship and clutter pixels to enhance the network’s ship discrimination. Specifically, Doppler characteristics distinguish between moving and stationary targets, while scattering characteristics reveal fundamental differences between targets and clutter. Second, at the network architecture level, a Mamba-CNN hybrid Multi-scale ship detection Network (MCMN) is designed to improve multi-scale ship detection in complex scenarios. It uses a Multi-scale Information Perception Module (MIPM) to adaptively aggregate multi-scale features and a Local-Global Feature Enhancement Module (LGFEM) based on Mamba for long-range context modeling. MCMN remains efficient through feature grouping, pointwise and depthwise convolutions, meeting real-time requirements. Finally, extensive experiments on the GF-3 and SSDD datasets demonstrate the superiority of DDS and MCMN. DDS effectively distinguishes ships from clutter across scenarios. As an input feature, it boosts average F1-score and AP by 4.3% and 4.3%, respectively, over HV intensity, and outperforms other polarization features. MCMN achieves state-of-the-art results, improving AP by 1.2% and 0.8% on the two datasets while reducing parameters by 1.29M, FLOPs by 1.5G, and inference time by 59.2%.
Published: 2026-01-09T09:38:18+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.863 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gui Gao; Caiyi Li; Xi Zhang; Bingxiu Yao; Zhen Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004"&gt;10.1016/j.isprsjprs.2026.01.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.863 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection is crucial for both military and civilian applications and is a key use of polarimetric SAR (PolSAR). While convolutional neural networks (CNNs) enhance PolSAR ship detection with powerful feature extraction, existing approaches still face challenges in discriminating targets from clutter, detecting multi-scale objects in complex scenes, and achieving real-time detection. To address these issues, we propose a Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering. First, at the input feature level, a Dual-perception feature of Doppler and Scattering (DDS) is introduced, effectively differentiating ship and clutter pixels to enhance the network’s ship discrimination. Specifically, Doppler characteristics distinguish between moving and stationary targets, while scattering characteristics reveal fundamental differences between targets and clutter. Second, at the network architecture level, a Mamba-CNN hybrid Multi-scale ship detection Network (MCMN) is designed to improve multi-scale ship detection in complex scenarios. It uses a Multi-scale Information Perception Module (MIPM) to adaptively aggregate multi-scale features and a Local-Global Feature Enhancement Module (LGFEM) based on Mamba for long-range context modeling. MCMN remains efficient through feature grouping, pointwise and depthwise convolutions, meeting real-time requirements. Finally, extensive experiments on the GF-3 and SSDD datasets demonstrate the superiority of DDS and MCMN. DDS effectively distinguishes ships from clutter across scenarios. As an input feature, it boosts average F1-score and AP by 4.3% and 4.3%, respectively, over HV intensity, and outperforms other polarization features. MCMN achieves state-of-the-art results, improving AP by 1.2% and 0.8% on the two datasets while reducing parameters by 1.29M, FLOPs by 1.5G, and inference time by 59.2%.&lt;/p&gt;</content:encoded></item><item><title>Enhancing cloud detection across multiple satellite sensors using a combined Swin Transformer and UPerNet deep learning model</title><link>https://doi.org/10.1016/j.rse.2025.115206</link><guid>10.1016/j.rse.2025.115206</guid><pubDate>Fri, 09 Jan 2026 20:33:37 +0000</pubDate><dc:creator>Shulin Pang</dc:creator><dc:creator>Zhanqing Li</dc:creator><dc:creator>Lin Sun</dc:creator><dc:creator>Biao Cao</dc:creator><dc:creator>Zhihui Wang</dc:creator><dc:creator>Xinyuan Xi</dc:creator><dc:creator>Xiaohang Shi</dc:creator><dc:creator>Jing Xu</dc:creator><dc:creator>Jing Wei</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115206</prism:doi><description>Cloud detection is crucial in many applications of satellite remote sensing data. Traditional cloud detection methods typically operate at the pixel level, relying on empirically tuned thresholds or, more recently, machine learning classification schemes based on training datasets. Motivated by the success of the Transformer with its self-attention mechanism and convolutional neural networks for enhanced feature extraction, we propose a new encoder-decoder method that captures global and regional contexts with multi-scale features. This new model takes advantage of two advanced deep-learning techniques, the Swin Transformer and UPerNet (named STUPmask), demonstrating improved cloud detection accuracy and strong adaptability to diverse imagery types, spanning spectral bands from visible to thermal infrared and spatial resolutions from meters to kilometers, across a wide range of surface types, including bright scenes such as ice and desert, globally. Training and validation of the STUPmask model are conducted using data obtained from the Landsat 8 and Sentinel-2 Manually Cloud Validation Mask datasets on a global scale. STUPmask accurately estimates cloud amount with a marginal difference against reference masks (0.27 % for Landsat 8 and −0.81 % for Sentinel-2). Additionally, the model captures cloud distribution with a high overall classification accuracy (97.51 % for Landsat 8 and 96.27 % for Sentinel-2). Notably, it excels in detecting broken, thin, and semi-transparent clouds across diverse surfaces, including bright surfaces like urban and barren lands, especially with acceptable accuracy over snow and ice. These encompass the majority of challenging scenes encountered by cloud identification methods. It also adapts to cross-sensor satellite data with varying spatial resolutions (4 m–2 km) from both Low-Earth-Orbit (LEO) and Geostationary-Earth-Orbit (GEO) platforms (including GaoFen-2, MODIS, and Himawari-8), with an overall accuracy of 94.21–97.11 %. The demonstrated successes in the automatic identification of clouds with a variety of satellite imagery of different spectral channels and spatial resolutions render the method versatile for a wide range of remote sensing studies.
Published: 2026-01-09T20:33:37+00:00
Venue: Remote Sensing of Environment
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shulin Pang; Zhanqing Li; Lin Sun; Biao Cao; Zhihui Wang; Xinyuan Xi; Xiaohang Shi; Jing Xu; Jing Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115206"&gt;10.1016/j.rse.2025.115206&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Cloud detection is crucial in many applications of satellite remote sensing data. Traditional cloud detection methods typically operate at the pixel level, relying on empirically tuned thresholds or, more recently, machine learning classification schemes based on training datasets. Motivated by the success of the Transformer with its self-attention mechanism and convolutional neural networks for enhanced feature extraction, we propose a new encoder-decoder method that captures global and regional contexts with multi-scale features. This new model takes advantage of two advanced deep-learning techniques, the Swin Transformer and UPerNet (named STUPmask), demonstrating improved cloud detection accuracy and strong adaptability to diverse imagery types, spanning spectral bands from visible to thermal infrared and spatial resolutions from meters to kilometers, across a wide range of surface types, including bright scenes such as ice and desert, globally. Training and validation of the STUPmask model are conducted using data obtained from the Landsat 8 and Sentinel-2 Manually Cloud Validation Mask datasets on a global scale. STUPmask accurately estimates cloud amount with a marginal difference against reference masks (0.27 % for Landsat 8 and −0.81 % for Sentinel-2). Additionally, the model captures cloud distribution with a high overall classification accuracy (97.51 % for Landsat 8 and 96.27 % for Sentinel-2). Notably, it excels in detecting broken, thin, and semi-transparent clouds across diverse surfaces, including bright surfaces like urban and barren lands, especially with acceptable accuracy over snow and ice. These encompass the majority of challenging scenes encountered by cloud identification methods. It also adapts to cross-sensor satellite data with varying spatial resolutions (4 m–2 km) from both Low-Earth-Orbit (LEO) and Geostationary-Earth-Orbit (GEO) platforms (including GaoFen-2, MODIS, and Himawari-8), with an overall accuracy of 94.21–97.11 %. The demonstrated successes in the automatic identification of clouds with a variety of satellite imagery of different spectral channels and spatial resolutions render the method versatile for a wide range of remote sensing studies.&lt;/p&gt;</content:encoded></item><item><title>AgriFM: A multi-source temporal remote sensing foundation model for Agriculture mapping</title><link>https://doi.org/10.1016/j.rse.2026.115234</link><guid>10.1016/j.rse.2026.115234</guid><pubDate>Fri, 09 Jan 2026 11:12:22 +0000</pubDate><dc:creator>Wenyuan Li</dc:creator><dc:creator>Shunlin Liang</dc:creator><dc:creator>Keyan Chen</dc:creator><dc:creator>Yongzhe Chen</dc:creator><dc:creator>Han Ma</dc:creator><dc:creator>Jianglei Xu</dc:creator><dc:creator>Yichuan Ma</dc:creator><dc:creator>Yuxiang Zhang</dc:creator><dc:creator>Shikang Guan</dc:creator><dc:creator>Husheng Fang</dc:creator><dc:creator>Zhenwei Shi</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115234</prism:doi><description>Climate change and population growth intensify the demand for precise agriculture mapping to enhance food security. Such mapping tasks require robust modeling of multi-scale spatiotemporal patterns from fine field textures to landscape context, and from short-term phenology to full growing-season dynamics. Existing methods often process spatial and temporal features separately, limiting their ability to capture essential agricultural dynamics. While transformer-based remote sensing foundation models (RSFMs) offer unified spatiotemporal modeling ability, most of them remain suboptimal: they either use fixed windows that ignore multi-scale crop characteristics or neglect temporal information entirely. To address these gaps, we propose AgriFM, a multi-source, multi-temporal foundation model for agriculture mapping. AgriFM introduces a synchronized spatiotemporal downsampling strategy within a Video Swin Transformer backbone, enabling efficient handling of long and variable-length satellite time series while preserving multi-scale spatial and phenological information. It is pre-trained on a globally representative dataset comprising over 25 million samples from MODIS, Landsat-8/9, and Sentinel-2 with land cover fractions as pre-training supervision. AgriFM further integrates a versatile decoder specifically designed to dynamically fuse multi-source features from different stages of backbone and accommodate varying temporal lengths, thereby supporting consistent and scalable agriculture mapping across diverse satellite sources and task requirements. It supports diverse tasks including agricultural land mapping, field boundary delineation, agricultural land use/land cover mapping, and specific crop mapping (e.g., winter wheat and paddy rice) with different data sources. Comprehensive evaluations show that AgriFM consistently outperforms existing deep learning models and general-purpose RSFMs across multiple agriculture mapping tasks. Codes and models are available at https://github.com/flyakon/AgriFM and https://glass.hku.hk
Published: 2026-01-09T11:12:22+00:00
Venue: Remote Sensing of Environment
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenyuan Li; Shunlin Liang; Keyan Chen; Yongzhe Chen; Han Ma; Jianglei Xu; Yichuan Ma; Yuxiang Zhang; Shikang Guan; Husheng Fang; Zhenwei Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115234"&gt;10.1016/j.rse.2026.115234&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Climate change and population growth intensify the demand for precise agriculture mapping to enhance food security. Such mapping tasks require robust modeling of multi-scale spatiotemporal patterns from fine field textures to landscape context, and from short-term phenology to full growing-season dynamics. Existing methods often process spatial and temporal features separately, limiting their ability to capture essential agricultural dynamics. While transformer-based remote sensing foundation models (RSFMs) offer unified spatiotemporal modeling ability, most of them remain suboptimal: they either use fixed windows that ignore multi-scale crop characteristics or neglect temporal information entirely. To address these gaps, we propose AgriFM, a multi-source, multi-temporal foundation model for agriculture mapping. AgriFM introduces a synchronized spatiotemporal downsampling strategy within a Video Swin Transformer backbone, enabling efficient handling of long and variable-length satellite time series while preserving multi-scale spatial and phenological information. It is pre-trained on a globally representative dataset comprising over 25 million samples from MODIS, Landsat-8/9, and Sentinel-2 with land cover fractions as pre-training supervision. AgriFM further integrates a versatile decoder specifically designed to dynamically fuse multi-source features from different stages of backbone and accommodate varying temporal lengths, thereby supporting consistent and scalable agriculture mapping across diverse satellite sources and task requirements. It supports diverse tasks including agricultural land mapping, field boundary delineation, agricultural land use/land cover mapping, and specific crop mapping (e.g., winter wheat and paddy rice) with different data sources. Comprehensive evaluations show that AgriFM consistently outperforms existing deep learning models and general-purpose RSFMs across multiple agriculture mapping tasks. Codes and models are available at https://github.com/flyakon/AgriFM and https://glass.hku.hk&lt;/p&gt;</content:encoded></item><item><title>Delving into Pre-training for Domain Transfer: A Broad Study of Pre-training for Domain Generalization and Domain Adaptation</title><link>https://doi.org/10.1007/s11263-025-02590-5</link><guid>10.1007/s11263-025-02590-5</guid><pubDate>Fri, 09 Jan 2026 18:22:43 +0000</pubDate><dc:creator>Jungmyung Wi</dc:creator><dc:creator>Youngkyun Jang</dc:creator><dc:creator>Dujin Lee</dc:creator><dc:creator>Myeongseok Nam</dc:creator><dc:creator>Donghyun Kim</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02590-5</prism:doi><description>As deep learning models suffer from domain shifts, domain transfer methods have been developed to learn robust and reliable feature representations on unseen domains. Existing domain transfer methods, such as domain adaptation and domain generalization, focused on developing new adaptation or alignment algorithms, typically utilizing outdated ResNet backbones pre-trained on ImageNet-1K. However, the impact of recent pre-training approaches on domain transfer has not been thoroughly investigated. In this work, we provide a broad study and in-depth analysis of pre-training for domain adaptation and generalization from four distinct perspectives; network architectures, sizes, pre-training objectives, and pre-training datasets. Our extensive experiments cover a variety of domain transfer settings, including domain generalization, unsupervised domain adaptation, source free domain adaptation, and universal domain adaptation. Our study reveals two key findings: (1) state-of-the-art pre-training has a greater impact on performance than advanced generalization or adaptation techniques, (2) domain adaptation baselines tend to overfit to older pre-training backbones, indicating that top-performing methods under previous settings may no longer be optimal with modern pre-training, and (3) these trends are also observed in other tasks, such as object detection and semantic segmentation. Furthermore, we investigate what makes pre-training effective for domain transfer. Interestingly, our findings suggest that the performance gains are largely due to the presence of a significantly higher number of classes in recent pre-training datasets (e.g., ImageNet-22K) that closely resemble those in downstream tasks, rather than solely the result of large-scale data. In addition, we examine potential train/test contamination between web-scale pre-training datasets and downstream benchmarks and find that such data leakage has only a negligible impact on evaluation. We hope this work highlights the importance of pre-training for domain transfer and offers valuable insights for future domain transfer research.
Published: 2026-01-09T18:22:43+00:00
Venue: International Journal of Computer Vision
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jungmyung Wi; Youngkyun Jang; Dujin Lee; Myeongseok Nam; Donghyun Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02590-5"&gt;10.1007/s11263-025-02590-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;As deep learning models suffer from domain shifts, domain transfer methods have been developed to learn robust and reliable feature representations on unseen domains. Existing domain transfer methods, such as domain adaptation and domain generalization, focused on developing new adaptation or alignment algorithms, typically utilizing outdated ResNet backbones pre-trained on ImageNet-1K. However, the impact of recent pre-training approaches on domain transfer has not been thoroughly investigated. In this work, we provide a broad study and in-depth analysis of pre-training for domain adaptation and generalization from four distinct perspectives; network architectures, sizes, pre-training objectives, and pre-training datasets. Our extensive experiments cover a variety of domain transfer settings, including domain generalization, unsupervised domain adaptation, source free domain adaptation, and universal domain adaptation. Our study reveals two key findings: (1) state-of-the-art pre-training has a greater impact on performance than advanced generalization or adaptation techniques, (2) domain adaptation baselines tend to overfit to older pre-training backbones, indicating that top-performing methods under previous settings may no longer be optimal with modern pre-training, and (3) these trends are also observed in other tasks, such as object detection and semantic segmentation. Furthermore, we investigate what makes pre-training effective for domain transfer. Interestingly, our findings suggest that the performance gains are largely due to the presence of a significantly higher number of classes in recent pre-training datasets (e.g., ImageNet-22K) that closely resemble those in downstream tasks, rather than solely the result of large-scale data. In addition, we examine potential train/test contamination between web-scale pre-training datasets and downstream benchmarks and find that such data leakage has only a negligible impact on evaluation. We hope this work highlights the importance of pre-training for domain transfer and offers valuable insights for future domain transfer research.&lt;/p&gt;</content:encoded></item><item><title>X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval</title><link>https://doi.org/10.1016/j.eswa.2026.131169</link><guid>10.1016/j.eswa.2026.131169</guid><pubDate>Fri, 09 Jan 2026 08:03:00 +0000</pubDate><dc:creator>Aparna H</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:creator>Avik Hati</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131169</prism:doi><description>Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.
Published: 2026-01-09T08:03:00+00:00
Venue: Expert Systems with Applications
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aparna H; Biplab Banerjee; Avik Hati&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131169"&gt;10.1016/j.eswa.2026.131169&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.&lt;/p&gt;</content:encoded></item><item><title>MambaFPN: A SSM-based Feature Pyramid Network for Object Detection</title><link>https://doi.org/10.1016/j.neunet.2026.108544</link><guid>10.1016/j.neunet.2026.108544</guid><pubDate>Sat, 10 Jan 2026 00:23:34 +0000</pubDate><dc:creator>Le Liang</dc:creator><dc:creator>Cheng Wang</dc:creator><dc:creator>Lefei Zhang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108544</prism:doi><description>Object detection is a fundamental task in computer vision, aiming to localize and classify objects within images. Feature pyramid networks (FPNs) play a crucial role in modern object detectors by constructing hierarchical multi-scale feature maps to effectively handle objects of varying sizes. However, most existing advanced FPN methods rely heavily on convolutional neural networks (CNNs), which struggle to capture global context information. To address this limitation, we propose leveraging vision mamba blocks to enhance global modeling capabilities. The vanilla vision mamba block, through its state space mechanism, enables global context modeling for every spatial pixel within a single feature map. Building on this, we first use vision mamba blocks to extract global information from individual feature maps in the hierarchy. Subsequently, additional vision mamba blocks facilitate inter-scale information exchange among multi-scale feature maps, ensuring comprehensive global context integration. The proposed method, termed MambaFPN, significantly enhances object detector performance. For instance, it improves the Average Precision (AP) of vanilla FPN from 38.6 to 39.4, with fewer parameters. This demonstrates the effectiveness and efficiency of MambaFPN in advancing object detection.
Published: 2026-01-10T00:23:34+00:00
Venue: Neural Networks
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Le Liang; Cheng Wang; Lefei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108544"&gt;10.1016/j.neunet.2026.108544&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection is a fundamental task in computer vision, aiming to localize and classify objects within images. Feature pyramid networks (FPNs) play a crucial role in modern object detectors by constructing hierarchical multi-scale feature maps to effectively handle objects of varying sizes. However, most existing advanced FPN methods rely heavily on convolutional neural networks (CNNs), which struggle to capture global context information. To address this limitation, we propose leveraging vision mamba blocks to enhance global modeling capabilities. The vanilla vision mamba block, through its state space mechanism, enables global context modeling for every spatial pixel within a single feature map. Building on this, we first use vision mamba blocks to extract global information from individual feature maps in the hierarchy. Subsequently, additional vision mamba blocks facilitate inter-scale information exchange among multi-scale feature maps, ensuring comprehensive global context integration. The proposed method, termed MambaFPN, significantly enhances object detector performance. For instance, it improves the Average Precision (AP) of vanilla FPN from 38.6 to 39.4, with fewer parameters. This demonstrates the effectiveness and efficiency of MambaFPN in advancing object detection.&lt;/p&gt;</content:encoded></item><item><title>Unraveling Domain Styles for Enhanced Cross-Domain Generalization</title><link>https://doi.org/10.1016/j.knosys.2026.115302</link><guid>10.1016/j.knosys.2026.115302</guid><pubDate>Sat, 10 Jan 2026 16:17:21 +0000</pubDate><dc:creator>Zhonghua Yao</dc:creator><dc:creator>Juncheng Lian</dc:creator><dc:creator>Qiang Zhang</dc:creator><dc:creator>Yanming Guo</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115302</prism:doi><description>Domain generalization is aimed at enabling deep neural networks trained on multiple source domains to generalize effectively to unseen target domains. In this study, we propose a unified and novel framework for domain generalization in image classification. Unlike existing approaches that only adapt domain-invariant features or augment styles, our method implicitly separates style and semantic representations via three novel modules: (1) a Dynamic Mapping Module that implicitly preserves style information while maintaining semantic consistency, (2) a Spatial Regrouping Weighting Module that selectively emphasizes domain-adaptive spatial semantics via attention-guided regrouping, and (3) a Distribution Metrics Alignment Module that aligns high-order feature distributions across domains to reduce domain shift. Unlike existing works that passively align domains or suppress domain-specific cues, our framework actively leverages these for robust generalization. Extensive experiments across three standard domain generalization(DG) benchmarks, namely, Digits-DG, PACS, and Office-Home, demonstrate that our method achieves state-of-the-art performance.
Published: 2026-01-10T16:17:21+00:00
Venue: Knowledge-Based Systems
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhonghua Yao; Juncheng Lian; Qiang Zhang; Yanming Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115302"&gt;10.1016/j.knosys.2026.115302&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Domain generalization is aimed at enabling deep neural networks trained on multiple source domains to generalize effectively to unseen target domains. In this study, we propose a unified and novel framework for domain generalization in image classification. Unlike existing approaches that only adapt domain-invariant features or augment styles, our method implicitly separates style and semantic representations via three novel modules: (1) a Dynamic Mapping Module that implicitly preserves style information while maintaining semantic consistency, (2) a Spatial Regrouping Weighting Module that selectively emphasizes domain-adaptive spatial semantics via attention-guided regrouping, and (3) a Distribution Metrics Alignment Module that aligns high-order feature distributions across domains to reduce domain shift. Unlike existing works that passively align domains or suppress domain-specific cues, our framework actively leverages these for robust generalization. Extensive experiments across three standard domain generalization(DG) benchmarks, namely, Digits-DG, PACS, and Office-Home, demonstrate that our method achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection</title><link>https://arxiv.org/abs/2601.04381v1</link><guid>http://arxiv.org/abs/2601.04381v1</guid><pubDate>Wed, 07 Jan 2026 20:41:26 +0000</pubDate><dc:creator>Maxim Clouser</dc:creator><dc:creator>Kia Khezeli</dc:creator><dc:creator>John Kalantari</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.
Published: 2026-01-07T20:41:26+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maxim Clouser; Kia Khezeli; John Kalantari&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.&lt;/p&gt;</content:encoded></item><item><title>SCMT-Net: Spatial Curvature and Motion Temporal Feature Synergy Network for Multi-Frame Infrared Small Target Detection</title><link>https://doi.org/10.3390/rs18020215</link><guid>10.3390/rs18020215</guid><pubDate>Fri, 09 Jan 2026 11:45:33 +0000</pubDate><dc:creator>Ruiqi Yang</dc:creator><dc:creator>Yuan Liu</dc:creator><dc:creator>Ming Zhu</dc:creator><dc:creator>Huiping Zhu</dc:creator><dc:creator>Yuanfu Yuan</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020215</prism:doi><description>Infrared small target (IRST) detection remains a challenging task due to extremely small target sizes, low signal-to-noise ratios (SNR), and complex background clutter. Existing methods often fail to balance reliable detection with low false alarm rates due to limited spatial–temporal modeling. To address this, we propose a multi-frame network that synergistically integrates spatial curvature and temporal motion consistency. Specifically, in the single-frame stage, a Gaussian Curvature Attention (GCA) module is introduced to exploit spatial curvature and geometric saliency, enhancing the discriminability of weak targets. In the multi-frame stage, a Motion-Aware Encoding Block (MAEB) utilizes MotionPool3D to capture temporal motion consistency and extract salient motion regions, while a Temporal Consistency Enhancement Module (TCEM) further refines cross-frame features to effectively suppress noise. Extensive experiments demonstrate that the proposed method achieves advanced overall performance. In particular, under low-SNR conditions, the method improves the detection rate by 0.29% while maintaining a low false alarm rate, providing an effective solution for the stable detection of weak and small targets.
Published: 2026-01-09T11:45:33+00:00
Venue: Remote Sensing
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiqi Yang; Yuan Liu; Ming Zhu; Huiping Zhu; Yuanfu Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020215"&gt;10.3390/rs18020215&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target (IRST) detection remains a challenging task due to extremely small target sizes, low signal-to-noise ratios (SNR), and complex background clutter. Existing methods often fail to balance reliable detection with low false alarm rates due to limited spatial–temporal modeling. To address this, we propose a multi-frame network that synergistically integrates spatial curvature and temporal motion consistency. Specifically, in the single-frame stage, a Gaussian Curvature Attention (GCA) module is introduced to exploit spatial curvature and geometric saliency, enhancing the discriminability of weak targets. In the multi-frame stage, a Motion-Aware Encoding Block (MAEB) utilizes MotionPool3D to capture temporal motion consistency and extract salient motion regions, while a Temporal Consistency Enhancement Module (TCEM) further refines cross-frame features to effectively suppress noise. Extensive experiments demonstrate that the proposed method achieves advanced overall performance. In particular, under low-SNR conditions, the method improves the detection rate by 0.29% while maintaining a low false alarm rate, providing an effective solution for the stable detection of weak and small targets.&lt;/p&gt;</content:encoded></item><item><title>Research on Infrared Small Target Detection Technology Based on DCS-YOLO Algorithm</title><link>https://doi.org/10.1016/j.dsp.2026.105898</link><guid>10.1016/j.dsp.2026.105898</guid><pubDate>Fri, 09 Jan 2026 00:18:13 +0000</pubDate><dc:creator>Meng Yin</dc:creator><dc:creator>Binghe Sun</dc:creator><dc:creator>Rugang Wang</dc:creator><dc:creator>Yuanyuan Wang</dc:creator><dc:creator>Feng Zhou</dc:creator><dc:creator>Xuesheng Bian</dc:creator><prism:publicationName>Digital Signal Processing</prism:publicationName><prism:doi>10.1016/j.dsp.2026.105898</prism:doi><description>To address the challenges of weak features, susceptibility to complex background interference in infrared small targets, and the high computational cost of existing specialized detection models, this paper proposes the Dual-Domain Fusion and Class-Aware Self-supervised YOLO (DCS-YOLO). This framework leverages dual-domain feature fusion and class-aware self-supervised learning for semantic enhancement. During feature extraction, a Class-aware Self-supervised Semantic Fusion Module (CSSFM) utilizes a class-aware self-supervised architecture as a deep semantic guide for generating discriminative semantic features, thereby enhancing the perception of faint target characteristics. Additionally, a Dual-domain Aware Enhancement Module (A2C2f_DDA) is designed, which analyzes the high-frequency components of small targets and employs a spatial-frequency domain feature complementary fusion strategy to sharpen feature capture while suppressing background clutter. For feature upsampling and fusion, a Multi-dimensional Selective Feature Pyramid Network (MSFPN) employs a frequency-domain, spatial, and channel three-dimensional cooperative selection mechanism, integrated with deep semantic information, to enhance feature integration across dimensions and improve detection performance in complex scenes. Furthermore, lightweight components including GSConv, VoVGSCSP, and LSCD-Detect are incorporated to reduce computational complexity and model parameters. Comprehensive evaluations on the IRSTD-1K, RealScene-ISTD, and SIRST-v2 datasets demonstrate the effectiveness of the proposed algorithm, achieving mAP@0.5 scores of 80.7%, 90.2%, and 93.3%, respectively. The results indicate that the algorithm effectively utilizes frequency-domain analysis and semantic enhancement, providing a powerful and efficient solution for infrared small target detection in complex scenarios while maintaining a favorable balance between accuracy and computational cost.
Published: 2026-01-09T00:18:13+00:00
Venue: Digital Signal Processing
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Yin; Binghe Sun; Rugang Wang; Yuanyuan Wang; Feng Zhou; Xuesheng Bian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Digital Signal Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.dsp.2026.105898"&gt;10.1016/j.dsp.2026.105898&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;To address the challenges of weak features, susceptibility to complex background interference in infrared small targets, and the high computational cost of existing specialized detection models, this paper proposes the Dual-Domain Fusion and Class-Aware Self-supervised YOLO (DCS-YOLO). This framework leverages dual-domain feature fusion and class-aware self-supervised learning for semantic enhancement. During feature extraction, a Class-aware Self-supervised Semantic Fusion Module (CSSFM) utilizes a class-aware self-supervised architecture as a deep semantic guide for generating discriminative semantic features, thereby enhancing the perception of faint target characteristics. Additionally, a Dual-domain Aware Enhancement Module (A2C2f_DDA) is designed, which analyzes the high-frequency components of small targets and employs a spatial-frequency domain feature complementary fusion strategy to sharpen feature capture while suppressing background clutter. For feature upsampling and fusion, a Multi-dimensional Selective Feature Pyramid Network (MSFPN) employs a frequency-domain, spatial, and channel three-dimensional cooperative selection mechanism, integrated with deep semantic information, to enhance feature integration across dimensions and improve detection performance in complex scenes. Furthermore, lightweight components including GSConv, VoVGSCSP, and LSCD-Detect are incorporated to reduce computational complexity and model parameters. Comprehensive evaluations on the IRSTD-1K, RealScene-ISTD, and SIRST-v2 datasets demonstrate the effectiveness of the proposed algorithm, achieving mAP@0.5 scores of 80.7%, 90.2%, and 93.3%, respectively. The results indicate that the algorithm effectively utilizes frequency-domain analysis and semantic enhancement, providing a powerful and efficient solution for infrared small target detection in complex scenarios while maintaining a favorable balance between accuracy and computational cost.&lt;/p&gt;</content:encoded></item><item><title>Complex convolutional sparse coding InSAR phase filtering Incorporating directional gradients and second-order difference regularization</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.016</link><guid>10.1016/j.isprsjprs.2025.12.016</guid><pubDate>Sat, 10 Jan 2026 20:22:59 +0000</pubDate><dc:creator>Pengcheng Hu</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Junhuan Peng</dc:creator><dc:creator>Xu Ma</dc:creator><dc:creator>Yuhan Su</dc:creator><dc:creator>Xiaoman Qi</dc:creator><dc:creator>Xinwei Jiang</dc:creator><dc:creator>Wenwen Wang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.016</prism:doi><description>Interferometric Synthetic Aperture Radar (InSAR) is a technology that can effectively obtain ground information, conduct large-scale topography mapping, and monitor surface deformation. However, InSAR data is interfered by speckle noise caused by radar echo signal fading, ground background clutter, and decoherence, which affects the InSAR interferometric phase quality and thus reduces the accuracy of InSAR results. The existing Complex Convolutional Sparse Coding Gradient Regularization (ComCSC-GR) method incorporates gradient regularization by considering the sparse coefficient matrix’s gradients in both row (azimuth) and column (range) directions. It is an advanced and effective interferogram phase filtering method that can improve the interferogram quality. However, this method does not take into account the variation characteristics of the diagonal gradient and the second-order difference information (caused by edge mutations). As a result, the interferogram still exhibits problems such as staircase artifacts in high-noise and low-coherence areas, uneven interferograms (caused by a large number of residual points), and unclear phase edge structure. This article introduces multiple directional gradients and second-order differential Laplacian operator information, and construct two models: “Complex Convolutional Sparse Coding Model with L 2 -norm Regularization of Directional Gradients and Laplacian Operator (ComCSC-RCDL) ” and “Complex Convolutional Sparse Coding Model Coupled with L 1 -norm Total Variation Regularization (ComCSC-RCDL-TV)”. These methods enhance the fidelity of phase texture and edge structure, and improve the quality of InSAR interferogram filtering phase in low-coherence scenarios. Comparative experiments were conducted using simulated data, real data from Sentinel-1 and LuTan-1 (LT-1), and advanced methods including ComCSC-GR and InSAR-BM3D (real data experiments included comparison experiments before and after removing the interferogram orbit error). The results show that the proposed model method performs better than the comparative model, verifying the effectiveness of the proposed model.
Published: 2026-01-10T20:22:59+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengcheng Hu; Xu Li; Junhuan Peng; Xu Ma; Yuhan Su; Xiaoman Qi; Xinwei Jiang; Wenwen Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.016"&gt;10.1016/j.isprsjprs.2025.12.016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Interferometric Synthetic Aperture Radar (InSAR) is a technology that can effectively obtain ground information, conduct large-scale topography mapping, and monitor surface deformation. However, InSAR data is interfered by speckle noise caused by radar echo signal fading, ground background clutter, and decoherence, which affects the InSAR interferometric phase quality and thus reduces the accuracy of InSAR results. The existing Complex Convolutional Sparse Coding Gradient Regularization (ComCSC-GR) method incorporates gradient regularization by considering the sparse coefficient matrix’s gradients in both row (azimuth) and column (range) directions. It is an advanced and effective interferogram phase filtering method that can improve the interferogram quality. However, this method does not take into account the variation characteristics of the diagonal gradient and the second-order difference information (caused by edge mutations). As a result, the interferogram still exhibits problems such as staircase artifacts in high-noise and low-coherence areas, uneven interferograms (caused by a large number of residual points), and unclear phase edge structure. This article introduces multiple directional gradients and second-order differential Laplacian operator information, and construct two models: “Complex Convolutional Sparse Coding Model with L 2 -norm Regularization of Directional Gradients and Laplacian Operator (ComCSC-RCDL) ” and “Complex Convolutional Sparse Coding Model Coupled with L 1 -norm Total Variation Regularization (ComCSC-RCDL-TV)”. These methods enhance the fidelity of phase texture and edge structure, and improve the quality of InSAR interferogram filtering phase in low-coherence scenarios. Comparative experiments were conducted using simulated data, real data from Sentinel-1 and LuTan-1 (LT-1), and advanced methods including ComCSC-GR and InSAR-BM3D (real data experiments included comparison experiments before and after removing the interferogram orbit error). The results show that the proposed model method performs better than the comparative model, verifying the effectiveness of the proposed model.&lt;/p&gt;</content:encoded></item><item><title>Region-based Deep Metric Learning for Tackling Class Overlap in Online Semi-Supervised Data Stream Classification</title><link>https://doi.org/10.1016/j.inffus.2026.104126</link><guid>10.1016/j.inffus.2026.104126</guid><pubDate>Fri, 09 Jan 2026 16:48:04 +0000</pubDate><dc:creator>Zhonglin Wu</dc:creator><dc:creator>Hongliang Wang</dc:creator><dc:creator>Tongze Zhang</dc:creator><dc:creator>Hongyuan Liu</dc:creator><dc:creator>Jinxia Guo</dc:creator><dc:creator>Qinli Yang</dc:creator><dc:creator>Junming Shao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104126</prism:doi><description>Class overlap in data streams presents a significant challenge for real-time classification, particularly when confronted with the high dimensionality and evolving distributions inherent in such streams. Traditional classification methods, typically designed for static datasets, struggle to adapt to the dynamic nature of data streams, where both high-dimensional feature spaces and class imbalance exacerbate the complexity of classifying overlapping regions. In this paper, we propose a novel deep metric learning framework specifically tailored to address the challenges of class overlap in high-dimensional data streams. Our approach introduces two key innovations. First, we develop a multi-anchor sample mining mechanism based on neighborhood rough set theory, which partitions the data into non-overlapping and overlapping regions. By utilizing region-specific triplet-margin losses and hinge embedding loss, we construct a more refined discriminative metric space that significantly enhances the separation of overlapping classes. Furthermore, we introduce a dynamic, density-aware real-time label propagation mechanism with class-imbalance compensation. This component integrates real-time distribution estimation with a nonlinear adaptive threshold controller, enabling dual adaptivity: (1) dynamically re-weighting density contributions via inverse-frequency scaling to mitigate the dominance of majority classes and (2) adjusting threshold boundaries for frequent classes while relaxing propagation criteria for rare classes through nonlinear adjustments. Empirical evaluations on both synthetic and real-world data streams demonstrate that our method not only improves balanced accuracy but also enhances robustness in the presence of class overlap and class imbalance, outperforming state-of-the-art techniques.
Published: 2026-01-09T16:48:04+00:00
Venue: Information Fusion
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhonglin Wu; Hongliang Wang; Tongze Zhang; Hongyuan Liu; Jinxia Guo; Qinli Yang; Junming Shao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104126"&gt;10.1016/j.inffus.2026.104126&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Class overlap in data streams presents a significant challenge for real-time classification, particularly when confronted with the high dimensionality and evolving distributions inherent in such streams. Traditional classification methods, typically designed for static datasets, struggle to adapt to the dynamic nature of data streams, where both high-dimensional feature spaces and class imbalance exacerbate the complexity of classifying overlapping regions. In this paper, we propose a novel deep metric learning framework specifically tailored to address the challenges of class overlap in high-dimensional data streams. Our approach introduces two key innovations. First, we develop a multi-anchor sample mining mechanism based on neighborhood rough set theory, which partitions the data into non-overlapping and overlapping regions. By utilizing region-specific triplet-margin losses and hinge embedding loss, we construct a more refined discriminative metric space that significantly enhances the separation of overlapping classes. Furthermore, we introduce a dynamic, density-aware real-time label propagation mechanism with class-imbalance compensation. This component integrates real-time distribution estimation with a nonlinear adaptive threshold controller, enabling dual adaptivity: (1) dynamically re-weighting density contributions via inverse-frequency scaling to mitigate the dominance of majority classes and (2) adjusting threshold boundaries for frequent classes while relaxing propagation criteria for rare classes through nonlinear adjustments. Empirical evaluations on both synthetic and real-world data streams demonstrate that our method not only improves balanced accuracy but also enhances robustness in the presence of class overlap and class imbalance, outperforming state-of-the-art techniques.&lt;/p&gt;</content:encoded></item><item><title>City-Facade: A city-level large-scale point cloud building facade dataset for semantic &amp;amp; instance segmentation</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.003</link><guid>10.1016/j.isprsjprs.2026.01.003</guid><pubDate>Fri, 09 Jan 2026 23:48:31 +0000</pubDate><dc:creator>Yiping Chen</dc:creator><dc:creator>Jonathan Li</dc:creator><dc:creator>Ting Han</dc:creator><dc:creator>Huifang Feng</dc:creator><dc:creator>Jun Chen</dc:creator><dc:creator>Cheng Wang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.003</prism:doi><description>With the growing demand for high-quality 3D urban scene understanding in applications such as building information modeling (BIM) and digital twins, large-scale and well-annotated 3D datasets have become essential for advancing scientific research and algorithm development. However, existing building facade datasets are predominantly image-based, suffering from drawbacks such as a lack of spatial information and sensitivity to lighting and weather conditions. Moreover, publicly available large-scale labeled datasets of building point clouds still remain scarce and have a relatively small coverage area. To this end, we introduce a city-level building facade point cloud dataset named City-Facade for semantic-level and instance-level segmentation. Firstly, the paper conducts a comprehensive review and analysis of existing urban &amp; building point cloud datasets and point cloud segmentation algorithms. Secondly, we present a large-scale building facade dataset with approximately 200 millions of labeled 3D point clouds (over 60 km roads) belonging to urban scenarios, realized to facilitate the development and evaluation of semantic and instance level algorithms in the urban understanding. Finally, baseline experiments for semantic and instance segmentation are conducted to encourage further research. The proposed dataset is accessible at https://github.com/gorgeouseping/City-Facade , comprising the dataset and segmentation baselines for better comparison and presentation of strengths and weaknesses of different methods. Additionally, the data will undergo continuous improvement and updates based on feedback from the community.
Published: 2026-01-09T23:48:31+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiping Chen; Jonathan Li; Ting Han; Huifang Feng; Jun Chen; Cheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.003"&gt;10.1016/j.isprsjprs.2026.01.003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;With the growing demand for high-quality 3D urban scene understanding in applications such as building information modeling (BIM) and digital twins, large-scale and well-annotated 3D datasets have become essential for advancing scientific research and algorithm development. However, existing building facade datasets are predominantly image-based, suffering from drawbacks such as a lack of spatial information and sensitivity to lighting and weather conditions. Moreover, publicly available large-scale labeled datasets of building point clouds still remain scarce and have a relatively small coverage area. To this end, we introduce a city-level building facade point cloud dataset named City-Facade for semantic-level and instance-level segmentation. Firstly, the paper conducts a comprehensive review and analysis of existing urban &amp;amp; building point cloud datasets and point cloud segmentation algorithms. Secondly, we present a large-scale building facade dataset with approximately 200 millions of labeled 3D point clouds (over 60 km roads) belonging to urban scenarios, realized to facilitate the development and evaluation of semantic and instance level algorithms in the urban understanding. Finally, baseline experiments for semantic and instance segmentation are conducted to encourage further research. The proposed dataset is accessible at https://github.com/gorgeouseping/City-Facade , comprising the dataset and segmentation baselines for better comparison and presentation of strengths and weaknesses of different methods. Additionally, the data will undergo continuous improvement and updates based on feedback from the community.&lt;/p&gt;</content:encoded></item><item><title>YOFOR: You Only Focus on Object Regions for Tiny Object Detection in Aerial Images</title><link>https://doi.org/10.1016/j.neunet.2026.108571</link><guid>10.1016/j.neunet.2026.108571</guid><pubDate>Sat, 10 Jan 2026 16:16:44 +0000</pubDate><dc:creator>Heng Hu</dc:creator><dc:creator>Hao-Zhe Wang</dc:creator><dc:creator>Si-Bao Chen</dc:creator><dc:creator>Jin Tang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108571</prism:doi><description>With development of deep learning methods, performance of object detection has been greatly improved. However, the high resolution of remotely sensed images, the complexity of the background, the uneven distribution of objects, and the uneven number of objects among them lead to unsatisfactory detection results of existing detectors. Facing these challenges, we propose YOFOR (You Only Focus on Object Regions), an adaptive local sensing enhancement network. It contains three components: adaptive local sensing module, fuzzy enhancement module and class balance module. Among them, adaptive local sensing module can adaptively localize dense object regions and dynamically crop dense object regions on view, which effectively solves problem of uneven distribution of objects. Fuzzy enhancement module further enhances object region by weakening the background interference, thus improving detection performance. Class balancing module, which analyzes dataset to obtain distribution of long-tailed classes, takes into account direction of tailed classes and distance around object, and operates on tailed classes within a certain range to alleviate long-tailed class problem and further improve detection performance. All three components are unsupervised and can be easily inserted into existing networks. Extensive experiments on the VisDrone, DOTA, and AI-TOD datasets demonstrate the effectiveness and adaptability of the method.
Published: 2026-01-10T16:16:44+00:00
Venue: Neural Networks
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Heng Hu; Hao-Zhe Wang; Si-Bao Chen; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108571"&gt;10.1016/j.neunet.2026.108571&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;With development of deep learning methods, performance of object detection has been greatly improved. However, the high resolution of remotely sensed images, the complexity of the background, the uneven distribution of objects, and the uneven number of objects among them lead to unsatisfactory detection results of existing detectors. Facing these challenges, we propose YOFOR (You Only Focus on Object Regions), an adaptive local sensing enhancement network. It contains three components: adaptive local sensing module, fuzzy enhancement module and class balance module. Among them, adaptive local sensing module can adaptively localize dense object regions and dynamically crop dense object regions on view, which effectively solves problem of uneven distribution of objects. Fuzzy enhancement module further enhances object region by weakening the background interference, thus improving detection performance. Class balancing module, which analyzes dataset to obtain distribution of long-tailed classes, takes into account direction of tailed classes and distance around object, and operates on tailed classes within a certain range to alleviate long-tailed class problem and further improve detection performance. All three components are unsupervised and can be easily inserted into existing networks. Extensive experiments on the VisDrone, DOTA, and AI-TOD datasets demonstrate the effectiveness and adaptability of the method.&lt;/p&gt;</content:encoded></item><item><title>Frequency-Aware and Lifting-Based Efficient Transformer for Person Search</title><link>https://doi.org/10.1016/j.eswa.2026.131090</link><guid>10.1016/j.eswa.2026.131090</guid><pubDate>Sat, 10 Jan 2026 00:24:32 +0000</pubDate><dc:creator>Qilin Shu</dc:creator><dc:creator>Qixian Zhang</dc:creator><dc:creator>Duoqian Miao</dc:creator><dc:creator>Qi Zhang</dc:creator><dc:creator>Hongyun Zhang</dc:creator><dc:creator>Cairong Zhao</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131090</prism:doi><description>The person search task aims to locate a target person within a set of scene images. In recent years, transformer-based models in this field have made some progress. However, they still face two primary challenges: 1) the self-attention mechanism tends to suppress high-frequency components in the features, which severely impacts model performance; 2) the computational cost of transformers is relatively high. To address these issues, we propose a novel Frequency-Aware and Lifting-Based Efficient Transformer (FLET) method for person search. FLET is designed to enhance the discriminative feature extraction capabilities of transformers while reducing computational overhead and improving efficiency. Specifically, we develop a three-stage framework that progressively optimizes both detection and re-identification performance. Our model enhances the perception of high-frequency features by learning from augmented inputs. The augmented inputs are generated via High-Pass Filtering (HPF) and contain additional high-frequency components. Furthermore, we replace the self-attention layers in the transformer with a Learnable Lifting Block (LLB) to capture multiscale features. LLB not only lowers the computational complexity but also alleviates the suppression of high-frequency features and enhances the ability to exploit multiscale information. Extensive experiments demonstrate that FLET achieves state-of-the-art performance on both the CUHK-SYSU and PRW datasets.
Published: 2026-01-10T00:24:32+00:00
Venue: Expert Systems with Applications
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qilin Shu; Qixian Zhang; Duoqian Miao; Qi Zhang; Hongyun Zhang; Cairong Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131090"&gt;10.1016/j.eswa.2026.131090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;The person search task aims to locate a target person within a set of scene images. In recent years, transformer-based models in this field have made some progress. However, they still face two primary challenges: 1) the self-attention mechanism tends to suppress high-frequency components in the features, which severely impacts model performance; 2) the computational cost of transformers is relatively high. To address these issues, we propose a novel Frequency-Aware and Lifting-Based Efficient Transformer (FLET) method for person search. FLET is designed to enhance the discriminative feature extraction capabilities of transformers while reducing computational overhead and improving efficiency. Specifically, we develop a three-stage framework that progressively optimizes both detection and re-identification performance. Our model enhances the perception of high-frequency features by learning from augmented inputs. The augmented inputs are generated via High-Pass Filtering (HPF) and contain additional high-frequency components. Furthermore, we replace the self-attention layers in the transformer with a Learnable Lifting Block (LLB) to capture multiscale features. LLB not only lowers the computational complexity but also alleviates the suppression of high-frequency features and enhances the ability to exploit multiscale information. Extensive experiments demonstrate that FLET achieves state-of-the-art performance on both the CUHK-SYSU and PRW datasets.&lt;/p&gt;</content:encoded></item><item><title>SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection</title><link>https://arxiv.org/abs/2601.04968v1</link><guid>http://arxiv.org/abs/2601.04968v1</guid><pubDate>Thu, 08 Jan 2026 14:16:11 +0000</pubDate><dc:creator>Maximilian Pittner</dc:creator><dc:creator>Joel Janai</dc:creator><dc:creator>Mario Faigle</dc:creator><dc:creator>Alexandru Paul Condurache</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.
Published: 2026-01-08T14:16:11+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maximilian Pittner; Joel Janai; Mario Faigle; Alexandru Paul Condurache&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment</title><link>https://arxiv.org/abs/2601.04571v1</link><guid>http://arxiv.org/abs/2601.04571v1</guid><pubDate>Thu, 08 Jan 2026 04:02:49 +0000</pubDate><dc:creator>Delong Zeng</dc:creator><dc:creator>Yuexiang Xie</dc:creator><dc:creator>Yaliang Li</dc:creator><dc:creator>Ying Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.
Published: 2026-01-08T04:02:49+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Delong Zeng; Yuexiang Xie; Yaliang Li; Ying Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.&lt;/p&gt;</content:encoded></item><item><title>Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection</title><link>https://arxiv.org/abs/2601.03617v1</link><guid>http://arxiv.org/abs/2601.03617v1</guid><pubDate>Wed, 07 Jan 2026 05:57:19 +0000</pubDate><dc:creator>Samson Oseiwe Ajadalu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.
Published: 2026-01-07T05:57:19+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Samson Oseiwe Ajadalu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.&lt;/p&gt;</content:encoded></item><item><title>Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images</title><link>https://arxiv.org/abs/2601.04127v1</link><guid>http://arxiv.org/abs/2601.04127v1</guid><pubDate>Wed, 07 Jan 2026 17:41:11 +0000</pubDate><dc:creator>Leandro Stival</dc:creator><dc:creator>Ricardo da Silva Torres</dc:creator><dc:creator>Helio Pedrini</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on
Published: 2026-01-07T17:41:11+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Leandro Stival; Ricardo da Silva Torres; Helio Pedrini&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on&lt;/p&gt;</content:encoded></item><item><title>From Rays to Projections: Better Inputs for Feed-Forward View Synthesis</title><link>https://arxiv.org/abs/2601.05116v1</link><guid>http://arxiv.org/abs/2601.05116v1</guid><pubDate>Thu, 08 Jan 2026 17:03:44 +0000</pubDate><dc:creator>Zirui Wu</dc:creator><dc:creator>Zeren Jiang</dc:creator><dc:creator>Martin R. Oswald</dc:creator><dc:creator>Jie Song</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.
Published: 2026-01-08T17:03:44+00:00
Venue: arXiv
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zirui Wu; Zeren Jiang; Martin R. Oswald; Jie Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.&lt;/p&gt;</content:encoded></item><item><title>A Lightweight Dual-View Network for Sand-Dust Degraded Image Enhancement</title><link>https://doi.org/10.1016/j.knosys.2026.115308</link><guid>10.1016/j.knosys.2026.115308</guid><pubDate>Sat, 10 Jan 2026 00:24:28 +0000</pubDate><dc:creator>Guxue Gao</dc:creator><dc:creator>Yang Xiao</dc:creator><dc:creator>Xiaopeng Wen</dc:creator><dc:creator>Chunyun Sun</dc:creator><dc:creator>Yuanyuan Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115308</prism:doi><description>To address the issue that current supervised sand-dust image enhancement networks require large parameters and consume substantial computational resources and storage space, we propose a lightweight dual-view sand-dust image network. The proposed dual-view sharpening encoder and the original encoder are designed to provide complementary feature information, thereby maximizing the diversity of extracted features. At the encoder stage, a parameter-free feature modulation module is introduced and selectively embedded into the encoder branches to enhance feature extraction capability. In the decoding stage, a contextual attention integration module is designed to improve image contrast and enhance regional details by adaptively leveraging variance-based weighting and long-range pixel dependencies. These modules collectively strengthen feature representation and network reconstruction capacity while significantly reducing parameter overhead. Experimental results demonstrate that the proposed network can effectively enhance sand-dust images with fewer network parameters while ensuring performance. Additionally, the proposed algorithm generalizes well to haze and turbid underwater image enhancement. The processed images also improve the detection accuracy of targets such as vehicles and pedestrians, indicating its strong application potential.
Published: 2026-01-10T00:24:28+00:00
Venue: Knowledge-Based Systems
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guxue Gao; Yang Xiao; Xiaopeng Wen; Chunyun Sun; Yuanyuan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115308"&gt;10.1016/j.knosys.2026.115308&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;To address the issue that current supervised sand-dust image enhancement networks require large parameters and consume substantial computational resources and storage space, we propose a lightweight dual-view sand-dust image network. The proposed dual-view sharpening encoder and the original encoder are designed to provide complementary feature information, thereby maximizing the diversity of extracted features. At the encoder stage, a parameter-free feature modulation module is introduced and selectively embedded into the encoder branches to enhance feature extraction capability. In the decoding stage, a contextual attention integration module is designed to improve image contrast and enhance regional details by adaptively leveraging variance-based weighting and long-range pixel dependencies. These modules collectively strengthen feature representation and network reconstruction capacity while significantly reducing parameter overhead. Experimental results demonstrate that the proposed network can effectively enhance sand-dust images with fewer network parameters while ensuring performance. Additionally, the proposed algorithm generalizes well to haze and turbid underwater image enhancement. The processed images also improve the detection accuracy of targets such as vehicles and pedestrians, indicating its strong application potential.&lt;/p&gt;</content:encoded></item><item><title>Noise-Robust Tiny Object Localization with Flows</title><link>https://doi.org/10.1016/j.patcog.2026.113041</link><guid>10.1016/j.patcog.2026.113041</guid><pubDate>Fri, 09 Jan 2026 00:01:17 +0000</pubDate><dc:creator>Huixin Sun</dc:creator><dc:creator>Linlin Yang</dc:creator><dc:creator>Ronyu Chen</dc:creator><dc:creator>Kerui Gu</dc:creator><dc:creator>Baochang Zhang</dc:creator><dc:creator>Angela Yao</dc:creator><dc:creator>Xianbin Cao</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113041</prism:doi><description>Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach’s effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.
Published: 2026-01-09T00:01:17+00:00
Venue: Pattern Recognition
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huixin Sun; Linlin Yang; Ronyu Chen; Kerui Gu; Baochang Zhang; Angela Yao; Xianbin Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113041"&gt;10.1016/j.patcog.2026.113041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach’s effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.&lt;/p&gt;</content:encoded></item><item><title>Enhancing UAV small target detection: A balanced accuracy-efficiency algorithm with tiered feature focus</title><link>https://doi.org/10.1016/j.imavis.2026.105897</link><guid>10.1016/j.imavis.2026.105897</guid><pubDate>Sat, 10 Jan 2026 07:39:26 +0000</pubDate><dc:creator>Hanwei Guo</dc:creator><dc:creator>Shugang Liu</dc:creator><prism:publicationName>Image and Vision Computing</prism:publicationName><prism:doi>10.1016/j.imavis.2026.105897</prism:doi><description>Small target detection in unmanned aerial vehicle (UAV) imagery is crucial for both military and civilian applications. However, achieving a balance between detection performance, efficiency, and lightweight architecture remains challenging. This paper introduces TF-DEIM-DFINE, a tiered focused small target detection model designed specifically for UAV tasks.We propose the Convolutional Gated-Visual Mamba (CG-VIM) module to enhance global dependency capture and local detail extraction through long sequence modeling, along with the Half-Channel Single-Head Attention (HCSA) module for global modeling, which improves fine-grained representation while reducing computational redundancy. Additionally, our Tiered Focus-Feature Pyramid Networks (TF-FPN) improve the representational capability of high-frequency information in multi-scale features without significantly increasing computational overhead. Experimental results on the VisDrone dataset demonstrate a 4.7% improvement in AP M " role="presentation"&gt; M M and a 5.8% improvement in AP metrics, with a 37% reduction in parameter count and only a 6% increase in GFLOPs, maintaining unchanged FPS. These results highlight TF-DEIM-DFINE’s ability to improve detection accuracy while preserving a lightweight and efficient structure
Published: 2026-01-10T07:39:26+00:00
Venue: Image and Vision Computing
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanwei Guo; Shugang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Image and Vision Computing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.imavis.2026.105897"&gt;10.1016/j.imavis.2026.105897&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Small target detection in unmanned aerial vehicle (UAV) imagery is crucial for both military and civilian applications. However, achieving a balance between detection performance, efficiency, and lightweight architecture remains challenging. This paper introduces TF-DEIM-DFINE, a tiered focused small target detection model designed specifically for UAV tasks.We propose the Convolutional Gated-Visual Mamba (CG-VIM) module to enhance global dependency capture and local detail extraction through long sequence modeling, along with the Half-Channel Single-Head Attention (HCSA) module for global modeling, which improves fine-grained representation while reducing computational redundancy. Additionally, our Tiered Focus-Feature Pyramid Networks (TF-FPN) improve the representational capability of high-frequency information in multi-scale features without significantly increasing computational overhead. Experimental results on the VisDrone dataset demonstrate a 4.7% improvement in AP M &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; M M and a 5.8% improvement in AP metrics, with a 37% reduction in parameter count and only a 6% increase in GFLOPs, maintaining unchanged FPS. These results highlight TF-DEIM-DFINE’s ability to improve detection accuracy while preserving a lightweight and efficient structure&lt;/p&gt;</content:encoded></item><item><title>Integrating Distribution Matching into Semi-Supervised Contrastive Learning for Labeled and Unlabeled Data</title><link>https://arxiv.org/abs/2601.04518v1</link><guid>http://arxiv.org/abs/2601.04518v1</guid><pubDate>Thu, 08 Jan 2026 02:32:12 +0000</pubDate><dc:creator>Shogo Nakayama</dc:creator><dc:creator>Masahiro Okuda</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/ITC-CSCC66376.2025.11137694</prism:doi><description>The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.
Published: 2026-01-08T02:32:12+00:00
Venue: arXiv
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shogo Nakayama; Masahiro Okuda&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/ITC-CSCC66376.2025.11137694"&gt;10.1109/ITC-CSCC66376.2025.11137694&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;The advancement of deep learning has greatly improved supervised image classification. However, labeling data is costly, prompting research into unsupervised learning methods such as contrastive learning. In real-world scenarios, fully unlabeled datasets are rare, making semi-supervised learning (SSL) highly relevant in scenarios where a small amount of labeled data coexists with a large volume of unlabeled data. A well-known semi-supervised contrastive learning approach involves assigning pseudo-labels to unlabeled data. This study aims to enhance pseudo-label-based SSL by incorporating distribution matching between labeled and unlabeled feature embeddings to improve image classification accuracy across multiple datasets.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Attention Distillation for Robust Few-Shot Segmentation under Environmental Perturbations</title><link>https://arxiv.org/abs/2601.03596v1</link><guid>http://arxiv.org/abs/2601.03596v1</guid><pubDate>Wed, 07 Jan 2026 05:27:12 +0000</pubDate><dc:creator>Qianyu Guo</dc:creator><dc:creator>Jingrong Wu</dc:creator><dc:creator>Jieji Ren</dc:creator><dc:creator>Weifeng Ge</dc:creator><dc:creator>Wenqiang Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot segmentation (FSS) aims to rapidly learn novel class concepts from limited examples to segment specific targets in unseen images, and has been widely applied in areas such as medical diagnosis and industrial inspection. However, existing studies largely overlook the complex environmental factors encountered in real world scenarios-such as illumination, background, and camera viewpoint-which can substantially increase the difficulty of test images. As a result, models trained under laboratory conditions often fall short of practical deployment requirements. To bridge this gap, in this paper, an environment-robust FSS setting is introduced that explicitly incorporates challenging test cases arising from complex environments-such as motion blur, small objects, and camouflaged targets-to enhance model's robustness under realistic, dynamic conditions. An environment robust FSS benchmark (ER-FSS) is established, covering eight datasets across multiple real world scenarios. In addition, an Adaptive Attention Distillation (AAD) method is proposed, which repeatedly contrasts and distills key shared semantics between known (support) and unknown (query) images to derive class-specific attention for novel categories. This strengthens the model's ability to focus on the correct targets in complex environments, thereby improving environmental robustness. Comparative experiments show that AAD improves mIoU by 3.3% - 8.5% across all datasets and settings, demonstrating superior performance and strong generalization. The source code and dataset are available at: https://github.com/guoqianyu-alberta/Adaptive-Attention-Distillation-for-FSS.
Published: 2026-01-07T05:27:12+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qianyu Guo; Jingrong Wu; Jieji Ren; Weifeng Ge; Wenqiang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot segmentation (FSS) aims to rapidly learn novel class concepts from limited examples to segment specific targets in unseen images, and has been widely applied in areas such as medical diagnosis and industrial inspection. However, existing studies largely overlook the complex environmental factors encountered in real world scenarios-such as illumination, background, and camera viewpoint-which can substantially increase the difficulty of test images. As a result, models trained under laboratory conditions often fall short of practical deployment requirements. To bridge this gap, in this paper, an environment-robust FSS setting is introduced that explicitly incorporates challenging test cases arising from complex environments-such as motion blur, small objects, and camouflaged targets-to enhance model&amp;#x27;s robustness under realistic, dynamic conditions. An environment robust FSS benchmark (ER-FSS) is established, covering eight datasets across multiple real world scenarios. In addition, an Adaptive Attention Distillation (AAD) method is proposed, which repeatedly contrasts and distills key shared semantics between known (support) and unknown (query) images to derive class-specific attention for novel categories. This strengthens the model&amp;#x27;s ability to focus on the correct targets in complex environments, thereby improving environmental robustness. Comparative experiments show that AAD improves mIoU by 3.3% - 8.5% across all datasets and settings, demonstrating superior performance and strong generalization. The source code and dataset are available at: https://github.com/guoqianyu-alberta/Adaptive-Attention-Distillation-for-FSS.&lt;/p&gt;</content:encoded></item><item><title>MRFP-TDG: A detection transformer with hybrid encoder and position-aware decoder for photovoltaic cell defect detection</title><link>https://doi.org/10.1016/j.ins.2026.123092</link><guid>10.1016/j.ins.2026.123092</guid><pubDate>Fri, 09 Jan 2026 07:54:03 +0000</pubDate><dc:creator>Wenfu Huang</dc:creator><dc:creator>Jie Zhao</dc:creator><dc:creator>Ran Wei</dc:creator><dc:creator>Jingyuan Yang</dc:creator><dc:creator>Jing Ruan</dc:creator><dc:creator>Li Fan</dc:creator><dc:creator>Xinwen Zhou</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2026.123092</prism:doi><description>Defects arising during photovoltaic (PV) cell manufacturing critically compromise performance and operational safety. Existing computer vision-based detection methods struggle with defects characterized by small scales, dense distributions, and background confusion. To address this, we propose MRFP-TDG, a Detection Transformer-based detector featuring an optimized hybrid encoder for enhanced fine-grained feature extraction and background suppression. Specifically, our Multi-scale Receptive Field Projection (MRFP) module leverages channel-split depthwise separable convolutions with multi-scale kernels to project backbone features into tokens while preserving spatial relationships, significantly improving small-defect detection. The Token-Driven Gathering (TDG) module further integrates spatial attention to fuse multi-scale tokens, compensating for tokenization-induced spatial information loss while suppressing background noise. Furthermore, a relation position embedding mechanism in the decoder models positional relationships of bounding boxes across layers, accelerating convergence during iterative refinement. Evaluated on the public PVEL-AD dataset, MRFP-TDG achieves 95.1% mAP@50 in nine-category defect detection, outperforming state-of-the-art PV defect detectors in both accuracy and efficiency. Specifically, it surpasses the baseline model by 1.0% mAP while requiring only 59.6% of the computational cost compared with the best-performing SOTA method.
Published: 2026-01-09T07:54:03+00:00
Venue: Information Sciences
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenfu Huang; Jie Zhao; Ran Wei; Jingyuan Yang; Jing Ruan; Li Fan; Xinwen Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2026.123092"&gt;10.1016/j.ins.2026.123092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;Defects arising during photovoltaic (PV) cell manufacturing critically compromise performance and operational safety. Existing computer vision-based detection methods struggle with defects characterized by small scales, dense distributions, and background confusion. To address this, we propose MRFP-TDG, a Detection Transformer-based detector featuring an optimized hybrid encoder for enhanced fine-grained feature extraction and background suppression. Specifically, our Multi-scale Receptive Field Projection (MRFP) module leverages channel-split depthwise separable convolutions with multi-scale kernels to project backbone features into tokens while preserving spatial relationships, significantly improving small-defect detection. The Token-Driven Gathering (TDG) module further integrates spatial attention to fuse multi-scale tokens, compensating for tokenization-induced spatial information loss while suppressing background noise. Furthermore, a relation position embedding mechanism in the decoder models positional relationships of bounding boxes across layers, accelerating convergence during iterative refinement. Evaluated on the public PVEL-AD dataset, MRFP-TDG achieves 95.1% mAP@50 in nine-category defect detection, outperforming state-of-the-art PV defect detectors in both accuracy and efficiency. Specifically, it surpasses the baseline model by 1.0% mAP while requiring only 59.6% of the computational cost compared with the best-performing SOTA method.&lt;/p&gt;</content:encoded></item><item><title>Multi-Directional Decision Fusion for Black-Box Source-Free Anomaly Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113038</link><guid>10.1016/j.patcog.2026.113038</guid><pubDate>Fri, 09 Jan 2026 07:54:21 +0000</pubDate><dc:creator>Yu Gao</dc:creator><dc:creator>Shilong Sun</dc:creator><dc:creator>Zhanpei Zhang</dc:creator><dc:creator>Jinxing Li</dc:creator><dc:creator>Guangming Lu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113038</prism:doi><description>With growing data-privacy concerns in industrial anomaly detection, source-free domain adaptation aims to transfer the pre-trained source model to the target domain without source data. However, the source model is often unavailable due to its commercial value. This paper investigates a more challenging yet practical problem, namely black-box source-free domain adaptation, where only the outputs of the source model and unlabeled target data are available. Specifically, our method comprises two key stages: self-learning based pseudo-label and cluster separation based classifier-hypothesis. In the first stage, we fuse instance-directional and class-directional decisions to generate pseudo-labels for target samples, transferring detection knowledge from the source model to the target model while mitigating the adverse effects of severe category-imbalance. Additionally, the spatial regularization is introduced to enhance the learning of discriminative target-features. Finally, a simple yet effective mechanism is established to correct the pseudo-labels by progressively fusing the outputs of the target model. In the second stage, the pseudo-label learning is discarded in favor of exploring the semantic structure. The cluster separation is designed to make the average outputs of different clusters orthogonal for realizing cluster transfer. Extensive experiments demonstrate the superiority of our method.
Published: 2026-01-09T07:54:21+00:00
Venue: Pattern Recognition
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Gao; Shilong Sun; Zhanpei Zhang; Jinxing Li; Guangming Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113038"&gt;10.1016/j.patcog.2026.113038&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;With growing data-privacy concerns in industrial anomaly detection, source-free domain adaptation aims to transfer the pre-trained source model to the target domain without source data. However, the source model is often unavailable due to its commercial value. This paper investigates a more challenging yet practical problem, namely black-box source-free domain adaptation, where only the outputs of the source model and unlabeled target data are available. Specifically, our method comprises two key stages: self-learning based pseudo-label and cluster separation based classifier-hypothesis. In the first stage, we fuse instance-directional and class-directional decisions to generate pseudo-labels for target samples, transferring detection knowledge from the source model to the target model while mitigating the adverse effects of severe category-imbalance. Additionally, the spatial regularization is introduced to enhance the learning of discriminative target-features. Finally, a simple yet effective mechanism is established to correct the pseudo-labels by progressively fusing the outputs of the target model. In the second stage, the pseudo-label learning is discarded in favor of exploring the semantic structure. The cluster separation is designed to make the average outputs of different clusters orthogonal for realizing cluster transfer. Extensive experiments demonstrate the superiority of our method.&lt;/p&gt;</content:encoded></item><item><title>Training-free and Zero-shot Regeneration for Hallucination Mitigation in MLLMs: Representation Understanding Perspective</title><link>https://doi.org/10.1016/j.eswa.2026.131102</link><guid>10.1016/j.eswa.2026.131102</guid><pubDate>Sat, 10 Jan 2026 00:24:28 +0000</pubDate><dc:creator>Dong Zhang</dc:creator><dc:creator>Yuansheng Ma</dc:creator><dc:creator>Linqin Li</dc:creator><dc:creator>Shoushan Li</dc:creator><dc:creator>Erik Cambria</dc:creator><dc:creator>Guodong Zhou</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131102</prism:doi><description>Hallucinations in multimodal large language models (MLLMs) are urgent problems to be solved in the new era of artificial general intelligence (AGI). Compared with traditional large language models (LLMs), besides handling language understanding and modeling, we also need to consider the detection and position determination of objects in vision. Therefore, to tackle the hallucination issues, the existing studies attempt to employ few-shot learning on the following perspectives: 1) limit the length of the generated response, 2) iteratively generate multiple candidates or select from multiple candidates via beam search, 3) locally edit the possible parts of primary response, and 4) leverage external knowledge to augment the generation capability. To address the above potential weaknesses, this paper proposes a multimodal training-free and zero-shot regeneration approach by obtain various multimodal evidences and globally improving the raw response to alleviate hallucinations in MLLMs ( Mtzr ). Specifically, we first extract the entity-level evidences by object-based pre-trained models with in-context learning. Then, we mine the attribute-level evidences inside each entity and cross different entities with heterogeneous in-context learning based on both uni- and multimodal pre-trained models. Finally, towards the obtained multimodal evidences, we regenerate the response with augmented context by residually connecting both the input text and image. For better understanding, we provide theoretical explanations with universal approximation to support why our approach can bring about smaller hallucination. Detailed experimental results and extensive analysis demonstrate that our approach is very suitable for mitigating hallucination in MLLMs.
Published: 2026-01-10T00:24:28+00:00
Venue: Expert Systems with Applications
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dong Zhang; Yuansheng Ma; Linqin Li; Shoushan Li; Erik Cambria; Guodong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131102"&gt;10.1016/j.eswa.2026.131102&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Hallucinations in multimodal large language models (MLLMs) are urgent problems to be solved in the new era of artificial general intelligence (AGI). Compared with traditional large language models (LLMs), besides handling language understanding and modeling, we also need to consider the detection and position determination of objects in vision. Therefore, to tackle the hallucination issues, the existing studies attempt to employ few-shot learning on the following perspectives: 1) limit the length of the generated response, 2) iteratively generate multiple candidates or select from multiple candidates via beam search, 3) locally edit the possible parts of primary response, and 4) leverage external knowledge to augment the generation capability. To address the above potential weaknesses, this paper proposes a multimodal training-free and zero-shot regeneration approach by obtain various multimodal evidences and globally improving the raw response to alleviate hallucinations in MLLMs ( Mtzr ). Specifically, we first extract the entity-level evidences by object-based pre-trained models with in-context learning. Then, we mine the attribute-level evidences inside each entity and cross different entities with heterogeneous in-context learning based on both uni- and multimodal pre-trained models. Finally, towards the obtained multimodal evidences, we regenerate the response with augmented context by residually connecting both the input text and image. For better understanding, we provide theoretical explanations with universal approximation to support why our approach can bring about smaller hallucination. Detailed experimental results and extensive analysis demonstrate that our approach is very suitable for mitigating hallucination in MLLMs.&lt;/p&gt;</content:encoded></item><item><title>Detector-Augmented SAMURAI for Long-Duration Drone Tracking</title><link>https://arxiv.org/abs/2601.04798v1</link><guid>http://arxiv.org/abs/2601.04798v1</guid><pubDate>Thu, 08 Jan 2026 10:27:05 +0000</pubDate><dc:creator>Tamara R. Lenhard</dc:creator><dc:creator>Andreas Weinmann</dc:creator><dc:creator>Hichem Snoussi</dc:creator><dc:creator>Tobias Koch</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.
Published: 2026-01-08T10:27:05+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tamara R. Lenhard; Andreas Weinmann; Hichem Snoussi; Tobias Koch&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI&amp;#x27;s potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI&amp;#x27;s zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.&lt;/p&gt;</content:encoded></item><item><title>Co-Training Vision-Language Models for Remote Sensing Multi-Task Learning</title><link>https://doi.org/10.3390/rs18020222</link><guid>10.3390/rs18020222</guid><pubDate>Fri, 09 Jan 2026 16:03:16 +0000</pubDate><dc:creator>Qingyun Li</dc:creator><dc:creator>Shuran Ma</dc:creator><dc:creator>Junwei Luo</dc:creator><dc:creator>Yi Yu</dc:creator><dc:creator>Yue Zhou</dc:creator><dc:creator>Fengxiang Wang</dc:creator><dc:creator>Xudong Lu</dc:creator><dc:creator>Xiaoxing Wang</dc:creator><dc:creator>Xin He</dc:creator><dc:creator>Yushi Chen</dc:creator><dc:creator>Xue Yang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020222</prism:doi><description>With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.
Published: 2026-01-09T16:03:16+00:00
Venue: Remote Sensing
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingyun Li; Shuran Ma; Junwei Luo; Yi Yu; Yue Zhou; Fengxiang Wang; Xudong Lu; Xiaoxing Wang; Xin He; Yushi Chen; Xue Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020222"&gt;10.3390/rs18020222&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision-language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation procedure, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data procedure effectively addresses complex RS data enviroments and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model’s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.&lt;/p&gt;</content:encoded></item></channel></rss>