<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 07 Feb 2026 03:26:26 +0000</lastBuildDate><item><title>Adaptive image zoom-in with bounding box transformation for UAV object detection</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.036</link><guid>10.1016/j.isprsjprs.2026.01.036</guid><pubDate>Thu, 05 Feb 2026 11:10:41 +0000</pubDate><dc:creator>Tao Wang</dc:creator><dc:creator>Chenyu Lin</dc:creator><dc:creator>Chenwei Tang</dc:creator><dc:creator>Jizhe Zhou</dc:creator><dc:creator>Deng Xiong</dc:creator><dc:creator>Jianan Li</dc:creator><dc:creator>Jian Zhao</dc:creator><dc:creator>Jiancheng Lv</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.036</prism:doi><description>Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: (i) How to conduct non-uniform zooming on each image efficiently? (ii) How to enable object detection training and inference with the zoomed image space? Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code .
Published: 2026-02-05T11:10:41+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Wang; Chenyu Lin; Chenwei Tang; Jizhe Zhou; Deng Xiong; Jianan Li; Jian Zhao; Jiancheng Lv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.036"&gt;10.1016/j.isprsjprs.2026.01.036&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: (i) How to conduct non-uniform zooming on each image efficiently? (ii) How to enable object detection training and inference with the zoomed image space? Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code .&lt;/p&gt;</content:encoded></item><item><title>Visual language models show widespread visual deficits on neuropsychological tests</title><link>https://doi.org/10.1038/s42256-026-01179-y</link><guid>10.1038/s42256-026-01179-y</guid><pubDate>Fri, 06 Feb 2026 10:03:29 +0000</pubDate><dc:creator>Gene Tangtartharakul</dc:creator><dc:creator>Katherine R. Storrs</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-026-01179-y</prism:doi><description>Visual language models (VLMs) show remarkable performance in visual reasoning tasks, successfully tackling college-level challenges that require a high-level understanding of images. However, some recent reports of VLMs struggling to reason about elemental visual concepts such as orientation, position, continuity and occlusion suggest a potential gulf between human and VLM vision. Currently, few assessments enable a direct comparison between human and VLM performance, which limits our ability to measure alignment between the two systems. Here we use the toolkit of neuropsychology to systematically evaluate the capabilities of three state-of-the-art VLMs across low, mid and high visual domains. Using 51 tests drawn from 6 clinical and experimental psychology batteries, we characterize the visual abilities of leading VLMs relative to normative performance in healthy adults. While the models excel in straightforward object recognition tasks, we find widespread deficits in low- and mid-level visual abilities that would be considered clinically significant in humans. These selective deficits, profiled through validated test batteries, suggest that an artificial system can achieve complex object recognition without developing foundational visual concepts that in humans require no explicit training. Tangtartharakul and Storrs use standardized neuropsychological tests to compare human visual abilities with those of visual language models (VLMs). They report that while VLMs excel in high-level object recognition, they show deficits in low- and mid-level visual abilities.
Published: 2026-02-06T10:03:29+00:00
Venue: Nature Machine Intelligence
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gene Tangtartharakul; Katherine R. Storrs&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-026-01179-y"&gt;10.1038/s42256-026-01179-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Visual language models (VLMs) show remarkable performance in visual reasoning tasks, successfully tackling college-level challenges that require a high-level understanding of images. However, some recent reports of VLMs struggling to reason about elemental visual concepts such as orientation, position, continuity and occlusion suggest a potential gulf between human and VLM vision. Currently, few assessments enable a direct comparison between human and VLM performance, which limits our ability to measure alignment between the two systems. Here we use the toolkit of neuropsychology to systematically evaluate the capabilities of three state-of-the-art VLMs across low, mid and high visual domains. Using 51 tests drawn from 6 clinical and experimental psychology batteries, we characterize the visual abilities of leading VLMs relative to normative performance in healthy adults. While the models excel in straightforward object recognition tasks, we find widespread deficits in low- and mid-level visual abilities that would be considered clinically significant in humans. These selective deficits, profiled through validated test batteries, suggest that an artificial system can achieve complex object recognition without developing foundational visual concepts that in humans require no explicit training. Tangtartharakul and Storrs use standardized neuropsychological tests to compare human visual abilities with those of visual language models (VLMs). They report that while VLMs excel in high-level object recognition, they show deficits in low- and mid-level visual abilities.&lt;/p&gt;</content:encoded></item><item><title>Cross-modal contrastive learning for 3D point cloud-text fusion via implicit semantic alignment</title><link>https://doi.org/10.1016/j.inffus.2026.104208</link><guid>10.1016/j.inffus.2026.104208</guid><pubDate>Thu, 05 Feb 2026 01:09:00 +0000</pubDate><dc:creator>Xiangtian Zheng</dc:creator><dc:creator>Chen Ji</dc:creator><dc:creator>Wei Cai</dc:creator><dc:creator>Xianghua Tang</dc:creator><dc:creator>Xiaolin Yang</dc:creator><dc:creator>Liang Cheng</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104208</prism:doi><description>While Transformers have historically dominated point cloud analysis, their quadratic computational complexity O ( N 2 ) poses a significant bottleneck for processing large-scale 3D data. Recently, State Space Models (SSMs), particularly Mamba, have emerged as a potent alternative due to their linear complexity and robust long-range modeling capabilities. In this work, we propose PST-Mamba, a novel multimodal SSM-based framework designed for efficient and holistic point cloud understanding. To bridge the dimensionality gap between unstructured 3D geometry and 1D sequential SSMs, we introduce Semantic-Guided Manifold Unfolding. Unlike rigid geometric scanning, this strategy utilizes implicit textual embeddings to guide the projection of point clouds into 1D sequences, effectively preserving both topological continuity and semantic coherence. Furthermore, we integrate implicit semantic prompts to enrich the geometric features with high-level contextual priors, facilitating a deep fusion of visual and linguistic information. Extensive experiments demonstrate that PST-Mamba achieves state-of-the-art performance across multiple benchmarks (e.g., 95.21% OA on ModelNet40 and 90.09% on ScanObjectNN PB-T50-RS). Remarkably, compared to leading Transformer-based models, PST-Mamba reduces parameters by 54.8% and FLOPs by 70.5%, showcasing its immense potential for large-scale, real-time 3D multimodal applications.
Published: 2026-02-05T01:09:00+00:00
Venue: Information Fusion
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangtian Zheng; Chen Ji; Wei Cai; Xianghua Tang; Xiaolin Yang; Liang Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104208"&gt;10.1016/j.inffus.2026.104208&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;While Transformers have historically dominated point cloud analysis, their quadratic computational complexity O ( N 2 ) poses a significant bottleneck for processing large-scale 3D data. Recently, State Space Models (SSMs), particularly Mamba, have emerged as a potent alternative due to their linear complexity and robust long-range modeling capabilities. In this work, we propose PST-Mamba, a novel multimodal SSM-based framework designed for efficient and holistic point cloud understanding. To bridge the dimensionality gap between unstructured 3D geometry and 1D sequential SSMs, we introduce Semantic-Guided Manifold Unfolding. Unlike rigid geometric scanning, this strategy utilizes implicit textual embeddings to guide the projection of point clouds into 1D sequences, effectively preserving both topological continuity and semantic coherence. Furthermore, we integrate implicit semantic prompts to enrich the geometric features with high-level contextual priors, facilitating a deep fusion of visual and linguistic information. Extensive experiments demonstrate that PST-Mamba achieves state-of-the-art performance across multiple benchmarks (e.g., 95.21% OA on ModelNet40 and 90.09% on ScanObjectNN PB-T50-RS). Remarkably, compared to leading Transformer-based models, PST-Mamba reduces parameters by 54.8% and FLOPs by 70.5%, showcasing its immense potential for large-scale, real-time 3D multimodal applications.&lt;/p&gt;</content:encoded></item><item><title>Identifying spatial single-cell-level interactions with graph transformer</title><link>https://doi.org/10.1038/s42256-026-01191-2</link><guid>10.1038/s42256-026-01191-2</guid><pubDate>Fri, 06 Feb 2026 10:02:54 +0000</pubDate><dc:creator>Xiangzheng Cheng</dc:creator><dc:creator>Suoqin Jin</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-026-01191-2</prism:doi><description>Identifying cell–cell interactions from imaging-based spatial transcriptomics suffers from limited gene panels. A new self-supervised graph transformer-based method can resolve spatial single-cell-level interactions without requiring known ligand–receptor pairs.
Published: 2026-02-06T10:02:54+00:00
Venue: Nature Machine Intelligence
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangzheng Cheng; Suoqin Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-026-01191-2"&gt;10.1038/s42256-026-01191-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Identifying cell–cell interactions from imaging-based spatial transcriptomics suffers from limited gene panels. A new self-supervised graph transformer-based method can resolve spatial single-cell-level interactions without requiring known ligand–receptor pairs.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Class-Incremental SAR Target Recognition Based on Dynamic Task-Adaptive Classifier</title><link>https://doi.org/10.3390/rs18030527</link><guid>10.3390/rs18030527</guid><pubDate>Fri, 06 Feb 2026 11:10:38 +0000</pubDate><dc:creator>Dan Li</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Yong Li</dc:creator><dc:creator>Wei Cheng</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030527</prism:doi><description>Current synthetic aperture radar automatic target recognition (SAR ATR) tasks face challenges including limited training samples and poor generalization capability to novel classes. To address these issues, few-shot class-incremental learning (FSCIL) has emerged as a promising research direction. Few-shot learning facilitates the expedited adaptation to novel tasks utilizing a limited number of labeled samples, whereas incremental learning concentrates on the continuous refinement of the model as new categories are incorporated without eradicating previously learned knowledge. Although both methodologies present potential resolutions to the challenges of sample scarcity and class evolution in SAR target recognition, they are not without their own set of difficulties. Fine-tuning with emerging classes can perturb the feature distribution of established classes, culminating in catastrophic forgetting, while training exclusively on a handful of new samples can induce bias towards older classes, leading to distribution collapse and overfitting. To surmount these limitations and satisfy practical application requirements, we propose a Few-Shot Class-Incremental SAR Target Recognition method based on a Dynamic Task-Adaptive Classifier (DTAC). This approach underscores task adaptability through a feature extraction module, a task information encoding module, and a classifier generation module. The feature extraction module discerns both target-specific and task-specific characteristics, while the task information encoding module modulates the network parameters of the classifier generation module based on pertinent task information, thereby improving adaptability. Our innovative classifier generation module, honed with task-specific insights, dynamically assembles classifiers tailored to the current task, effectively accommodating a variety of scenarios and novel class samples. Our extensive experiments on SAR datasets demonstrate that our proposed method generally outperforms the baselines in few-shot class incremental SAR target recognition.
Published: 2026-02-06T11:10:38+00:00
Venue: Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dan Li; Feng Zhao; Yong Li; Wei Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030527"&gt;10.3390/rs18030527&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Current synthetic aperture radar automatic target recognition (SAR ATR) tasks face challenges including limited training samples and poor generalization capability to novel classes. To address these issues, few-shot class-incremental learning (FSCIL) has emerged as a promising research direction. Few-shot learning facilitates the expedited adaptation to novel tasks utilizing a limited number of labeled samples, whereas incremental learning concentrates on the continuous refinement of the model as new categories are incorporated without eradicating previously learned knowledge. Although both methodologies present potential resolutions to the challenges of sample scarcity and class evolution in SAR target recognition, they are not without their own set of difficulties. Fine-tuning with emerging classes can perturb the feature distribution of established classes, culminating in catastrophic forgetting, while training exclusively on a handful of new samples can induce bias towards older classes, leading to distribution collapse and overfitting. To surmount these limitations and satisfy practical application requirements, we propose a Few-Shot Class-Incremental SAR Target Recognition method based on a Dynamic Task-Adaptive Classifier (DTAC). This approach underscores task adaptability through a feature extraction module, a task information encoding module, and a classifier generation module. The feature extraction module discerns both target-specific and task-specific characteristics, while the task information encoding module modulates the network parameters of the classifier generation module based on pertinent task information, thereby improving adaptability. Our innovative classifier generation module, honed with task-specific insights, dynamically assembles classifiers tailored to the current task, effectively accommodating a variety of scenarios and novel class samples. Our extensive experiments on SAR datasets demonstrate that our proposed method generally outperforms the baselines in few-shot class incremental SAR target recognition.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Alignment and Fusion: A Survey</title><link>https://doi.org/10.1007/s11263-025-02667-1</link><guid>10.1007/s11263-025-02667-1</guid><pubDate>Fri, 06 Feb 2026 03:27:20 +0000</pubDate><dc:creator>Songtao Li</dc:creator><dc:creator>Hao Tang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02667-1</prism:doi><description>This survey provides a comprehensive overview of recent advances in multimodal alignment and fusion within the field of machine learning, driven by the increasing availability and diversity of data modalities such as text, images, audio, and video. Unlike previous surveys that often focus on specific modalities or limited fusion strategies, our work presents a structure-centric and method-driven framework that emphasizes generalizable techniques. We systematically categorize and analyze key approaches to alignment and fusion through both structural perspectives—data-level, feature-level, and output-level fusion—and methodological paradigms—including statistical, kernel-based, graphical, generative, contrastive, attention-based, and large language model (LLM)-based methods, drawing insights from an extensive review of over 260 relevant studies. Furthermore, this survey highlights critical challenges such as cross-modal misalignment, computational bottlenecks, data quality issues, and the modality gap, along with recent efforts to address them. Applications ranging from social media analysis and medical imaging to emotion recognition and embodied AI are explored to illustrate the real-world impact of robust multimodal systems. The insights provided aim to guide future research toward optimizing multimodal learning systems for improved scalability, robustness, and generalizability across diverse domains.
Published: 2026-02-06T03:27:20+00:00
Venue: International Journal of Computer Vision
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songtao Li; Hao Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02667-1"&gt;10.1007/s11263-025-02667-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;This survey provides a comprehensive overview of recent advances in multimodal alignment and fusion within the field of machine learning, driven by the increasing availability and diversity of data modalities such as text, images, audio, and video. Unlike previous surveys that often focus on specific modalities or limited fusion strategies, our work presents a structure-centric and method-driven framework that emphasizes generalizable techniques. We systematically categorize and analyze key approaches to alignment and fusion through both structural perspectives—data-level, feature-level, and output-level fusion—and methodological paradigms—including statistical, kernel-based, graphical, generative, contrastive, attention-based, and large language model (LLM)-based methods, drawing insights from an extensive review of over 260 relevant studies. Furthermore, this survey highlights critical challenges such as cross-modal misalignment, computational bottlenecks, data quality issues, and the modality gap, along with recent efforts to address them. Applications ranging from social media analysis and medical imaging to emotion recognition and embodied AI are explored to illustrate the real-world impact of robust multimodal systems. The insights provided aim to guide future research toward optimizing multimodal learning systems for improved scalability, robustness, and generalizability across diverse domains.&lt;/p&gt;</content:encoded></item><item><title>基于双分类头的遥感图像精细化目标检测方法</title><link>https://doi.org/10.11834/jrs.20254243</link><guid>10.11834/jrs.20254243</guid><pubDate>Fri, 06 Feb 2026 02:20:15 +0000</pubDate><dc:creator>ZHANG Feng</dc:creator><dc:creator>TENG Shuhua</dc:creator><dc:creator>HAN Xing</dc:creator><dc:creator>WANG Yingqian</dc:creator><dc:creator>WANG Xueying</dc:creator><prism:publicationName>National Remote Sensing Bulletin</prism:publicationName><prism:doi>10.11834/jrs.20254243</prism:doi><description>2026年1月30日湖南第一师范学院电子信息学院的张锋、滕书华团队在《遥感学报》发文，介绍了其在遥感图像目标精细化检测领域的研究进展，张锋、滕书华专家提出了一种基于双分类头的遥感图像精细化目标检测方法，为解决相似数据利用不充分、错误标签影响模型精度和相似类别难以区分等问题提供了有效解决方案。
Published: 2026-02-06T02:20:15+00:00
Venue: National Remote Sensing Bulletin
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; ZHANG Feng; TENG Shuhua; HAN Xing; WANG Yingqian; WANG Xueying&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; National Remote Sensing Bulletin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jrs.20254243"&gt;10.11834/jrs.20254243&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;2026年1月30日湖南第一师范学院电子信息学院的张锋、滕书华团队在《遥感学报》发文，介绍了其在遥感图像目标精细化检测领域的研究进展，张锋、滕书华专家提出了一种基于双分类头的遥感图像精细化目标检测方法，为解决相似数据利用不充分、错误标签影响模型精度和相似类别难以区分等问题提供了有效解决方案。&lt;/p&gt;</content:encoded></item><item><title>Transformer-Masked Autoencoder (MAE) for Robust Medical Image Classification: A Comprehensive Survey</title><link>https://doi.org/10.1016/j.eswa.2026.131462</link><guid>10.1016/j.eswa.2026.131462</guid><pubDate>Fri, 06 Feb 2026 00:31:36 +0000</pubDate><dc:creator>Ernest Asimeng</dc:creator><dc:creator>Jun Chen</dc:creator><dc:creator>Kai Han</dc:creator><dc:creator>Chongwen Lyu</dc:creator><dc:creator>Zhe Liu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131462</prism:doi><description>Medical image classification underpins screening, diagnosis, and treatment planning, but conventional CNN pipelines remain label-hungry and brittle under noise, occlusion, and domain shift. Transformer-based masked autoencoders (MAEs) offer a compelling alternative by exploiting large unlabeled archives to learn anatomy-aware representations. This survey systematically synthesizes various studies on MAE-based encoders for X-ray, MRI, CT, ultrasound, and histopathology. We propose a modality-aware taxonomy spanning ViT/Swin backbones, masking strategies, and training regimes, and harmonize reported results against strong CNN and contrastive baselines. Across modalities, MAE initialization consistently improves label efficiency, calibration, and cross-scanner transfer, especially when combined with parameter-efficient fine-tuning. We distill practical design heuristics (mask ratios, encoder &amp; decoder depth, PEFT rank), highlight recurrent pitfalls in evaluation and reporting, and outline future directions in domain-aware masking, scalable 3D pretraining, cross-modal co-masking, and clinically grounded interpretability. The codes for to generate the results can be found in this https://github.com/ekwadwo1/Medical-MAE-Survey .
Published: 2026-02-06T00:31:36+00:00
Venue: Expert Systems with Applications
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ernest Asimeng; Jun Chen; Kai Han; Chongwen Lyu; Zhe Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131462"&gt;10.1016/j.eswa.2026.131462&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Medical image classification underpins screening, diagnosis, and treatment planning, but conventional CNN pipelines remain label-hungry and brittle under noise, occlusion, and domain shift. Transformer-based masked autoencoders (MAEs) offer a compelling alternative by exploiting large unlabeled archives to learn anatomy-aware representations. This survey systematically synthesizes various studies on MAE-based encoders for X-ray, MRI, CT, ultrasound, and histopathology. We propose a modality-aware taxonomy spanning ViT/Swin backbones, masking strategies, and training regimes, and harmonize reported results against strong CNN and contrastive baselines. Across modalities, MAE initialization consistently improves label efficiency, calibration, and cross-scanner transfer, especially when combined with parameter-efficient fine-tuning. We distill practical design heuristics (mask ratios, encoder &amp;amp; decoder depth, PEFT rank), highlight recurrent pitfalls in evaluation and reporting, and outline future directions in domain-aware masking, scalable 3D pretraining, cross-modal co-masking, and clinically grounded interpretability. The codes for to generate the results can be found in this https://github.com/ekwadwo1/Medical-MAE-Survey .&lt;/p&gt;</content:encoded></item><item><title>Panoptic-VSNet: Visual-Semantic Prior Knowledge-Driven Multimodal 3D Panoptic Segmentation</title><link>https://doi.org/10.1016/j.patcog.2026.113239</link><guid>10.1016/j.patcog.2026.113239</guid><pubDate>Thu, 05 Feb 2026 00:51:08 +0000</pubDate><dc:creator>Xiao Li</dc:creator><dc:creator>Hui Li</dc:creator><dc:creator>Xiangzhen Kong</dc:creator><dc:creator>Yuang Ji</dc:creator><dc:creator>Zhiyu Liu</dc:creator><dc:creator>Hao Liu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113239</prism:doi><description>Precise and robust perception is critical for ensuring the safe operation of autonomous vehicles. However, current methods are constrained by sparse image-LiDAR alignment, insufficient annotations, and ineffective structural discrepancy modeling, causing semantic degradation and generalization deficiency. Therefore, we propose Panoptic-VSNet, a visual-semantic prior knowledge-driven multimodal 3D panoptic segmentation network. Firstly, we design a progressive fusion semantic alignment module that effectively aggregates visual prior features obtained from the large Visual-Language model, establishing a point-semantic region association, thereby enhancing semantic awareness. Secondly, we propose an instance-aware superpixel cross-modal fusion module that incorporates instance prior knowledge, forming a unified representation with spatial precision and class consistency. Finally, we introduce a correlation-aware adaptive panoptic segmentation network that reduces parameter count while dynamically capturing contextual information and enhancing local details, thereby improving panoptic perception capabilities. Experimental evaluations on benchmark datasets show that Panoptic-VSNet outperforms state-of-the-art methods. Code is available at https://github.com/lixiao0125/panoptic-vsnet.git .
Published: 2026-02-05T00:51:08+00:00
Venue: Pattern Recognition
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiao Li; Hui Li; Xiangzhen Kong; Yuang Ji; Zhiyu Liu; Hao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113239"&gt;10.1016/j.patcog.2026.113239&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Precise and robust perception is critical for ensuring the safe operation of autonomous vehicles. However, current methods are constrained by sparse image-LiDAR alignment, insufficient annotations, and ineffective structural discrepancy modeling, causing semantic degradation and generalization deficiency. Therefore, we propose Panoptic-VSNet, a visual-semantic prior knowledge-driven multimodal 3D panoptic segmentation network. Firstly, we design a progressive fusion semantic alignment module that effectively aggregates visual prior features obtained from the large Visual-Language model, establishing a point-semantic region association, thereby enhancing semantic awareness. Secondly, we propose an instance-aware superpixel cross-modal fusion module that incorporates instance prior knowledge, forming a unified representation with spatial precision and class consistency. Finally, we introduce a correlation-aware adaptive panoptic segmentation network that reduces parameter count while dynamically capturing contextual information and enhancing local details, thereby improving panoptic perception capabilities. Experimental evaluations on benchmark datasets show that Panoptic-VSNet outperforms state-of-the-art methods. Code is available at https://github.com/lixiao0125/panoptic-vsnet.git .&lt;/p&gt;</content:encoded></item><item><title>RFAConv: Receptive-Field Attention Convolution for Improving Convolutional Neural Networks</title><link>https://doi.org/10.1016/j.patcog.2026.113208</link><guid>10.1016/j.patcog.2026.113208</guid><pubDate>Thu, 05 Feb 2026 16:01:16 +0000</pubDate><dc:creator>Xin Zhang</dc:creator><dc:creator>Chen Liu</dc:creator><dc:creator>Tingting Song</dc:creator><dc:creator>Degang Yang</dc:creator><dc:creator>Yichen Ye</dc:creator><dc:creator>Ke Li</dc:creator><dc:creator>Yingze Song</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113208</prism:doi><description>In the realm of deep learning, spatial attention mechanisms have emerged as a vital method for enhancing the performance of convolutional neural networks. However, these mechanisms possess inherent limitations that cannot be overlooked. This work delves into the mechanism of spatial attention and reveals a new insight. It is that the mechanism essentially addresses the issue of convolutional parameter sharing. By addressing this issue, the convolutional kernel can efficiently extract features by employing varying weights at distinct locations. However, current spatial attention mechanisms focus on shallow attention to spatial features, which is insufficient to address the fundamental challenge of parameter sharing in convolutions involving larger kernels. In response to this challenge, we introduce a novel attention mechanism known as Receptive-Field Attention (RFA). Compared to existing spatial attention methods, RFA not only concentrates on the receptive-field spatial features but also offers effective attention weights for large convolutional kernels. Building upon the RFA concept, a Receptive-Field Attention Convolution (RFAConv) is proposed to supplant the conventional standard convolution. Notably, it offers nearly negligible increment of computational overhead and parameters, while significantly improving network performance. Furthermore, this work reveals that current spatial attention mechanisms require enhanced prioritization of receptive-field spatial features to optimize network performance. To validate the advantages of the proposed methods, we conduct many experiments across several authoritative datasets, including ImageNet, COCO, VOC, and Roboflow. The results demonstrate that the proposed methods bring about significant advancements in tasks, such as image classification, object detection, and semantic segmentation, surpassing convolutional operations constructed using current spatial attention mechanisms. Presently, the code and pre-trained models for the associated tasks have been made publicly available at https://github.com/Liuchen1997/RFAConv .
Published: 2026-02-05T16:01:16+00:00
Venue: Pattern Recognition
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Zhang; Chen Liu; Tingting Song; Degang Yang; Yichen Ye; Ke Li; Yingze Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113208"&gt;10.1016/j.patcog.2026.113208&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;In the realm of deep learning, spatial attention mechanisms have emerged as a vital method for enhancing the performance of convolutional neural networks. However, these mechanisms possess inherent limitations that cannot be overlooked. This work delves into the mechanism of spatial attention and reveals a new insight. It is that the mechanism essentially addresses the issue of convolutional parameter sharing. By addressing this issue, the convolutional kernel can efficiently extract features by employing varying weights at distinct locations. However, current spatial attention mechanisms focus on shallow attention to spatial features, which is insufficient to address the fundamental challenge of parameter sharing in convolutions involving larger kernels. In response to this challenge, we introduce a novel attention mechanism known as Receptive-Field Attention (RFA). Compared to existing spatial attention methods, RFA not only concentrates on the receptive-field spatial features but also offers effective attention weights for large convolutional kernels. Building upon the RFA concept, a Receptive-Field Attention Convolution (RFAConv) is proposed to supplant the conventional standard convolution. Notably, it offers nearly negligible increment of computational overhead and parameters, while significantly improving network performance. Furthermore, this work reveals that current spatial attention mechanisms require enhanced prioritization of receptive-field spatial features to optimize network performance. To validate the advantages of the proposed methods, we conduct many experiments across several authoritative datasets, including ImageNet, COCO, VOC, and Roboflow. The results demonstrate that the proposed methods bring about significant advancements in tasks, such as image classification, object detection, and semantic segmentation, surpassing convolutional operations constructed using current spatial attention mechanisms. Presently, the code and pre-trained models for the associated tasks have been made publicly available at https://github.com/Liuchen1997/RFAConv .&lt;/p&gt;</content:encoded></item><item><title>AutoIT: Automated Image Tagging with Random Perturbation</title><link>https://doi.org/10.1007/s11263-026-02737-y</link><guid>10.1007/s11263-026-02737-y</guid><pubDate>Fri, 06 Feb 2026 04:54:30 +0000</pubDate><dc:creator>Xuelin Zhu</dc:creator><dc:creator>Jianshu Li</dc:creator><dc:creator>Jian Liu</dc:creator><dc:creator>Dongqi Tang</dc:creator><dc:creator>Jiawei Ge</dc:creator><dc:creator>Weijia Liu</dc:creator><dc:creator>Bo Liu</dc:creator><dc:creator>Jiuxin Cao</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-026-02737-y</prism:doi><description>Vision-language pre-training (VLP) models have been explored as a means to bridge the text and image modalities, allowing to learn visual classifiers using only texts for image tagging. However, existing methods rely heavily on prompt tuning, which becomes computationally prohibitive when managing a vast array of candidate labels. In this study, we present a lightweight adapter network paired with an effective random perturbation mechanism, facilitating the creation of label classifiers with augmented cross-modal transfer capabilities. Together with large language models for multi-label text generation, a fully automated pipeline for image tagging is developed without relying on any manually curated data. Through comprehensive experiments on public benchmarks, we empirically reveal the nature of random perturbation in improving cross-modal alignment within the adapter’s embedding space. Our findings also emphasize the critical role of pre-trained embeddings’ magnitude in enhancing cross-modal classifier performance, challenging the prevailing focus on normalization of the embedding space. Alongside empirical results concerning the impact of both the quantity and quality of generated texts and the efficiency of the adapter, our pivotal insights into the automated image tagging paradigm are expected to advance future research efforts within the community.
Published: 2026-02-06T04:54:30+00:00
Venue: International Journal of Computer Vision
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuelin Zhu; Jianshu Li; Jian Liu; Dongqi Tang; Jiawei Ge; Weijia Liu; Bo Liu; Jiuxin Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-026-02737-y"&gt;10.1007/s11263-026-02737-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Vision-language pre-training (VLP) models have been explored as a means to bridge the text and image modalities, allowing to learn visual classifiers using only texts for image tagging. However, existing methods rely heavily on prompt tuning, which becomes computationally prohibitive when managing a vast array of candidate labels. In this study, we present a lightweight adapter network paired with an effective random perturbation mechanism, facilitating the creation of label classifiers with augmented cross-modal transfer capabilities. Together with large language models for multi-label text generation, a fully automated pipeline for image tagging is developed without relying on any manually curated data. Through comprehensive experiments on public benchmarks, we empirically reveal the nature of random perturbation in improving cross-modal alignment within the adapter’s embedding space. Our findings also emphasize the critical role of pre-trained embeddings’ magnitude in enhancing cross-modal classifier performance, challenging the prevailing focus on normalization of the embedding space. Alongside empirical results concerning the impact of both the quantity and quality of generated texts and the efficiency of the adapter, our pivotal insights into the automated image tagging paradigm are expected to advance future research efforts within the community.&lt;/p&gt;</content:encoded></item><item><title>L2M-Reg: Building-level uncertainty-aware registration of outdoor LiDAR point clouds and semantic 3D city models</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.005</link><guid>10.1016/j.isprsjprs.2026.02.005</guid><pubDate>Fri, 06 Feb 2026 09:05:04 +0000</pubDate><dc:creator>Ziyang Xu</dc:creator><dc:creator>Benedikt Schwab</dc:creator><dc:creator>Yihui Yang</dc:creator><dc:creator>Thomas H. Kolbe</dc:creator><dc:creator>Christoph Holst</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.005</prism:doi><description>Accurate registration between LiDAR (Light Detection and Ranging) point clouds and semantic 3D city models is a fundamental topic in urban digital twinning and a prerequisite for downstream tasks, such as digital construction, change detection, and model refinement. However, achieving accurate LiDAR-to-Model registration at the individual building level remains challenging, particularly due to the generalization uncertainty in semantic 3D city models at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing L2M-Reg, a plane-based fine registration method that explicitly accounts for model uncertainty. L2M-Reg consists of three key steps: establishing reliable plane correspondence, building a pseudo-plane-constrained Gauss–Helmert model, and adaptively estimating vertical translation. Overall, extensive experiments on five real-world datasets demonstrate that L2M-Reg is both more accurate and computationally efficient than current leading ICP-based and plane-based methods. Therefore, L2M-Reg provides a novel building-level solution regarding LiDAR-to-Model registration when model uncertainty is present. The datasets and code for L2M-Reg can be found: https://github.com/Ziyang-Geodesy/L2M-Reg .
Published: 2026-02-06T09:05:04+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyang Xu; Benedikt Schwab; Yihui Yang; Thomas H. Kolbe; Christoph Holst&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.005"&gt;10.1016/j.isprsjprs.2026.02.005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Accurate registration between LiDAR (Light Detection and Ranging) point clouds and semantic 3D city models is a fundamental topic in urban digital twinning and a prerequisite for downstream tasks, such as digital construction, change detection, and model refinement. However, achieving accurate LiDAR-to-Model registration at the individual building level remains challenging, particularly due to the generalization uncertainty in semantic 3D city models at the Level of Detail 2 (LoD2). This paper addresses this gap by proposing L2M-Reg, a plane-based fine registration method that explicitly accounts for model uncertainty. L2M-Reg consists of three key steps: establishing reliable plane correspondence, building a pseudo-plane-constrained Gauss–Helmert model, and adaptively estimating vertical translation. Overall, extensive experiments on five real-world datasets demonstrate that L2M-Reg is both more accurate and computationally efficient than current leading ICP-based and plane-based methods. Therefore, L2M-Reg provides a novel building-level solution regarding LiDAR-to-Model registration when model uncertainty is present. The datasets and code for L2M-Reg can be found: https://github.com/Ziyang-Geodesy/L2M-Reg .&lt;/p&gt;</content:encoded></item><item><title>基于功率迁移的极化SAR五分量散射分解方法</title><link>https://doi.org/10.11834/jrs.20255144</link><guid>10.11834/jrs.20255144</guid><pubDate>Fri, 06 Feb 2026 10:14:24 +0000</pubDate><dc:creator>LIU Yuxuan</dc:creator><dc:creator>FU Haiqiang</dc:creator><dc:creator>ZHU Jianjun</dc:creator><prism:publicationName>National Remote Sensing Bulletin</prism:publicationName><prism:doi>10.11834/jrs.20255144</prism:doi><description>2026年1月5日中南大学地球科学与信息物理学院的刘宇轩、付海强团队在《遥感学报》发文，介绍了其在极化分解方法领域的研究进展，通过重新解释极化取向角补偿为功率迁移过程，构建五分量分解模型，有效提升复杂场景下的散射机制识别能力。
Published: 2026-02-06T10:14:24+00:00
Venue: National Remote Sensing Bulletin
Score: 0.776 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; LIU Yuxuan; FU Haiqiang; ZHU Jianjun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; National Remote Sensing Bulletin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jrs.20255144"&gt;10.11834/jrs.20255144&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (consider)&lt;/p&gt;
&lt;p&gt;2026年1月5日中南大学地球科学与信息物理学院的刘宇轩、付海强团队在《遥感学报》发文，介绍了其在极化分解方法领域的研究进展，通过重新解释极化取向角补偿为功率迁移过程，构建五分量分解模型，有效提升复杂场景下的散射机制识别能力。&lt;/p&gt;</content:encoded></item><item><title>SGC: A Self-Guided Cascade Multitask Model for Low-Light Object Detection</title><link>https://doi.org/10.1016/j.knosys.2026.115462</link><guid>10.1016/j.knosys.2026.115462</guid><pubDate>Thu, 05 Feb 2026 17:18:22 +0000</pubDate><dc:creator>Jiakun Jin</dc:creator><dc:creator>Junchao Zhang</dc:creator><dc:creator>Yidong Luo</dc:creator><dc:creator>Jiandong Tian</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115462</prism:doi><description>Outdoor scenes often suffer from insufficient and non-uniform illumination, leading to object detection (OD) failures. This issue has garnered research attention, with the mainstream solution being to improve the model’s feature extraction capability through cascaded feature enhancement modules. However, such approaches increase the model’s complexity and the enhancement effect is highly dependent on the similarity between the training and testing data. Alternatively, some methods incorporate parallel low-light image enhancement (LLE) modules to guide the training of object detection models. Nevertheless, due to the lack of object detection datasets containing paired bright and low-light images, these methods often require manually selecting appropriate pre-trained LLE models for different scenes, making end-to-end training challenging. In this paper, we aim to build an end-to-end LLE&amp;OD cascade multitask model that leverages the strengths of both approaches. We use a new data augmentation techniques to synthesize low-light images from normal-light object detection datasets. To mutually train the cascade model, a new self-guided loss is designed. By deconstruction and reorganization of the multitask model, the self-guided loss effectively steering the model away from local optima for single tasks, enabling the model to achieve superior performance compared to many state-of-the-art methods on several publicly available night scene datasets, as well as on a daytime scene dataset. The source code of the proposed method will be available at https://github.com/225ceV/SGC .
Published: 2026-02-05T17:18:22+00:00
Venue: Knowledge-Based Systems
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiakun Jin; Junchao Zhang; Yidong Luo; Jiandong Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115462"&gt;10.1016/j.knosys.2026.115462&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;Outdoor scenes often suffer from insufficient and non-uniform illumination, leading to object detection (OD) failures. This issue has garnered research attention, with the mainstream solution being to improve the model’s feature extraction capability through cascaded feature enhancement modules. However, such approaches increase the model’s complexity and the enhancement effect is highly dependent on the similarity between the training and testing data. Alternatively, some methods incorporate parallel low-light image enhancement (LLE) modules to guide the training of object detection models. Nevertheless, due to the lack of object detection datasets containing paired bright and low-light images, these methods often require manually selecting appropriate pre-trained LLE models for different scenes, making end-to-end training challenging. In this paper, we aim to build an end-to-end LLE&amp;amp;OD cascade multitask model that leverages the strengths of both approaches. We use a new data augmentation techniques to synthesize low-light images from normal-light object detection datasets. To mutually train the cascade model, a new self-guided loss is designed. By deconstruction and reorganization of the multitask model, the self-guided loss effectively steering the model away from local optima for single tasks, enabling the model to achieve superior performance compared to many state-of-the-art methods on several publicly available night scene datasets, as well as on a daytime scene dataset. The source code of the proposed method will be available at https://github.com/225ceV/SGC .&lt;/p&gt;</content:encoded></item><item><title>Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review</title><link>https://doi.org/10.1007/s11263-025-02663-5</link><guid>10.1007/s11263-025-02663-5</guid><pubDate>Fri, 06 Feb 2026 04:39:30 +0000</pubDate><dc:creator>Thang-Anh-Quan Nguyen</dc:creator><dc:creator>Amine Bourki</dc:creator><dc:creator>Mátyás Macudzinski</dc:creator><dc:creator>Anthony Brunel</dc:creator><dc:creator>Mohammed Bennamoun</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02663-5</prism:doi><description>This review thoroughly examines the role of semantically-aware Neural Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of over 250 scholarly papers. It explores how NeRFs adeptly infer 3D representations for both stationary and dynamic objects in a scene. This capability is pivotal for generating high-quality new viewpoints, completing missing scene details (inpainting), conducting comprehensive scene segmentation (panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and extracting object-centric 3D models. A significant aspect of this study is the application of semantic labels as viewpoint-invariant functions, which effectively map spatial coordinates to a spectrum of semantic labels, thus facilitating the recognition of distinct objects within the scene. Overall, this survey highlights the progression and diverse applications of semantically-aware neural radiance fields in the context of visual scene interpretation.
Published: 2026-02-06T04:39:30+00:00
Venue: International Journal of Computer Vision
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thang-Anh-Quan Nguyen; Amine Bourki; Mátyás Macudzinski; Anthony Brunel; Mohammed Bennamoun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02663-5"&gt;10.1007/s11263-025-02663-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;This review thoroughly examines the role of semantically-aware Neural Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of over 250 scholarly papers. It explores how NeRFs adeptly infer 3D representations for both stationary and dynamic objects in a scene. This capability is pivotal for generating high-quality new viewpoints, completing missing scene details (inpainting), conducting comprehensive scene segmentation (panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and extracting object-centric 3D models. A significant aspect of this study is the application of semantic labels as viewpoint-invariant functions, which effectively map spatial coordinates to a spectrum of semantic labels, thus facilitating the recognition of distinct objects within the scene. Overall, this survey highlights the progression and diverse applications of semantically-aware neural radiance fields in the context of visual scene interpretation.&lt;/p&gt;</content:encoded></item><item><title>PC-YOLO: Moving Target Detection in Video SAR via YOLO on Principal Components</title><link>https://doi.org/10.3390/rs18030510</link><guid>10.3390/rs18030510</guid><pubDate>Thu, 05 Feb 2026 11:13:01 +0000</pubDate><dc:creator>Yu Han</dc:creator><dc:creator>Xinrong Wang</dc:creator><dc:creator>Jiaqing Jiang</dc:creator><dc:creator>Chao Xue</dc:creator><dc:creator>Rui Qin</dc:creator><dc:creator>Ganggang Dong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030510</prism:doi><description>Video synthetic aperture radar could provide more valuable information than static images. However, it suffers from several difficulties, such as strong clutter, low signal-to-noise ratio, and variable target scale. The task of moving target detection is therefore difficult to achieve. To solve these problems, this paper proposes a model and data co-driven learning method called look once on principal components (PC-YOLO). Unlike preceding works, we regarded the imaging scenario as a combination of low-rank and sparse scenes in theory. The former models the global, slowly varying background information, while the latter expresses the localized anomalies. These were then separated using the principal component decomposition technique to reduce the clutter while simultaneously enhancing the moving targets. The resulting principal components were then handled by an improved version of the look once framework. Since the moving targets featured various scales and weak scattering coefficients, the hierarchical attention mechanism and the cross-scale feature fusion strategy were introduced to further improve the detection performance. Finally, multiple rounds of experiments were performed to verify the proposed method, with the results proving that it could achieve more than 30% improvement in mAP compared to classical methods.
Published: 2026-02-05T11:13:01+00:00
Venue: Remote Sensing
Score: 0.773 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Han; Xinrong Wang; Jiaqing Jiang; Chao Xue; Rui Qin; Ganggang Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030510"&gt;10.3390/rs18030510&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (consider)&lt;/p&gt;
&lt;p&gt;Video synthetic aperture radar could provide more valuable information than static images. However, it suffers from several difficulties, such as strong clutter, low signal-to-noise ratio, and variable target scale. The task of moving target detection is therefore difficult to achieve. To solve these problems, this paper proposes a model and data co-driven learning method called look once on principal components (PC-YOLO). Unlike preceding works, we regarded the imaging scenario as a combination of low-rank and sparse scenes in theory. The former models the global, slowly varying background information, while the latter expresses the localized anomalies. These were then separated using the principal component decomposition technique to reduce the clutter while simultaneously enhancing the moving targets. The resulting principal components were then handled by an improved version of the look once framework. Since the moving targets featured various scales and weak scattering coefficients, the hierarchical attention mechanism and the cross-scale feature fusion strategy were introduced to further improve the detection performance. Finally, multiple rounds of experiments were performed to verify the proposed method, with the results proving that it could achieve more than 30% improvement in mAP compared to classical methods.&lt;/p&gt;</content:encoded></item><item><title>Understanding multimodal sentiment with deep modality interaction learning</title><link>https://doi.org/10.1016/j.patcog.2026.113236</link><guid>10.1016/j.patcog.2026.113236</guid><pubDate>Fri, 06 Feb 2026 00:14:37 +0000</pubDate><dc:creator>Jie Mu</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Jian Xu</dc:creator><dc:creator>Wenqi Liu</dc:creator><dc:creator>Zhizheng Sun</dc:creator><dc:creator>Wei Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113236</prism:doi><description>Multimodal sentiment analysis (MSA), which detects the sentimental polarities from multimodal data, is a crucial task in data mining and pattern recognition. Most MSA methods apply attention mechanisms to achieve better performance. However, the attention-based methods have two limitations: (1) their encoding modules based on the attention mechanism fail to fully consider the semantic-related information shared by image and text, which prevents the models from discovering sentiment features; (2) the attention-based methods either consider intra-modal interaction or inter-modal interaction but cannot consider intra- and inter-modal interaction simultaneously, which makes the models unable to fuse different modal features well. To overcome these limitations, this paper proposes a deep modality interaction network (DMINet) for understanding multimodal sentiment. First, we raise a cross-modal information interaction strategy to preserve the semantic-related information by maximizing the mutual information between image and text. Second, we design an image-text interactive graph module to simultaneously consider the intra- and inter-modal interaction by constructing a cross-modal graph. In addition, to address the problem that mutual information is difficult to calculate, we derive a cross-modal sub-boundary to compute the mutual information. Experimental results on 4 publicly available multimodal datasets demonstrate that DMINet outperforms 18 existing methods in multimodal sentiment analysis, achieving up to a 19-percentage-point improvement over several baseline models.
Published: 2026-02-06T00:14:37+00:00
Venue: Pattern Recognition
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Mu; Jing Zhang; Jian Xu; Wenqi Liu; Zhizheng Sun; Wei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113236"&gt;10.1016/j.patcog.2026.113236&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal sentiment analysis (MSA), which detects the sentimental polarities from multimodal data, is a crucial task in data mining and pattern recognition. Most MSA methods apply attention mechanisms to achieve better performance. However, the attention-based methods have two limitations: (1) their encoding modules based on the attention mechanism fail to fully consider the semantic-related information shared by image and text, which prevents the models from discovering sentiment features; (2) the attention-based methods either consider intra-modal interaction or inter-modal interaction but cannot consider intra- and inter-modal interaction simultaneously, which makes the models unable to fuse different modal features well. To overcome these limitations, this paper proposes a deep modality interaction network (DMINet) for understanding multimodal sentiment. First, we raise a cross-modal information interaction strategy to preserve the semantic-related information by maximizing the mutual information between image and text. Second, we design an image-text interactive graph module to simultaneously consider the intra- and inter-modal interaction by constructing a cross-modal graph. In addition, to address the problem that mutual information is difficult to calculate, we derive a cross-modal sub-boundary to compute the mutual information. Experimental results on 4 publicly available multimodal datasets demonstrate that DMINet outperforms 18 existing methods in multimodal sentiment analysis, achieving up to a 19-percentage-point improvement over several baseline models.&lt;/p&gt;</content:encoded></item><item><title>Multimodal remote sensing change detection: An image matching perspective</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.004</link><guid>10.1016/j.isprsjprs.2026.02.004</guid><pubDate>Fri, 06 Feb 2026 11:43:10 +0000</pubDate><dc:creator>Hongruixuan Chen</dc:creator><dc:creator>Cuiling Lan</dc:creator><dc:creator>Jian Song</dc:creator><dc:creator>Damian Ibañez</dc:creator><dc:creator>Junshi Xia</dc:creator><dc:creator>Konrad Schindler</dc:creator><dc:creator>Naoto Yokoya</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.004</prism:doi><description>Change Detection (CD) between images with different modalities is a fundamental capability for remote sensing. In this work, we pinpoint the commonalities between Multimodal Change Detection (MCD) and Multimodal Image Matching (MIM). Accordingly, we present a new unsupervised CD framework designed from the perspective of Image Matching (IM), called IM4CD. It unifies the IM and CD tasks into a single, coherent framework. In this framework, we abandon the prevalent strategy in MCD to compare per-pixel image features, since it is in practice quite difficult to design features that are truly invariant across modalities. Instead, we propose to compute similarity by local template matching and utilize the spatial offset of response peaks to represent change intensity between images with different modalities, and then to integrate it tightly with the co-registration of the two images, which anyway includes such a matching step. In this way, the same off-the-shelf descriptors used for MIM also support MCD. In other words, we first extract modality-independent features, then detect salient points to obtain initial pairs of corresponding Control Points (CP). When matching those points to accurately register the images, CP pairs located in unchanged areas show low residuals, whereas those in changed areas show high residuals. The CPs can then be connected into a Conditional Random Field (CRF), leveraging modality-independent structural relationships to estimate dense change maps. Experimental results show the effectiveness of our method, including robustness to registration errors, its compatibility with different image descriptors, and promising potential for challenging real-world disaster response scenarios.
Published: 2026-02-06T11:43:10+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongruixuan Chen; Cuiling Lan; Jian Song; Damian Ibañez; Junshi Xia; Konrad Schindler; Naoto Yokoya&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.004"&gt;10.1016/j.isprsjprs.2026.02.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;Change Detection (CD) between images with different modalities is a fundamental capability for remote sensing. In this work, we pinpoint the commonalities between Multimodal Change Detection (MCD) and Multimodal Image Matching (MIM). Accordingly, we present a new unsupervised CD framework designed from the perspective of Image Matching (IM), called IM4CD. It unifies the IM and CD tasks into a single, coherent framework. In this framework, we abandon the prevalent strategy in MCD to compare per-pixel image features, since it is in practice quite difficult to design features that are truly invariant across modalities. Instead, we propose to compute similarity by local template matching and utilize the spatial offset of response peaks to represent change intensity between images with different modalities, and then to integrate it tightly with the co-registration of the two images, which anyway includes such a matching step. In this way, the same off-the-shelf descriptors used for MIM also support MCD. In other words, we first extract modality-independent features, then detect salient points to obtain initial pairs of corresponding Control Points (CP). When matching those points to accurately register the images, CP pairs located in unchanged areas show low residuals, whereas those in changed areas show high residuals. The CPs can then be connected into a Conditional Random Field (CRF), leveraging modality-independent structural relationships to estimate dense change maps. Experimental results show the effectiveness of our method, including robustness to registration errors, its compatibility with different image descriptors, and promising potential for challenging real-world disaster response scenarios.&lt;/p&gt;</content:encoded></item><item><title>Adapting Vision Foundation Models with Lightweight Trident Decoder for Remote Sensing Change Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131472</link><guid>10.1016/j.eswa.2026.131472</guid><pubDate>Fri, 06 Feb 2026 00:31:39 +0000</pubDate><dc:creator>Wenhui Ye</dc:creator><dc:creator>Weimin Lei</dc:creator><dc:creator>Wenchao Zhang</dc:creator><dc:creator>Wei Zhang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131472</prism:doi><description>The field of remote sensing image change detection has attracted significant interest. However, prevailing methods tend to concentrate on the basic extraction of change features, neglecting the crucial role of stable semantic patterns (e.g., ”features that remain consistent across time”) in bi-temporal images (image pairs taken at different times) for enhancing change feature extraction precision. Moreover, existing change detection networks often grapple with striking an optimal balance between accuracy and complexity. To surmount these challenges, we leverage the advanced feature extraction capabilities of pre-trained Vision Foundation Models (FastSAM), introducing an innovative lightweight Trident Decoder framework for remote sensing change detection, termed LTCD. Our framework encompasses three pioneering fully convolutional modules: the Attention-CDut Module, Trident Decoder Module, and Segment-Head Module. The Attention-CD Module enriches the semantic content of bi-temporal image features via cross-attention, which precedes the decoding phase, effectively decoupling change features. The Trident Decoder Module intensifies the influence of stable semantic patterns on change features within the feature pyramid, resulting in a distinctive trident-shaped feature output. Furthermore, the Segment-Head Module integrates shallow auxiliary tasks under a deep supervision learning scheme to bolster feature interaction. Experimental results demonstrate that our work outperforms state-of-the-art (SOTA) approaches, including both CNN-based and Transformer-based methods, across five challenging benchmark datasets, achieving improved accuracy with a model size 10 × smaller than previous SOTA models.
Published: 2026-02-06T00:31:39+00:00
Venue: Expert Systems with Applications
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhui Ye; Weimin Lei; Wenchao Zhang; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131472"&gt;10.1016/j.eswa.2026.131472&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;The field of remote sensing image change detection has attracted significant interest. However, prevailing methods tend to concentrate on the basic extraction of change features, neglecting the crucial role of stable semantic patterns (e.g., ”features that remain consistent across time”) in bi-temporal images (image pairs taken at different times) for enhancing change feature extraction precision. Moreover, existing change detection networks often grapple with striking an optimal balance between accuracy and complexity. To surmount these challenges, we leverage the advanced feature extraction capabilities of pre-trained Vision Foundation Models (FastSAM), introducing an innovative lightweight Trident Decoder framework for remote sensing change detection, termed LTCD. Our framework encompasses three pioneering fully convolutional modules: the Attention-CDut Module, Trident Decoder Module, and Segment-Head Module. The Attention-CD Module enriches the semantic content of bi-temporal image features via cross-attention, which precedes the decoding phase, effectively decoupling change features. The Trident Decoder Module intensifies the influence of stable semantic patterns on change features within the feature pyramid, resulting in a distinctive trident-shaped feature output. Furthermore, the Segment-Head Module integrates shallow auxiliary tasks under a deep supervision learning scheme to bolster feature interaction. Experimental results demonstrate that our work outperforms state-of-the-art (SOTA) approaches, including both CNN-based and Transformer-based methods, across five challenging benchmark datasets, achieving improved accuracy with a model size 10 × smaller than previous SOTA models.&lt;/p&gt;</content:encoded></item><item><title>DyLA-YOLO: An accurate sonar target detection approach via YOLO based linear attention and dynamic upsampling</title><link>https://doi.org/10.1016/j.sigpro.2026.110546</link><guid>10.1016/j.sigpro.2026.110546</guid><pubDate>Thu, 05 Feb 2026 16:02:59 +0000</pubDate><dc:creator>Hao Wu</dc:creator><dc:creator>Shui Yu</dc:creator><dc:creator>Zhengliang Hu</dc:creator><dc:creator>Zhongtao Liu</dc:creator><dc:creator>Jingxuan Wang</dc:creator><dc:creator>Hongna Zhu</dc:creator><prism:publicationName>Signal Processing</prism:publicationName><prism:doi>10.1016/j.sigpro.2026.110546</prism:doi><description>Accurate sonar target detection (STD) with deep learning methods is attracting extensive research interest in the underwater perception. The inherent speckle noise, low signal-to-noise ratio (SNR), intense shadow effects, and spatial inhomogeneity of sonar images lead to the sparsification of discriminative features extracted by deep learning networks. To improve the accuracy of the STD, we propose a creative model, termed DyLA-YOLO, via YOLOv11 based linear attention and dynamic upsampling. Therein, the C3k2_MLLA with Mamba-inspired linear attention (MILA) is provided to enhances sparse feature extraction, and the DySample dynamic sampling is designed to realize lossless feature transmission and repair boundary features. Our model is verified on the typical Underwater Acoustic Target Detection (UATD) dataset and the Marine Debris Forward-Looking Sonar (MDFLS) dataset with high accuracy, and compared with other methods. This method provides a promising approach for accurate STD.
Published: 2026-02-05T16:02:59+00:00
Venue: Signal Processing
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wu; Shui Yu; Zhengliang Hu; Zhongtao Liu; Jingxuan Wang; Hongna Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Signal Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.sigpro.2026.110546"&gt;10.1016/j.sigpro.2026.110546&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Accurate sonar target detection (STD) with deep learning methods is attracting extensive research interest in the underwater perception. The inherent speckle noise, low signal-to-noise ratio (SNR), intense shadow effects, and spatial inhomogeneity of sonar images lead to the sparsification of discriminative features extracted by deep learning networks. To improve the accuracy of the STD, we propose a creative model, termed DyLA-YOLO, via YOLOv11 based linear attention and dynamic upsampling. Therein, the C3k2_MLLA with Mamba-inspired linear attention (MILA) is provided to enhances sparse feature extraction, and the DySample dynamic sampling is designed to realize lossless feature transmission and repair boundary features. Our model is verified on the typical Underwater Acoustic Target Detection (UATD) dataset and the Marine Debris Forward-Looking Sonar (MDFLS) dataset with high accuracy, and compared with other methods. This method provides a promising approach for accurate STD.&lt;/p&gt;</content:encoded></item><item><title>基于SAR图像的双时相变化检测研究综述</title><link>https://doi.org/10.11834/jrs.20255072</link><guid>10.11834/jrs.20255072</guid><pubDate>Fri, 06 Feb 2026 02:20:15 +0000</pubDate><dc:creator>LIU Yuting</dc:creator><dc:creator>LI Shihua</dc:creator><dc:creator>HE Ze</dc:creator><dc:creator>LIU Kaitong</dc:creator><prism:publicationName>National Remote Sensing Bulletin</prism:publicationName><prism:doi>10.11834/jrs.20255072</prism:doi><description>2026年1月30日电子科技大学资源与环境学院的刘玉婷、李世华团队在《遥感学报》发文，介绍了其在SAR图像变化检测领域的研究进展，该团队构建了双时相变化检测一般流程，梳理了主流数据和方法，为该领域研究提供了重要参考。
Published: 2026-02-06T02:20:15+00:00
Venue: National Remote Sensing Bulletin
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; LIU Yuting; LI Shihua; HE Ze; LIU Kaitong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; National Remote Sensing Bulletin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jrs.20255072"&gt;10.11834/jrs.20255072&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;2026年1月30日电子科技大学资源与环境学院的刘玉婷、李世华团队在《遥感学报》发文，介绍了其在SAR图像变化检测领域的研究进展，该团队构建了双时相变化检测一般流程，梳理了主流数据和方法，为该领域研究提供了重要参考。&lt;/p&gt;</content:encoded></item><item><title>Improving Episodic Few-shot Visual Question Answering via Spatial and Frequency Domain Dual-calibration</title><link>https://doi.org/10.1016/j.patcog.2026.113165</link><guid>10.1016/j.patcog.2026.113165</guid><pubDate>Thu, 05 Feb 2026 00:50:32 +0000</pubDate><dc:creator>Jing Zhang</dc:creator><dc:creator>Yifan Wei</dc:creator><dc:creator>Yunzuo Hu</dc:creator><dc:creator>Zhe Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113165</prism:doi><description>Considering that the frequency domain information in the image can make up for the deficiency of the spatial domain information in the global structure representation, we proposed a novel Dual-domain Feature and Distribution dual-calibration Network (DFDN) for episodic few shot visual question answering to achieve a deep and comprehensive understanding of the image content and cross-modal reasoning. In DFDN, spatial and frequency-domain information are mutually calibrated to achieve complementary information advantages, and more effective cross-modal reasoning is achieved through dual calibration of both features and distributions. A dual-domain feature calibration module is proposed, which employs mutual mapping and dynamic masking techniques to extract task-relevant features, and calibrate dual-domain information at the feature level. Meanwhile, a new dual-domain mutual distillation distribution calibration module is proposed to achieve mutual calibration of data distributions across spatial and frequency domains, further improving the cross-modal reasoning ability of DFDN. Experimental results across four public benchmark datasets demonstrated that DFDN achieved excellent performance and outperformed current state-of-the-art methods on episodic few shot visual question answering. Code is available at anonymous account: https://github.com/Harold1810/DFDN .
Published: 2026-02-05T00:50:32+00:00
Venue: Pattern Recognition
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Zhang; Yifan Wei; Yunzuo Hu; Zhe Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113165"&gt;10.1016/j.patcog.2026.113165&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Considering that the frequency domain information in the image can make up for the deficiency of the spatial domain information in the global structure representation, we proposed a novel Dual-domain Feature and Distribution dual-calibration Network (DFDN) for episodic few shot visual question answering to achieve a deep and comprehensive understanding of the image content and cross-modal reasoning. In DFDN, spatial and frequency-domain information are mutually calibrated to achieve complementary information advantages, and more effective cross-modal reasoning is achieved through dual calibration of both features and distributions. A dual-domain feature calibration module is proposed, which employs mutual mapping and dynamic masking techniques to extract task-relevant features, and calibrate dual-domain information at the feature level. Meanwhile, a new dual-domain mutual distillation distribution calibration module is proposed to achieve mutual calibration of data distributions across spatial and frequency domains, further improving the cross-modal reasoning ability of DFDN. Experimental results across four public benchmark datasets demonstrated that DFDN achieved excellent performance and outperformed current state-of-the-art methods on episodic few shot visual question answering. Code is available at anonymous account: https://github.com/Harold1810/DFDN .&lt;/p&gt;</content:encoded></item><item><title>A novel scheme for estimating surface solar radiation based on Fengyun-2</title><link>https://doi.org/10.1016/j.rse.2026.115295</link><guid>10.1016/j.rse.2026.115295</guid><pubDate>Thu, 05 Feb 2026 11:40:23 +0000</pubDate><dc:creator>Jinyan Yang</dc:creator><dc:creator>Liang Hong</dc:creator><dc:creator>Junmei He</dc:creator><dc:creator>Bing Hu</dc:creator><dc:creator>Chuanming Yuan</dc:creator><dc:creator>Wenjun Tang</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115295</prism:doi><description>Traditional surface solar radiation (SSR) estimation methods are limited by sparse ground stations, inconsistent data quality, and inadequate representativeness, while cloud-parameter-based physical retrievals are prone to errors under complex cloud conditions. To address these challenges, we propose a DenseNet-based, cloud-transmittance-driven framework that, for the first time, employs the high-accuracy Himawari-8 radiation product as the training reference instead of ground observations, and uses cloud transmittance as the model target for estimating SSR across China. This strategy eliminates the reliance on sparse ground stations and mitigates errors from cloud-phase retrievals, thereby substantially enhancing model stability. When evaluated against observations from 120 China Meteorological Administration (CMA) stations, the model achieved an hourly correlation coefficient (R) of 0.92, a root mean square error (RMSE) of 107.2 W m −2 , and a mean bias error (MBE) of −0.2 W m −2 . The proposed approach demonstrated superior performance relative to several existing SSR products derived from Fengyun-2. The results demonstrate that integrating high-precision satellite products as training references into deep-learning frameworks, while avoiding complex cloud parameter estimation, is a viable pathway to producing high-resolution, stable, and internally consistent radiation products from geostationary meteorological satellites, offering a new perspective for SSR estimation.
Published: 2026-02-05T11:40:23+00:00
Venue: Remote Sensing of Environment
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinyan Yang; Liang Hong; Junmei He; Bing Hu; Chuanming Yuan; Wenjun Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115295"&gt;10.1016/j.rse.2026.115295&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Traditional surface solar radiation (SSR) estimation methods are limited by sparse ground stations, inconsistent data quality, and inadequate representativeness, while cloud-parameter-based physical retrievals are prone to errors under complex cloud conditions. To address these challenges, we propose a DenseNet-based, cloud-transmittance-driven framework that, for the first time, employs the high-accuracy Himawari-8 radiation product as the training reference instead of ground observations, and uses cloud transmittance as the model target for estimating SSR across China. This strategy eliminates the reliance on sparse ground stations and mitigates errors from cloud-phase retrievals, thereby substantially enhancing model stability. When evaluated against observations from 120 China Meteorological Administration (CMA) stations, the model achieved an hourly correlation coefficient (R) of 0.92, a root mean square error (RMSE) of 107.2 W m −2 , and a mean bias error (MBE) of −0.2 W m −2 . The proposed approach demonstrated superior performance relative to several existing SSR products derived from Fengyun-2. The results demonstrate that integrating high-precision satellite products as training references into deep-learning frameworks, while avoiding complex cloud parameter estimation, is a viable pathway to producing high-resolution, stable, and internally consistent radiation products from geostationary meteorological satellites, offering a new perspective for SSR estimation.&lt;/p&gt;</content:encoded></item><item><title>ConMamba: Contrastive Vision Mamba for Plant Disease Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113177</link><guid>10.1016/j.patcog.2026.113177</guid><pubDate>Fri, 06 Feb 2026 00:28:22 +0000</pubDate><dc:creator>Abdullah Al Mamun</dc:creator><dc:creator>Miaohua Zhang</dc:creator><dc:creator>David Ahmedt-Aristizabal</dc:creator><dc:creator>Zeeshan Hayder</dc:creator><dc:creator>Mohammad Awrangjeb</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113177</prism:doi><description>Plant Disease Detection (PDD) is a key aspect of precision agriculture. However, existing deep learning methods often rely on extensively annotated datasets, which are time-consuming and costly to generate. Self-supervised Learning (SSL) offers a promising alternative by exploiting the abundance of unlabeled data. However, most existing SSL approaches suffer from high computational costs due to convolutional neural networks or transformer-based architectures. Additionally, they struggle to capture long-range dependencies in visual representation and rely on static loss functions that fail to align local and global features effectively. To address these challenges, we propose ConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates the Vision Mamba Encoder (VME), which employs a bidirectional State Space Model (SSM) to capture long-range dependencies efficiently. Furthermore, we introduce a dual-level contrastive loss with dynamic weight adjustment to optimize local-global feature alignment. Experimental results on three benchmark datasets demonstrate that ConMamba significantly outperforms state-of-the-art methods across multiple evaluation metrics. This provides an efficient and robust solution for PDD.
Published: 2026-02-06T00:28:22+00:00
Venue: Pattern Recognition
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Abdullah Al Mamun; Miaohua Zhang; David Ahmedt-Aristizabal; Zeeshan Hayder; Mohammad Awrangjeb&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113177"&gt;10.1016/j.patcog.2026.113177&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Plant Disease Detection (PDD) is a key aspect of precision agriculture. However, existing deep learning methods often rely on extensively annotated datasets, which are time-consuming and costly to generate. Self-supervised Learning (SSL) offers a promising alternative by exploiting the abundance of unlabeled data. However, most existing SSL approaches suffer from high computational costs due to convolutional neural networks or transformer-based architectures. Additionally, they struggle to capture long-range dependencies in visual representation and rely on static loss functions that fail to align local and global features effectively. To address these challenges, we propose ConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates the Vision Mamba Encoder (VME), which employs a bidirectional State Space Model (SSM) to capture long-range dependencies efficiently. Furthermore, we introduce a dual-level contrastive loss with dynamic weight adjustment to optimize local-global feature alignment. Experimental results on three benchmark datasets demonstrate that ConMamba significantly outperforms state-of-the-art methods across multiple evaluation metrics. This provides an efficient and robust solution for PDD.&lt;/p&gt;</content:encoded></item><item><title>空间模式引导的土地对象参数遥感智能计算研究</title><link>https://doi.org/10.11834/jrs.20255188</link><guid>10.11834/jrs.20255188</guid><pubDate>Fri, 06 Feb 2026 10:14:24 +0000</pubDate><dc:creator>LI Manjia</dc:creator><dc:creator>WU Tianjun</dc:creator><dc:creator>LUO Jiancheng</dc:creator><dc:creator>ZHANG Jing</dc:creator><dc:creator>LU Xuanzhi</dc:creator><dc:creator>LI Ziqi</dc:creator><dc:creator>FANG Zhiyang</dc:creator><prism:publicationName>National Remote Sensing Bulletin</prism:publicationName><prism:doi>10.11834/jrs.20255188</prism:doi><description>2026年1月5日长安大学土地工程学院的吴田军团队和中国科学院空天信息创新研究院遥感与数字地球全国重点实验室的李曼嘉团队在《遥感学报》发文，介绍了其在土地空间目标对象状态监测领域的研究进展，建立了耦合空间模式的地理目标对象认知与参数计算框架，为地表空间数字化表达及参数计算提供有效解决方案。
Published: 2026-02-06T10:14:24+00:00
Venue: National Remote Sensing Bulletin
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; LI Manjia; WU Tianjun; LUO Jiancheng; ZHANG Jing; LU Xuanzhi; LI Ziqi; FANG Zhiyang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; National Remote Sensing Bulletin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jrs.20255188"&gt;10.11834/jrs.20255188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;2026年1月5日长安大学土地工程学院的吴田军团队和中国科学院空天信息创新研究院遥感与数字地球全国重点实验室的李曼嘉团队在《遥感学报》发文，介绍了其在土地空间目标对象状态监测领域的研究进展，建立了耦合空间模式的地理目标对象认知与参数计算框架，为地表空间数字化表达及参数计算提供有效解决方案。&lt;/p&gt;</content:encoded></item><item><title>Lightweight dual-encoder deep learning integrating Sentinel-1 and Sentinel-2 for paddy field mapping</title><link>https://doi.org/10.1016/j.rsase.2026.101895</link><guid>10.1016/j.rsase.2026.101895</guid><pubDate>Fri, 06 Feb 2026 00:25:58 +0000</pubDate><dc:creator>Bagus Setyawan Wijaya</dc:creator><dc:creator>Rinaldi Munir</dc:creator><dc:creator>Nugraha Priya Utama</dc:creator><prism:publicationName>Remote Sensing Applications: Society and Environment</prism:publicationName><prism:doi>10.1016/j.rsase.2026.101895</prism:doi><description>Timely and accurate paddy field mapping remains challenging in tropical regions due to persistent cloud cover and complex cropping patterns. We propose DSSNet , a lightweight dual-encoder semantic segmentation framework that fuses Sentinel-1 SAR and Sentinel-2 optical imagery. DSSNet leverages modality-specific backbones from different architectural paradigms: EfficientNet-B0 , a convolutional, and MaxVit-T , a transformer-based encoder. To further enhance multimodal feature discrimination, we introduce two axial attention mechanisms — Axial Spatial Attention (ASA) and Axial Channel Attention (ACA) — to selectively emphasize directional spatial patterns and inter-channel relationships. Evaluated on imagery from Indonesia rice-growing regions during the 2019 season, DSSNet achieves an F1-score of 0.8982, pixel accuracy of 0.8998, and mIoU of 0.8156, outperforming ten benchmark models. These findings underscore the operational feasibility of lightweight dual-paradigm fusion architectures for large-scale, in-season agricultural mapping under complex environmental conditions. Our code and model will be publicly available at https://github.com/project4earth/DSSNet .
Published: 2026-02-06T00:25:58+00:00
Venue: Remote Sensing Applications: Society and Environment
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bagus Setyawan Wijaya; Rinaldi Munir; Nugraha Priya Utama&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing Applications: Society and Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rsase.2026.101895"&gt;10.1016/j.rsase.2026.101895&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Timely and accurate paddy field mapping remains challenging in tropical regions due to persistent cloud cover and complex cropping patterns. We propose DSSNet , a lightweight dual-encoder semantic segmentation framework that fuses Sentinel-1 SAR and Sentinel-2 optical imagery. DSSNet leverages modality-specific backbones from different architectural paradigms: EfficientNet-B0 , a convolutional, and MaxVit-T , a transformer-based encoder. To further enhance multimodal feature discrimination, we introduce two axial attention mechanisms — Axial Spatial Attention (ASA) and Axial Channel Attention (ACA) — to selectively emphasize directional spatial patterns and inter-channel relationships. Evaluated on imagery from Indonesia rice-growing regions during the 2019 season, DSSNet achieves an F1-score of 0.8982, pixel accuracy of 0.8998, and mIoU of 0.8156, outperforming ten benchmark models. These findings underscore the operational feasibility of lightweight dual-paradigm fusion architectures for large-scale, in-season agricultural mapping under complex environmental conditions. Our code and model will be publicly available at https://github.com/project4earth/DSSNet .&lt;/p&gt;</content:encoded></item><item><title>CSSA: A Cross-Modal Spatial–Semantic Alignment Framework for Remote Sensing Image Captioning</title><link>https://doi.org/10.3390/rs18030522</link><guid>10.3390/rs18030522</guid><pubDate>Thu, 05 Feb 2026 15:24:19 +0000</pubDate><dc:creator>Xiao Han</dc:creator><dc:creator>Zhaoji Wu</dc:creator><dc:creator>Yunpeng Li</dc:creator><dc:creator>Xiangrong Zhang</dc:creator><dc:creator>Guanchun Wang</dc:creator><dc:creator>Biao Hou</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030522</prism:doi><description>Remote sensing image captioning (RSIC) aims to generate natural language descriptions for the given remote sensing image, which requires a comprehensive and in-depth understanding of image content and summarizes it with sentences. Most RSIC methods have successful vision feature extraction, but the representation of spatial features or fusion features fails to fully consider cross-modal differences between remote sensing images and texts, resulting in unsatisfactory performance. Thus, we propose a novel cross-modal spatial–semantic alignment (CSSA) framework for an RSIC task, which consists of a multi-branch cross-modal contrastive learning (MCCL) mechanism and a dynamic geometry Transformer (DG-former) module. Specifically, compared to discrete text, remote sensing images present a noisy property, interfering with the extraction of valid vision features. Therefore, we present an MCCL mechanism to learn consistent representation between image and text, achieving cross-modal semantic alignment. In addition, most objects are scattered in remote sensing images and exhibit a sparsity property due to the overhead view. However, the Transformer structure mines the objects’ relationships without considering the geometry information of the objects, leading to suboptimal capture of the spatial structure. To address this, a DG-former is designed to realize spatial alignment by introducing geometry information. We conduct experiments on three publicly available datasets (Sydney-Captions, UCM-Captions and RSICD), and the superior results demonstrate its effectiveness.
Published: 2026-02-05T15:24:19+00:00
Venue: Remote Sensing
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiao Han; Zhaoji Wu; Yunpeng Li; Xiangrong Zhang; Guanchun Wang; Biao Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030522"&gt;10.3390/rs18030522&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image captioning (RSIC) aims to generate natural language descriptions for the given remote sensing image, which requires a comprehensive and in-depth understanding of image content and summarizes it with sentences. Most RSIC methods have successful vision feature extraction, but the representation of spatial features or fusion features fails to fully consider cross-modal differences between remote sensing images and texts, resulting in unsatisfactory performance. Thus, we propose a novel cross-modal spatial–semantic alignment (CSSA) framework for an RSIC task, which consists of a multi-branch cross-modal contrastive learning (MCCL) mechanism and a dynamic geometry Transformer (DG-former) module. Specifically, compared to discrete text, remote sensing images present a noisy property, interfering with the extraction of valid vision features. Therefore, we present an MCCL mechanism to learn consistent representation between image and text, achieving cross-modal semantic alignment. In addition, most objects are scattered in remote sensing images and exhibit a sparsity property due to the overhead view. However, the Transformer structure mines the objects’ relationships without considering the geometry information of the objects, leading to suboptimal capture of the spatial structure. To address this, a DG-former is designed to realize spatial alignment by introducing geometry information. We conduct experiments on three publicly available datasets (Sydney-Captions, UCM-Captions and RSICD), and the superior results demonstrate its effectiveness.&lt;/p&gt;</content:encoded></item><item><title>Seeing Like Argus: Multi-Perspective Global–Local Context Learning for Remote Sensing Semantic Segmentation</title><link>https://doi.org/10.3390/rs18030521</link><guid>10.3390/rs18030521</guid><pubDate>Thu, 05 Feb 2026 15:24:19 +0000</pubDate><dc:creator>Hongbing Chen</dc:creator><dc:creator>Yizhe Feng</dc:creator><dc:creator>Kun Wang</dc:creator><dc:creator>Mingrui Liao</dc:creator><dc:creator>Haoting Zhai</dc:creator><dc:creator>Tian Xia</dc:creator><dc:creator>Yubo Zhang</dc:creator><dc:creator>Jianhua Jiao</dc:creator><dc:creator>Changji Wen</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030521</prism:doi><description>Accurate semantic segmentation of high-resolution remote sensing imagery is crucial for applications such as land cover mapping, urban development monitoring, and disaster response. However, remote sensing data still present inherent challenges, including complex spatial structures, significant intra-class variability, and diverse object scales, which demand models capable of capturing rich contextual information from both local and global regions. To address these issues, we propose ArgusNet, a novel segmentation framework that enhances multi-scale representations through a series of carefully designed fusion mechanisms. At the core of ArgusNet lies the synergistic integration of Adaptive Windowed Additive Attention (AWAA) and 2D Selective Scan (SS2D). Specifically, our AWAA extends additive attention into a window-based structure with a dynamic routing mechanism, enabling multi-perspective local feature interaction via multiple global query vectors. Furthermore, we introduce a decoder optimization strategy incorporating three-stage feature fusion and a Macro Guidance Module (MGM) to improve spatial detail preservation and semantic consistency. Experiments on benchmark remote sensing datasets demonstrate that ArgusNet achieves competitive and improved segmentation performance compared to state-of-the-art methods, particularly in scenarios requiring fine-grained object delineation and robust multi-scale contextual understanding.
Published: 2026-02-05T15:24:19+00:00
Venue: Remote Sensing
Score: 0.760 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongbing Chen; Yizhe Feng; Kun Wang; Mingrui Liao; Haoting Zhai; Tian Xia; Yubo Zhang; Jianhua Jiao; Changji Wen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030521"&gt;10.3390/rs18030521&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.760 (consider)&lt;/p&gt;
&lt;p&gt;Accurate semantic segmentation of high-resolution remote sensing imagery is crucial for applications such as land cover mapping, urban development monitoring, and disaster response. However, remote sensing data still present inherent challenges, including complex spatial structures, significant intra-class variability, and diverse object scales, which demand models capable of capturing rich contextual information from both local and global regions. To address these issues, we propose ArgusNet, a novel segmentation framework that enhances multi-scale representations through a series of carefully designed fusion mechanisms. At the core of ArgusNet lies the synergistic integration of Adaptive Windowed Additive Attention (AWAA) and 2D Selective Scan (SS2D). Specifically, our AWAA extends additive attention into a window-based structure with a dynamic routing mechanism, enabling multi-perspective local feature interaction via multiple global query vectors. Furthermore, we introduce a decoder optimization strategy incorporating three-stage feature fusion and a Macro Guidance Module (MGM) to improve spatial detail preservation and semantic consistency. Experiments on benchmark remote sensing datasets demonstrate that ArgusNet achieves competitive and improved segmentation performance compared to state-of-the-art methods, particularly in scenarios requiring fine-grained object delineation and robust multi-scale contextual understanding.&lt;/p&gt;</content:encoded></item><item><title>Remote sensing of tropical forest recovery: A review and decision-support framework for multi-sensor integration</title><link>https://doi.org/10.1016/j.rse.2026.115257</link><guid>10.1016/j.rse.2026.115257</guid><pubDate>Thu, 05 Feb 2026 19:58:13 +0000</pubDate><dc:creator>Chima J. Iheaturu</dc:creator><dc:creator>Giulia F. Curatola Fernández</dc:creator><dc:creator>Vladimir R. Wingate</dc:creator><dc:creator>Felicia O. Akinyemi</dc:creator><dc:creator>Chukwuma J. Okolie</dc:creator><dc:creator>Chinwe Ifejika Speranza</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115257</prism:doi><description>Tropical forest recovery plays a vital role in mitigating climate change and conserving biodiversity, yet accurately monitoring its ecological success remains a persistent challenge. Common proxies such as canopy cover or greenness often overestimate recovery by conflating rapid structural regrowth with the slower processes of compositional reassembly and functional reintegration. This review synthesizes recent advances in remote sensing that enable more comprehensive tracking of tropical forest recovery across structural, compositional, and functional dimensions and at multiple spatial and temporal scales. We evaluate the capabilities and limitations of key sensor types: LiDAR for mapping canopy structure and estimating biomass; optical sensors for assessing spectral diversity and phenological variation; synthetic aperture radar (SAR) for reliable structural monitoring under all weather conditions; passive microwave sensors capture plant water content, with Vegetation Optical Depth (VOD) serving as a valuable proxy for hydrological function; and thermal sensors for tracking evapotranspiration and plant stress. Crucially, we highlight how integrating these complementary data streams (e.g., fusing LiDAR with hyperspectral or SAR with VOD) overcomes single-sensor blind spots, revealing decoupled recovery trajectories and avoiding misleading \
Published: 2026-02-05T19:58:13+00:00
Venue: Remote Sensing of Environment
Score: 0.759 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chima J. Iheaturu; Giulia F. Curatola Fernández; Vladimir R. Wingate; Felicia O. Akinyemi; Chukwuma J. Okolie; Chinwe Ifejika Speranza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115257"&gt;10.1016/j.rse.2026.115257&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (consider)&lt;/p&gt;
&lt;p&gt;Tropical forest recovery plays a vital role in mitigating climate change and conserving biodiversity, yet accurately monitoring its ecological success remains a persistent challenge. Common proxies such as canopy cover or greenness often overestimate recovery by conflating rapid structural regrowth with the slower processes of compositional reassembly and functional reintegration. This review synthesizes recent advances in remote sensing that enable more comprehensive tracking of tropical forest recovery across structural, compositional, and functional dimensions and at multiple spatial and temporal scales. We evaluate the capabilities and limitations of key sensor types: LiDAR for mapping canopy structure and estimating biomass; optical sensors for assessing spectral diversity and phenological variation; synthetic aperture radar (SAR) for reliable structural monitoring under all weather conditions; passive microwave sensors capture plant water content, with Vegetation Optical Depth (VOD) serving as a valuable proxy for hydrological function; and thermal sensors for tracking evapotranspiration and plant stress. Crucially, we highlight how integrating these complementary data streams (e.g., fusing LiDAR with hyperspectral or SAR with VOD) overcomes single-sensor blind spots, revealing decoupled recovery trajectories and avoiding misleading \&lt;/p&gt;</content:encoded></item><item><title>Text2AIRS: Fine-Grained Airplane Image Generation in Remote Sensing from Nature Language</title><link>https://doi.org/10.3390/rs18030511</link><guid>10.3390/rs18030511</guid><pubDate>Thu, 05 Feb 2026 11:13:01 +0000</pubDate><dc:creator>Yunuo Yang</dc:creator><dc:creator>Youwei Cheng</dc:creator><dc:creator>Jinlong Hu</dc:creator><dc:creator>Yan Xia</dc:creator><dc:creator>Yu Zang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030511</prism:doi><description>Airplanes are the most popular investigation objects as a dynamic and critical component in remote sensing images. Accurately identifying and monitoring airplane behaviors is crucial for effective air traffic management. However, existing methods for interpreting fine-grained airplanes in remote sensing data depend heavily on large annotated datasets, which are both time-consuming and prone to errors due to the detailed nature of labeling individual points. In this paper, we introduce Text2AIRS, a novel method that generates fine-grained and realistic Airplane Images in Remote Sensing from textual descriptions. Text2AIRS significantly simplifies the process of generating diverse aircraft types, requiring limited texts and allowing for extensive variability in the generated images. Specifically, Text2AIRS is the first to incorporate ground sample distance into the text-to-image stable diffusion model, both at the data and feature levels. Extensive experiments demonstrate our Text2AIRS surpasses the state-of-the-art by a large margin on the Fair1M benchmark dataset. Furthermore, utilizing the fine-grained airplane images generated by Text2AIRS, the existing SOTA object detector achieves 6.12% performance improvement, showing the practical impact of our approach.
Published: 2026-02-05T11:13:01+00:00
Venue: Remote Sensing
Score: 0.759 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunuo Yang; Youwei Cheng; Jinlong Hu; Yan Xia; Yu Zang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030511"&gt;10.3390/rs18030511&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (consider)&lt;/p&gt;
&lt;p&gt;Airplanes are the most popular investigation objects as a dynamic and critical component in remote sensing images. Accurately identifying and monitoring airplane behaviors is crucial for effective air traffic management. However, existing methods for interpreting fine-grained airplanes in remote sensing data depend heavily on large annotated datasets, which are both time-consuming and prone to errors due to the detailed nature of labeling individual points. In this paper, we introduce Text2AIRS, a novel method that generates fine-grained and realistic Airplane Images in Remote Sensing from textual descriptions. Text2AIRS significantly simplifies the process of generating diverse aircraft types, requiring limited texts and allowing for extensive variability in the generated images. Specifically, Text2AIRS is the first to incorporate ground sample distance into the text-to-image stable diffusion model, both at the data and feature levels. Extensive experiments demonstrate our Text2AIRS surpasses the state-of-the-art by a large margin on the Fair1M benchmark dataset. Furthermore, utilizing the fine-grained airplane images generated by Text2AIRS, the existing SOTA object detector achieves 6.12% performance improvement, showing the practical impact of our approach.&lt;/p&gt;</content:encoded></item></channel></rss>