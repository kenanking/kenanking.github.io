<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 11 Dec 2025 02:46:54 +0000</lastBuildDate><item><title>Towards Efficient Semi-Supervised Object Detection with Detection Transformer</title><link>https://doi.org/10.1109/tpami.2025.3642123</link><guid>10.1109/tpami.2025.3642123</guid><pubDate>Wed, 10 Dec 2025 18:32:22 +0000</pubDate><dc:creator>Jiacheng Zhang</dc:creator><dc:creator>Jiaming Li</dc:creator><dc:creator>Xiangru Lin</dc:creator><dc:creator>Wei Zhang</dc:creator><dc:creator>Xiao Tan</dc:creator><dc:creator>Hongbo Gao</dc:creator><dc:creator>Jingdong Wang</dc:creator><dc:creator>Guanbin Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642123</prism:doi><description>Semi-supervised object detection (SSOD) mitigates the annotation burden in object detection by leveraging unlabeled data, providing a scalable solution for modern perception systems. Concurrently, detection transformers (DETRs) have emerged as a popular end-to-end framework, offering advantages such as non-maximum suppression (NMS)-free inference. However, existing SSODmethods are predominantly designed for conventional detectors, leaving the exploration of DETR-based SSOD largely uncharted. This paper presents a systematic study to bridge this gap. We begin by identifying two principal obstacles in semi-supervised DETR training: (1) the inherent one-to-one assignment mechanism of DETRs is highly sensitive to noisy pseudo-labels, which impedes training efficiency; and (2) the query-based decoder architecture complicates the design of an effective consistency regularization scheme, limiting further performance gains. To address these challenges, we propose Semi-DETR++, a novel framework for efficient SSOD with DETRs. Our approach introduces a stage-wise hybrid matching strategy that enhances robustness to noisy pseudo-labels by synergistically combining one-to-many and one-to-one assignments while preserving NMS-free inference. Furthermore, based on our observation of the unique layer-wise decoding behavior in DETRs, we develop a simple yet effective re-decode query consistency training method to regularize the decoder. Extensive experiments demonstrate that Semi-DETR++ enables more efficient semi-supervised learning across various DETR architectures, outperforming existing methods by significant margins. The proposed components are also flexible and versatile, showing superior generalization by readily extending to semi-supervised segmentation tasks. Code is available at https://github.com/JCZ404/Semi-DETR.
Published: 2025-12-10T18:32:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.840 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiacheng Zhang; Jiaming Li; Xiangru Lin; Wei Zhang; Xiao Tan; Hongbo Gao; Jingdong Wang; Guanbin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642123"&gt;10.1109/tpami.2025.3642123&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.840 (must_read)&lt;/p&gt;
&lt;p&gt;Semi-supervised object detection (SSOD) mitigates the annotation burden in object detection by leveraging unlabeled data, providing a scalable solution for modern perception systems. Concurrently, detection transformers (DETRs) have emerged as a popular end-to-end framework, offering advantages such as non-maximum suppression (NMS)-free inference. However, existing SSODmethods are predominantly designed for conventional detectors, leaving the exploration of DETR-based SSOD largely uncharted. This paper presents a systematic study to bridge this gap. We begin by identifying two principal obstacles in semi-supervised DETR training: (1) the inherent one-to-one assignment mechanism of DETRs is highly sensitive to noisy pseudo-labels, which impedes training efficiency; and (2) the query-based decoder architecture complicates the design of an effective consistency regularization scheme, limiting further performance gains. To address these challenges, we propose Semi-DETR++, a novel framework for efficient SSOD with DETRs. Our approach introduces a stage-wise hybrid matching strategy that enhances robustness to noisy pseudo-labels by synergistically combining one-to-many and one-to-one assignments while preserving NMS-free inference. Furthermore, based on our observation of the unique layer-wise decoding behavior in DETRs, we develop a simple yet effective re-decode query consistency training method to regularize the decoder. Extensive experiments demonstrate that Semi-DETR++ enables more efficient semi-supervised learning across various DETR architectures, outperforming existing methods by significant margins. The proposed components are also flexible and versatile, showing superior generalization by readily extending to semi-supervised segmentation tasks. Code is available at https://github.com/JCZ404/Semi-DETR.&lt;/p&gt;</content:encoded></item><item><title>Optical-to-SAR Domain Adaptation with Inversion Regularization for Unsupervised Ship Detection</title><link>https://doi.org/10.1016/j.knosys.2025.115044</link><guid>10.1016/j.knosys.2025.115044</guid><pubDate>Tue, 09 Dec 2025 07:42:52 +0000</pubDate><dc:creator>Shijie Wang</dc:creator><dc:creator>Yuanfei Huang</dc:creator><dc:creator>Ping Wang</dc:creator><dc:creator>Lei Lu</dc:creator><dc:creator>Hua Huang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115044</prism:doi><description>Due to the challenges of synthetic aperture radar (SAR) data acquisition and the high cost of manual annotation, utilizing labeled optical images to learn from unlabeled SAR data has received great attention. Cross-modal domain adaptation (DA) from optical to SAR imagery presents a particularly difficult problem because of the inherent modality gap between these two imaging paradigms. To address the problem of unsupervised ship detection in SAR images, we conduct domain adaptation experiments from ship images in the DIOR dataset to the SSDD dataset. However, traditional domain adaptation methods are insufficient to address the significant modality differences between optical and SAR images. Although the unconstrained feature alignment strategy is effective between domains with small differences, it inadvertently expels SAR features from the supervised recognition space, ultimately reducing detection performance. To mitigate this issue, we propose a new framework, DAIR, which integrates an innovative inversion regularization module (IRM) and a task-correlation enhancement (TCE) strategy to improve domain adaptation. Specifically, IRM acts as a feature-space regularizer to counteract deviation caused by aggressive alignment, while TCE explicitly models task interdependency to alleviate the effects of task independence. Evaluated on the DIOR and SSDD datasets, our method achieved improvements of 5% and 8% in AP50 and APm, respectively, over the baseline method DA Faster R-CNN. In few-shot scenarios, our method attained gains of 14.1%, 10.6%, and 15.4% in AP50 compared to the state-of-the-art method under 3-shot, 5-shot, and 10-shot settings, which demonstrates stronger generalization ability with limited data. Source code and models are available at https://github.com/whatbb/DAIR/tree/main
Published: 2025-12-09T07:42:52+00:00
Venue: Knowledge-Based Systems
Score: 0.840 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shijie Wang; Yuanfei Huang; Ping Wang; Lei Lu; Hua Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115044"&gt;10.1016/j.knosys.2025.115044&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.840 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the challenges of synthetic aperture radar (SAR) data acquisition and the high cost of manual annotation, utilizing labeled optical images to learn from unlabeled SAR data has received great attention. Cross-modal domain adaptation (DA) from optical to SAR imagery presents a particularly difficult problem because of the inherent modality gap between these two imaging paradigms. To address the problem of unsupervised ship detection in SAR images, we conduct domain adaptation experiments from ship images in the DIOR dataset to the SSDD dataset. However, traditional domain adaptation methods are insufficient to address the significant modality differences between optical and SAR images. Although the unconstrained feature alignment strategy is effective between domains with small differences, it inadvertently expels SAR features from the supervised recognition space, ultimately reducing detection performance. To mitigate this issue, we propose a new framework, DAIR, which integrates an innovative inversion regularization module (IRM) and a task-correlation enhancement (TCE) strategy to improve domain adaptation. Specifically, IRM acts as a feature-space regularizer to counteract deviation caused by aggressive alignment, while TCE explicitly models task interdependency to alleviate the effects of task independence. Evaluated on the DIOR and SSDD datasets, our method achieved improvements of 5% and 8% in AP50 and APm, respectively, over the baseline method DA Faster R-CNN. In few-shot scenarios, our method attained gains of 14.1%, 10.6%, and 15.4% in AP50 compared to the state-of-the-art method under 3-shot, 5-shot, and 10-shot settings, which demonstrates stronger generalization ability with limited data. Source code and models are available at https://github.com/whatbb/DAIR/tree/main&lt;/p&gt;</content:encoded></item><item><title>Diverse semantic representation learning based on vision-language models for zero-shot indoor scene recognition</title><link>https://doi.org/10.1016/j.inffus.2025.104049</link><guid>10.1016/j.inffus.2025.104049</guid><pubDate>Wed, 10 Dec 2025 00:35:42 +0000</pubDate><dc:creator>Chen Wang</dc:creator><dc:creator>Guohua Peng</dc:creator><dc:creator>Bernard De Baets</dc:creator><dc:creator>Xiong Pan</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104049</prism:doi><description>Recently, vision-language models, such as the well-known CLIP, have demonstrated remarkable generalization ability in various zero-shot recognition tasks. However, their performance on zero-shot fine-grained recognition, especially for indoor scenes, remains limited due to the high between-class semantic similarity. To address this challenge, we propose a Diverse Semantic Representation Learning (DSRL) method based on the pre-trained vision-language model for zero-shot indoor scene recognition. Specifically, we first design a meaningful prompt text for indoor scene images to extract semantic features based on the CLIP text encoder. Then, in order to explore diverse visual-related semantic features, we introduce a visual-guided semantic feature learning method based on the CLIP image encoder, which refines the diverse visual prototypes through contrastive learning. Next, these features are fused by a multi-head attention fusion strategy, generating the diverse semantic representations. Finally, a dual reconstruction loss and a cross-entropy loss are constructed to facilitate knowledge transfer for zero-shot learning. In the testing phase, inspired by the convergent evolution theory, we revise the visual-guided semantic feature learning method to obtain the diverse semantic representations for unseen images. Extensive experiments on three indoor scene datasets demonstrate that DSRL achieves the state-of-the-art performance in zero-shot indoor scene recognition.
Published: 2025-12-10T00:35:42+00:00
Venue: Information Fusion
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Wang; Guohua Peng; Bernard De Baets; Xiong Pan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104049"&gt;10.1016/j.inffus.2025.104049&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, vision-language models, such as the well-known CLIP, have demonstrated remarkable generalization ability in various zero-shot recognition tasks. However, their performance on zero-shot fine-grained recognition, especially for indoor scenes, remains limited due to the high between-class semantic similarity. To address this challenge, we propose a Diverse Semantic Representation Learning (DSRL) method based on the pre-trained vision-language model for zero-shot indoor scene recognition. Specifically, we first design a meaningful prompt text for indoor scene images to extract semantic features based on the CLIP text encoder. Then, in order to explore diverse visual-related semantic features, we introduce a visual-guided semantic feature learning method based on the CLIP image encoder, which refines the diverse visual prototypes through contrastive learning. Next, these features are fused by a multi-head attention fusion strategy, generating the diverse semantic representations. Finally, a dual reconstruction loss and a cross-entropy loss are constructed to facilitate knowledge transfer for zero-shot learning. In the testing phase, inspired by the convergent evolution theory, we revise the visual-guided semantic feature learning method to obtain the diverse semantic representations for unseen images. Extensive experiments on three indoor scene datasets demonstrate that DSRL achieves the state-of-the-art performance in zero-shot indoor scene recognition.&lt;/p&gt;</content:encoded></item><item><title>A Survey of Small Sea-Surface Target Detection for Maritime Search and Rescue</title><link>https://doi.org/10.1109/tits.2025.3635199</link><guid>10.1109/tits.2025.3635199</guid><pubDate>Wed, 10 Dec 2025 18:34:12 +0000</pubDate><dc:creator>Jianchuan Yin</dc:creator><dc:creator>Guokang Xu</dc:creator><dc:creator>Ning Wang</dc:creator><dc:creator>Nini Wang</dc:creator><dc:creator>Zeguo Zhang</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3635199</prism:doi><description>The detection of small surface targets plays a critical role in maritime search and rescue (SAR) operations, ensuring the safety of people and property at sea. This paper provides a comprehensive review of the latest advancements and research in small sea surface target detection for maritime SAR missions. Deep learning-based models facilitate accurate target detection and localization by transforming image or video frames into high-dimensional abstract representations, enabling effective detection in complex sea surface environments. However, challenges such as occlusion, blurring, and reflections on the sea surface significantly complicate small target detection. To address these challenges, this paper summarizes a range of effective approaches, including context information, multi-scale learning, anchor-free detection, super-resolution, attention mechanisms, and sample-oriented approaches. These approaches aim to enhance the performance of small target detection in applications such as uncrewed aerial vehicles (UAV) and uncrewed supply vessels. Furthermore, this paper classifies small target datasets, providing a detailed overview based on their collection methods and application scenarios, while highlighting representative datasets. Through a thorough analysis of both methodologies and datasets, this paper offers valuable insights and directions for the future development of small target detection technology in maritime search and rescue operations.
Published: 2025-12-10T18:34:12+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianchuan Yin; Guokang Xu; Ning Wang; Nini Wang; Zeguo Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3635199"&gt;10.1109/tits.2025.3635199&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;The detection of small surface targets plays a critical role in maritime search and rescue (SAR) operations, ensuring the safety of people and property at sea. This paper provides a comprehensive review of the latest advancements and research in small sea surface target detection for maritime SAR missions. Deep learning-based models facilitate accurate target detection and localization by transforming image or video frames into high-dimensional abstract representations, enabling effective detection in complex sea surface environments. However, challenges such as occlusion, blurring, and reflections on the sea surface significantly complicate small target detection. To address these challenges, this paper summarizes a range of effective approaches, including context information, multi-scale learning, anchor-free detection, super-resolution, attention mechanisms, and sample-oriented approaches. These approaches aim to enhance the performance of small target detection in applications such as uncrewed aerial vehicles (UAV) and uncrewed supply vessels. Furthermore, this paper classifies small target datasets, providing a detailed overview based on their collection methods and application scenarios, while highlighting representative datasets. Through a thorough analysis of both methodologies and datasets, this paper offers valuable insights and directions for the future development of small target detection technology in maritime search and rescue operations.&lt;/p&gt;</content:encoded></item><item><title>SaSAM: Scale-aware segmentation anything model for multimodal remote sensing images</title><link>https://doi.org/10.1016/j.inffus.2025.104054</link><guid>10.1016/j.inffus.2025.104054</guid><pubDate>Tue, 09 Dec 2025 07:46:35 +0000</pubDate><dc:creator>You Ma</dc:creator><dc:creator>Hongwei Tong</dc:creator><dc:creator>Lin Chai</dc:creator><dc:creator>Shihan Mao</dc:creator><dc:creator>Yucheng Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104054</prism:doi><description>The segment anything model (SAM) has achieved remarkable progress in remote sensing image segmentation tasks due to its exceptional segmentation performance and generalization ability. However, existing SAM-based methods primarily focus on modeling single-modal data, which limits the feature representation of the model in complex scenarios. Additionally, these methods fail to fully exploit multi-scale information during fine-tuning and lack collaborative utilization of multi-level features. To address these issues, we propose the scale-aware SAM (SaSAM) framework, aiming to explore the potential of SAM for semantic segmentation of multimodal remote sensing images. Specifically, we first employ a dual attention feature fusion module to integrate multimodal features into a unified feature, enabling the adaptation of SAM to multimodal tasks without altering its original structure. Next, we design a mixture of multi-scale LoRA experts module that captures object features at different scales through multiple lightweight LoRA experts to enhance the scale-aware capability of the model. Subsequently, we introduce a multi-level feature adaptive aggregation module to fully utilize the multi-granularity features in the SAM encoder. Finally, the multi-scale features are input into the masked decoder to generate accurate segmentation results. Extensive experiments on three multimodal remote sensing datasets demonstrate the superiority of our method. The source code is available at https://github.com/MaYou1997/SaSAM .
Published: 2025-12-09T07:46:35+00:00
Venue: Information Fusion
Score: 0.829 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; You Ma; Hongwei Tong; Lin Chai; Shihan Mao; Yucheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104054"&gt;10.1016/j.inffus.2025.104054&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.829 (must_read)&lt;/p&gt;
&lt;p&gt;The segment anything model (SAM) has achieved remarkable progress in remote sensing image segmentation tasks due to its exceptional segmentation performance and generalization ability. However, existing SAM-based methods primarily focus on modeling single-modal data, which limits the feature representation of the model in complex scenarios. Additionally, these methods fail to fully exploit multi-scale information during fine-tuning and lack collaborative utilization of multi-level features. To address these issues, we propose the scale-aware SAM (SaSAM) framework, aiming to explore the potential of SAM for semantic segmentation of multimodal remote sensing images. Specifically, we first employ a dual attention feature fusion module to integrate multimodal features into a unified feature, enabling the adaptation of SAM to multimodal tasks without altering its original structure. Next, we design a mixture of multi-scale LoRA experts module that captures object features at different scales through multiple lightweight LoRA experts to enhance the scale-aware capability of the model. Subsequently, we introduce a multi-level feature adaptive aggregation module to fully utilize the multi-granularity features in the SAM encoder. Finally, the multi-scale features are input into the masked decoder to generate accurate segmentation results. Extensive experiments on three multimodal remote sensing datasets demonstrate the superiority of our method. The source code is available at https://github.com/MaYou1997/SaSAM .&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Object Detection via Spatial-Channel State Space Model</title><link>https://doi.org/10.1109/tcsvt.2025.3642750</link><guid>10.1109/tcsvt.2025.3642750</guid><pubDate>Wed, 10 Dec 2025 18:34:28 +0000</pubDate><dc:creator>Zhimeng Xin</dc:creator><dc:creator>Tianxu Wu</dc:creator><dc:creator>Yixiong Zou</dc:creator><dc:creator>Shiming Chen</dc:creator><dc:creator>Dingjie Fu</dc:creator><dc:creator>Xinge You</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642750</prism:doi><description>Due to the limited training samples in few-shot object detection (FSOD), we observe that current methods may struggle to accurately extract effective features from each channel. Specifically, this issue manifests in two aspects: i) channels with high weights may not necessarily be effective, and ii) channels with low weights may still hold significant value. To handle this problem, we consider utilizing inter-channel correlation to ensure that the novel model can effectively highlight relevant channels and rectify incorrect ones, thereby strengthening channel quality. Since the channel sequence is also 1-dimensional, its similarity with the temporal sequence inspires us to take Mamba for modeling the correlation in the channel sequence Based on this concept, we propose the Spatial-Channel State Space Modeling (SCSM) module for spatial-channel-sequence modeling to accurately extract effective features from each channel. In SCSM, we design the Spatial Feature Modeling (SFM) module to ensure the quality of spatial feature representations. We then introduce the Channel State Modeling (CSM) module, which treats channels as a 1-dimensional sequence and take mamba to capture the correlation between channels. Extensive experiments on the VOC and COCO datasets show that the SCSM module enables the novel detector to improve the quality of channel feature representations and achieve state-of-the-art performance. Code is released at https://github.com/zhimengXin/SCSM.
Published: 2025-12-10T18:34:28+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhimeng Xin; Tianxu Wu; Yixiong Zou; Shiming Chen; Dingjie Fu; Xinge You&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642750"&gt;10.1109/tcsvt.2025.3642750&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the limited training samples in few-shot object detection (FSOD), we observe that current methods may struggle to accurately extract effective features from each channel. Specifically, this issue manifests in two aspects: i) channels with high weights may not necessarily be effective, and ii) channels with low weights may still hold significant value. To handle this problem, we consider utilizing inter-channel correlation to ensure that the novel model can effectively highlight relevant channels and rectify incorrect ones, thereby strengthening channel quality. Since the channel sequence is also 1-dimensional, its similarity with the temporal sequence inspires us to take Mamba for modeling the correlation in the channel sequence Based on this concept, we propose the Spatial-Channel State Space Modeling (SCSM) module for spatial-channel-sequence modeling to accurately extract effective features from each channel. In SCSM, we design the Spatial Feature Modeling (SFM) module to ensure the quality of spatial feature representations. We then introduce the Channel State Modeling (CSM) module, which treats channels as a 1-dimensional sequence and take mamba to capture the correlation between channels. Extensive experiments on the VOC and COCO datasets show that the SCSM module enables the novel detector to improve the quality of channel feature representations and achieve state-of-the-art performance. Code is released at https://github.com/zhimengXin/SCSM.&lt;/p&gt;</content:encoded></item><item><title>NSDSAM: Noise-Suppression-Driven SAM for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3642125</link><guid>10.1109/tgrs.2025.3642125</guid><pubDate>Tue, 09 Dec 2025 18:32:55 +0000</pubDate><dc:creator>Wenxiao Xu</dc:creator><dc:creator>Qiyuan Yin</dc:creator><dc:creator>Chen Wu</dc:creator><dc:creator>Dianjie Lu</dc:creator><dc:creator>Guijuan Zhang</dc:creator><dc:creator>Zhuoran Zheng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3642125</prism:doi><description>Although Segment Anything Model (SAM) have recently achieved remarkable progress, their generalization capability in infrared small target detection remains limited due to the inherently high noise levels in infrared imagery. To preserve the generalization and noise suppression ability of the model, we propose an method called NSDSAM, a noise-suppression-driven approach that enhances SAM at both internal and external levels. Internally, we develop a Hybrid Adapter for suppressing the noise of feature maps, consisting of an MLP adapter and a self-attention adapter. The self-attention adapter first performs entropy-aware reconstruction of features from noisy inputs and employs a gating mechanism for soft-attention fusion, mitigating SAM’s sensitivity to noise. Externally, we design a Spatial-Frequency hybrid Module (SFHM) that jointly processes spatial and frequency domains to overcome the self-attention model’s bias toward low-frequency components, further strengthening the suppression of background clutter and noise. Extensive experiments on multiple infrared datasets demonstrate that the proposed method achieves state-of-the-art (SOTA) performance in infrared small target detection. The project code is available upon acceptance.
Published: 2025-12-09T18:32:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxiao Xu; Qiyuan Yin; Chen Wu; Dianjie Lu; Guijuan Zhang; Zhuoran Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3642125"&gt;10.1109/tgrs.2025.3642125&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Although Segment Anything Model (SAM) have recently achieved remarkable progress, their generalization capability in infrared small target detection remains limited due to the inherently high noise levels in infrared imagery. To preserve the generalization and noise suppression ability of the model, we propose an method called NSDSAM, a noise-suppression-driven approach that enhances SAM at both internal and external levels. Internally, we develop a Hybrid Adapter for suppressing the noise of feature maps, consisting of an MLP adapter and a self-attention adapter. The self-attention adapter first performs entropy-aware reconstruction of features from noisy inputs and employs a gating mechanism for soft-attention fusion, mitigating SAM’s sensitivity to noise. Externally, we design a Spatial-Frequency hybrid Module (SFHM) that jointly processes spatial and frequency domains to overcome the self-attention model’s bias toward low-frequency components, further strengthening the suppression of background clutter and noise. Extensive experiments on multiple infrared datasets demonstrate that the proposed method achieves state-of-the-art (SOTA) performance in infrared small target detection. The project code is available upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation</title><link>https://doi.org/10.1109/tip.2025.3639996</link><guid>10.1109/tip.2025.3639996</guid><pubDate>Wed, 10 Dec 2025 18:34:53 +0000</pubDate><dc:creator>Sule Bai</dc:creator><dc:creator>Yong Liu</dc:creator><dc:creator>Yifei Han</dc:creator><dc:creator>Haoji Zhang</dc:creator><dc:creator>Yansong Tang</dc:creator><dc:creator>Jie Zhou</dc:creator><dc:creator>Jiwen Lu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639996</prism:doi><description>Recent advancements in pre-trained vision-language models like CLIP, have enabled the task of open-vocabulary segmentation. CLIP demonstrates impressive zero-shot capabilities in various downstream tasks that require holistic image understanding. However, due to the image-level contrastive learning and fully global feature interaction, ViT-based CLIP struggles to capture local details, resulting in poor performance in segmentation tasks. Our analysis of ViT-based CLIP reveals that anomaly tokens emerge during the forward process, attracting disproportionate attention from normal patch tokens and thereby diminishing spatial awareness. To address this issue, we propose Self-Calibrated CLIP (SC-CLIP), a training-free method that calibrates CLIP to generate finer representations while preserving its original generalization ability—without introducing new parameters or relying on additional backbones. Specifically, we mitigate the negative impact of anomaly tokens from two complementary perspectives. First, we explicitly identify the anomaly tokens and replace them based on local context. Second, we reduce their influence on normal tokens by enhancing feature discriminability and attention correlation, leveraging the inherent semantic consistency within CLIP’s mid-level features. In addition, we introduce a two-pass strategy that effectively integrates multi-level features to enrich local details under the training-free setting. Together, these strategies enhance CLIP’s feature representations with improved granularity and semantic coherence. Experimental results demonstrate the effectiveness of SC-CLIP, achieving state-of-the-art results across all datasets and surpassing previous methods by 9.5%. Notably, SC-CLIP boosts the performance of vanilla CLIP ViT-L/14 by 6.8 times. Furthermore, we discuss our method’s applicability to other vision–language models and tasks for a comprehensive evaluation. Our source code is available at https://github.com/SuleBai/SC-CLIP.
Published: 2025-12-10T18:34:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sule Bai; Yong Liu; Yifei Han; Haoji Zhang; Yansong Tang; Jie Zhou; Jiwen Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639996"&gt;10.1109/tip.2025.3639996&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in pre-trained vision-language models like CLIP, have enabled the task of open-vocabulary segmentation. CLIP demonstrates impressive zero-shot capabilities in various downstream tasks that require holistic image understanding. However, due to the image-level contrastive learning and fully global feature interaction, ViT-based CLIP struggles to capture local details, resulting in poor performance in segmentation tasks. Our analysis of ViT-based CLIP reveals that anomaly tokens emerge during the forward process, attracting disproportionate attention from normal patch tokens and thereby diminishing spatial awareness. To address this issue, we propose Self-Calibrated CLIP (SC-CLIP), a training-free method that calibrates CLIP to generate finer representations while preserving its original generalization ability—without introducing new parameters or relying on additional backbones. Specifically, we mitigate the negative impact of anomaly tokens from two complementary perspectives. First, we explicitly identify the anomaly tokens and replace them based on local context. Second, we reduce their influence on normal tokens by enhancing feature discriminability and attention correlation, leveraging the inherent semantic consistency within CLIP’s mid-level features. In addition, we introduce a two-pass strategy that effectively integrates multi-level features to enrich local details under the training-free setting. Together, these strategies enhance CLIP’s feature representations with improved granularity and semantic coherence. Experimental results demonstrate the effectiveness of SC-CLIP, achieving state-of-the-art results across all datasets and surpassing previous methods by 9.5%. Notably, SC-CLIP boosts the performance of vanilla CLIP ViT-L/14 by 6.8 times. Furthermore, we discuss our method’s applicability to other vision–language models and tasks for a comprehensive evaluation. Our source code is available at https://github.com/SuleBai/SC-CLIP.&lt;/p&gt;</content:encoded></item><item><title>Gradient-Guided Learning Network for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2512.09497v1</link><guid>http://arxiv.org/abs/2512.09497v1</guid><pubDate>Wed, 10 Dec 2025 10:21:08 +0000</pubDate><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Chuang Yu</dc:creator><dc:creator>Zelin Shi</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Yingdi Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/LGRS.2023.3308783</prism:doi><description>Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net
Published: 2025-12-10T10:21:08+00:00
Venue: arXiv
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinmiao Zhao; Chuang Yu; Zelin Shi; Yunpeng Liu; Yingdi Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/LGRS.2023.3308783"&gt;10.1109/LGRS.2023.3308783&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net&lt;/p&gt;</content:encoded></item><item><title>InstructIVF: Degradation-Aware Fusion of Infrared and Visible Images Guided by Diverse Textual Instructions</title><link>https://doi.org/10.1016/j.inffus.2025.104050</link><guid>10.1016/j.inffus.2025.104050</guid><pubDate>Tue, 09 Dec 2025 16:21:25 +0000</pubDate><dc:creator>Wenxia Yin</dc:creator><dc:creator>Xinyi Zeng</dc:creator><dc:creator>Xi Wu</dc:creator><dc:creator>Daoqiang Zhang</dc:creator><dc:creator>Yan Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104050</prism:doi><description>Recently, several pioneering degradation-resistant infrared and visible image fusion methods have been proposed that leverage human-interactive text instructions to generate high-quality fused images under varying degradation conditions. Although these methods have made notable strides, two key challenges remain. First, due to user subjectivity, text instructions conveying the same meaning may vary significantly in form. Thus, handling free-form user instructions rather than fixed-format inputs remains challenging. Second, the complex semantic relationships between visual and linguistic features are yet to be fully explored. To address these issues, we propose InstructIVF, a degradation-aware image fusion framework guided by diverse textual instructions. Specifically, we design a set of structured construction guidelines and leverage the powerful semantic understanding capabilities of LLMs to assist in generating a wide range of text instructions, resulting in an optimized set of 6,000 diverse textual samples. Furthermore, to comprehensively capture valuable vision-language cues, we propose a heterogeneous feature alignment module enabling bidirectional, multi-granularity interactions between visual regions and textual sequences. Extensive qualitative and quantitative experiments demonstrate that our method effectively adapts to diverse user-defined instruction styles and outperforms 11 state-of-the-art methods across diverse degradation scenarios.
Published: 2025-12-09T16:21:25+00:00
Venue: Information Fusion
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxia Yin; Xinyi Zeng; Xi Wu; Daoqiang Zhang; Yan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104050"&gt;10.1016/j.inffus.2025.104050&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, several pioneering degradation-resistant infrared and visible image fusion methods have been proposed that leverage human-interactive text instructions to generate high-quality fused images under varying degradation conditions. Although these methods have made notable strides, two key challenges remain. First, due to user subjectivity, text instructions conveying the same meaning may vary significantly in form. Thus, handling free-form user instructions rather than fixed-format inputs remains challenging. Second, the complex semantic relationships between visual and linguistic features are yet to be fully explored. To address these issues, we propose InstructIVF, a degradation-aware image fusion framework guided by diverse textual instructions. Specifically, we design a set of structured construction guidelines and leverage the powerful semantic understanding capabilities of LLMs to assist in generating a wide range of text instructions, resulting in an optimized set of 6,000 diverse textual samples. Furthermore, to comprehensively capture valuable vision-language cues, we propose a heterogeneous feature alignment module enabling bidirectional, multi-granularity interactions between visual regions and textual sequences. Extensive qualitative and quantitative experiments demonstrate that our method effectively adapts to diverse user-defined instruction styles and outperforms 11 state-of-the-art methods across diverse degradation scenarios.&lt;/p&gt;</content:encoded></item><item><title>OMAP2-YOLO: A Ship Target Detection Algorithm for SAR Images With Multi-Scale Ships and Complex Environments</title><link>https://doi.org/10.1109/taes.2025.3641915</link><guid>10.1109/taes.2025.3641915</guid><pubDate>Wed, 10 Dec 2025 18:34:42 +0000</pubDate><dc:creator>Chuanbiao Qiu</dc:creator><dc:creator>Ying Zhang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3641915</prism:doi><description>Synthetic Aperture Radar (SAR) is pivotal in shipping management, maritime activity monitoring, and military intelligence. However, existing target detection methods often perform poorly in SAR scenarios due to complex background environments, multi-scale target variations, speckle noise in images, and occlusions. In order to tackle these challenges, we present a detection model named OMAP2-YOLO. First, we boost feature extraction efficiency through reparameterization of the backbone network with OREPA,based on the YOLOv8 architecture. Next, we introduce the MultiSEAM module, which integrates various attention mechanisms to mitigate the interference of complex backgrounds and occlusions on target detection. We then design the QFE module to build a more refined feature pyramid. This module enables the model to effectively detect targets at multiple scales and enhances detection performance by utilizing fine-grained features from the P2 layer. Finally, we propose the Focaler_SIoU loss function, which offers improved convergence and enhances sensitivity to small targets. Extensive experiments on the HRSID and SSDD datasets demonstrate the exceptional robustness and reliability of the OMAP2-YOLO model.Compared to baseline methods, the proposed model achieves notable performance improvements on both the HRSID and SSDD datasets. On the HRSID dataset, it improves Detection Rate (DR) by 3%, F1-Score (F1) by 2%, and Average Precision (AP) by 3.1%, while reducing False Alarm Rate (FAR) by 0.2213. On the SSDD dataset, it achieves improvements of 6% in DR, 11% in F1, and 8.5% in AP. These advancements underscore the potential of OMAP2-YOLO for real-time maritime surveillance and management.
Published: 2025-12-10T18:34:42+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuanbiao Qiu; Ying Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3641915"&gt;10.1109/taes.2025.3641915&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is pivotal in shipping management, maritime activity monitoring, and military intelligence. However, existing target detection methods often perform poorly in SAR scenarios due to complex background environments, multi-scale target variations, speckle noise in images, and occlusions. In order to tackle these challenges, we present a detection model named OMAP2-YOLO. First, we boost feature extraction efficiency through reparameterization of the backbone network with OREPA,based on the YOLOv8 architecture. Next, we introduce the MultiSEAM module, which integrates various attention mechanisms to mitigate the interference of complex backgrounds and occlusions on target detection. We then design the QFE module to build a more refined feature pyramid. This module enables the model to effectively detect targets at multiple scales and enhances detection performance by utilizing fine-grained features from the P2 layer. Finally, we propose the Focaler_SIoU loss function, which offers improved convergence and enhances sensitivity to small targets. Extensive experiments on the HRSID and SSDD datasets demonstrate the exceptional robustness and reliability of the OMAP2-YOLO model.Compared to baseline methods, the proposed model achieves notable performance improvements on both the HRSID and SSDD datasets. On the HRSID dataset, it improves Detection Rate (DR) by 3%, F1-Score (F1) by 2%, and Average Precision (AP) by 3.1%, while reducing False Alarm Rate (FAR) by 0.2213. On the SSDD dataset, it achieves improvements of 6% in DR, 11% in F1, and 8.5% in AP. These advancements underscore the potential of OMAP2-YOLO for real-time maritime surveillance and management.&lt;/p&gt;</content:encoded></item><item><title>SpaceFormer: Spatial Position Contextual Semantics Embedding for Multi-View 3D Object Detection</title><link>https://doi.org/10.1109/tits.2025.3639680</link><guid>10.1109/tits.2025.3639680</guid><pubDate>Wed, 10 Dec 2025 18:34:12 +0000</pubDate><dc:creator>Jiaqi Zhao</dc:creator><dc:creator>Huanfeng Hu</dc:creator><dc:creator>Wen-Liang Du</dc:creator><dc:creator>Yong Zhou</dc:creator><dc:creator>Kunyang Sun</dc:creator><dc:creator>Rui Yao</dc:creator><dc:creator>Abdulmotaleb El Saddik</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3639680</prism:doi><description>3D object detection aims to accurately localize and recognize objects in 3D space. It serves as a fundamental task for reliable perception in intelligent transportation systems, enabling the monitoring of diverse traffic participants such as vehicles, pedestrians, cyclists, and public transport. Recently, transformer-based methods have gained significant attention in multi-view 3D object detection due to their strong global reasoning capabilities. However, their limited capacity to model spatial positional information hinders accurate object localization, especially in complex and large-scale scenes. To address this limitation, SpaceFormer is proposed as a novel transformer-based multi-view 3D object detector. Specifically, a Contextual Visual Prompts Learning strategy is proposed to enhance the perception of small and sparse traffic participants by incorporating contextual priors. To further suppress background interference, a Semantics-guided Depth Estimation method is proposed to refine depth representations using high-level semantic information. Furthermore, a Spatial Position Embedding mechanism is proposed to improve the spatial localization capability of the transformer by integrating geometric position and polar spatial embedding. Extensive experiments on the nuScenes benchmark demonstrate that SpaceFormer achieves state-of-the-art performance with 55.5% mAP and 62.9% NDS. These improvements indicate not only methodological advances but also practical benefits for intelligent transportation systems, enhancing safety, reliability, and efficiency in real-world deployments.
Published: 2025-12-10T18:34:12+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Zhao; Huanfeng Hu; Wen-Liang Du; Yong Zhou; Kunyang Sun; Rui Yao; Abdulmotaleb El Saddik&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3639680"&gt;10.1109/tits.2025.3639680&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detection aims to accurately localize and recognize objects in 3D space. It serves as a fundamental task for reliable perception in intelligent transportation systems, enabling the monitoring of diverse traffic participants such as vehicles, pedestrians, cyclists, and public transport. Recently, transformer-based methods have gained significant attention in multi-view 3D object detection due to their strong global reasoning capabilities. However, their limited capacity to model spatial positional information hinders accurate object localization, especially in complex and large-scale scenes. To address this limitation, SpaceFormer is proposed as a novel transformer-based multi-view 3D object detector. Specifically, a Contextual Visual Prompts Learning strategy is proposed to enhance the perception of small and sparse traffic participants by incorporating contextual priors. To further suppress background interference, a Semantics-guided Depth Estimation method is proposed to refine depth representations using high-level semantic information. Furthermore, a Spatial Position Embedding mechanism is proposed to improve the spatial localization capability of the transformer by integrating geometric position and polar spatial embedding. Extensive experiments on the nuScenes benchmark demonstrate that SpaceFormer achieves state-of-the-art performance with 55.5% mAP and 62.9% NDS. These improvements indicate not only methodological advances but also practical benefits for intelligent transportation systems, enhancing safety, reliability, and efficiency in real-world deployments.&lt;/p&gt;</content:encoded></item><item><title>Test-time Correction: An Online 3D Detection System via Visual Prompting</title><link>https://doi.org/10.1109/tpami.2025.3642076</link><guid>10.1109/tpami.2025.3642076</guid><pubDate>Tue, 09 Dec 2025 18:32:53 +0000</pubDate><dc:creator>Hanxue Zhang</dc:creator><dc:creator>Zetong Yang</dc:creator><dc:creator>Yanan Sun</dc:creator><dc:creator>Li Chen</dc:creator><dc:creator>Fei Xia</dc:creator><dc:creator>Fatma Güney</dc:creator><dc:creator>Hongyang Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642076</prism:doi><description>This paper introduces Test-time Correction (TTC), an online 3D detection system designed to rectify test-time errors using various auxiliary feedback, aiming to enhance the safety of deployed autonomous driving systems. Unlike conventional offline 3D detectors that remain fixed during inference, TTC enables immediate online error correction without retraining, allowing autonomous vehicles to adapt to new scenarios and reduce deployment risks. To achieve this, we equip existing 3D detectors with an Online Adapter (OA) module-a prompt-driven query generator for real-time correction. At the core of OA module are visual prompts: image-based descriptions of objects of interest derived from auxiliary feedback such as mismatches with 2D detections, road descriptions, or user clicks. These visual prompts, collected from risky objects during inference, are maintained in a visual prompt buffer to enable continuous correction in future frames. By leveraging this mechanism, TTC consistently detects risky objects, achieving reliable, adaptive, and versatile driving autonomy. Extensive experiments show that TTC significantly improves instant error rectification over frozen 3D detectors, even under limited labels, zero-shot settings, and adverse conditions. We hope this work inspires future research on post-deployment online rectification systems for autonomous driving.
Published: 2025-12-09T18:32:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanxue Zhang; Zetong Yang; Yanan Sun; Li Chen; Fei Xia; Fatma Güney; Hongyang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642076"&gt;10.1109/tpami.2025.3642076&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;This paper introduces Test-time Correction (TTC), an online 3D detection system designed to rectify test-time errors using various auxiliary feedback, aiming to enhance the safety of deployed autonomous driving systems. Unlike conventional offline 3D detectors that remain fixed during inference, TTC enables immediate online error correction without retraining, allowing autonomous vehicles to adapt to new scenarios and reduce deployment risks. To achieve this, we equip existing 3D detectors with an Online Adapter (OA) module-a prompt-driven query generator for real-time correction. At the core of OA module are visual prompts: image-based descriptions of objects of interest derived from auxiliary feedback such as mismatches with 2D detections, road descriptions, or user clicks. These visual prompts, collected from risky objects during inference, are maintained in a visual prompt buffer to enable continuous correction in future frames. By leveraging this mechanism, TTC consistently detects risky objects, achieving reliable, adaptive, and versatile driving autonomy. Extensive experiments show that TTC significantly improves instant error rectification over frozen 3D detectors, even under limited labels, zero-shot settings, and adverse conditions. We hope this work inspires future research on post-deployment online rectification systems for autonomous driving.&lt;/p&gt;</content:encoded></item><item><title>Large Visual Language Models Continual Learning with Dynamic Mixture-of-Experts</title><link>https://doi.org/10.1109/tip.2025.3639925</link><guid>10.1109/tip.2025.3639925</guid><pubDate>Wed, 10 Dec 2025 18:34:53 +0000</pubDate><dc:creator>Yizhou Chen</dc:creator><dc:creator>Xihao Huang</dc:creator><dc:creator>Wei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639925</prism:doi><description>In dynamic and evolving application scenarios, the ability of visual language models to continuously learn from new data while preserving historical knowledge is critically important. Existing continual learning methods for large visual language models (LVLMs) often restrict the number of tasks they can handle, causing performance to decline as tasks continue to increase. In this paper, we propose a novel continual learning framework that adapts to the growing number of tasks, enabling visual language models to handle a dynamic range of open-set tasks while overcoming the catastrophic forgetting problem of learning new tasks at the expense of forgetting old ones.Our method builds on a pre-trained CLIP model and incorporates a dynamic mixture-of-experts (MoE) layer, enabling flexible adaptation to a wide range of open-set tasks. We design an elastic expert weight management strategy to effectively mitigate the catastrophic forgetting problem. Furthermore, we optimize the LoRA experts with adaptive ranks to achieve a balanced trade-off between model complexity and representational capacity. Extensive experiments across diverse settings demonstrate that our proposed method significantly reduces the number of tunable parameters while consistently surpassing state-of-the-art methods in new task learning capability and maintaining performance on historical tasks.
Published: 2025-12-10T18:34:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yizhou Chen; Xihao Huang; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639925"&gt;10.1109/tip.2025.3639925&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;In dynamic and evolving application scenarios, the ability of visual language models to continuously learn from new data while preserving historical knowledge is critically important. Existing continual learning methods for large visual language models (LVLMs) often restrict the number of tasks they can handle, causing performance to decline as tasks continue to increase. In this paper, we propose a novel continual learning framework that adapts to the growing number of tasks, enabling visual language models to handle a dynamic range of open-set tasks while overcoming the catastrophic forgetting problem of learning new tasks at the expense of forgetting old ones.Our method builds on a pre-trained CLIP model and incorporates a dynamic mixture-of-experts (MoE) layer, enabling flexible adaptation to a wide range of open-set tasks. We design an elastic expert weight management strategy to effectively mitigate the catastrophic forgetting problem. Furthermore, we optimize the LoRA experts with adaptive ranks to achieve a balanced trade-off between model complexity and representational capacity. Extensive experiments across diverse settings demonstrate that our proposed method significantly reduces the number of tunable parameters while consistently surpassing state-of-the-art methods in new task learning capability and maintaining performance on historical tasks.&lt;/p&gt;</content:encoded></item><item><title>DINet: Depth-guided and Iterative Refinement Network for Salient Object Detection in Optical Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3641927</link><guid>10.1109/tgrs.2025.3641927</guid><pubDate>Tue, 09 Dec 2025 18:32:55 +0000</pubDate><dc:creator>Xihang Hu</dc:creator><dc:creator>Fuming Sun</dc:creator><dc:creator>Xiaoli Zhang</dc:creator><dc:creator>Chuanmin Jia</dc:creator><dc:creator>Siwei Ma</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3641927</prism:doi><description>Optical remote sensing images (ORSI) feature unique scenes and complex imaging conditions. Specifically, they exhibit substantial variations in object scale, quantity, structure, and distribution. Consequently, salient object detection in ORSI (ORSI-SOD) is pivotal in ORSI content perception and understanding. Additionally, the limitations of the single modality impede the advancement of ORSI-SOD. To tackle these issues, we propose a Depth-guided and Iterative Refinement Network (DINet) for ORSI-SOD. By incorporating depth information as auxiliary cues, we introduce a multi-modal strategy for ORSI-SOD, resulting in improved accuracy in the localization and segmentation of salient objects. To address the variability of salient objects, we design an Aggregation Perception Enhancement (APE) Module. This module integrates complementary cues from cross-modal features using multi-dimensional attention mechanisms. By fostering cross-modal interactions, the APE module effectively preserves both detail and spatial location information. Furthermore, we propose an Iterative Guidance Refinement Decoder to handle boundary uncertainty. The decoder uses initial predictions to guide the decoding phase and iteratively refine results. Simultaneously, it minimizes noise from depth cues, yielding predictions with more accurate boundaries. Experimental comparisons with 22 state-of-the-art methods show that DINet exhibits superior performance while maintaining lightweight (11.98M) and real-time (55FPS) capabilities.
Published: 2025-12-09T18:32:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xihang Hu; Fuming Sun; Xiaoli Zhang; Chuanmin Jia; Siwei Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3641927"&gt;10.1109/tgrs.2025.3641927&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Optical remote sensing images (ORSI) feature unique scenes and complex imaging conditions. Specifically, they exhibit substantial variations in object scale, quantity, structure, and distribution. Consequently, salient object detection in ORSI (ORSI-SOD) is pivotal in ORSI content perception and understanding. Additionally, the limitations of the single modality impede the advancement of ORSI-SOD. To tackle these issues, we propose a Depth-guided and Iterative Refinement Network (DINet) for ORSI-SOD. By incorporating depth information as auxiliary cues, we introduce a multi-modal strategy for ORSI-SOD, resulting in improved accuracy in the localization and segmentation of salient objects. To address the variability of salient objects, we design an Aggregation Perception Enhancement (APE) Module. This module integrates complementary cues from cross-modal features using multi-dimensional attention mechanisms. By fostering cross-modal interactions, the APE module effectively preserves both detail and spatial location information. Furthermore, we propose an Iterative Guidance Refinement Decoder to handle boundary uncertainty. The decoder uses initial predictions to guide the decoding phase and iteratively refine results. Simultaneously, it minimizes noise from depth cues, yielding predictions with more accurate boundaries. Experimental comparisons with 22 state-of-the-art methods show that DINet exhibits superior performance while maintaining lightweight (11.98M) and real-time (55FPS) capabilities.&lt;/p&gt;</content:encoded></item><item><title>MPRANet: Multi-scale perception and reference attention network for lightweight SAR target recognition</title><link>https://doi.org/10.1016/j.neucom.2025.132310</link><guid>10.1016/j.neucom.2025.132310</guid><pubDate>Wed, 10 Dec 2025 08:03:05 +0000</pubDate><dc:creator>Yonggang Qian</dc:creator><dc:creator>Yinghua Wang</dc:creator><dc:creator>Hongwei Liu</dc:creator><dc:creator>Zelong Wang</dc:creator><dc:creator>Feipeng Yu</dc:creator><dc:creator>Chunhui Qu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132310</prism:doi><description>Deep learning methods have been widely used in Synthetic Aperture Radar Automatic Target Recognition (SAR ATR). However, challenges remain due to limited SAR data and computational constraints on mobile devices, which hinder model training and deployment. In this paper, we propose a Multi-scale Perception and Reference Attention Network (MPRANet) for lightweight SAR ATR, which is a hybrid structure combining convolutional networks and transformers, built upon the ShuffleNetV2 network. Specifically, MPRANet introduces two key improvements compared to the CNN-based ShuffleNetV2. Firstly, we replace the depthwise convolutions (DWConv) in the downsampling and basic units of ShuffleNetV2 with the Multi-scale Parameter-Shared Convolution (MPConv) module. MPConv enables the extraction of multi-scale features of SAR targets with almost no additional parameters, thereby enhancing the network’s feature extraction capabilities. Secondly, we propose a lightweight Reference Attention Transformer (RAformer) to capture global information, addressing the issue of insufficient channel feature interaction in ShuffleNetV2. In RAformer, a Local Linear Mapping Unit (LMU) is designed to perform linear mappings, reducing the introduction of redundant features while ensuring its lightweight and efficient nature. RAformer contains two modules: the Reference Vector Attention (RVA) module, which efficiently models attention relationships, and the Lightweight Feedforward Neural Network (LW-FFN) module, which enhances the network’s ability to capture nonlinear representations. We evaluated the performance of MPRANet using publicly available SAR datasets, including the MSTAR dataset, OpenSARShip dataset, and SAR-AIRcraft-1.0 dataset. The experimental results demonstrate that MPRANet consistently achieves superior recognition performance compared to other lightweight networks of similar complexity.
Published: 2025-12-10T08:03:05+00:00
Venue: Neurocomputing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yonggang Qian; Yinghua Wang; Hongwei Liu; Zelong Wang; Feipeng Yu; Chunhui Qu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132310"&gt;10.1016/j.neucom.2025.132310&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning methods have been widely used in Synthetic Aperture Radar Automatic Target Recognition (SAR ATR). However, challenges remain due to limited SAR data and computational constraints on mobile devices, which hinder model training and deployment. In this paper, we propose a Multi-scale Perception and Reference Attention Network (MPRANet) for lightweight SAR ATR, which is a hybrid structure combining convolutional networks and transformers, built upon the ShuffleNetV2 network. Specifically, MPRANet introduces two key improvements compared to the CNN-based ShuffleNetV2. Firstly, we replace the depthwise convolutions (DWConv) in the downsampling and basic units of ShuffleNetV2 with the Multi-scale Parameter-Shared Convolution (MPConv) module. MPConv enables the extraction of multi-scale features of SAR targets with almost no additional parameters, thereby enhancing the network’s feature extraction capabilities. Secondly, we propose a lightweight Reference Attention Transformer (RAformer) to capture global information, addressing the issue of insufficient channel feature interaction in ShuffleNetV2. In RAformer, a Local Linear Mapping Unit (LMU) is designed to perform linear mappings, reducing the introduction of redundant features while ensuring its lightweight and efficient nature. RAformer contains two modules: the Reference Vector Attention (RVA) module, which efficiently models attention relationships, and the Lightweight Feedforward Neural Network (LW-FFN) module, which enhances the network’s ability to capture nonlinear representations. We evaluated the performance of MPRANet using publicly available SAR datasets, including the MSTAR dataset, OpenSARShip dataset, and SAR-AIRcraft-1.0 dataset. The experimental results demonstrate that MPRANet consistently achieves superior recognition performance compared to other lightweight networks of similar complexity.&lt;/p&gt;</content:encoded></item><item><title>Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement</title><link>https://arxiv.org/abs/2512.07611v1</link><guid>http://arxiv.org/abs/2512.07611v1</guid><pubDate>Mon, 08 Dec 2025 14:58:19 +0000</pubDate><dc:creator>Yongsheng Lian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.
Published: 2025-12-08T14:58:19+00:00
Venue: arXiv
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongsheng Lian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.&lt;/p&gt;</content:encoded></item><item><title>A lightweight network for foreign object detection in railway overhead contact lines based on optical imagery</title><link>https://doi.org/10.1016/j.eswa.2025.130718</link><guid>10.1016/j.eswa.2025.130718</guid><pubDate>Wed, 10 Dec 2025 08:05:55 +0000</pubDate><dc:creator>Fengqiang Xu</dc:creator><dc:creator>Li Diao</dc:creator><dc:creator>Haolin Yang</dc:creator><dc:creator>Renxuan Xiong</dc:creator><dc:creator>Jinhao Cao</dc:creator><dc:creator>Yanjuan Wang</dc:creator><dc:creator>Fengqi Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130718</prism:doi><description>Foreign object detection in railway overhead contact lines (ROCL) is vital for train operation safety. However, traditional object detection models often struggle to balance accuracy and efficiency, particularly under complex backgrounds and varying object shapes. To overcome these challenges, we propose a lightweight object detection model, LWOD-DETR, tailored for ROCL foreign object detection. Firstly, a reparameterizable cross-stage partial (RepCSP) module is designed to enhance the model’s feature extraction capability with reduced parameter count and computational cost. Secondly, a cascaded group attention (CGA) mechanism is employed into the encoder to improve the model’s ability to capture and utilize essential image features while suppressing redundant information. Thirdly, an efficient multi-scale feature fusion (EMSFF) module is proposed to improve the detection accuracy of foreign objects across different scales and simultaneously reduce the model’s parameter count. In addition, a gating mechanism based on the HardSigmoid activation function is introduced to optimize the upsampling and downsampling modules. This mechanism enables the model to selectively activate or suppress different feature channels through learning, thereby enhancing the model’s nonlinear representation capability and improving the efficiency of multi-scale feature fusion. Experimental results on two ROCL foreign body datasets demonstrate that our proposed method outperforms the state-of-the-art methods in terms of AP metric (RCPSSFO: 0.607, RailFOD23: 0.841) while maintaining low parameter and GFLOPs (13.57M, 45.8G).
Published: 2025-12-10T08:05:55+00:00
Venue: Expert Systems with Applications
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fengqiang Xu; Li Diao; Haolin Yang; Renxuan Xiong; Jinhao Cao; Yanjuan Wang; Fengqi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130718"&gt;10.1016/j.eswa.2025.130718&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Foreign object detection in railway overhead contact lines (ROCL) is vital for train operation safety. However, traditional object detection models often struggle to balance accuracy and efficiency, particularly under complex backgrounds and varying object shapes. To overcome these challenges, we propose a lightweight object detection model, LWOD-DETR, tailored for ROCL foreign object detection. Firstly, a reparameterizable cross-stage partial (RepCSP) module is designed to enhance the model’s feature extraction capability with reduced parameter count and computational cost. Secondly, a cascaded group attention (CGA) mechanism is employed into the encoder to improve the model’s ability to capture and utilize essential image features while suppressing redundant information. Thirdly, an efficient multi-scale feature fusion (EMSFF) module is proposed to improve the detection accuracy of foreign objects across different scales and simultaneously reduce the model’s parameter count. In addition, a gating mechanism based on the HardSigmoid activation function is introduced to optimize the upsampling and downsampling modules. This mechanism enables the model to selectively activate or suppress different feature channels through learning, thereby enhancing the model’s nonlinear representation capability and improving the efficiency of multi-scale feature fusion. Experimental results on two ROCL foreign body datasets demonstrate that our proposed method outperforms the state-of-the-art methods in terms of AP metric (RCPSSFO: 0.607, RailFOD23: 0.841) while maintaining low parameter and GFLOPs (13.57M, 45.8G).&lt;/p&gt;</content:encoded></item><item><title>Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving</title><link>https://arxiv.org/abs/2512.09296v1</link><guid>http://arxiv.org/abs/2512.09296v1</guid><pubDate>Wed, 10 Dec 2025 03:46:57 +0000</pubDate><dc:creator>Songhan Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.
Published: 2025-12-10T03:46:57+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songhan Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model&amp;#x27;s contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.&lt;/p&gt;</content:encoded></item><item><title>Hands-on Evaluation of Visual Transformers for Object Recognition and Detection</title><link>https://arxiv.org/abs/2512.09579v1</link><guid>http://arxiv.org/abs/2512.09579v1</guid><pubDate>Wed, 10 Dec 2025 12:15:48 +0000</pubDate><dc:creator>Dimitrios N. Vlachogiannis</dc:creator><dc:creator>Dimitrios A. Koutsomitropoulos</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.
Published: 2025-12-10T12:15:48+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dimitrios N. Vlachogiannis; Dimitrios A. Koutsomitropoulos&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.&lt;/p&gt;</content:encoded></item><item><title>LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery</title><link>https://arxiv.org/abs/2512.09700v1</link><guid>http://arxiv.org/abs/2512.09700v1</guid><pubDate>Wed, 10 Dec 2025 14:48:58 +0000</pubDate><dc:creator>Seon-Hoon Kim</dc:creator><dc:creator>Hyeji Sim</dc:creator><dc:creator>Youeyun Jung</dc:creator><dc:creator>Ok-Chul Jung</dc:creator><dc:creator>Yerin Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.
Published: 2025-12-10T14:48:58+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seon-Hoon Kim; Hyeji Sim; Youeyun Jung; Ok-Chul Jung; Yerin Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset</title><link>https://doi.org/10.1109/tip.2025.3639998</link><guid>10.1109/tip.2025.3639998</guid><pubDate>Tue, 09 Dec 2025 18:35:41 +0000</pubDate><dc:creator>Zhiyuan You</dc:creator><dc:creator>Jinjin Gu</dc:creator><dc:creator>Xin Cai</dc:creator><dc:creator>Zheyuan Li</dc:creator><dc:creator>Kaiwen Zhu</dc:creator><dc:creator>Chao Dong</dc:creator><dc:creator>Tianfan Xue</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639998</prism:doi><description>With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Enhanced Descriptive image Quality Assessment (EDQA). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named EDQA-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that EDQA significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released.
Published: 2025-12-09T18:35:41+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiyuan You; Jinjin Gu; Xin Cai; Zheyuan Li; Kaiwen Zhu; Chao Dong; Tianfan Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639998"&gt;10.1109/tip.2025.3639998&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce Enhanced Descriptive image Quality Assessment (EDQA). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named EDQA-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that EDQA significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Datasets and codes will be released.&lt;/p&gt;</content:encoded></item><item><title>DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection</title><link>https://arxiv.org/abs/2512.07078v1</link><guid>http://arxiv.org/abs/2512.07078v1</guid><pubDate>Mon, 08 Dec 2025 01:25:10 +0000</pubDate><dc:creator>Bo Gao</dc:creator><dc:creator>Jingcheng Tong</dc:creator><dc:creator>Xingsheng Chen</dc:creator><dc:creator>Han Yu</dc:creator><dc:creator>Zichen Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.
Published: 2025-12-08T01:25:10+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Gao; Jingcheng Tong; Xingsheng Chen; Han Yu; Zichen Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.&lt;/p&gt;</content:encoded></item><item><title>Time Series Domain Adaptation Via Latent Invariant Causal Mechanism</title><link>https://doi.org/10.1109/tpami.2025.3642245</link><guid>10.1109/tpami.2025.3642245</guid><pubDate>Wed, 10 Dec 2025 18:32:22 +0000</pubDate><dc:creator>Ruichu Cai</dc:creator><dc:creator>Junxian Huang</dc:creator><dc:creator>Zhenhui Yang</dc:creator><dc:creator>Zijian Li</dc:creator><dc:creator>Emadeldeen Eldele</dc:creator><dc:creator>Min Wu</dc:creator><dc:creator>Fuchun Sun</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642245</prism:doi><description>Time series domain adaptation aims to transfer the complex temporal dependence from the labeled source domain to the unlabeled target domain. Recent advances leverage the stable causal mechanism over observed variables to model the domain-invariant temporal dependence. However, modeling precise causal structures in high-dimensional data, such as videos, remains challenging. Additionally, direct causal edges may not exist among observed variables (e.g., pixels). These limitations hinder the applicability of existing approaches to real-world scenarios. To address these challenges, we find that the high-dimension time series data are generated from the low-dimension latent variables, which motivates us to model the causal mechanisms of the temporal latent process. Based on this intuition, we propose a latent causal mechanism identification framework that guarantees the uniqueness of the reconstructed latent causal structures. Specifically, we first identify latent variables by utilizing sufficient changes in historical information. Moreover, by enforcing the sparsity of the relationships of latent variables, we can achieve identifiable latent causal structures. Built on the theoretical results, we develop the Latent Causality Alignment (LCA) model that leverages variational inference, which incorporates an intra-domain latent sparsity constraint for latent structure reconstruction and an inter-domain latent sparsity constraint for domain-invariant structure reconstruction. Experiment results on eight benchmarks show a general improvement in the domain-adaptive time series classification and forecasting tasks, highlighting the effectiveness of our method in real-world scenarios. Codes are available at https://github.com/DMIRLAB-Group/LCA.
Published: 2025-12-10T18:32:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruichu Cai; Junxian Huang; Zhenhui Yang; Zijian Li; Emadeldeen Eldele; Min Wu; Fuchun Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642245"&gt;10.1109/tpami.2025.3642245&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Time series domain adaptation aims to transfer the complex temporal dependence from the labeled source domain to the unlabeled target domain. Recent advances leverage the stable causal mechanism over observed variables to model the domain-invariant temporal dependence. However, modeling precise causal structures in high-dimensional data, such as videos, remains challenging. Additionally, direct causal edges may not exist among observed variables (e.g., pixels). These limitations hinder the applicability of existing approaches to real-world scenarios. To address these challenges, we find that the high-dimension time series data are generated from the low-dimension latent variables, which motivates us to model the causal mechanisms of the temporal latent process. Based on this intuition, we propose a latent causal mechanism identification framework that guarantees the uniqueness of the reconstructed latent causal structures. Specifically, we first identify latent variables by utilizing sufficient changes in historical information. Moreover, by enforcing the sparsity of the relationships of latent variables, we can achieve identifiable latent causal structures. Built on the theoretical results, we develop the Latent Causality Alignment (LCA) model that leverages variational inference, which incorporates an intra-domain latent sparsity constraint for latent structure reconstruction and an inter-domain latent sparsity constraint for domain-invariant structure reconstruction. Experiment results on eight benchmarks show a general improvement in the domain-adaptive time series classification and forecasting tasks, highlighting the effectiveness of our method in real-world scenarios. Codes are available at https://github.com/DMIRLAB-Group/LCA.&lt;/p&gt;</content:encoded></item><item><title>Beyond Pillars: Advancing 3D Object Detection with Salient Voxel Enhancement of LiDAR-4D Radar Fusion</title><link>https://doi.org/10.1016/j.patcog.2025.112841</link><guid>10.1016/j.patcog.2025.112841</guid><pubDate>Tue, 09 Dec 2025 00:21:13 +0000</pubDate><dc:creator>Pengfei Yang</dc:creator><dc:creator>Feng Wu</dc:creator><dc:creator>Minyang Liu</dc:creator><dc:creator>Ting Zhong</dc:creator><dc:creator>Fan Zhou</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112841</prism:doi><description>The fusion of LiDAR and 4D radar has emerged as a promising solution for robust and accurate 3D object detection in complex and adverse conditions. Existing methods typically rely on pillar-based representations, which, although computationally efficient, fail to provide fine-grained structural details necessary for precise object localization and recognition. In contrast, voxel-based representations offer richer spatial information but face challenges such as background noise and data quality disparity. To address these limitations, we propose SVEFusion, a voxel-based 3D object detection framework that integrates LiDAR and 4D radar data using a salient voxel enhancement mechanism. Our method introduces an adaptive feature alignment module and a novel spatial neighborhood attention module for efficient early-stage multi-modal voxel feature integration. Furthermore, we design a salient voxel enhancement mechanism that assigns higher weights to foreground voxels using a multi-scale weight prediction strategy, progressively refining weight accuracy with supervision loss. Experimental results demonstrate that SVEFusion significantly outperforms state-of-the-art methods, establishing a new benchmark in multi-modal 3D object detection. The source code and network weighting for reproducibility are available at https://github.com/icdm-adteam/SVEFusion .
Published: 2025-12-09T00:21:13+00:00
Venue: Pattern Recognition
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Yang; Feng Wu; Minyang Liu; Ting Zhong; Fan Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112841"&gt;10.1016/j.patcog.2025.112841&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;The fusion of LiDAR and 4D radar has emerged as a promising solution for robust and accurate 3D object detection in complex and adverse conditions. Existing methods typically rely on pillar-based representations, which, although computationally efficient, fail to provide fine-grained structural details necessary for precise object localization and recognition. In contrast, voxel-based representations offer richer spatial information but face challenges such as background noise and data quality disparity. To address these limitations, we propose SVEFusion, a voxel-based 3D object detection framework that integrates LiDAR and 4D radar data using a salient voxel enhancement mechanism. Our method introduces an adaptive feature alignment module and a novel spatial neighborhood attention module for efficient early-stage multi-modal voxel feature integration. Furthermore, we design a salient voxel enhancement mechanism that assigns higher weights to foreground voxels using a multi-scale weight prediction strategy, progressively refining weight accuracy with supervision loss. Experimental results demonstrate that SVEFusion significantly outperforms state-of-the-art methods, establishing a new benchmark in multi-modal 3D object detection. The source code and network weighting for reproducibility are available at https://github.com/icdm-adteam/SVEFusion .&lt;/p&gt;</content:encoded></item><item><title>TrajDiff: Trajectory Prediction with Diffusion Probabilistic Models</title><link>https://doi.org/10.1109/tip.2025.3640001</link><guid>10.1109/tip.2025.3640001</guid><pubDate>Wed, 10 Dec 2025 18:34:53 +0000</pubDate><dc:creator>Changzhi Yang</dc:creator><dc:creator>Huihui Pan</dc:creator><dc:creator>Jue Wang</dc:creator><dc:creator>Yuanduo Hong</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3640001</prism:doi><description>Diffusion probabilistic models (DPMs) have recently achieved brilliant achievements in computer vision. Inspired by the success of DPMs, we present TrajDiff, a model based on conditional diffusion probabilistic models for agent future trajectory prediction, which speculates the agent future states through a series of stochastic iterative denoising processes. Specifically, we map the trajectory prediction task into the latent heatmap space, translating hard keypoint prediction into soft cluster center learning. The core architecture is a U-shaped encoder-decoder network (U-Net) that is trained with a denoising objective. During inference, conditioned on the observed past trajectory heatmaps, random pure Gaussian noise is initialized to drive the reverse sampling process. The U-Net iteratively removes various levels of Gaussian noise from initialized images, resembling Langevin dynamics, and generates multi-modal predicted future trajectory heatmaps. Furthermore, we introduce a novel residual block with a mutual attention mechanism that can elegantly consider the interactions between the agent and the surrounding environment at multiple scales, assisting in generating physically and socially acceptable trajectories. We verify TrajDiff on the Stanford Drone Dataset and the ETH and UCY Datasets. The experimental results show that TrajDiff outperforms previous state-of-the-art methods with considerable accuracy gains, while significantly reducing computational requirements.
Published: 2025-12-10T18:34:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changzhi Yang; Huihui Pan; Jue Wang; Yuanduo Hong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3640001"&gt;10.1109/tip.2025.3640001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion probabilistic models (DPMs) have recently achieved brilliant achievements in computer vision. Inspired by the success of DPMs, we present TrajDiff, a model based on conditional diffusion probabilistic models for agent future trajectory prediction, which speculates the agent future states through a series of stochastic iterative denoising processes. Specifically, we map the trajectory prediction task into the latent heatmap space, translating hard keypoint prediction into soft cluster center learning. The core architecture is a U-shaped encoder-decoder network (U-Net) that is trained with a denoising objective. During inference, conditioned on the observed past trajectory heatmaps, random pure Gaussian noise is initialized to drive the reverse sampling process. The U-Net iteratively removes various levels of Gaussian noise from initialized images, resembling Langevin dynamics, and generates multi-modal predicted future trajectory heatmaps. Furthermore, we introduce a novel residual block with a mutual attention mechanism that can elegantly consider the interactions between the agent and the surrounding environment at multiple scales, assisting in generating physically and socially acceptable trajectories. We verify TrajDiff on the Stanford Drone Dataset and the ETH and UCY Datasets. The experimental results show that TrajDiff outperforms previous state-of-the-art methods with considerable accuracy gains, while significantly reducing computational requirements.&lt;/p&gt;</content:encoded></item><item><title>Multi-level Alignment Network for Unsupervised Domain Adaptive Multi-modality Object Re-identification</title><link>https://doi.org/10.1016/j.knosys.2025.115015</link><guid>10.1016/j.knosys.2025.115015</guid><pubDate>Tue, 09 Dec 2025 07:42:51 +0000</pubDate><dc:creator>Yusong Sheng</dc:creator><dc:creator>Yuhe Ding</dc:creator><dc:creator>Aihua Zheng</dc:creator><dc:creator>Ziqi Liu</dc:creator><dc:creator>Zi Wang</dc:creator><dc:creator>Jin Tang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115015</prism:doi><description>Existing multi-modality object re-identification methods demonstrate robust performance in complex environments, but this is predominantly contingent upon the test and training data sharing an identical distribution. However, performance degrades significantly when applied to the real world (target domains) with distribution differences from the training data (source domain). Single-modality domain adaptation methods that do not account for multi-modality domain gaps and complementary modality information do not achieve satisfactory performance. To alleviate this, we first introduce the task of unsupervised domain adaptation multi-modality object ReID, aiming to address the challenge of distribution shift in multi-modality scenarios and its impact on model performance. We further propose a novel Multi-level Alignment Network (MAN), which performs alignment strategies at the pseudo-label level, domain level, and modality level by leveraging multi-modality information consistency, multi-modality distribution discrepancy, and multi-modality information diversity. Specifically, Consistency-driven Pseudo-label Alignment (CPA) aims to mitigate the effects of pseudo-label noise from clustering by aligning pseudo-labels and filtering reliable samples based on consistency scores. Prototype-guided Domain Distribution Alignment (PDA) narrows the domain gap between the source and target domains by minimizing the distribution distance between the prototype of one domain and the instances of another domain. Margin-preserved modality distribution alignment (MMA) aligns modality distributions within the same domain by keeping the distribution of instances to a marginal distance from the prototype distribution and preserves modality diversity and complementary information. Experiments conducted on vehicle and person datasets WMVeID863, RGBNT100, RGBNT201, and Market1501-MM validate the effectiveness of the proposed method.
Published: 2025-12-09T07:42:51+00:00
Venue: Knowledge-Based Systems
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yusong Sheng; Yuhe Ding; Aihua Zheng; Ziqi Liu; Zi Wang; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115015"&gt;10.1016/j.knosys.2025.115015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Existing multi-modality object re-identification methods demonstrate robust performance in complex environments, but this is predominantly contingent upon the test and training data sharing an identical distribution. However, performance degrades significantly when applied to the real world (target domains) with distribution differences from the training data (source domain). Single-modality domain adaptation methods that do not account for multi-modality domain gaps and complementary modality information do not achieve satisfactory performance. To alleviate this, we first introduce the task of unsupervised domain adaptation multi-modality object ReID, aiming to address the challenge of distribution shift in multi-modality scenarios and its impact on model performance. We further propose a novel Multi-level Alignment Network (MAN), which performs alignment strategies at the pseudo-label level, domain level, and modality level by leveraging multi-modality information consistency, multi-modality distribution discrepancy, and multi-modality information diversity. Specifically, Consistency-driven Pseudo-label Alignment (CPA) aims to mitigate the effects of pseudo-label noise from clustering by aligning pseudo-labels and filtering reliable samples based on consistency scores. Prototype-guided Domain Distribution Alignment (PDA) narrows the domain gap between the source and target domains by minimizing the distribution distance between the prototype of one domain and the instances of another domain. Margin-preserved modality distribution alignment (MMA) aligns modality distributions within the same domain by keeping the distribution of instances to a marginal distance from the prototype distribution and preserves modality diversity and complementary information. Experiments conducted on vehicle and person datasets WMVeID863, RGBNT100, RGBNT201, and Market1501-MM validate the effectiveness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Space-time Gaussian Surfels for High-Fidelity Dynamic Objects Segmentation and Representation</title><link>https://doi.org/10.1109/tcsvt.2025.3642694</link><guid>10.1109/tcsvt.2025.3642694</guid><pubDate>Wed, 10 Dec 2025 18:34:28 +0000</pubDate><dc:creator>Xiaoyun Zheng</dc:creator><dc:creator>Xufeng Li</dc:creator><dc:creator>Liwei Liao</dc:creator><dc:creator>Feng Gao</dc:creator><dc:creator>Shiqi Wang</dc:creator><dc:creator>Ronggang Wang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642694</prism:doi><description>We introduce ST-ObjGS, a method using Space-time Gaussian surfels for accurate object segmentation within 4D representations. Our approach addresses the limitations of current Gaussian-based methods, which primarily focus on static 3D scene understanding and struggle with geometrically accurate object segmentation in complex dynamic scenes. To ensure robust object-level segmentation, we first integrate Grounded SAM 2, which enables text prompt-based object selection and tracking. We then learn a set of Gaussian surfels for object geometry representation and employ a marginal 1D Gaussian for dynamic modeling at each timestamp. To improve geometric quality when modeling surfaces, we use depth and surface normal for geometric regularization. Furthermore, to address continuity and flickering issues in complex scenes, we implement dynamic-aware regularization to maintain temporal consistency. This approach allows us to capture object motion and morphing over time while maintaining spatial coherence. To the best of our knowledge, ST-ObjGS is the first self-supervised approach using Space-time Gaussian surfels for consistent segmentation of dynamic 3D objects in real-world scenes. Extensive experiments on standard benchmarks including PKU-DyMVHumans, Plenoptic Video, Google Immersive, and CMU Panoptic datasets demonstrate that ST-ObjGS produces more precise object masks than its Gaussian-based counterparts and significantly outperforms supervised single-view baselines.
Published: 2025-12-10T18:34:28+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyun Zheng; Xufeng Li; Liwei Liao; Feng Gao; Shiqi Wang; Ronggang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642694"&gt;10.1109/tcsvt.2025.3642694&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce ST-ObjGS, a method using Space-time Gaussian surfels for accurate object segmentation within 4D representations. Our approach addresses the limitations of current Gaussian-based methods, which primarily focus on static 3D scene understanding and struggle with geometrically accurate object segmentation in complex dynamic scenes. To ensure robust object-level segmentation, we first integrate Grounded SAM 2, which enables text prompt-based object selection and tracking. We then learn a set of Gaussian surfels for object geometry representation and employ a marginal 1D Gaussian for dynamic modeling at each timestamp. To improve geometric quality when modeling surfaces, we use depth and surface normal for geometric regularization. Furthermore, to address continuity and flickering issues in complex scenes, we implement dynamic-aware regularization to maintain temporal consistency. This approach allows us to capture object motion and morphing over time while maintaining spatial coherence. To the best of our knowledge, ST-ObjGS is the first self-supervised approach using Space-time Gaussian surfels for consistent segmentation of dynamic 3D objects in real-world scenes. Extensive experiments on standard benchmarks including PKU-DyMVHumans, Plenoptic Video, Google Immersive, and CMU Panoptic datasets demonstrate that ST-ObjGS produces more precise object masks than its Gaussian-based counterparts and significantly outperforms supervised single-view baselines.&lt;/p&gt;</content:encoded></item><item><title>rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection</title><link>https://arxiv.org/abs/2512.08300v1</link><guid>http://arxiv.org/abs/2512.08300v1</guid><pubDate>Tue, 09 Dec 2025 06:55:39 +0000</pubDate><dc:creator>Sijia Chen</dc:creator><dc:creator>Baochun Li</dc:creator><dc:creator>Di Niu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.
Published: 2025-12-09T06:55:39+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sijia Chen; Baochun Li; Di Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha&amp;#x27;&amp;#x27; moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM&amp;#x27;s CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.&lt;/p&gt;</content:encoded></item><item><title>MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images</title><link>https://arxiv.org/abs/2512.09489v1</link><guid>http://arxiv.org/abs/2512.09489v1</guid><pubDate>Wed, 10 Dec 2025 10:07:06 +0000</pubDate><dc:creator>Shuaihao Han</dc:creator><dc:creator>Tingfa Xu</dc:creator><dc:creator>Peifu Liu</dc:creator><dc:creator>Jianan Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.
Published: 2025-12-10T10:07:06+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuaihao Han; Tingfa Xu; Peifu Liu; Jianan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.&lt;/p&gt;</content:encoded></item></channel></rss>