<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 07 Dec 2025 02:48:41 +0000</lastBuildDate><item><title>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</title><link>https://doi.org/10.1109/tpami.2025.3640589</link><guid>10.1109/tpami.2025.3640589</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Xiang Xu</dc:creator><dc:creator>Lingdong Kong</dc:creator><dc:creator>Hui Shuai</dc:creator><dc:creator>Wenwei Zhang</dc:creator><dc:creator>Liang Pan</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Ziwei Liu</dc:creator><dc:creator>Qingshan Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640589</prism:doi><description>LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.870 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Xu; Lingdong Kong; Hui Shuai; Wenwei Zhang; Liang Pan; Kai Chen; Ziwei Liu; Qingshan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640589"&gt;10.1109/tpami.2025.3640589&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.870 (must_read)&lt;/p&gt;
&lt;p&gt;LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.&lt;/p&gt;</content:encoded></item><item><title>AttBEV: Enhancing Multi-Modal 3D Object Detection with CBAM Attention in BEVFusion for Autonomous Driving</title><link>https://doi.org/10.1109/lra.2025.3641130</link><guid>10.1109/lra.2025.3641130</guid><pubDate>Fri, 05 Dec 2025 18:40:34 +0000</pubDate><dc:creator>Na Zhang</dc:creator><dc:creator>Edmundo Guerra</dc:creator><dc:creator>Antoni Grau</dc:creator><prism:publicationName>IEEE Robotics and Automation Letters</prism:publicationName><prism:doi>10.1109/lra.2025.3641130</prism:doi><description>Multimodal fusion has an important research value in environmental perception for autonomous driving. Among them, BEVFusion has become one of the mainstream framework for LiDAR camera fusion by unifying multimodal features in the bird's-eye view (BEV) space. However, its performance is limited by inefficient cross-modal interaction and information loss during BEV projection, especially for dynamic objects and edge cases. To address these limitations, we propose AttBEV, an advanced fusion architecture that introduces a CBAM at the feature fusion layer: a lightweight attention mechanism that improves the model's ability to capture key information through dynamic feature calibration of channel and spatial dimensions. Extensive experiments on the nuScenes dataset demonstrate that AttBEV achieves superior performance compared to BEVFusion on most evaluation metrics. NDS reaches 0.6795, which is 2.63% higher than BEVFusion's 0.6532, and mAP reaches 0.6426, which is 1.79% higher than BEVFusion's 0.6247. In general, AttBEV outperforms existing methods in both model accuracy and generalization ability and significantly improves the performance of 3D object detection in autonomous driving scenarios.
Published: 2025-12-05T18:40:34+00:00
Venue: IEEE Robotics and Automation Letters
Score: 0.833 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Na Zhang; Edmundo Guerra; Antoni Grau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Robotics and Automation Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lra.2025.3641130"&gt;10.1109/lra.2025.3641130&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.833 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal fusion has an important research value in environmental perception for autonomous driving. Among them, BEVFusion has become one of the mainstream framework for LiDAR camera fusion by unifying multimodal features in the bird&amp;#x27;s-eye view (BEV) space. However, its performance is limited by inefficient cross-modal interaction and information loss during BEV projection, especially for dynamic objects and edge cases. To address these limitations, we propose AttBEV, an advanced fusion architecture that introduces a CBAM at the feature fusion layer: a lightweight attention mechanism that improves the model&amp;#x27;s ability to capture key information through dynamic feature calibration of channel and spatial dimensions. Extensive experiments on the nuScenes dataset demonstrate that AttBEV achieves superior performance compared to BEVFusion on most evaluation metrics. NDS reaches 0.6795, which is 2.63% higher than BEVFusion&amp;#x27;s 0.6532, and mAP reaches 0.6426, which is 1.79% higher than BEVFusion&amp;#x27;s 0.6247. In general, AttBEV outperforms existing methods in both model accuracy and generalization ability and significantly improves the performance of 3D object detection in autonomous driving scenarios.&lt;/p&gt;</content:encoded></item><item><title>PreciseVideo: A Dual-Process Framework for Zero-Shot Text-to-Video Generation with Quantitative Content Control</title><link>https://doi.org/10.1016/j.inffus.2025.104030</link><guid>10.1016/j.inffus.2025.104030</guid><pubDate>Sat, 06 Dec 2025 00:36:18 +0000</pubDate><dc:creator>Lizhi Dang</dc:creator><dc:creator>Ting Liang</dc:creator><dc:creator>Huixin Zhang</dc:creator><dc:creator>Ruihao Zhang</dc:creator><dc:creator>Yingping Hong</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104030</prism:doi><description>Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .
Published: 2025-12-06T00:36:18+00:00
Venue: Information Fusion
Score: 0.832 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lizhi Dang; Ting Liang; Huixin Zhang; Ruihao Zhang; Yingping Hong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104030"&gt;10.1016/j.inffus.2025.104030&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.832 (must_read)&lt;/p&gt;
&lt;p&gt;Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .&lt;/p&gt;</content:encoded></item><item><title>Harnessing Lightweight Transformer With Contextual Synergic Enhancement for Efficient 3D Medical Image Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3640233</link><guid>10.1109/tpami.2025.3640233</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Xinyu Liu</dc:creator><dc:creator>Zhen Chen</dc:creator><dc:creator>Wuyang Li</dc:creator><dc:creator>Chenxin Li</dc:creator><dc:creator>Yixuan Yuan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640233</prism:doi><description>Transformers have shown remarkable performance in 3D medical image segmentation, but their high computational requirements and need for large amounts of labeled data limit their applicability. To address these challenges, we consider two crucial aspects: model efficiency and data efficiency. Specifically, we propose Light-UNETR, a lightweight transformer designed to achieve model efficiency. Light-UNETR features a Lightweight Dimension Reductive Attention (LIDR) module, which reduces spatial and channel dimensions while capturing both global and local features via multi-branch attention. Additionally, we introduce a Compact Gated Linear Unit (CGLU) to selectively control channel interaction with minimal parameters. Furthermore, we introduce a Contextual Synergic Enhancement (CSE) learning strategy, which aims to boost the data efficiency of Transformers. It first leverages the extrinsic contextual information to support the learning of unlabeled data with Attention-Guided Replacement, then applies Spatial Masking Consistency that utilizes intrinsic contextual information to enhance the spatial context reasoning for unlabeled data. Extensive experiments on various benchmarks demonstrate the superiority of our approach in both performance and efficiency. For example, with only 10% labeled data on the Left Atrial Segmentation dataset, our method surpasses BCP by 1.43% Jaccard while drastically reducing the FLOPs by 90.8% and parameters by 85.8%. Code is released at https://github.com/CUHK-AIM-Group/Light-UNETR.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Liu; Zhen Chen; Wuyang Li; Chenxin Li; Yixuan Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640233"&gt;10.1109/tpami.2025.3640233&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have shown remarkable performance in 3D medical image segmentation, but their high computational requirements and need for large amounts of labeled data limit their applicability. To address these challenges, we consider two crucial aspects: model efficiency and data efficiency. Specifically, we propose Light-UNETR, a lightweight transformer designed to achieve model efficiency. Light-UNETR features a Lightweight Dimension Reductive Attention (LIDR) module, which reduces spatial and channel dimensions while capturing both global and local features via multi-branch attention. Additionally, we introduce a Compact Gated Linear Unit (CGLU) to selectively control channel interaction with minimal parameters. Furthermore, we introduce a Contextual Synergic Enhancement (CSE) learning strategy, which aims to boost the data efficiency of Transformers. It first leverages the extrinsic contextual information to support the learning of unlabeled data with Attention-Guided Replacement, then applies Spatial Masking Consistency that utilizes intrinsic contextual information to enhance the spatial context reasoning for unlabeled data. Extensive experiments on various benchmarks demonstrate the superiority of our approach in both performance and efficiency. For example, with only 10% labeled data on the Left Atrial Segmentation dataset, our method surpasses BCP by 1.43% Jaccard while drastically reducing the FLOPs by 90.8% and parameters by 85.8%. Code is released at https://github.com/CUHK-AIM-Group/Light-UNETR.&lt;/p&gt;</content:encoded></item><item><title>EquivFisheye: A Spherical Fusion Framework for Panoramic 3D Perception with Surround-View Fisheye Cameras</title><link>https://doi.org/10.1016/j.inffus.2025.104024</link><guid>10.1016/j.inffus.2025.104024</guid><pubDate>Sat, 06 Dec 2025 07:56:17 +0000</pubDate><dc:creator>Zhao Yang</dc:creator><dc:creator>Xinglin Pu</dc:creator><dc:creator>Weixiang Xu</dc:creator><dc:creator>Zezhong Qian</dc:creator><dc:creator>Kang Ke</dc:creator><dc:creator>Haonan Zhang</dc:creator><dc:creator>Longjun Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104024</prism:doi><description>Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.
Published: 2025-12-06T07:56:17+00:00
Venue: Information Fusion
Score: 0.829 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Yang; Xinglin Pu; Weixiang Xu; Zezhong Qian; Kang Ke; Haonan Zhang; Longjun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104024"&gt;10.1016/j.inffus.2025.104024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.829 (must_read)&lt;/p&gt;
&lt;p&gt;Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.&lt;/p&gt;</content:encoded></item><item><title>Optical Context Compression Is Just (Bad) Autoencoding</title><link>https://arxiv.org/abs/2512.03643v1</link><guid>http://arxiv.org/abs/2512.03643v1</guid><pubDate>Wed, 03 Dec 2025 10:27:27 +0000</pubDate><dc:creator>Ivan Yee Lee</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Taylor Berg-Kirkpatrick</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding
Published: 2025-12-03T10:27:27+00:00
Venue: arXiv
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ivan Yee Lee; Cheng Yang; Taylor Berg-Kirkpatrick&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR&amp;#x27;s reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding&lt;/p&gt;</content:encoded></item><item><title>WMRNet: Wavelet Mamba with Reversible Structure for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tip.2025.3637729</link><guid>10.1109/tip.2025.3637729</guid><pubDate>Fri, 05 Dec 2025 18:41:09 +0000</pubDate><dc:creator>Mingjin Zhang</dc:creator><dc:creator>Xiaolong Li</dc:creator><dc:creator>Jie Guo</dc:creator><dc:creator>Yunsong Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3637729</prism:doi><description>Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods.
Published: 2025-12-05T18:41:09+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingjin Zhang; Xiaolong Li; Jie Guo; Yunsong Li; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3637729"&gt;10.1109/tip.2025.3637729&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Heatmap Pooling Network for Action Recognition From RGB Videos</title><link>https://doi.org/10.1109/tpami.2025.3640697</link><guid>10.1109/tpami.2025.3640697</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Mengyuan Liu</dc:creator><dc:creator>Jinfu Liu</dc:creator><dc:creator>Yongkang Jiang</dc:creator><dc:creator>Bin He</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640697</prism:doi><description>Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengyuan Liu; Jinfu Liu; Yongkang Jiang; Bin He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640697"&gt;10.1109/tpami.2025.3640697&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Domain Adaptive Object Detection via Discriminative Instance Teacher</title><link>https://doi.org/10.1016/j.eswa.2025.130656</link><guid>10.1016/j.eswa.2025.130656</guid><pubDate>Fri, 05 Dec 2025 00:36:16 +0000</pubDate><dc:creator>Yiming Ge</dc:creator><dc:creator>Hui Liu</dc:creator><dc:creator>Yanjie Hu</dc:creator><dc:creator>Jie Zhao</dc:creator><dc:creator>Junzhao Du</dc:creator><dc:creator>Ertong Shang</dc:creator><dc:creator>Zhaocheng Niu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130656</prism:doi><description>Domain adaptive object detection (DAOD) poses significant challenges due to pronounced domain shifts. Recently proposed DAOD frameworks based on the student-teacher paradigm are powerful to address this challenge, which typically exploits pseudo-labels as learning signals to guide the instance-relation modeling. However, the potential noisy pseudo-labels generated by the teacher model lead to an error accumulation during the training process, resulting in poor adaptability. Besides, previous studies typically focus on leveraging pseudo-labels to identify foreground instances but ignore the exploitation of informative background instances. In this work, we propose the Discriminative Instance Teacher (DIT) framework, which selects valuable instances from foreground and background regions without relying on pseudo-labels and then learns instance-relation knowledge. Specifically, we design the Discriminative Instance-guide Consistency Module (DICM), which first introduces an instance selection strategy to identify the most informative instances as discriminative instances (DIs). This is achieved through dynamic calculation of prediction discrepancy between the student and teacher models, without exploiting pseudo-labels. Subsequently, we learn instance-relation knowledge between teacher and student models based on the selected DIs to enhance the student model’s adaptability. Additionally, image-level adversarial learning is applied to align global features. Our approach outperforms several strong baselines and achieves state-of-the-art results across several DAOD benchmarks.
Published: 2025-12-05T00:36:16+00:00
Venue: Expert Systems with Applications
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Ge; Hui Liu; Yanjie Hu; Jie Zhao; Junzhao Du; Ertong Shang; Zhaocheng Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130656"&gt;10.1016/j.eswa.2025.130656&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Domain adaptive object detection (DAOD) poses significant challenges due to pronounced domain shifts. Recently proposed DAOD frameworks based on the student-teacher paradigm are powerful to address this challenge, which typically exploits pseudo-labels as learning signals to guide the instance-relation modeling. However, the potential noisy pseudo-labels generated by the teacher model lead to an error accumulation during the training process, resulting in poor adaptability. Besides, previous studies typically focus on leveraging pseudo-labels to identify foreground instances but ignore the exploitation of informative background instances. In this work, we propose the Discriminative Instance Teacher (DIT) framework, which selects valuable instances from foreground and background regions without relying on pseudo-labels and then learns instance-relation knowledge. Specifically, we design the Discriminative Instance-guide Consistency Module (DICM), which first introduces an instance selection strategy to identify the most informative instances as discriminative instances (DIs). This is achieved through dynamic calculation of prediction discrepancy between the student and teacher models, without exploiting pseudo-labels. Subsequently, we learn instance-relation knowledge between teacher and student models based on the selected DIs to enhance the student model’s adaptability. Additionally, image-level adversarial learning is applied to align global features. Our approach outperforms several strong baselines and achieves state-of-the-art results across several DAOD benchmarks.&lt;/p&gt;</content:encoded></item><item><title>ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers</title><link>https://arxiv.org/abs/2512.03673v1</link><guid>http://arxiv.org/abs/2512.03673v1</guid><pubDate>Wed, 03 Dec 2025 11:02:16 +0000</pubDate><dc:creator>Feice Huang</dc:creator><dc:creator>Zuliang Han</dc:creator><dc:creator>Xing Zhou</dc:creator><dc:creator>Yihuang Chen</dc:creator><dc:creator>Lifei Zhu</dc:creator><dc:creator>Haoqian Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.
Published: 2025-12-03T11:02:16+00:00
Venue: arXiv
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feice Huang; Zuliang Han; Xing Zhou; Yihuang Chen; Lifei Zhu; Haoqian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.&lt;/p&gt;</content:encoded></item><item><title>Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.04520v1</link><guid>http://arxiv.org/abs/2512.04520v1</guid><pubDate>Thu, 04 Dec 2025 07:08:21 +0000</pubDate><dc:creator>Chenlin Xu</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Lituan Wang</dc:creator><dc:creator>Xinyu Pu</dc:creator><dc:creator>Pengfei Ma</dc:creator><dc:creator>Guangwu Qian</dc:creator><dc:creator>Zizhou Wang</dc:creator><dc:creator>Yan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM's zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.
Published: 2025-12-04T07:08:21+00:00
Venue: arXiv
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenlin Xu; Lei Zhang; Lituan Wang; Xinyu Pu; Pengfei Ma; Guangwu Qian; Zizhou Wang; Yan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM&amp;#x27;s zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.&lt;/p&gt;</content:encoded></item><item><title>MSG-CLIP: Enhancing CLIP’s Ability to Learn Fine-grained Structural Associations through Multi-modal Scene Graph Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112794</link><guid>10.1016/j.patcog.2025.112794</guid><pubDate>Sat, 06 Dec 2025 23:14:20 +0000</pubDate><dc:creator>Xiaotian Lv</dc:creator><dc:creator>Yue Zhao</dc:creator><dc:creator>Hanlong Yin</dc:creator><dc:creator>Yifei Chen</dc:creator><dc:creator>Jianxing Liu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112794</prism:doi><description>As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.
Published: 2025-12-06T23:14:20+00:00
Venue: Pattern Recognition
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaotian Lv; Yue Zhao; Hanlong Yin; Yifei Chen; Jianxing Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112794"&gt;10.1016/j.patcog.2025.112794&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.&lt;/p&gt;</content:encoded></item><item><title>LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging</title><link>https://arxiv.org/abs/2512.04939v1</link><guid>http://arxiv.org/abs/2512.04939v1</guid><pubDate>Thu, 04 Dec 2025 16:07:02 +0000</pubDate><dc:creator>Zhijian Shu</dc:creator><dc:creator>Cheng Lin</dc:creator><dc:creator>Tao Xie</dc:creator><dc:creator>Wei Yin</dc:creator><dc:creator>Ben Li</dc:creator><dc:creator>Zhiyuan Pu</dc:creator><dc:creator>Weize Li</dc:creator><dc:creator>Yao Yao</dc:creator><dc:creator>Xun Cao</dc:creator><dc:creator>Xiaoyang Guo</dc:creator><dc:creator>Xiao-Xiao Long</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/
Published: 2025-12-04T16:07:02+00:00
Venue: arXiv
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhijian Shu; Cheng Lin; Tao Xie; Wei Yin; Ben Li; Zhiyuan Pu; Weize Li; Yao Yao; Xun Cao; Xiaoyang Guo; Xiao-Xiao Long&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token&amp;#x27;s geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT&amp;#x27;s core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT&amp;#x27;s effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/&lt;/p&gt;</content:encoded></item><item><title>Multi-modal Collaborative Learning with Vision Foundation Model Prompt Boosts 3D Semi-supervised Semantic Segmentation</title><link>https://doi.org/10.1016/j.inffus.2025.104019</link><guid>10.1016/j.inffus.2025.104019</guid><pubDate>Fri, 05 Dec 2025 00:44:45 +0000</pubDate><dc:creator>Xiang He</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Baidan Li</dc:creator><dc:creator>Zhiyuan Xu</dc:creator><dc:creator>Qimin Xu</dc:creator><dc:creator>Hongwei Lu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104019</prism:doi><description>3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.
Published: 2025-12-05T00:44:45+00:00
Venue: Information Fusion
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang He; Xu Li; Baidan Li; Zhiyuan Xu; Qimin Xu; Hongwei Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104019"&gt;10.1016/j.inffus.2025.104019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.&lt;/p&gt;</content:encoded></item><item><title>MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms</title><link>https://arxiv.org/abs/2512.03640v1</link><guid>http://arxiv.org/abs/2512.03640v1</guid><pubDate>Wed, 03 Dec 2025 10:22:27 +0000</pubDate><dc:creator>Jiahao Zhang</dc:creator><dc:creator>Xiao Zhao</dc:creator><dc:creator>Guangyu Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1007/978-981-96-2061-6_29</prism:doi><description>Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.
Published: 2025-12-03T10:22:27+00:00
Venue: arXiv
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Zhang; Xiao Zhao; Guangyu Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/978-981-96-2061-6_29"&gt;10.1007/978-981-96-2061-6_29&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network&amp;#x27;s ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet&amp;#x27;s superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.&lt;/p&gt;</content:encoded></item><item><title>Difference Decomposition Networks for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2512.03470v1</link><guid>http://arxiv.org/abs/2512.03470v1</guid><pubDate>Wed, 03 Dec 2025 05:52:06 +0000</pubDate><dc:creator>Chen Hu</dc:creator><dc:creator>Mingyu Zhou</dc:creator><dc:creator>Shuai Yuan</dc:creator><dc:creator>Hongbo Hu</dc:creator><dc:creator>Xiangyu Qiu</dc:creator><dc:creator>Junhai Luo</dc:creator><dc:creator>Tian Pu</dc:creator><dc:creator>Xiyin Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.
Published: 2025-12-03T05:52:06+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Hu; Mingyu Zhou; Shuai Yuan; Hongbo Hu; Xiangyu Qiu; Junhai Luo; Tian Pu; Xiyin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.&lt;/p&gt;</content:encoded></item><item><title>融合小波卷积与频域注意力的小目标检测改进</title><link>https://doi.org/10.11834/jig.250293</link><guid>10.11834/jig.250293</guid><pubDate>Fri, 05 Dec 2025 08:29:30 +0000</pubDate><dc:creator>Liu Xu</dc:creator><dc:creator>Song Peibo</dc:creator><dc:creator>Bao Fangxun</dc:creator><dc:creator>Du Hongwei</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250293</prism:doi><description>目的无人机拍摄图像存在小目标数量多，易受恶劣天气等噪声污染的特点，针对无人机拍摄图像的小目标检测技术在军用领域和商用领域都发挥着重要作用。然而，现有的目标检测方法在定位小目标方面仍然存在检测精度低的问题。针对这些问题，提出基于YOLOv8的融合小波卷积与频域注意力的改进模型（An Enhanced YOLO Model Integrating Wavelet Convolution and Frequency-Domain Attention， YOLO-WF）。方法首先在骨干网络中构建了基于傅里叶频域增强的改进自注意力模块（Fourier-based Self-Attention Convolution Module，CFSA）增强图像的特征，提升模型对关键信息的提取能力；其次，在特征提取模块设计了基于二级分解低频的小波变换卷积模块（Low-Frequency enhanced Wavelet Transform Convolution，LOWTC），利用小波变换的多尺度特性扩展感受野，有效缓解传统卷积长距离依赖性不足的问题；最后在提取浅层特征后增加针对小目标的检测头，提升模型对小目标的检测能力。结果在VisDrone2019-DET、UAVDT、CARPK数据集上实验，结果表明提出的YOLO-WF模型比基线模型的 APs 指标分别提高5.5个、3.08个、6.8个百分点，达到19.9%、38.54%和33.3%。 AP50 和 APm 指标也均有提升，以VisDrone2019-DET为例， AP50 、 APm 分别达到47.1%、40.3%，相比基线模型分别提高3.5、3.0个百分点，且参数量下降0.4%。结论YOLO-WF通过频域-小波融合策略，显著提升了中小目标的检测精度，且未引入额外存储负担，可直接迁移至其他航拍检测任务。
Published: 2025-12-05T08:29:30+00:00
Venue: Journal of Image and Graphics
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liu Xu; Song Peibo; Bao Fangxun; Du Hongwei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250293"&gt;10.11834/jig.250293&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;目的无人机拍摄图像存在小目标数量多，易受恶劣天气等噪声污染的特点，针对无人机拍摄图像的小目标检测技术在军用领域和商用领域都发挥着重要作用。然而，现有的目标检测方法在定位小目标方面仍然存在检测精度低的问题。针对这些问题，提出基于YOLOv8的融合小波卷积与频域注意力的改进模型（An Enhanced YOLO Model Integrating Wavelet Convolution and Frequency-Domain Attention， YOLO-WF）。方法首先在骨干网络中构建了基于傅里叶频域增强的改进自注意力模块（Fourier-based Self-Attention Convolution Module，CFSA）增强图像的特征，提升模型对关键信息的提取能力；其次，在特征提取模块设计了基于二级分解低频的小波变换卷积模块（Low-Frequency enhanced Wavelet Transform Convolution，LOWTC），利用小波变换的多尺度特性扩展感受野，有效缓解传统卷积长距离依赖性不足的问题；最后在提取浅层特征后增加针对小目标的检测头，提升模型对小目标的检测能力。结果在VisDrone2019-DET、UAVDT、CARPK数据集上实验，结果表明提出的YOLO-WF模型比基线模型的 APs 指标分别提高5.5个、3.08个、6.8个百分点，达到19.9%、38.54%和33.3%。 AP50 和 APm 指标也均有提升，以VisDrone2019-DET为例， AP50 、 APm 分别达到47.1%、40.3%，相比基线模型分别提高3.5、3.0个百分点，且参数量下降0.4%。结论YOLO-WF通过频域-小波融合策略，显著提升了中小目标的检测精度，且未引入额外存储负担，可直接迁移至其他航拍检测任务。&lt;/p&gt;</content:encoded></item><item><title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title><link>https://arxiv.org/abs/2512.04581v1</link><guid>http://arxiv.org/abs/2512.04581v1</guid><pubDate>Thu, 04 Dec 2025 08:49:23 +0000</pubDate><dc:creator>Houzhang Fang</dc:creator><dc:creator>Chenxing Wu</dc:creator><dc:creator>Kun Bai</dc:creator><dc:creator>Tianqi Chen</dc:creator><dc:creator>Xiaolin Wang</dc:creator><dc:creator>Xiyang Liu</dc:creator><dc:creator>Yi Chang</dc:creator><dc:creator>Luxin Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
Published: 2025-12-04T08:49:23+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Houzhang Fang; Chenxing Wu; Kun Bai; Tianqi Chen; Xiaolin Wang; Xiyang Liu; Yi Chang; Luxin Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network&amp;#x27;s focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.&lt;/p&gt;</content:encoded></item><item><title>Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows</title><link>https://arxiv.org/abs/2512.04954v1</link><guid>http://arxiv.org/abs/2512.04954v1</guid><pubDate>Thu, 04 Dec 2025 16:22:53 +0000</pubDate><dc:creator>Rajneil Baruah</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.
Published: 2025-12-04T16:22:53+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rajneil Baruah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.&lt;/p&gt;</content:encoded></item><item><title>LPATR-Net: Learnable Piecewise Affine Transformation Regression Assisted Data-Driven Dehazing Framework</title><link>https://doi.org/10.1109/tip.2025.3637687</link><guid>10.1109/tip.2025.3637687</guid><pubDate>Fri, 05 Dec 2025 18:41:09 +0000</pubDate><dc:creator>Yuelong Li</dc:creator><dc:creator>Fei Chen</dc:creator><dc:creator>Zhenwei Liu</dc:creator><dc:creator>Tianyu Zang</dc:creator><dc:creator>Jianming Wang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3637687</prism:doi><description>Nowadays, data-driven learning based deep neural network (DNN) is the most dominant SOTA image dehazing framework. Here, learning to perfectly simulate the underlying mapping rules (from hazy to clear) told by massive paired training data is its core driving force. However, under genuine scenarios, it is extremely hard to guarantee the 100% qualification of all collected ground truth (GT) haze-free data. That’s because natural weather is hardly controlled, and many weathers are actually in a chaotic status existing between foggy and fog-free. Thus, unlike most supervised learning issues, the image dehazing society is born with the torture of part of faulty ground truth no-haze samples. Therefore, totally trusting training data and solely pursuing more fitting powerful data-driven model may not be a wise solution. To cope with this thorny challenge, in this paper, instead of faithfully pursuing for fitting capacity promotion, we on the contrary choose to intentionally cut down the fitting flexibility to achieve higher-level robustness. That is the LPATR-Net, a novel dehazing framework specially armed with fitting power suppression mechanism to resist intrinsic annoying faulty GT. This solution does not involve any extra manually labeling. Specifically, the LPATR-Net architecture is created completely around elaborately designed fitting-restrained learnable piecewise affine transformation regression. Since such low-order linear regression structure genetically can only fit for majority of data, the interference of minority of unqualified GT samples is expected to be effectively suppressed. Through further coupled with a highly customized multi-concerns high-accuracy dehazing fitting companion component, All-Mattering, proposed LPATR-Net elegantly achieves the seamless integration of traditional majority determining fixed-form regression and modern all freedom data-driven deep learning. Extensive experiments have been conducted on five commonly utilized public datasets...
Published: 2025-12-05T18:41:09+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuelong Li; Fei Chen; Zhenwei Liu; Tianyu Zang; Jianming Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3637687"&gt;10.1109/tip.2025.3637687&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Nowadays, data-driven learning based deep neural network (DNN) is the most dominant SOTA image dehazing framework. Here, learning to perfectly simulate the underlying mapping rules (from hazy to clear) told by massive paired training data is its core driving force. However, under genuine scenarios, it is extremely hard to guarantee the 100% qualification of all collected ground truth (GT) haze-free data. That’s because natural weather is hardly controlled, and many weathers are actually in a chaotic status existing between foggy and fog-free. Thus, unlike most supervised learning issues, the image dehazing society is born with the torture of part of faulty ground truth no-haze samples. Therefore, totally trusting training data and solely pursuing more fitting powerful data-driven model may not be a wise solution. To cope with this thorny challenge, in this paper, instead of faithfully pursuing for fitting capacity promotion, we on the contrary choose to intentionally cut down the fitting flexibility to achieve higher-level robustness. That is the LPATR-Net, a novel dehazing framework specially armed with fitting power suppression mechanism to resist intrinsic annoying faulty GT. This solution does not involve any extra manually labeling. Specifically, the LPATR-Net architecture is created completely around elaborately designed fitting-restrained learnable piecewise affine transformation regression. Since such low-order linear regression structure genetically can only fit for majority of data, the interference of minority of unqualified GT samples is expected to be effectively suppressed. Through further coupled with a highly customized multi-concerns high-accuracy dehazing fitting companion component, All-Mattering, proposed LPATR-Net elegantly achieves the seamless integration of traditional majority determining fixed-form regression and modern all freedom data-driven deep learning. Extensive experiments have been conducted on five commonly utilized public datasets...&lt;/p&gt;</content:encoded></item><item><title>SAR-D-FINE: A Context-Aware Detector for Small and Densely Packed Ship Detection in SAR Imagery</title><link>https://doi.org/10.1109/lgrs.2025.3640683</link><guid>10.1109/lgrs.2025.3640683</guid><pubDate>Fri, 05 Dec 2025 18:41:23 +0000</pubDate><dc:creator>Xiaobing Fan</dc:creator><dc:creator>Bowen Xing</dc:creator><dc:creator>Xingchen Wang</dc:creator><dc:creator>Hongdan Liu</dc:creator><dc:creator>Chuanxu Yan</dc:creator><dc:creator>Pengfei Zhi</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3640683</prism:doi><description>Synthetic Aperture Radar (SAR) provides all-weather imaging, yet small-scale, densely clustered ships remain difficult to detect because coherent speckle noise and coastal clutter often mask target echoes. Current detectors, derived mainly from optical imaging methods, fail to extract weak signatures from minute vessels and to separate closely spaced targets from background clutter, leading to frequent missed detections and elevated false-alarm rates. This paper presents SAR-D-FINE, a context-aware detection framework tailored for SAR imagery. Extending the D-FINE architecture, we design a hybrid backbone with a StarStage module that strengthens nonlinear feature extraction under heavy noise. A Focusing Diffusion Encoder, integrating a Multi-Kernel Aggregation Module (MKAM) and a parameter-free Shuffle-and-Shift Upsampling (SSU) unit, is adopted to aggregate multi-scale features without sacrificing fine spatial details. Experimental results on SSDD and HRSID indicate that SAR-D-FINE surpasses existing methods, including dedicated SAR detectors such as YOLO-SARSI and SW-Net, achieving AP improvements of 2.0% and 1.8% over the baseline, respectively. The results confirm the advantages of the proposed model, particularly for detecting small, densely distributed vessels.
Published: 2025-12-05T18:41:23+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaobing Fan; Bowen Xing; Xingchen Wang; Hongdan Liu; Chuanxu Yan; Pengfei Zhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3640683"&gt;10.1109/lgrs.2025.3640683&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) provides all-weather imaging, yet small-scale, densely clustered ships remain difficult to detect because coherent speckle noise and coastal clutter often mask target echoes. Current detectors, derived mainly from optical imaging methods, fail to extract weak signatures from minute vessels and to separate closely spaced targets from background clutter, leading to frequent missed detections and elevated false-alarm rates. This paper presents SAR-D-FINE, a context-aware detection framework tailored for SAR imagery. Extending the D-FINE architecture, we design a hybrid backbone with a StarStage module that strengthens nonlinear feature extraction under heavy noise. A Focusing Diffusion Encoder, integrating a Multi-Kernel Aggregation Module (MKAM) and a parameter-free Shuffle-and-Shift Upsampling (SSU) unit, is adopted to aggregate multi-scale features without sacrificing fine spatial details. Experimental results on SSDD and HRSID indicate that SAR-D-FINE surpasses existing methods, including dedicated SAR detectors such as YOLO-SARSI and SW-Net, achieving AP improvements of 2.0% and 1.8% over the baseline, respectively. The results confirm the advantages of the proposed model, particularly for detecting small, densely distributed vessels.&lt;/p&gt;</content:encoded></item><item><title>Triple-Way Visual Modulation for Zero-Shot Sketch-Based Image Retrieval</title><link>https://doi.org/10.1109/tcsvt.2025.3640868</link><guid>10.1109/tcsvt.2025.3640868</guid><pubDate>Fri, 05 Dec 2025 18:40:41 +0000</pubDate><dc:creator>Haoxiang Zhang</dc:creator><dc:creator>Qiqi Kou</dc:creator><dc:creator>He Jiang</dc:creator><dc:creator>Tianshu Song</dc:creator><dc:creator>Liangliang Chen</dc:creator><dc:creator>Deqiang Cheng</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3640868</prism:doi><description>Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) seeks to correlate unseen hand-drawn sketches with unseen real images by leveraging trained models on visible categories. Recent CLIP-based models, which primarily focus on visual-textual interaction, have demonstrated strong competitiveness in ZS-SBIR. However, they still fall short in the exploration of cross-modal visual representations, especially in terms of cross-modal visual shared and specific information. Differing from the aforementioned researches, we start with class-level, prompt-level, and patch-level visual information, committed to unlocking the potential of visual feature representation. On this foundation, we introduce the vision-centric Triple-way visual modulATion (TAT) framework to enhance the model’s perception of visual shared and specific information. Specifically, we establish unified multi-modal perception by integrating visual-level modality prompter into the CLIP architecture. We then conduct triple-way modulation modeling on prompt, token, and patch levels to effectively mine shared and specific features. Lastly, we develop an enhanced calibration strategy incorporating prompt-aware, token-aware, and logit-aware alignment modules to amplify the model’s proficiency in probing shared-specific features. We thoroughly test our approach to confirm its excellence and the efficacy of individual components. The comparison results on the popular datasets Sketchy, Sketchyv2, Tuberlin, and QuickDraw show that the developed algorithm significantly surpasses the current state-of-the-art technologies.
Published: 2025-12-05T18:40:41+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoxiang Zhang; Qiqi Kou; He Jiang; Tianshu Song; Liangliang Chen; Deqiang Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3640868"&gt;10.1109/tcsvt.2025.3640868&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) seeks to correlate unseen hand-drawn sketches with unseen real images by leveraging trained models on visible categories. Recent CLIP-based models, which primarily focus on visual-textual interaction, have demonstrated strong competitiveness in ZS-SBIR. However, they still fall short in the exploration of cross-modal visual representations, especially in terms of cross-modal visual shared and specific information. Differing from the aforementioned researches, we start with class-level, prompt-level, and patch-level visual information, committed to unlocking the potential of visual feature representation. On this foundation, we introduce the vision-centric Triple-way visual modulATion (TAT) framework to enhance the model’s perception of visual shared and specific information. Specifically, we establish unified multi-modal perception by integrating visual-level modality prompter into the CLIP architecture. We then conduct triple-way modulation modeling on prompt, token, and patch levels to effectively mine shared and specific features. Lastly, we develop an enhanced calibration strategy incorporating prompt-aware, token-aware, and logit-aware alignment modules to amplify the model’s proficiency in probing shared-specific features. We thoroughly test our approach to confirm its excellence and the efficacy of individual components. The comparison results on the popular datasets Sketchy, Sketchyv2, Tuberlin, and QuickDraw show that the developed algorithm significantly surpasses the current state-of-the-art technologies.&lt;/p&gt;</content:encoded></item><item><title>Diminishing Returns in Self-Supervised Learning</title><link>https://arxiv.org/abs/2512.03862v1</link><guid>http://arxiv.org/abs/2512.03862v1</guid><pubDate>Wed, 03 Dec 2025 15:11:44 +0000</pubDate><dc:creator>Oli Bridge</dc:creator><dc:creator>Huey Sun</dc:creator><dc:creator>Botond Branyicskai-Nagy</dc:creator><dc:creator>Charles D'Ornano</dc:creator><dc:creator>Shomit Basu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.
Published: 2025-12-03T15:11:44+00:00
Venue: arXiv
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Oli Bridge; Huey Sun; Botond Branyicskai-Nagy; Charles D&amp;#x27;Ornano; Shomit Basu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.&lt;/p&gt;</content:encoded></item><item><title>CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning</title><link>https://doi.org/10.1016/j.inffus.2025.104027</link><guid>10.1016/j.inffus.2025.104027</guid><pubDate>Fri, 05 Dec 2025 00:44:23 +0000</pubDate><dc:creator>Wenjie Li</dc:creator><dc:creator>Yujie Zhang</dc:creator><dc:creator>Haoran Sun</dc:creator><dc:creator>Yueqi Li</dc:creator><dc:creator>Fanrui Zhang</dc:creator><dc:creator>Mengzhe Xu</dc:creator><dc:creator>Victoria Borja Clausich</dc:creator><dc:creator>Sade Mellin</dc:creator><dc:creator>Renhao Yang</dc:creator><dc:creator>Chenrun Wang</dc:creator><dc:creator>Jethro Zih-Shuo Wang</dc:creator><dc:creator>Shiyi Yao</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Yidong Xu</dc:creator><dc:creator>Hanyu Wang</dc:creator><dc:creator>Yilin Huang</dc:creator><dc:creator>Angela Lin Wang</dc:creator><dc:creator>Chen Shi</dc:creator><dc:creator>Yin Zhang</dc:creator><dc:creator>Jianan Guo</dc:creator><dc:creator>Luqi Yang</dc:creator><dc:creator>Renxuan Li</dc:creator><dc:creator>Yang Xu</dc:creator><dc:creator>Jiawei Liu</dc:creator><dc:creator>Yao Zhang</dc:creator><dc:creator>Lei Liu</dc:creator><dc:creator>Carlos Gutiérrez Sanromán</dc:creator><dc:creator>Lei Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104027</prism:doi><description>Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on ”one-time” diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind , the first generative model to achieve interleaved ”think-answer” reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On a real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. CX-Mind establishes a new paradigm for constructing interpretable, and high-performing medical MLLMs.
Published: 2025-12-05T00:44:23+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenjie Li; Yujie Zhang; Haoran Sun; Yueqi Li; Fanrui Zhang; Mengzhe Xu; Victoria Borja Clausich; Sade Mellin; Renhao Yang; Chenrun Wang; Jethro Zih-Shuo Wang; Shiyi Yao; Gen Li; Yidong Xu; Hanyu Wang; Yilin Huang; Angela Lin Wang; Chen Shi; Yin Zhang; Jianan Guo; Luqi Yang; Renxuan Li; Yang Xu; Jiawei Liu; Yao Zhang; Lei Liu; Carlos Gutiérrez Sanromán; Lei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104027"&gt;10.1016/j.inffus.2025.104027&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on ”one-time” diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind , the first generative model to achieve interleaved ”think-answer” reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On a real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. CX-Mind establishes a new paradigm for constructing interpretable, and high-performing medical MLLMs.&lt;/p&gt;</content:encoded></item><item><title>EEformer: Early Exiting for Transformer with Global-Local Exits and Progressive Fine-Tuning</title><link>https://doi.org/10.1109/tmm.2025.3639997</link><guid>10.1109/tmm.2025.3639997</guid><pubDate>Fri, 05 Dec 2025 18:39:50 +0000</pubDate><dc:creator>Guanyu Xu</dc:creator><dc:creator>Jiawei Hao</dc:creator><dc:creator>Yong Luo</dc:creator><dc:creator>Li Shen</dc:creator><dc:creator>Han Hu</dc:creator><dc:creator>Dan Zeng</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639997</prism:doi><description>Recently, the efficient deployment and acceleration of transformer-based pre-trained models (TPMs) on resource-constrained edge devices for multimedia services have gained significant interest. Although early exiting is a feasible solution, it may lead to extra computational cost and substantial performance degradation compared to the original models. To tackle these issues, we propose a framework termed EEformer, which incorporates global-local heads (GLHs) into intermediate layers to construct the early exiting dynamic neural network (EDNN). The GLH can efficiently extract global and local information from hidden states produced by the backbone layer, thereby achieving a better performance-efficiency trade-off for the EDNN. Moreover, we propose a novel progressive fine-tuning strategy to steadily improve the efficiency of the EDNN while maintaining its performance comparable to the original mode through three fine-tuning stages. We conduct extensive experiments on image classification and natural language processing tasks, demonstrating the superiority of the proposed framework. In particular, the proposed framework achieves 1.87× speed-up while maintaining 99.0% performance on the CIFAR-100 dataset, and 3.05× speed-up while maintaining 98.5% performance on the SST-2 dataset.
Published: 2025-12-05T18:39:50+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guanyu Xu; Jiawei Hao; Yong Luo; Li Shen; Han Hu; Dan Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639997"&gt;10.1109/tmm.2025.3639997&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, the efficient deployment and acceleration of transformer-based pre-trained models (TPMs) on resource-constrained edge devices for multimedia services have gained significant interest. Although early exiting is a feasible solution, it may lead to extra computational cost and substantial performance degradation compared to the original models. To tackle these issues, we propose a framework termed EEformer, which incorporates global-local heads (GLHs) into intermediate layers to construct the early exiting dynamic neural network (EDNN). The GLH can efficiently extract global and local information from hidden states produced by the backbone layer, thereby achieving a better performance-efficiency trade-off for the EDNN. Moreover, we propose a novel progressive fine-tuning strategy to steadily improve the efficiency of the EDNN while maintaining its performance comparable to the original mode through three fine-tuning stages. We conduct extensive experiments on image classification and natural language processing tasks, demonstrating the superiority of the proposed framework. In particular, the proposed framework achieves 1.87× speed-up while maintaining 99.0% performance on the CIFAR-100 dataset, and 3.05× speed-up while maintaining 98.5% performance on the SST-2 dataset.&lt;/p&gt;</content:encoded></item><item><title>用于红外与可见光图像融合的多层级Mamba网络</title><link>https://doi.org/10.11834/jig.250243</link><guid>10.11834/jig.250243</guid><pubDate>Fri, 05 Dec 2025 08:29:31 +0000</pubDate><dc:creator>Yang Tianyu</dc:creator><dc:creator>Huo Hongtao</dc:creator><dc:creator>Guo Baofeng</dc:creator><dc:creator>Zheng Bowen</dc:creator><dc:creator>Liu Xiaowen</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250243</prism:doi><description>目的现有融合方法普遍存在多层级语义信息的表征退化问题，且缺乏有效的跨层级特征交互机制，导致浅层细节与深层语义信息在融合过程中难以完全耦合。此外，基于Transformer的融合方法在全局特征建模的过程中需要消耗大量计算资源。针对上述问题，本文提出了一种用于红外与可见光图像融合的多层级Mamba网络。方法融合网络通过构建多层级特征框架，对多分辨率源图像进行全局特征建模与跨层级特征交互，实现了跨模态图像细粒度语义信息的有效保留。同时，特征编码阶段设计F-Mamba模块，在维持线性复杂度的同时，实现了全局特征提取。此外，模型通过设计跨层级特征聚合模块，实现了不同层级间视觉特征与语义信息的深度对齐。结果对比实验在MSRS、LLVIP和RoadScene数据集上与13种传统以及深度学习融合方法进行比较。主观评价方面，融合结果在目标细节特征恢复以及视觉质量方面具有显著优势。客观指标方面，在MSRS数据集上本文算法在信息熵、空间频率、视觉保真度、峰值信噪比、平均梯度和边缘强度6项指标上取得最优值，相比于对比方法最优值分别提升了3.03%，1.56%，15.89%，7.26%，2.61%，1.62%。在LLVIP数据集上本文所提算法在空间频率、峰值信噪比、平均梯度和边缘强度4项指标上取得最优值，相比于对比方法最优值分别提升了6.42 %，0.45%，6.47%，7.23%。在RoadScene数据集上本文所提算法在平均梯度和边缘强度2项指标上仍取得最优值。消融实验验证了本文融合网络各组件的有效性。此外，运行效率对比实验和语义分割实验，进一步验证了本文方法在计算效率和深层语义信息保留方面的优势。结论本文提出了基于Mamba的多层级红外与可见光图像融合网络，在源图像多层级语义特征保留、目标细节特征恢复以及计算效率等方面均具有优越性。
Published: 2025-12-05T08:29:31+00:00
Venue: Journal of Image and Graphics
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Tianyu; Huo Hongtao; Guo Baofeng; Zheng Bowen; Liu Xiaowen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250243"&gt;10.11834/jig.250243&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;目的现有融合方法普遍存在多层级语义信息的表征退化问题，且缺乏有效的跨层级特征交互机制，导致浅层细节与深层语义信息在融合过程中难以完全耦合。此外，基于Transformer的融合方法在全局特征建模的过程中需要消耗大量计算资源。针对上述问题，本文提出了一种用于红外与可见光图像融合的多层级Mamba网络。方法融合网络通过构建多层级特征框架，对多分辨率源图像进行全局特征建模与跨层级特征交互，实现了跨模态图像细粒度语义信息的有效保留。同时，特征编码阶段设计F-Mamba模块，在维持线性复杂度的同时，实现了全局特征提取。此外，模型通过设计跨层级特征聚合模块，实现了不同层级间视觉特征与语义信息的深度对齐。结果对比实验在MSRS、LLVIP和RoadScene数据集上与13种传统以及深度学习融合方法进行比较。主观评价方面，融合结果在目标细节特征恢复以及视觉质量方面具有显著优势。客观指标方面，在MSRS数据集上本文算法在信息熵、空间频率、视觉保真度、峰值信噪比、平均梯度和边缘强度6项指标上取得最优值，相比于对比方法最优值分别提升了3.03%，1.56%，15.89%，7.26%，2.61%，1.62%。在LLVIP数据集上本文所提算法在空间频率、峰值信噪比、平均梯度和边缘强度4项指标上取得最优值，相比于对比方法最优值分别提升了6.42 %，0.45%，6.47%，7.23%。在RoadScene数据集上本文所提算法在平均梯度和边缘强度2项指标上仍取得最优值。消融实验验证了本文融合网络各组件的有效性。此外，运行效率对比实验和语义分割实验，进一步验证了本文方法在计算效率和深层语义信息保留方面的优势。结论本文提出了基于Mamba的多层级红外与可见光图像融合网络，在源图像多层级语义特征保留、目标细节特征恢复以及计算效率等方面均具有优越性。&lt;/p&gt;</content:encoded></item><item><title>BlurDM: A Blur Diffusion Model for Image Deblurring</title><link>https://arxiv.org/abs/2512.03979v1</link><guid>http://arxiv.org/abs/2512.03979v1</guid><pubDate>Wed, 03 Dec 2025 17:10:44 +0000</pubDate><dc:creator>Jin-Ting He</dc:creator><dc:creator>Fu-Jen Tsai</dc:creator><dc:creator>Yan-Tsung Peng</dc:creator><dc:creator>Min-Hung Chen</dc:creator><dc:creator>Chia-Wen Lin</dc:creator><dc:creator>Yen-Yu Lin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.
Published: 2025-12-03T17:10:44+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jin-Ting He; Fu-Jen Tsai; Yan-Tsung Peng; Min-Hung Chen; Chia-Wen Lin; Yen-Yu Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The source code is available at https://github.com/Jin-Ting-He/BlurDM.&lt;/p&gt;</content:encoded></item><item><title>GeoPE:A Unified Geometric Positional Embedding for Structured Tensors</title><link>https://arxiv.org/abs/2512.04963v1</link><guid>http://arxiv.org/abs/2512.04963v1</guid><pubDate>Thu, 04 Dec 2025 16:31:12 +0000</pubDate><dc:creator>Yupu Yao</dc:creator><dc:creator>Bowen Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.
Published: 2025-12-04T16:31:12+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yupu Yao; Bowen Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.&lt;/p&gt;</content:encoded></item><item><title>Towards Cognition-Driven 3D Object Detection: A LiDAR-Based Framework</title><link>https://doi.org/10.1016/j.knosys.2025.115051</link><guid>10.1016/j.knosys.2025.115051</guid><pubDate>Fri, 05 Dec 2025 17:34:55 +0000</pubDate><dc:creator>Yewei Shi</dc:creator><dc:creator>Baicang Guo</dc:creator><dc:creator>Lisheng Jin</dc:creator><dc:creator>Xiao Yang</dc:creator><dc:creator>Hongyu Zhang</dc:creator><dc:creator>Xingchen Liu</dc:creator><dc:creator>Menglin Li</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115051</prism:doi><description>With the continuous advancement of autonomous driving technology, increasingly stringent requirements are placed on the accuracy and robustness of 3D object detection. Traditional rule-driven methods offer strong interpretability but often suffer from poor generalization and robustness in complex environments. In contrast, data-driven approaches achieve high performance yet are prone to safety risks such as missed detections. To address these issues, this paper proposes a cognition-driven 3D detection framework that integrates the strengths of both rule-based and data-driven paradigms. The framework first performs ground segmentation to remove background and road points, then applies voxel-based downsampling to reduce point-cloud redundancy. Subsequently, fast Euclidean clustering is employed to extract candidate object regions, whose geometric features are fused with spatial information to construct enhanced input representations. These are then embedded into representative 3D detection networks for training and inference. Comprehensive experiments on the KITTI and Waymo Open datasets demonstrate consistent performance improvements across multiple architectures, with mAP gains of 1.18-4.80% on KITTI and 1.20-4.04% on Waymo for PillarNet, PV-RCNN, and IA-SSD. Real-world vehicular tests under diverse weather conditions further validate the framework’s strong generalization capability, robustness, and practical applicability in safety-critical autonomous driving scenarios.
Published: 2025-12-05T17:34:55+00:00
Venue: Knowledge-Based Systems
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yewei Shi; Baicang Guo; Lisheng Jin; Xiao Yang; Hongyu Zhang; Xingchen Liu; Menglin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115051"&gt;10.1016/j.knosys.2025.115051&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;With the continuous advancement of autonomous driving technology, increasingly stringent requirements are placed on the accuracy and robustness of 3D object detection. Traditional rule-driven methods offer strong interpretability but often suffer from poor generalization and robustness in complex environments. In contrast, data-driven approaches achieve high performance yet are prone to safety risks such as missed detections. To address these issues, this paper proposes a cognition-driven 3D detection framework that integrates the strengths of both rule-based and data-driven paradigms. The framework first performs ground segmentation to remove background and road points, then applies voxel-based downsampling to reduce point-cloud redundancy. Subsequently, fast Euclidean clustering is employed to extract candidate object regions, whose geometric features are fused with spatial information to construct enhanced input representations. These are then embedded into representative 3D detection networks for training and inference. Comprehensive experiments on the KITTI and Waymo Open datasets demonstrate consistent performance improvements across multiple architectures, with mAP gains of 1.18-4.80% on KITTI and 1.20-4.04% on Waymo for PillarNet, PV-RCNN, and IA-SSD. Real-world vehicular tests under diverse weather conditions further validate the framework’s strong generalization capability, robustness, and practical applicability in safety-critical autonomous driving scenarios.&lt;/p&gt;</content:encoded></item><item><title>Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning</title><link>https://arxiv.org/abs/2512.04359v1</link><guid>http://arxiv.org/abs/2512.04359v1</guid><pubDate>Thu, 04 Dec 2025 01:09:17 +0000</pubDate><dc:creator>Hongye Cao</dc:creator><dc:creator>Zhixin Bai</dc:creator><dc:creator>Ziyue Peng</dc:creator><dc:creator>Boyan Wang</dc:creator><dc:creator>Tianpei Yang</dc:creator><dc:creator>Jing Huo</dc:creator><dc:creator>Yuyao Zhang</dc:creator><dc:creator>Yang Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.
Published: 2025-12-04T01:09:17+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongye Cao; Zhixin Bai; Ziyue Peng; Boyan Wang; Tianpei Yang; Jing Huo; Yuyao Zhang; Yang Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.&lt;/p&gt;</content:encoded></item></channel></rss>