<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 08 Dec 2025 05:59:17 +0000</lastBuildDate><item><title>EquivFisheye: A Spherical Fusion Framework for Panoramic 3D Perception with Surround-View Fisheye Cameras</title><link>https://doi.org/10.1016/j.inffus.2025.104024</link><guid>10.1016/j.inffus.2025.104024</guid><pubDate>Sat, 06 Dec 2025 07:56:17 +0000</pubDate><dc:creator>Zhao Yang</dc:creator><dc:creator>Xinglin Pu</dc:creator><dc:creator>Weixiang Xu</dc:creator><dc:creator>Zezhong Qian</dc:creator><dc:creator>Kang Ke</dc:creator><dc:creator>Haonan Zhang</dc:creator><dc:creator>Longjun Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104024</prism:doi><description>Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.
Published: 2025-12-06T07:56:17+00:00
Venue: Information Fusion
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Yang; Xinglin Pu; Weixiang Xu; Zezhong Qian; Kang Ke; Haonan Zhang; Longjun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104024"&gt;10.1016/j.inffus.2025.104024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Language Models in Agriculture: A Tutorial and Survey</title><link>https://doi.org/10.1016/j.inffus.2025.104042</link><guid>10.1016/j.inffus.2025.104042</guid><pubDate>Sun, 07 Dec 2025 15:19:43 +0000</pubDate><dc:creator>Mohammadreza Haghighat</dc:creator><dc:creator>Alzayat Saleh</dc:creator><dc:creator>Mostafa Rahimi Azghadi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104042</prism:doi><description>The integration of artificial intelligence (AI) in agriculture is rapidly evolving, marked by increasing adoption of machine learning (ML), deep learning (DL), and the recent emergence of large language models (LLMs) and multimodal language models (MLMs). These technologies are transforming traditional agricultural practices through advanced data analysis and offering innovative solutions for smart agriculture. Although earlier methods often relied on unimodal data, primarily images, current research is shifting toward information fusion with multimodal AI systems that fuse text, imagery, and other types of data, such as agricultural knowledge graphs and videos of animal activities, for enhanced decision making. General-purpose MLMs struggle with agriculture-specific nuances due to domain gaps and limited multimodal datasets. The high computational demands of foundation models (FM) and domain-specific needs also restrict broader adoption. This study presents a tutorial on applying MLMs in agriculture, covering their main concepts. Building on this foundational knowledge, the tutorial surveys key developments in the literature and provides a comprehensive step-by-step guide for implementing and tailoring MLMs to agricultural applications. This will address the domain gaps by knowledge integration, synthetic multimodal data generation and efficient learning methods, thereby expanding their practical applications.
Published: 2025-12-07T15:19:43+00:00
Venue: Information Fusion
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohammadreza Haghighat; Alzayat Saleh; Mostafa Rahimi Azghadi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104042"&gt;10.1016/j.inffus.2025.104042&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;The integration of artificial intelligence (AI) in agriculture is rapidly evolving, marked by increasing adoption of machine learning (ML), deep learning (DL), and the recent emergence of large language models (LLMs) and multimodal language models (MLMs). These technologies are transforming traditional agricultural practices through advanced data analysis and offering innovative solutions for smart agriculture. Although earlier methods often relied on unimodal data, primarily images, current research is shifting toward information fusion with multimodal AI systems that fuse text, imagery, and other types of data, such as agricultural knowledge graphs and videos of animal activities, for enhanced decision making. General-purpose MLMs struggle with agriculture-specific nuances due to domain gaps and limited multimodal datasets. The high computational demands of foundation models (FM) and domain-specific needs also restrict broader adoption. This study presents a tutorial on applying MLMs in agriculture, covering their main concepts. Building on this foundational knowledge, the tutorial surveys key developments in the literature and provides a comprehensive step-by-step guide for implementing and tailoring MLMs to agricultural applications. This will address the domain gaps by knowledge integration, synthetic multimodal data generation and efficient learning methods, thereby expanding their practical applications.&lt;/p&gt;</content:encoded></item><item><title>Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning</title><link>https://arxiv.org/abs/2512.04359v1</link><guid>http://arxiv.org/abs/2512.04359v1</guid><pubDate>Thu, 04 Dec 2025 01:09:17 +0000</pubDate><dc:creator>Hongye Cao</dc:creator><dc:creator>Zhixin Bai</dc:creator><dc:creator>Ziyue Peng</dc:creator><dc:creator>Boyan Wang</dc:creator><dc:creator>Tianpei Yang</dc:creator><dc:creator>Jing Huo</dc:creator><dc:creator>Yuyao Zhang</dc:creator><dc:creator>Yang Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.
Published: 2025-12-04T01:09:17+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongye Cao; Zhixin Bai; Ziyue Peng; Boyan Wang; Tianpei Yang; Jing Huo; Yuyao Zhang; Yang Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.&lt;/p&gt;</content:encoded></item><item><title>MSG-CLIP: Enhancing CLIP’s Ability to Learn Fine-grained Structural Associations through Multi-modal Scene Graph Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112794</link><guid>10.1016/j.patcog.2025.112794</guid><pubDate>Sat, 06 Dec 2025 23:14:20 +0000</pubDate><dc:creator>Xiaotian Lv</dc:creator><dc:creator>Yue Zhao</dc:creator><dc:creator>Hanlong Yin</dc:creator><dc:creator>Yifei Chen</dc:creator><dc:creator>Jianxing Liu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112794</prism:doi><description>As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.
Published: 2025-12-06T23:14:20+00:00
Venue: Pattern Recognition
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaotian Lv; Yue Zhao; Hanlong Yin; Yifei Chen; Jianxing Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112794"&gt;10.1016/j.patcog.2025.112794&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.&lt;/p&gt;</content:encoded></item><item><title>PreciseVideo: A Dual-Process Framework for Zero-Shot Text-to-Video Generation with Quantitative Content Control</title><link>https://doi.org/10.1016/j.inffus.2025.104030</link><guid>10.1016/j.inffus.2025.104030</guid><pubDate>Sat, 06 Dec 2025 00:36:18 +0000</pubDate><dc:creator>Lizhi Dang</dc:creator><dc:creator>Ting Liang</dc:creator><dc:creator>Huixin Zhang</dc:creator><dc:creator>Ruihao Zhang</dc:creator><dc:creator>Yingping Hong</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104030</prism:doi><description>Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .
Published: 2025-12-06T00:36:18+00:00
Venue: Information Fusion
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lizhi Dang; Ting Liang; Huixin Zhang; Ruihao Zhang; Yingping Hong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104030"&gt;10.1016/j.inffus.2025.104030&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .&lt;/p&gt;</content:encoded></item><item><title>Algorithmic Thinking Theory</title><link>https://arxiv.org/abs/2512.04923v1</link><guid>http://arxiv.org/abs/2512.04923v1</guid><pubDate>Thu, 04 Dec 2025 15:55:55 +0000</pubDate><dc:creator>MohammadHossein Bateni</dc:creator><dc:creator>Vincent Cohen-Addad</dc:creator><dc:creator>Yuzhou Gu</dc:creator><dc:creator>Silvio Lattanzi</dc:creator><dc:creator>Simon Meierhans</dc:creator><dc:creator>Christopher Mohri</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.
  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.
Published: 2025-12-04T15:55:55+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; MohammadHossein Bateni; Vincent Cohen-Addad; Yuzhou Gu; Silvio Lattanzi; Simon Meierhans; Christopher Mohri&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.
  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.&lt;/p&gt;</content:encoded></item><item><title>Learning to Orchestrate Agents in Natural Language with the Conductor</title><link>https://arxiv.org/abs/2512.04388v1</link><guid>http://arxiv.org/abs/2512.04388v1</guid><pubDate>Thu, 04 Dec 2025 02:23:13 +0000</pubDate><dc:creator>Stefan Nielsen</dc:creator><dc:creator>Edoardo Cetin</dc:creator><dc:creator>Peter Schwendeman</dc:creator><dc:creator>Qi Sun</dc:creator><dc:creator>Jinglue Xu</dc:creator><dc:creator>Yujin Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.
Published: 2025-12-04T02:23:13+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Stefan Nielsen; Edoardo Cetin; Peter Schwendeman; Qi Sun; Jinglue Xu; Yujin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.&lt;/p&gt;</content:encoded></item><item><title>Rethinking Infrared Small Target Detection: A Foundation-Driven Efficient Paradigm</title><link>https://arxiv.org/abs/2512.05511v1</link><guid>http://arxiv.org/abs/2512.05511v1</guid><pubDate>Fri, 05 Dec 2025 08:12:35 +0000</pubDate><dc:creator>Chuang Yu</dc:creator><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Yaokun Li</dc:creator><dc:creator>Xiujun Shu</dc:creator><dc:creator>Yuanhao Feng</dc:creator><dc:creator>Bo Wang</dc:creator><dc:creator>Yimian Dai</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework
Published: 2025-12-05T08:12:35+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuang Yu; Jinmiao Zhao; Yunpeng Liu; Yaokun Li; Xiujun Shu; Yuanhao Feng; Bo Wang; Yimian Dai; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework&lt;/p&gt;</content:encoded></item><item><title>KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity</title><link>https://arxiv.org/abs/2512.05916v1</link><guid>http://arxiv.org/abs/2512.05916v1</guid><pubDate>Fri, 05 Dec 2025 17:51:10 +0000</pubDate><dc:creator>Damien Lesens</dc:creator><dc:creator>Beheshteh T. Rakhshan</dc:creator><dc:creator>Guillaume Rabusseau</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.
Published: 2025-12-05T17:51:10+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Damien Lesens; Beheshteh T. Rakhshan; Guillaume Rabusseau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.&lt;/p&gt;</content:encoded></item><item><title>TRINITY: An Evolved LLM Coordinator</title><link>https://arxiv.org/abs/2512.04695v1</link><guid>http://arxiv.org/abs/2512.04695v1</guid><pubDate>Thu, 04 Dec 2025 11:45:21 +0000</pubDate><dc:creator>Jinglue Xu</dc:creator><dc:creator>Qi Sun</dc:creator><dc:creator>Peter Schwendeman</dc:creator><dc:creator>Stefan Nielsen</dc:creator><dc:creator>Edoardo Cetin</dc:creator><dc:creator>Yujin Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately $0.6$B parameters) and a lightweight head (approximately $10$K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator's hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.
Published: 2025-12-04T11:45:21+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinglue Xu; Qi Sun; Peter Schwendeman; Stefan Nielsen; Edoardo Cetin; Yujin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately $0.6$B parameters) and a lightweight head (approximately $10$K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator&amp;#x27;s hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.&lt;/p&gt;</content:encoded></item><item><title>Classifier Guidance and Domain Cooperation for Multisource Unsupervised Domain Adaptation</title><link>https://doi.org/10.1016/j.knosys.2025.115057</link><guid>10.1016/j.knosys.2025.115057</guid><pubDate>Sun, 07 Dec 2025 23:04:51 +0000</pubDate><dc:creator>Ming Zhao</dc:creator><dc:creator>Yifan Lan</dc:creator><dc:creator>Yuwu Lu</dc:creator><dc:creator>Leyao Yuan</dc:creator><dc:creator>Wenmeng Zhang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115057</prism:doi><description>Traditional deep learning models usually rely on identical data distributions for their training and testing sets. Unfortunately, this assumption rarely holds in real-world applications. This challenge has led to the proposal of unsupervised domain adaptation (UDA). While centred on transfers between single source and target domains, most UDA methods do not capitalize on the wealth of labelled resources that are present across multiple source domains. This gap is addressed herein by focusing on multisource unsupervised domain adaptation (MUDA), where knowledge is transferred from multiple source domains to an unlabelled target domain. The existing MUDA methods focus primarily on reducing discrepancies between the source and target domains and often overlook the goal of improving the performance of the utilized classifier across different domains, which is equally crucial for effectively implementing domain adaptation. To overcome this limitation, we propose a novel approach called classifier guidance and domain cooperation (CGDC). Our method calculates the accuracy of a classifier in the target domain using a probability-inspired approach that considers data transformation costs and classifier precision in the high-level feature space of the target domain. This enables high-performance classifiers to guide classifiers with lower performance. Additionally, we introduce a mixup-based multi-information fusion strategy to create enriched domains by combining information derived from the source and target domains. A comparison with many methods on public datasets shows the effectiveness of our CGDC approach in image classification tasks. Specifically, CGDC achieves improvements of 4.3% on Office-Home and 1.9% on Office-31 over the best baseline methods.
Published: 2025-12-07T23:04:51+00:00
Venue: Knowledge-Based Systems
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Zhao; Yifan Lan; Yuwu Lu; Leyao Yuan; Wenmeng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115057"&gt;10.1016/j.knosys.2025.115057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Traditional deep learning models usually rely on identical data distributions for their training and testing sets. Unfortunately, this assumption rarely holds in real-world applications. This challenge has led to the proposal of unsupervised domain adaptation (UDA). While centred on transfers between single source and target domains, most UDA methods do not capitalize on the wealth of labelled resources that are present across multiple source domains. This gap is addressed herein by focusing on multisource unsupervised domain adaptation (MUDA), where knowledge is transferred from multiple source domains to an unlabelled target domain. The existing MUDA methods focus primarily on reducing discrepancies between the source and target domains and often overlook the goal of improving the performance of the utilized classifier across different domains, which is equally crucial for effectively implementing domain adaptation. To overcome this limitation, we propose a novel approach called classifier guidance and domain cooperation (CGDC). Our method calculates the accuracy of a classifier in the target domain using a probability-inspired approach that considers data transformation costs and classifier precision in the high-level feature space of the target domain. This enables high-performance classifiers to guide classifiers with lower performance. Additionally, we introduce a mixup-based multi-information fusion strategy to create enriched domains by combining information derived from the source and target domains. A comparison with many methods on public datasets shows the effectiveness of our CGDC approach in image classification tasks. Specifically, CGDC achieves improvements of 4.3% on Office-Home and 1.9% on Office-31 over the best baseline methods.&lt;/p&gt;</content:encoded></item><item><title>SparseLight: Dynamic Gradient-Optimized Softmax for Efficient Transformer Acceleration</title><link>https://doi.org/10.1016/j.knosys.2025.115050</link><guid>10.1016/j.knosys.2025.115050</guid><pubDate>Sat, 06 Dec 2025 00:19:58 +0000</pubDate><dc:creator>Kai Zhang</dc:creator><dc:creator>Chaoxiang Lan</dc:creator><dc:creator>Yazhang Xu</dc:creator><dc:creator>Zheyang Li</dc:creator><dc:creator>Wenming Tan</dc:creator><dc:creator>Ye Ren</dc:creator><dc:creator>Jilin Hu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115050</prism:doi><description>Benefiting from the self-attention mechanism’s capability to capture long-range dependencies, Transformer has achieved outstanding performance across various tasks such as NLP, computer vision, and speech tasks. Moreover, transformer-based Large Language Models (LLMs) have driven significant advancements in artificial intelligence. Within the self-attention mechanism, the softmax function plays a critical role in capturing the association of different tokens. However, hardware implementation of softmax is computationally expensive due to its exponential and division operations, especially for edge devices. Specifically, for long sequence inputs (e.g., over 8192 tokens), softmax accounts for over 23.5% of total computational time. In this paper, we propose SparseLight, a dynamic gradient-optimized sparse softmax method, which can significantly accelerate the inference of Transformers. To mitigate performance degradation caused by direct sparse softmax application, we formulate softmax sparsity as a mathematical optimization problem and solve it via gradient descent algorithm. Regrettably, Transformer often exhibits significant distributional differences between channels and tokens, leading to gradient optimization focusing excessively on outliers and causing performance drops. To tackle this issue, a balanced strategy is introduced to diminish the effect of outliers across channels and tokens. Theoretical analysis based on the condition number of the Fisher information matrix demonstrates the effectiveness of our approach. Extensive experiments on vision and language tasks show that SparseLight serves as an efficient drop-in replacement for standard softmax. Evaluations on GPUs demonstrate that SparseLight achieves a 18% speedup on LLaMA2-7B, highlighting its potential for real-world deployment. The code will be publicly available soon.
Published: 2025-12-06T00:19:58+00:00
Venue: Knowledge-Based Systems
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kai Zhang; Chaoxiang Lan; Yazhang Xu; Zheyang Li; Wenming Tan; Ye Ren; Jilin Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115050"&gt;10.1016/j.knosys.2025.115050&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Benefiting from the self-attention mechanism’s capability to capture long-range dependencies, Transformer has achieved outstanding performance across various tasks such as NLP, computer vision, and speech tasks. Moreover, transformer-based Large Language Models (LLMs) have driven significant advancements in artificial intelligence. Within the self-attention mechanism, the softmax function plays a critical role in capturing the association of different tokens. However, hardware implementation of softmax is computationally expensive due to its exponential and division operations, especially for edge devices. Specifically, for long sequence inputs (e.g., over 8192 tokens), softmax accounts for over 23.5% of total computational time. In this paper, we propose SparseLight, a dynamic gradient-optimized sparse softmax method, which can significantly accelerate the inference of Transformers. To mitigate performance degradation caused by direct sparse softmax application, we formulate softmax sparsity as a mathematical optimization problem and solve it via gradient descent algorithm. Regrettably, Transformer often exhibits significant distributional differences between channels and tokens, leading to gradient optimization focusing excessively on outliers and causing performance drops. To tackle this issue, a balanced strategy is introduced to diminish the effect of outliers across channels and tokens. Theoretical analysis based on the condition number of the Fisher information matrix demonstrates the effectiveness of our approach. Extensive experiments on vision and language tasks show that SparseLight serves as an efficient drop-in replacement for standard softmax. Evaluations on GPUs demonstrate that SparseLight achieves a 18% speedup on LLaMA2-7B, highlighting its potential for real-world deployment. The code will be publicly available soon.&lt;/p&gt;</content:encoded></item><item><title>MAdaKron: a Mixture-of-AdaKron Adapters</title><link>https://doi.org/10.1016/j.knosys.2025.115086</link><guid>10.1016/j.knosys.2025.115086</guid><pubDate>Sat, 06 Dec 2025 23:20:27 +0000</pubDate><dc:creator>Marco Braga</dc:creator><dc:creator>Alessandro Raganato</dc:creator><dc:creator>Gabriella Pasi</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115086</prism:doi><description>Adapting pre-trained Large Language Models to specific tasks has traditionally involved updating all of their parameters. Nonetheless, this technique becomes impractical for models containing billions of parameters. This has led to intensive research on Parameter-Efficient Fine-Tuning (PEFT) techniques, which aim to train a small fraction of the model’s parameters while maintaining comparable performance to Full Fine-Tuning. A popular method is the Adapter, i.e. small trainable layers added to pre-trained models. Recently, we present AdaKron, an Adapter-based PEFT technique, which leverages the Kronecker product to combine the outputs of two small networks, training less than 0.55% of the model’s parameters while outperforming Full Fine-Tuning. In this paper, we put forward a novel technique, called MAdaKron, a Mixture-of-AdaKron model, which combines AdaKron with a Mixture of Experts approach. MAdaKron combines the flexibility of a Mixture of Experts architecture with the efficiency given by AdaKron to further enhance its performance. We then extensively evaluate MAdaKron on eighteen Natural Language Understanding and Generation benchmarks, showing that it achieves performance on par or even better than recent state-of-the-art PEFT methods, while reducing the number of trainable parameters. These findings highlight MAdaKron as an efficient solution for Fine-Tuning LLMs, offering substantial computational cost reductions without losing performance.
Published: 2025-12-06T23:20:27+00:00
Venue: Knowledge-Based Systems
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Marco Braga; Alessandro Raganato; Gabriella Pasi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115086"&gt;10.1016/j.knosys.2025.115086&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Adapting pre-trained Large Language Models to specific tasks has traditionally involved updating all of their parameters. Nonetheless, this technique becomes impractical for models containing billions of parameters. This has led to intensive research on Parameter-Efficient Fine-Tuning (PEFT) techniques, which aim to train a small fraction of the model’s parameters while maintaining comparable performance to Full Fine-Tuning. A popular method is the Adapter, i.e. small trainable layers added to pre-trained models. Recently, we present AdaKron, an Adapter-based PEFT technique, which leverages the Kronecker product to combine the outputs of two small networks, training less than 0.55% of the model’s parameters while outperforming Full Fine-Tuning. In this paper, we put forward a novel technique, called MAdaKron, a Mixture-of-AdaKron model, which combines AdaKron with a Mixture of Experts approach. MAdaKron combines the flexibility of a Mixture of Experts architecture with the efficiency given by AdaKron to further enhance its performance. We then extensively evaluate MAdaKron on eighteen Natural Language Understanding and Generation benchmarks, showing that it achieves performance on par or even better than recent state-of-the-art PEFT methods, while reducing the number of trainable parameters. These findings highlight MAdaKron as an efficient solution for Fine-Tuning LLMs, offering substantial computational cost reductions without losing performance.&lt;/p&gt;</content:encoded></item><item><title>LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</title><link>https://arxiv.org/abs/2512.05663v1</link><guid>http://arxiv.org/abs/2512.05663v1</guid><pubDate>Fri, 05 Dec 2025 12:08:18 +0000</pubDate><dc:creator>Johannes Meier</dc:creator><dc:creator>Jonathan Michel</dc:creator><dc:creator>Oussema Dhaouadi</dc:creator><dc:creator>Yung-Hsu Yang</dc:creator><dc:creator>Christoph Reich</dc:creator><dc:creator>Zuria Bauer</dc:creator><dc:creator>Stefan Roth</dc:creator><dc:creator>Marc Pollefeys</dc:creator><dc:creator>Jacques Kaiser</dc:creator><dc:creator>Daniel Cremers</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.
Published: 2025-12-05T12:08:18+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Johannes Meier; Jonathan Michel; Oussema Dhaouadi; Yung-Hsu Yang; Christoph Reich; Zuria Bauer; Stefan Roth; Marc Pollefeys; Jacques Kaiser; Daniel Cremers&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.&lt;/p&gt;</content:encoded></item><item><title>MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models</title><link>https://arxiv.org/abs/2512.05530v1</link><guid>http://arxiv.org/abs/2512.05530v1</guid><pubDate>Fri, 05 Dec 2025 08:41:44 +0000</pubDate><dc:creator>Chuang Yu</dc:creator><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Mingxuan Zhao</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Xiujun Shu</dc:creator><dc:creator>Yuanhao Feng</dc:creator><dc:creator>Bo Wang</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -&gt; Rethink -&gt; Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND
Published: 2025-12-05T08:41:44+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuang Yu; Jinmiao Zhao; Mingxuan Zhao; Yunpeng Liu; Xiujun Shu; Yuanhao Feng; Bo Wang; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of &amp;quot;Understand -&amp;gt; Rethink -&amp;gt; Correct&amp;quot;, and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND&lt;/p&gt;</content:encoded></item><item><title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title><link>https://arxiv.org/abs/2512.04581v1</link><guid>http://arxiv.org/abs/2512.04581v1</guid><pubDate>Thu, 04 Dec 2025 08:49:23 +0000</pubDate><dc:creator>Houzhang Fang</dc:creator><dc:creator>Chenxing Wu</dc:creator><dc:creator>Kun Bai</dc:creator><dc:creator>Tianqi Chen</dc:creator><dc:creator>Xiaolin Wang</dc:creator><dc:creator>Xiyang Liu</dc:creator><dc:creator>Yi Chang</dc:creator><dc:creator>Luxin Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
Published: 2025-12-04T08:49:23+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Houzhang Fang; Chenxing Wu; Kun Bai; Tianqi Chen; Xiaolin Wang; Xiyang Liu; Yi Chang; Luxin Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network&amp;#x27;s focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.&lt;/p&gt;</content:encoded></item><item><title>Lightweight Image Super-Resolution Network with Adaptive Token Selection and Feature Enhancement</title><link>https://doi.org/10.1016/j.knosys.2025.115055</link><guid>10.1016/j.knosys.2025.115055</guid><pubDate>Sat, 06 Dec 2025 16:09:36 +0000</pubDate><dc:creator>Detian Huang</dc:creator><dc:creator>Mingxin Lin</dc:creator><dc:creator>Xinwei Gan</dc:creator><dc:creator>Luanyuan Dai</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115055</prism:doi><description>Transformer has demonstrated remarkable performance in image super-resolution due to its powerful long-range dependency modeling. However, existing transformer-based methods face two challenges. First, their self-attention mechanisms often treat all tokens equally, introducing redundant correlations and noise. Second, the use of window-based attention restricts the receptive field, hindering effective long-range dependency modeling. To address these issues, we introduce TSFE, a lightweight super-resolution network with adaptive Token Selection and Feature Enhancement. Specifically, we design a Token Selection Channel self-Attention (TSCA) that adaptively selects the top- K most relevant tokens along the channel dimension, preserving critical features while reducing redundant computations. Then, we present a Sparse Spatial Self-Attention (SSSA) that enhances critical features by suppressing low-attention weights and amplifying high-attention ones in the spatial dimension. To expand the receptive field, SSSA integrates a prior dictionary that guides the attention distribution with external prior, improving its dependency modeling capability. The collaboration of TSCA and SSSA components enables TSFE to achieve precise self-attention computation with low complexity. Extensive experiments validate that the proposed TSFE outperforms state-of-the-art methods across benchmarks.
Published: 2025-12-06T16:09:36+00:00
Venue: Knowledge-Based Systems
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Detian Huang; Mingxin Lin; Xinwei Gan; Luanyuan Dai; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115055"&gt;10.1016/j.knosys.2025.115055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer has demonstrated remarkable performance in image super-resolution due to its powerful long-range dependency modeling. However, existing transformer-based methods face two challenges. First, their self-attention mechanisms often treat all tokens equally, introducing redundant correlations and noise. Second, the use of window-based attention restricts the receptive field, hindering effective long-range dependency modeling. To address these issues, we introduce TSFE, a lightweight super-resolution network with adaptive Token Selection and Feature Enhancement. Specifically, we design a Token Selection Channel self-Attention (TSCA) that adaptively selects the top- K most relevant tokens along the channel dimension, preserving critical features while reducing redundant computations. Then, we present a Sparse Spatial Self-Attention (SSSA) that enhances critical features by suppressing low-attention weights and amplifying high-attention ones in the spatial dimension. To expand the receptive field, SSSA integrates a prior dictionary that guides the attention distribution with external prior, improving its dependency modeling capability. The collaboration of TSCA and SSSA components enables TSFE to achieve precise self-attention computation with low complexity. Extensive experiments validate that the proposed TSFE outperforms state-of-the-art methods across benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Finder, Evaluator, Explainer, Generator (FEEG): A Bloom’s Taxonomy-Based Query Classification Framework for LLMs and Generative AI</title><link>https://doi.org/10.1016/j.eswa.2025.130755</link><guid>10.1016/j.eswa.2025.130755</guid><pubDate>Sun, 07 Dec 2025 23:05:07 +0000</pubDate><dc:creator>Jim Samuel</dc:creator><dc:creator>Meng Ye</dc:creator><dc:creator>Tanya Khanna</dc:creator><dc:creator>Yi Yao</dc:creator><dc:creator>Xiao Lin</dc:creator><dc:creator>Rick Anderson</dc:creator><dc:creator>Anish Gupta</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130755</prism:doi><description>Generative artificial intelligence (AI) and natural language processing (NLP) applications, particularly those utilizing large language models (LLMs) with Retrieval-Augmented Generation (RAG), fine-tuning and their variations, are receiving widespread attention. However, despite their promise, the dependability of LLM applications remains uncertain. This study introduces a new dimension to improving LLM performance through systematic query classification. A taxonomy-based approach was developed to categorize queries by their likelihood of eliciting accurate responses, from those likely to succeed to those prone to failure. Bloom’s Taxonomy (BT) provided the conceptual direction for assessing and classifying queries using a curated dataset of question-answer pairs mapped to BT levels to evaluate RAG-LLM performance across categories. Building on insights from BT-based analysis, an innovative ‘Taxonomical Query Classifier’ (TQC) framework for LLMs is introduced: Finder (F), Evaluator (Ev), Explainer (Ex), and Generator (G) - collectively termed FEEG. FEEG-TQC classifies queries to improve predictability, accuracy, and enhance human-AI interaction. It differentiates queries based on intent, whether they (F) seek factual information, (Ev) evaluate content, (Ex) explain concepts, or (G) generate new content. By developing the concept of query category variations, this research provides a systematic approach to managing query quality and predictive estimations of LLM response accuracy. The findings contribute to the development of more effective human-AI interaction opportunities for LLM-based Gen AI systems, emphasizing query classification as an important factor in improving Gen AI systems.
Published: 2025-12-07T23:05:07+00:00
Venue: Expert Systems with Applications
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jim Samuel; Meng Ye; Tanya Khanna; Yi Yao; Xiao Lin; Rick Anderson; Anish Gupta&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130755"&gt;10.1016/j.eswa.2025.130755&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Generative artificial intelligence (AI) and natural language processing (NLP) applications, particularly those utilizing large language models (LLMs) with Retrieval-Augmented Generation (RAG), fine-tuning and their variations, are receiving widespread attention. However, despite their promise, the dependability of LLM applications remains uncertain. This study introduces a new dimension to improving LLM performance through systematic query classification. A taxonomy-based approach was developed to categorize queries by their likelihood of eliciting accurate responses, from those likely to succeed to those prone to failure. Bloom’s Taxonomy (BT) provided the conceptual direction for assessing and classifying queries using a curated dataset of question-answer pairs mapped to BT levels to evaluate RAG-LLM performance across categories. Building on insights from BT-based analysis, an innovative ‘Taxonomical Query Classifier’ (TQC) framework for LLMs is introduced: Finder (F), Evaluator (Ev), Explainer (Ex), and Generator (G) - collectively termed FEEG. FEEG-TQC classifies queries to improve predictability, accuracy, and enhance human-AI interaction. It differentiates queries based on intent, whether they (F) seek factual information, (Ev) evaluate content, (Ex) explain concepts, or (G) generate new content. By developing the concept of query category variations, this research provides a systematic approach to managing query quality and predictive estimations of LLM response accuracy. The findings contribute to the development of more effective human-AI interaction opportunities for LLM-based Gen AI systems, emphasizing query classification as an important factor in improving Gen AI systems.&lt;/p&gt;</content:encoded></item><item><title>EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</title><link>https://arxiv.org/abs/2512.04810v2</link><guid>http://arxiv.org/abs/2512.04810v2</guid><pubDate>Thu, 04 Dec 2025 14:01:53 +0000</pubDate><dc:creator>Xin He</dc:creator><dc:creator>Longhui Wei</dc:creator><dc:creator>Jianbo Ouyang</dc:creator><dc:creator>Lingxi Xie</dc:creator><dc:creator>Qi Tian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.
Published: 2025-12-04T14:01:53+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin He; Longhui Wei; Jianbo Ouyang; Lingxi Xie; Qi Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.&lt;/p&gt;</content:encoded></item><item><title>RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs</title><link>https://arxiv.org/abs/2512.05542v1</link><guid>http://arxiv.org/abs/2512.05542v1</guid><pubDate>Fri, 05 Dec 2025 08:55:39 +0000</pubDate><dc:creator>Jonathan Geuter</dc:creator><dc:creator>Gregor Kornhardt</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.
Published: 2025-12-05T08:55:39+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jonathan Geuter; Gregor Kornhardt&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.&lt;/p&gt;</content:encoded></item><item><title>Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models</title><link>https://arxiv.org/abs/2512.04395v1</link><guid>http://arxiv.org/abs/2512.04395v1</guid><pubDate>Thu, 04 Dec 2025 02:32:55 +0000</pubDate><dc:creator>Hieu Dinh Trung Pham</dc:creator><dc:creator>Huy Minh Nhat Nguyen</dc:creator><dc:creator>Cuong Tuan Nguyen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.
Published: 2025-12-04T02:32:55+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hieu Dinh Trung Pham; Huy Minh Nhat Nguyen; Cuong Tuan Nguyen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image&amp;#x27;s domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image&amp;#x27;s structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.&lt;/p&gt;</content:encoded></item><item><title>Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark</title><link>https://arxiv.org/abs/2512.05091v1</link><guid>http://arxiv.org/abs/2512.05091v1</guid><pubDate>Thu, 04 Dec 2025 18:55:34 +0000</pubDate><dc:creator>Haobo Yuan</dc:creator><dc:creator>Yueyi Sun</dc:creator><dc:creator>Yanwei Li</dc:creator><dc:creator>Tao Zhang</dc:creator><dc:creator>Xueqing Deng</dc:creator><dc:creator>Henghui Ding</dc:creator><dc:creator>Lu Qi</dc:creator><dc:creator>Anran Wang</dc:creator><dc:creator>Xiangtai Li</dc:creator><dc:creator>Ming-Hsuan Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.
Published: 2025-12-04T18:55:34+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haobo Yuan; Yueyi Sun; Yanwei Li; Tao Zhang; Xueqing Deng; Henghui Ding; Lu Qi; Anran Wang; Xiangtai Li; Ming-Hsuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.&lt;/p&gt;</content:encoded></item><item><title>Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space</title><link>https://arxiv.org/abs/2512.04601v1</link><guid>http://arxiv.org/abs/2512.04601v1</guid><pubDate>Thu, 04 Dec 2025 09:21:44 +0000</pubDate><dc:creator>Joey Hong</dc:creator><dc:creator>Kang Liu</dc:creator><dc:creator>Zhan Ling</dc:creator><dc:creator>Jiecao Chen</dc:creator><dc:creator>Sergey Levine</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.
Published: 2025-12-04T09:21:44+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Joey Hong; Kang Liu; Zhan Ling; Jiecao Chen; Sergey Levine&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.&lt;/p&gt;</content:encoded></item><item><title>Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity</title><link>https://arxiv.org/abs/2512.05962v1</link><guid>http://arxiv.org/abs/2512.05962v1</guid><pubDate>Fri, 05 Dec 2025 18:56:40 +0000</pubDate><dc:creator>Germán Kruszewski</dc:creator><dc:creator>Pierre Erbacher</dc:creator><dc:creator>Jos Rozen</dc:creator><dc:creator>Marc Dymetman</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.
Published: 2025-12-05T18:56:40+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Germán Kruszewski; Pierre Erbacher; Jos Rozen; Marc Dymetman&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the &amp;quot;mode-seeking&amp;quot; or &amp;quot;zero-forcing&amp;quot; Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.&lt;/p&gt;</content:encoded></item><item><title>Source-Free Domain Adaptation via Multimodal Space-Guided Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112827</link><guid>10.1016/j.patcog.2025.112827</guid><pubDate>Sun, 07 Dec 2025 15:13:39 +0000</pubDate><dc:creator>Lijuan Chen</dc:creator><dc:creator>Yunxiang Bai</dc:creator><dc:creator>Ying Hu</dc:creator><dc:creator>Qiong Wang</dc:creator><dc:creator>Xiaozhi Qi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112827</prism:doi><description>Conventional UDA requires access to the source domain, invalidating it in the information security and privacy protection scenarios. In contrast, Source-free Domain Adaptation (SFDA) involves transferring a pre-trained source model to an unlabeled target domain while the source data is absent. However, prior methods based on self-supervised learning have struggled to find a quality domain invariant representation space due to the lack of source data. To address this challenge, in this work, we propose leveraging the success of vision-language pre-trained (ViL) models (e.g., CLIP). To integrate the domain generality of the ViL model and the task specificity of source model more effectively, we introduce a novel M ulti M odal Space- G uided A lignment ( MMGA ) approach. Specifically, we start with a multimodal feature calibration for achieving coarse alignment between the target visual domain and the multimodal space. However, this ViL space is still not the domain invariant space, being trained on a large number of samples. To achieve further fine-grained alignment towards the domain invariant space, we have designed two methods: Potential category consistency and prediction consistency alignment. These methods push the potential categories distribution and the prediction distribution closer to the fused pseudo-supervision by the ViL model and the adapted source model, respectively. This strategy corrects the errors of feature alignment to the ViL space. Extensive experiments show that our MMGA approach significantly outperforms current state-of-the-art alternatives. The code and data are available at https://github.com/YunxiangBai0/MMGA/
Published: 2025-12-07T15:13:39+00:00
Venue: Pattern Recognition
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lijuan Chen; Yunxiang Bai; Ying Hu; Qiong Wang; Xiaozhi Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112827"&gt;10.1016/j.patcog.2025.112827&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Conventional UDA requires access to the source domain, invalidating it in the information security and privacy protection scenarios. In contrast, Source-free Domain Adaptation (SFDA) involves transferring a pre-trained source model to an unlabeled target domain while the source data is absent. However, prior methods based on self-supervised learning have struggled to find a quality domain invariant representation space due to the lack of source data. To address this challenge, in this work, we propose leveraging the success of vision-language pre-trained (ViL) models (e.g., CLIP). To integrate the domain generality of the ViL model and the task specificity of source model more effectively, we introduce a novel M ulti M odal Space- G uided A lignment ( MMGA ) approach. Specifically, we start with a multimodal feature calibration for achieving coarse alignment between the target visual domain and the multimodal space. However, this ViL space is still not the domain invariant space, being trained on a large number of samples. To achieve further fine-grained alignment towards the domain invariant space, we have designed two methods: Potential category consistency and prediction consistency alignment. These methods push the potential categories distribution and the prediction distribution closer to the fused pseudo-supervision by the ViL model and the adapted source model, respectively. This strategy corrects the errors of feature alignment to the ViL space. Extensive experiments show that our MMGA approach significantly outperforms current state-of-the-art alternatives. The code and data are available at https://github.com/YunxiangBai0/MMGA/&lt;/p&gt;</content:encoded></item><item><title>Sparse Attention Post-Training for Mechanistic Interpretability</title><link>https://arxiv.org/abs/2512.05865v1</link><guid>http://arxiv.org/abs/2512.05865v1</guid><pubDate>Fri, 05 Dec 2025 16:40:08 +0000</pubDate><dc:creator>Florent Draye</dc:creator><dc:creator>Anson Lei</dc:creator><dc:creator>Ingmar Posner</dc:creator><dc:creator>Bernhard Schölkopf</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.
Published: 2025-12-05T16:40:08+00:00
Venue: arXiv
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Florent Draye; Anson Lei; Ingmar Posner; Bernhard Schölkopf&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.&lt;/p&gt;</content:encoded></item><item><title>Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection</title><link>https://arxiv.org/abs/2512.04413v1</link><guid>http://arxiv.org/abs/2512.04413v1</guid><pubDate>Thu, 04 Dec 2025 03:18:42 +0000</pubDate><dc:creator>Xiangyi Gao</dc:creator><dc:creator>Danpei Zhao</dc:creator><dc:creator>Bo Yuan</dc:creator><dc:creator>Wentao Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TGRS.2025.3600098</prism:doi><description>Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.
Published: 2025-12-04T03:18:42+00:00
Venue: arXiv
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyi Gao; Danpei Zhao; Bo Yuan; Wentao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TGRS.2025.3600098"&gt;10.1109/TGRS.2025.3600098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.&lt;/p&gt;</content:encoded></item><item><title>GTM: Simulating the World of Tools for AI Agents</title><link>https://arxiv.org/abs/2512.04535v2</link><guid>http://arxiv.org/abs/2512.04535v2</guid><pubDate>Thu, 04 Dec 2025 07:33:04 +0000</pubDate><dc:creator>Zhenzhen Ren</dc:creator><dc:creator>Xinpeng Zhang</dc:creator><dc:creator>Zhenxing Qian</dc:creator><dc:creator>Yan Gao</dc:creator><dc:creator>Yu Shi</dc:creator><dc:creator>Shuxin Zheng</dc:creator><dc:creator>Jiyan He</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.
Published: 2025-12-04T07:33:04+00:00
Venue: arXiv
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenzhen Ren; Xinpeng Zhang; Zhenxing Qian; Yan Gao; Yu Shi; Shuxin Zheng; Jiyan He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.&lt;/p&gt;</content:encoded></item><item><title>GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis</title><link>https://arxiv.org/abs/2512.04456v1</link><guid>http://arxiv.org/abs/2512.04456v1</guid><pubDate>Thu, 04 Dec 2025 05:00:00 +0000</pubDate><dc:creator>Changjin Kim</dc:creator><dc:creator>HyeokJun Lee</dc:creator><dc:creator>YoungJoon Yoo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.
Published: 2025-12-04T05:00:00+00:00
Venue: arXiv
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changjin Kim; HyeokJun Lee; YoungJoon Yoo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model&amp;#x27;s backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.&lt;/p&gt;</content:encoded></item><item><title>PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering</title><link>https://arxiv.org/abs/2512.05336v1</link><guid>http://arxiv.org/abs/2512.05336v1</guid><pubDate>Fri, 05 Dec 2025 00:33:31 +0000</pubDate><dc:creator>Durga Prasad Maram</dc:creator><dc:creator>Kalpa Gunaratna</dc:creator><dc:creator>Vijay Srinivasan</dc:creator><dc:creator>Haris Jeelani</dc:creator><dc:creator>Srinivas Chappidi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.
Published: 2025-12-05T00:33:31+00:00
Venue: arXiv
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Durga Prasad Maram; Kalpa Gunaratna; Vijay Srinivasan; Haris Jeelani; Srinivas Chappidi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.&lt;/p&gt;</content:encoded></item></channel></rss>