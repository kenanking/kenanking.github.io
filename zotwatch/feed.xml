<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 12 Dec 2025 02:55:48 +0000</lastBuildDate><item><title>Towards Efficient Semi-Supervised Object Detection with Detection Transformer</title><link>https://doi.org/10.1109/tpami.2025.3642123</link><guid>10.1109/tpami.2025.3642123</guid><pubDate>Wed, 10 Dec 2025 18:32:22 +0000</pubDate><dc:creator>Jiacheng Zhang</dc:creator><dc:creator>Jiaming Li</dc:creator><dc:creator>Xiangru Lin</dc:creator><dc:creator>Wei Zhang</dc:creator><dc:creator>Xiao Tan</dc:creator><dc:creator>Hongbo Gao</dc:creator><dc:creator>Jingdong Wang</dc:creator><dc:creator>Guanbin Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642123</prism:doi><description>Semi-supervised object detection (SSOD) mitigates the annotation burden in object detection by leveraging unlabeled data, providing a scalable solution for modern perception systems. Concurrently, detection transformers (DETRs) have emerged as a popular end-to-end framework, offering advantages such as non-maximum suppression (NMS)-free inference. However, existing SSODmethods are predominantly designed for conventional detectors, leaving the exploration of DETR-based SSOD largely uncharted. This paper presents a systematic study to bridge this gap. We begin by identifying two principal obstacles in semi-supervised DETR training: (1) the inherent one-to-one assignment mechanism of DETRs is highly sensitive to noisy pseudo-labels, which impedes training efficiency; and (2) the query-based decoder architecture complicates the design of an effective consistency regularization scheme, limiting further performance gains. To address these challenges, we propose Semi-DETR++, a novel framework for efficient SSOD with DETRs. Our approach introduces a stage-wise hybrid matching strategy that enhances robustness to noisy pseudo-labels by synergistically combining one-to-many and one-to-one assignments while preserving NMS-free inference. Furthermore, based on our observation of the unique layer-wise decoding behavior in DETRs, we develop a simple yet effective re-decode query consistency training method to regularize the decoder. Extensive experiments demonstrate that Semi-DETR++ enables more efficient semi-supervised learning across various DETR architectures, outperforming existing methods by significant margins. The proposed components are also flexible and versatile, showing superior generalization by readily extending to semi-supervised segmentation tasks. Code is available at https://github.com/JCZ404/Semi-DETR.
Published: 2025-12-10T18:32:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.840 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiacheng Zhang; Jiaming Li; Xiangru Lin; Wei Zhang; Xiao Tan; Hongbo Gao; Jingdong Wang; Guanbin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642123"&gt;10.1109/tpami.2025.3642123&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.840 (must_read)&lt;/p&gt;
&lt;p&gt;Semi-supervised object detection (SSOD) mitigates the annotation burden in object detection by leveraging unlabeled data, providing a scalable solution for modern perception systems. Concurrently, detection transformers (DETRs) have emerged as a popular end-to-end framework, offering advantages such as non-maximum suppression (NMS)-free inference. However, existing SSODmethods are predominantly designed for conventional detectors, leaving the exploration of DETR-based SSOD largely uncharted. This paper presents a systematic study to bridge this gap. We begin by identifying two principal obstacles in semi-supervised DETR training: (1) the inherent one-to-one assignment mechanism of DETRs is highly sensitive to noisy pseudo-labels, which impedes training efficiency; and (2) the query-based decoder architecture complicates the design of an effective consistency regularization scheme, limiting further performance gains. To address these challenges, we propose Semi-DETR++, a novel framework for efficient SSOD with DETRs. Our approach introduces a stage-wise hybrid matching strategy that enhances robustness to noisy pseudo-labels by synergistically combining one-to-many and one-to-one assignments while preserving NMS-free inference. Furthermore, based on our observation of the unique layer-wise decoding behavior in DETRs, we develop a simple yet effective re-decode query consistency training method to regularize the decoder. Extensive experiments demonstrate that Semi-DETR++ enables more efficient semi-supervised learning across various DETR architectures, outperforming existing methods by significant margins. The proposed components are also flexible and versatile, showing superior generalization by readily extending to semi-supervised segmentation tasks. Code is available at https://github.com/JCZ404/Semi-DETR.&lt;/p&gt;</content:encoded></item><item><title>Diverse semantic representation learning based on vision-language models for zero-shot indoor scene recognition</title><link>https://doi.org/10.1016/j.inffus.2025.104049</link><guid>10.1016/j.inffus.2025.104049</guid><pubDate>Wed, 10 Dec 2025 00:35:42 +0000</pubDate><dc:creator>Chen Wang</dc:creator><dc:creator>Guohua Peng</dc:creator><dc:creator>Bernard De Baets</dc:creator><dc:creator>Xiong Pan</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104049</prism:doi><description>Recently, vision-language models, such as the well-known CLIP, have demonstrated remarkable generalization ability in various zero-shot recognition tasks. However, their performance on zero-shot fine-grained recognition, especially for indoor scenes, remains limited due to the high between-class semantic similarity. To address this challenge, we propose a Diverse Semantic Representation Learning (DSRL) method based on the pre-trained vision-language model for zero-shot indoor scene recognition. Specifically, we first design a meaningful prompt text for indoor scene images to extract semantic features based on the CLIP text encoder. Then, in order to explore diverse visual-related semantic features, we introduce a visual-guided semantic feature learning method based on the CLIP image encoder, which refines the diverse visual prototypes through contrastive learning. Next, these features are fused by a multi-head attention fusion strategy, generating the diverse semantic representations. Finally, a dual reconstruction loss and a cross-entropy loss are constructed to facilitate knowledge transfer for zero-shot learning. In the testing phase, inspired by the convergent evolution theory, we revise the visual-guided semantic feature learning method to obtain the diverse semantic representations for unseen images. Extensive experiments on three indoor scene datasets demonstrate that DSRL achieves the state-of-the-art performance in zero-shot indoor scene recognition.
Published: 2025-12-10T00:35:42+00:00
Venue: Information Fusion
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Wang; Guohua Peng; Bernard De Baets; Xiong Pan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104049"&gt;10.1016/j.inffus.2025.104049&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, vision-language models, such as the well-known CLIP, have demonstrated remarkable generalization ability in various zero-shot recognition tasks. However, their performance on zero-shot fine-grained recognition, especially for indoor scenes, remains limited due to the high between-class semantic similarity. To address this challenge, we propose a Diverse Semantic Representation Learning (DSRL) method based on the pre-trained vision-language model for zero-shot indoor scene recognition. Specifically, we first design a meaningful prompt text for indoor scene images to extract semantic features based on the CLIP text encoder. Then, in order to explore diverse visual-related semantic features, we introduce a visual-guided semantic feature learning method based on the CLIP image encoder, which refines the diverse visual prototypes through contrastive learning. Next, these features are fused by a multi-head attention fusion strategy, generating the diverse semantic representations. Finally, a dual reconstruction loss and a cross-entropy loss are constructed to facilitate knowledge transfer for zero-shot learning. In the testing phase, inspired by the convergent evolution theory, we revise the visual-guided semantic feature learning method to obtain the diverse semantic representations for unseen images. Extensive experiments on three indoor scene datasets demonstrate that DSRL achieves the state-of-the-art performance in zero-shot indoor scene recognition.&lt;/p&gt;</content:encoded></item><item><title>Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach</title><link>https://doi.org/10.1109/tpami.2025.3642842</link><guid>10.1109/tpami.2025.3642842</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Jiayang Li</dc:creator><dc:creator>Chengjie Jiang</dc:creator><dc:creator>Junjun Jiang</dc:creator><dc:creator>Pengwei Liang</dc:creator><dc:creator>Jiayi Ma</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642842</prism:doi><description>Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, an instruction-driven Diffusion Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayang Li; Chengjie Jiang; Junjun Jiang; Pengwei Liang; Jiayi Ma; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642842"&gt;10.1109/tpami.2025.3642842&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, an instruction-driven Diffusion Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.&lt;/p&gt;</content:encoded></item><item><title>S2AFormer: Strip Self-Attention for Efficient Vision Transformer</title><link>https://doi.org/10.1109/tip.2025.3639919</link><guid>10.1109/tip.2025.3639919</guid><pubDate>Thu, 11 Dec 2025 18:48:12 +0000</pubDate><dc:creator>Guoan Xu</dc:creator><dc:creator>Wenfeng Huang</dc:creator><dc:creator>Wenjing Jia</dc:creator><dc:creator>Jiamao Li</dc:creator><dc:creator>Guangwei Gao</dc:creator><dc:creator>Guo-Jun Qi</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639919</prism:doi><description>The Vision Transformer (ViT) has achieved remarkable success in computer vision due to its powerful token mixer, which effectively captures global dependencies among all tokens. However, the quadratic complexity of standard self-attention with respect to the number of tokens severely hampers its computational efficiency in practical deployment. Although recent hybrid approaches have sought to combine the strengths of convolutions and self-attention to improve the performance–efficiency trade-off, the costly pairwise token interactions and heavy matrix operations in conventional self-attention remain a critical bottleneck. To overcome this limitation, we introduce S2AFormer, an efficient Vision Transformer architecture built around a novel Strip Self-Attention (SSA) mechanism. Our design incorporates lightweight yet effective Hybrid Perception Blocks (HPBs) that seamlessly fuse the local inductive biases of CNNs with the global modeling capability of Transformer-style attention. The core innovation of SSA lies in simultaneously reducing the spatial resolution of the key (K) and value (V) tensors while compressing the channel dimension of the query (Q) and key (K) tensors. This joint spatial-and-channel compression dramatically lowers computational cost without sacrificing representational power, achieving an excellent balance between accuracy and efficiency. We extensively evaluate S2AFormer on a wide range of vision tasks, including image classification (ImageNet-1K), semantic segmentation (ADE20K), and object detection/instance segmentation (COCO). Experimental results consistently show that S2AFormer delivers substantial accuracy improvements together with superior inference speed and throughput across both GPU and non-GPU platforms, establishing it as a highly competitive solution in the landscape of efficient Vision Transformers.
Published: 2025-12-11T18:48:12+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guoan Xu; Wenfeng Huang; Wenjing Jia; Jiamao Li; Guangwei Gao; Guo-Jun Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639919"&gt;10.1109/tip.2025.3639919&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;The Vision Transformer (ViT) has achieved remarkable success in computer vision due to its powerful token mixer, which effectively captures global dependencies among all tokens. However, the quadratic complexity of standard self-attention with respect to the number of tokens severely hampers its computational efficiency in practical deployment. Although recent hybrid approaches have sought to combine the strengths of convolutions and self-attention to improve the performance–efficiency trade-off, the costly pairwise token interactions and heavy matrix operations in conventional self-attention remain a critical bottleneck. To overcome this limitation, we introduce S2AFormer, an efficient Vision Transformer architecture built around a novel Strip Self-Attention (SSA) mechanism. Our design incorporates lightweight yet effective Hybrid Perception Blocks (HPBs) that seamlessly fuse the local inductive biases of CNNs with the global modeling capability of Transformer-style attention. The core innovation of SSA lies in simultaneously reducing the spatial resolution of the key (K) and value (V) tensors while compressing the channel dimension of the query (Q) and key (K) tensors. This joint spatial-and-channel compression dramatically lowers computational cost without sacrificing representational power, achieving an excellent balance between accuracy and efficiency. We extensively evaluate S2AFormer on a wide range of vision tasks, including image classification (ImageNet-1K), semantic segmentation (ADE20K), and object detection/instance segmentation (COCO). Experimental results consistently show that S2AFormer delivers substantial accuracy improvements together with superior inference speed and throughput across both GPU and non-GPU platforms, establishing it as a highly competitive solution in the landscape of efficient Vision Transformers.&lt;/p&gt;</content:encoded></item><item><title>A Survey of Small Sea-Surface Target Detection for Maritime Search and Rescue</title><link>https://doi.org/10.1109/tits.2025.3635199</link><guid>10.1109/tits.2025.3635199</guid><pubDate>Wed, 10 Dec 2025 18:34:12 +0000</pubDate><dc:creator>Jianchuan Yin</dc:creator><dc:creator>Guokang Xu</dc:creator><dc:creator>Ning Wang</dc:creator><dc:creator>Nini Wang</dc:creator><dc:creator>Zeguo Zhang</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3635199</prism:doi><description>The detection of small surface targets plays a critical role in maritime search and rescue (SAR) operations, ensuring the safety of people and property at sea. This paper provides a comprehensive review of the latest advancements and research in small sea surface target detection for maritime SAR missions. Deep learning-based models facilitate accurate target detection and localization by transforming image or video frames into high-dimensional abstract representations, enabling effective detection in complex sea surface environments. However, challenges such as occlusion, blurring, and reflections on the sea surface significantly complicate small target detection. To address these challenges, this paper summarizes a range of effective approaches, including context information, multi-scale learning, anchor-free detection, super-resolution, attention mechanisms, and sample-oriented approaches. These approaches aim to enhance the performance of small target detection in applications such as uncrewed aerial vehicles (UAV) and uncrewed supply vessels. Furthermore, this paper classifies small target datasets, providing a detailed overview based on their collection methods and application scenarios, while highlighting representative datasets. Through a thorough analysis of both methodologies and datasets, this paper offers valuable insights and directions for the future development of small target detection technology in maritime search and rescue operations.
Published: 2025-12-10T18:34:12+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianchuan Yin; Guokang Xu; Ning Wang; Nini Wang; Zeguo Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3635199"&gt;10.1109/tits.2025.3635199&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;The detection of small surface targets plays a critical role in maritime search and rescue (SAR) operations, ensuring the safety of people and property at sea. This paper provides a comprehensive review of the latest advancements and research in small sea surface target detection for maritime SAR missions. Deep learning-based models facilitate accurate target detection and localization by transforming image or video frames into high-dimensional abstract representations, enabling effective detection in complex sea surface environments. However, challenges such as occlusion, blurring, and reflections on the sea surface significantly complicate small target detection. To address these challenges, this paper summarizes a range of effective approaches, including context information, multi-scale learning, anchor-free detection, super-resolution, attention mechanisms, and sample-oriented approaches. These approaches aim to enhance the performance of small target detection in applications such as uncrewed aerial vehicles (UAV) and uncrewed supply vessels. Furthermore, this paper classifies small target datasets, providing a detailed overview based on their collection methods and application scenarios, while highlighting representative datasets. Through a thorough analysis of both methodologies and datasets, this paper offers valuable insights and directions for the future development of small target detection technology in maritime search and rescue operations.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Object Detection via Spatial-Channel State Space Model</title><link>https://doi.org/10.1109/tcsvt.2025.3642750</link><guid>10.1109/tcsvt.2025.3642750</guid><pubDate>Wed, 10 Dec 2025 18:34:28 +0000</pubDate><dc:creator>Zhimeng Xin</dc:creator><dc:creator>Tianxu Wu</dc:creator><dc:creator>Yixiong Zou</dc:creator><dc:creator>Shiming Chen</dc:creator><dc:creator>Dingjie Fu</dc:creator><dc:creator>Xinge You</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642750</prism:doi><description>Due to the limited training samples in few-shot object detection (FSOD), we observe that current methods may struggle to accurately extract effective features from each channel. Specifically, this issue manifests in two aspects: i) channels with high weights may not necessarily be effective, and ii) channels with low weights may still hold significant value. To handle this problem, we consider utilizing inter-channel correlation to ensure that the novel model can effectively highlight relevant channels and rectify incorrect ones, thereby strengthening channel quality. Since the channel sequence is also 1-dimensional, its similarity with the temporal sequence inspires us to take Mamba for modeling the correlation in the channel sequence Based on this concept, we propose the Spatial-Channel State Space Modeling (SCSM) module for spatial-channel-sequence modeling to accurately extract effective features from each channel. In SCSM, we design the Spatial Feature Modeling (SFM) module to ensure the quality of spatial feature representations. We then introduce the Channel State Modeling (CSM) module, which treats channels as a 1-dimensional sequence and take mamba to capture the correlation between channels. Extensive experiments on the VOC and COCO datasets show that the SCSM module enables the novel detector to improve the quality of channel feature representations and achieve state-of-the-art performance. Code is released at https://github.com/zhimengXin/SCSM.
Published: 2025-12-10T18:34:28+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhimeng Xin; Tianxu Wu; Yixiong Zou; Shiming Chen; Dingjie Fu; Xinge You&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642750"&gt;10.1109/tcsvt.2025.3642750&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the limited training samples in few-shot object detection (FSOD), we observe that current methods may struggle to accurately extract effective features from each channel. Specifically, this issue manifests in two aspects: i) channels with high weights may not necessarily be effective, and ii) channels with low weights may still hold significant value. To handle this problem, we consider utilizing inter-channel correlation to ensure that the novel model can effectively highlight relevant channels and rectify incorrect ones, thereby strengthening channel quality. Since the channel sequence is also 1-dimensional, its similarity with the temporal sequence inspires us to take Mamba for modeling the correlation in the channel sequence Based on this concept, we propose the Spatial-Channel State Space Modeling (SCSM) module for spatial-channel-sequence modeling to accurately extract effective features from each channel. In SCSM, we design the Spatial Feature Modeling (SFM) module to ensure the quality of spatial feature representations. We then introduce the Channel State Modeling (CSM) module, which treats channels as a 1-dimensional sequence and take mamba to capture the correlation between channels. Extensive experiments on the VOC and COCO datasets show that the SCSM module enables the novel detector to improve the quality of channel feature representations and achieve state-of-the-art performance. Code is released at https://github.com/zhimengXin/SCSM.&lt;/p&gt;</content:encoded></item><item><title>Self-Calibrated CLIP for Training-Free Open-Vocabulary Segmentation</title><link>https://doi.org/10.1109/tip.2025.3639996</link><guid>10.1109/tip.2025.3639996</guid><pubDate>Wed, 10 Dec 2025 18:34:53 +0000</pubDate><dc:creator>Sule Bai</dc:creator><dc:creator>Yong Liu</dc:creator><dc:creator>Yifei Han</dc:creator><dc:creator>Haoji Zhang</dc:creator><dc:creator>Yansong Tang</dc:creator><dc:creator>Jie Zhou</dc:creator><dc:creator>Jiwen Lu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639996</prism:doi><description>Recent advancements in pre-trained vision-language models like CLIP, have enabled the task of open-vocabulary segmentation. CLIP demonstrates impressive zero-shot capabilities in various downstream tasks that require holistic image understanding. However, due to the image-level contrastive learning and fully global feature interaction, ViT-based CLIP struggles to capture local details, resulting in poor performance in segmentation tasks. Our analysis of ViT-based CLIP reveals that anomaly tokens emerge during the forward process, attracting disproportionate attention from normal patch tokens and thereby diminishing spatial awareness. To address this issue, we propose Self-Calibrated CLIP (SC-CLIP), a training-free method that calibrates CLIP to generate finer representations while preserving its original generalization ability—without introducing new parameters or relying on additional backbones. Specifically, we mitigate the negative impact of anomaly tokens from two complementary perspectives. First, we explicitly identify the anomaly tokens and replace them based on local context. Second, we reduce their influence on normal tokens by enhancing feature discriminability and attention correlation, leveraging the inherent semantic consistency within CLIP’s mid-level features. In addition, we introduce a two-pass strategy that effectively integrates multi-level features to enrich local details under the training-free setting. Together, these strategies enhance CLIP’s feature representations with improved granularity and semantic coherence. Experimental results demonstrate the effectiveness of SC-CLIP, achieving state-of-the-art results across all datasets and surpassing previous methods by 9.5%. Notably, SC-CLIP boosts the performance of vanilla CLIP ViT-L/14 by 6.8 times. Furthermore, we discuss our method’s applicability to other vision–language models and tasks for a comprehensive evaluation. Our source code is available at https://github.com/SuleBai/SC-CLIP.
Published: 2025-12-10T18:34:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sule Bai; Yong Liu; Yifei Han; Haoji Zhang; Yansong Tang; Jie Zhou; Jiwen Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639996"&gt;10.1109/tip.2025.3639996&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in pre-trained vision-language models like CLIP, have enabled the task of open-vocabulary segmentation. CLIP demonstrates impressive zero-shot capabilities in various downstream tasks that require holistic image understanding. However, due to the image-level contrastive learning and fully global feature interaction, ViT-based CLIP struggles to capture local details, resulting in poor performance in segmentation tasks. Our analysis of ViT-based CLIP reveals that anomaly tokens emerge during the forward process, attracting disproportionate attention from normal patch tokens and thereby diminishing spatial awareness. To address this issue, we propose Self-Calibrated CLIP (SC-CLIP), a training-free method that calibrates CLIP to generate finer representations while preserving its original generalization ability—without introducing new parameters or relying on additional backbones. Specifically, we mitigate the negative impact of anomaly tokens from two complementary perspectives. First, we explicitly identify the anomaly tokens and replace them based on local context. Second, we reduce their influence on normal tokens by enhancing feature discriminability and attention correlation, leveraging the inherent semantic consistency within CLIP’s mid-level features. In addition, we introduce a two-pass strategy that effectively integrates multi-level features to enrich local details under the training-free setting. Together, these strategies enhance CLIP’s feature representations with improved granularity and semantic coherence. Experimental results demonstrate the effectiveness of SC-CLIP, achieving state-of-the-art results across all datasets and surpassing previous methods by 9.5%. Notably, SC-CLIP boosts the performance of vanilla CLIP ViT-L/14 by 6.8 times. Furthermore, we discuss our method’s applicability to other vision–language models and tasks for a comprehensive evaluation. Our source code is available at https://github.com/SuleBai/SC-CLIP.&lt;/p&gt;</content:encoded></item><item><title>Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation</title><link>https://doi.org/10.1109/tpami.2025.3642821</link><guid>10.1109/tpami.2025.3642821</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Jinxing Zhou</dc:creator><dc:creator>Zhihui Li</dc:creator><dc:creator>Yongqiang Yu</dc:creator><dc:creator>Yanghao Zhou</dc:creator><dc:creator>Ruohao Guo</dc:creator><dc:creator>Guangyao Li</dc:creator><dc:creator>Yuxin Mao</dc:creator><dc:creator>Mingfei Han</dc:creator><dc:creator>Xiaojun Chang</dc:creator><dc:creator>Meng Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642821</prism:doi><description>Mainstream research in audio-visual learning has focused on designing task-specific expert models, primarily implemented through sophisticated multimodal fusion approaches. Recently, a few efforts have aimed to develop more task-independent or universal audiovisual embedding networks, encoding advanced representations for use in various audiovisual downstream tasks. This is typically achieved by fine-tuning large pretrained transformers, such as Swin-V2-L and HTS-AT, in a parameter-efficient manner through techniques such as tuning only a few adapter layers inserted into the pretrained transformer backbone. Although these methods are parameter-efficient, they suffer from significant training memory consumption due to gradient backpropagation through the deep transformer backbones, which limits accessibility for researchers with constrained computational resources. In this paper, we present Meta-Token Learning (Mettle), a simple and memory-efficient method for adapting large-scale pretrained transformer models to downstream audio-visual tasks. Instead of sequentially modifying the output feature distribution of the transformer backbone, Mettle utilizes a lightweight Layer-Centric Distillation (LCD) module to distill in parallel the intact audio or visual features embedded by each transformer layer into compact meta-tokens. This distillation process considers both pretrained knowledge preservation and task-specific adaptation. The obtained meta-tokens can be directly applied to classification tasks, such as audio-visual event localization and audio-visual video parsing. To further support fine-grained segmentation tasks, such as audio-visual segmentation, we introduce a Meta-Token Injection (MTI) module, which utilizes the audio and visual meta-tokens distilled from the top transformer layer to guide feature adaptation in earlier layers. Extensive experiments on multiple audiovisual benchmarks demonstrate that our method significantly reduces memory usage and training...
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinxing Zhou; Zhihui Li; Yongqiang Yu; Yanghao Zhou; Ruohao Guo; Guangyao Li; Yuxin Mao; Mingfei Han; Xiaojun Chang; Meng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642821"&gt;10.1109/tpami.2025.3642821&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Mainstream research in audio-visual learning has focused on designing task-specific expert models, primarily implemented through sophisticated multimodal fusion approaches. Recently, a few efforts have aimed to develop more task-independent or universal audiovisual embedding networks, encoding advanced representations for use in various audiovisual downstream tasks. This is typically achieved by fine-tuning large pretrained transformers, such as Swin-V2-L and HTS-AT, in a parameter-efficient manner through techniques such as tuning only a few adapter layers inserted into the pretrained transformer backbone. Although these methods are parameter-efficient, they suffer from significant training memory consumption due to gradient backpropagation through the deep transformer backbones, which limits accessibility for researchers with constrained computational resources. In this paper, we present Meta-Token Learning (Mettle), a simple and memory-efficient method for adapting large-scale pretrained transformer models to downstream audio-visual tasks. Instead of sequentially modifying the output feature distribution of the transformer backbone, Mettle utilizes a lightweight Layer-Centric Distillation (LCD) module to distill in parallel the intact audio or visual features embedded by each transformer layer into compact meta-tokens. This distillation process considers both pretrained knowledge preservation and task-specific adaptation. The obtained meta-tokens can be directly applied to classification tasks, such as audio-visual event localization and audio-visual video parsing. To further support fine-grained segmentation tasks, such as audio-visual segmentation, we introduce a Meta-Token Injection (MTI) module, which utilizes the audio and visual meta-tokens distilled from the top transformer layer to guide feature adaptation in earlier layers. Extensive experiments on multiple audiovisual benchmarks demonstrate that our method significantly reduces memory usage and training...&lt;/p&gt;</content:encoded></item><item><title>Gradient-Guided Learning Network for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2512.09497v1</link><guid>http://arxiv.org/abs/2512.09497v1</guid><pubDate>Wed, 10 Dec 2025 10:21:08 +0000</pubDate><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Chuang Yu</dc:creator><dc:creator>Zelin Shi</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Yingdi Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/LGRS.2023.3308783</prism:doi><description>Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net
Published: 2025-12-10T10:21:08+00:00
Venue: arXiv
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinmiao Zhao; Chuang Yu; Zelin Shi; Yunpeng Liu; Yingdi Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/LGRS.2023.3308783"&gt;10.1109/LGRS.2023.3308783&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net&lt;/p&gt;</content:encoded></item><item><title>Breaking Barriers, Localizing Saliency: A Large-scale Benchmark and Baseline for Condition-Constrained Salient Object Detection</title><link>https://doi.org/10.1109/tpami.2025.3642893</link><guid>10.1109/tpami.2025.3642893</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Runmin Cong</dc:creator><dc:creator>Zhiyang Chen</dc:creator><dc:creator>Hao Fang</dc:creator><dc:creator>Sam Kwong</dc:creator><dc:creator>Wei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642893</prism:doi><description>Salient Object Detection (SOD) aims to identify and segment the most prominent objects in an image. In real open environments, intelligent systems often encounter complex and challenging scenes, such as low-light, rain, snow, etc., which we call constrained conditions. These real situations pose more severe challenges to existing SOD models. However, there is no comprehensive and in-depth exploration of this field at both the data and model levels, and most of them focus on ideal situations or a single condition. To bridge this gap, we launch a new task, Condition-Constrained Salient Object Detection (CSOD), aimed at robustly and accurately locating salient objects in constrained environments. On the one hand, to compensate for the lack of datasets, we construct the first large-scale condition-constrained salient object detection dataset CSOD10K, comprising 10,000 pixel-level annotated images and over 100 categories of salient objects. This dataset is oriented towards the real environment and includes 8 real-world constrained scenes under 3 main constraint types, making it extremely challenging. On the other hand, we abandon the paradigm of “restoration before detection” and instead introduce a unified end-to-end framework CSSAM that fully explores scene attributes, eliminating the need for additional ground-truth restored images and reducing computational overhead. Specifically, we design a Scene Prior-Guided Adapter (SPGA), which injects scene priors to enable the foundation model to better adapt to downstream constrained scenes. To automatically decode salient objects, we propose a Hybrid Prompt Decoding Strategy (HPDS), which can effectively integrate multiple types of prompts to achieve adaptation to the SOD task. Extensive experiments show that our model significantly outperforms state-of-the-art methods on both the CSOD10K dataset and existing standard SOD benchmarks.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runmin Cong; Zhiyang Chen; Hao Fang; Sam Kwong; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642893"&gt;10.1109/tpami.2025.3642893&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Salient Object Detection (SOD) aims to identify and segment the most prominent objects in an image. In real open environments, intelligent systems often encounter complex and challenging scenes, such as low-light, rain, snow, etc., which we call constrained conditions. These real situations pose more severe challenges to existing SOD models. However, there is no comprehensive and in-depth exploration of this field at both the data and model levels, and most of them focus on ideal situations or a single condition. To bridge this gap, we launch a new task, Condition-Constrained Salient Object Detection (CSOD), aimed at robustly and accurately locating salient objects in constrained environments. On the one hand, to compensate for the lack of datasets, we construct the first large-scale condition-constrained salient object detection dataset CSOD10K, comprising 10,000 pixel-level annotated images and over 100 categories of salient objects. This dataset is oriented towards the real environment and includes 8 real-world constrained scenes under 3 main constraint types, making it extremely challenging. On the other hand, we abandon the paradigm of “restoration before detection” and instead introduce a unified end-to-end framework CSSAM that fully explores scene attributes, eliminating the need for additional ground-truth restored images and reducing computational overhead. Specifically, we design a Scene Prior-Guided Adapter (SPGA), which injects scene priors to enable the foundation model to better adapt to downstream constrained scenes. To automatically decode salient objects, we propose a Hybrid Prompt Decoding Strategy (HPDS), which can effectively integrate multiple types of prompts to achieve adaptation to the SOD task. Extensive experiments show that our model significantly outperforms state-of-the-art methods on both the CSOD10K dataset and existing standard SOD benchmarks.&lt;/p&gt;</content:encoded></item><item><title>OMAP2-YOLO: A Ship Target Detection Algorithm for SAR Images With Multi-Scale Ships and Complex Environments</title><link>https://doi.org/10.1109/taes.2025.3641915</link><guid>10.1109/taes.2025.3641915</guid><pubDate>Wed, 10 Dec 2025 18:34:42 +0000</pubDate><dc:creator>Chuanbiao Qiu</dc:creator><dc:creator>Ying Zhang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3641915</prism:doi><description>Synthetic Aperture Radar (SAR) is pivotal in shipping management, maritime activity monitoring, and military intelligence. However, existing target detection methods often perform poorly in SAR scenarios due to complex background environments, multi-scale target variations, speckle noise in images, and occlusions. In order to tackle these challenges, we present a detection model named OMAP2-YOLO. First, we boost feature extraction efficiency through reparameterization of the backbone network with OREPA,based on the YOLOv8 architecture. Next, we introduce the MultiSEAM module, which integrates various attention mechanisms to mitigate the interference of complex backgrounds and occlusions on target detection. We then design the QFE module to build a more refined feature pyramid. This module enables the model to effectively detect targets at multiple scales and enhances detection performance by utilizing fine-grained features from the P2 layer. Finally, we propose the Focaler_SIoU loss function, which offers improved convergence and enhances sensitivity to small targets. Extensive experiments on the HRSID and SSDD datasets demonstrate the exceptional robustness and reliability of the OMAP2-YOLO model.Compared to baseline methods, the proposed model achieves notable performance improvements on both the HRSID and SSDD datasets. On the HRSID dataset, it improves Detection Rate (DR) by 3%, F1-Score (F1) by 2%, and Average Precision (AP) by 3.1%, while reducing False Alarm Rate (FAR) by 0.2213. On the SSDD dataset, it achieves improvements of 6% in DR, 11% in F1, and 8.5% in AP. These advancements underscore the potential of OMAP2-YOLO for real-time maritime surveillance and management.
Published: 2025-12-10T18:34:42+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuanbiao Qiu; Ying Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3641915"&gt;10.1109/taes.2025.3641915&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is pivotal in shipping management, maritime activity monitoring, and military intelligence. However, existing target detection methods often perform poorly in SAR scenarios due to complex background environments, multi-scale target variations, speckle noise in images, and occlusions. In order to tackle these challenges, we present a detection model named OMAP2-YOLO. First, we boost feature extraction efficiency through reparameterization of the backbone network with OREPA,based on the YOLOv8 architecture. Next, we introduce the MultiSEAM module, which integrates various attention mechanisms to mitigate the interference of complex backgrounds and occlusions on target detection. We then design the QFE module to build a more refined feature pyramid. This module enables the model to effectively detect targets at multiple scales and enhances detection performance by utilizing fine-grained features from the P2 layer. Finally, we propose the Focaler_SIoU loss function, which offers improved convergence and enhances sensitivity to small targets. Extensive experiments on the HRSID and SSDD datasets demonstrate the exceptional robustness and reliability of the OMAP2-YOLO model.Compared to baseline methods, the proposed model achieves notable performance improvements on both the HRSID and SSDD datasets. On the HRSID dataset, it improves Detection Rate (DR) by 3%, F1-Score (F1) by 2%, and Average Precision (AP) by 3.1%, while reducing False Alarm Rate (FAR) by 0.2213. On the SSDD dataset, it achieves improvements of 6% in DR, 11% in F1, and 8.5% in AP. These advancements underscore the potential of OMAP2-YOLO for real-time maritime surveillance and management.&lt;/p&gt;</content:encoded></item><item><title>From Point to Flow: Enhancing Unsupervised Domain Adaptation with Flow Classification</title><link>https://doi.org/10.1109/tcsvt.2025.3642573</link><guid>10.1109/tcsvt.2025.3642573</guid><pubDate>Thu, 11 Dec 2025 18:47:52 +0000</pubDate><dc:creator>Lihua Zhou</dc:creator><dc:creator>Mao Ye</dc:creator><dc:creator>Nianxin Li</dc:creator><dc:creator>Song Tang</dc:creator><dc:creator>Xu-Qian Fan</dc:creator><dc:creator>Lei Deng</dc:creator><dc:creator>Zhen Lei</dc:creator><dc:creator>Xiatian Zhu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642573</prism:doi><description>Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Existing methods, whether based on distribution matching or self-supervised learning, often focus solely on classifying individual source samples, potentially overlooking discriminative information. To address this limitation, we propose FlowUDA, a novel plugin method that enhances existing UDA frameworks by constructing semantically invariant flows from individual source samples to corresponding target samples, forming cross-domain trajectories. By leveraging a diffusion network guided by ordinary differential equations, FlowUDA ensures these flows preserve the topological structure of the source domain, maintaining their distinguishability. Our method then classifies these flows by sampling points along them and transferring labels from source samples, effectively capturing spatial relationships between domains. In essence, FlowUDA transforms the traditional point-based classification on individual source samples into flow-based classification on flows, allowing the model to learn richer, more discriminative features that bridge the gap between source and target domains. Extensive experiments on standard benchmarks demonstrate that integrating FlowUDA into existing UDA methods leads to notable performance gains, highlighting its effectiveness in addressing domain shift challenges.
Published: 2025-12-11T18:47:52+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihua Zhou; Mao Ye; Nianxin Li; Song Tang; Xu-Qian Fan; Lei Deng; Zhen Lei; Xiatian Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642573"&gt;10.1109/tcsvt.2025.3642573&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Existing methods, whether based on distribution matching or self-supervised learning, often focus solely on classifying individual source samples, potentially overlooking discriminative information. To address this limitation, we propose FlowUDA, a novel plugin method that enhances existing UDA frameworks by constructing semantically invariant flows from individual source samples to corresponding target samples, forming cross-domain trajectories. By leveraging a diffusion network guided by ordinary differential equations, FlowUDA ensures these flows preserve the topological structure of the source domain, maintaining their distinguishability. Our method then classifies these flows by sampling points along them and transferring labels from source samples, effectively capturing spatial relationships between domains. In essence, FlowUDA transforms the traditional point-based classification on individual source samples into flow-based classification on flows, allowing the model to learn richer, more discriminative features that bridge the gap between source and target domains. Extensive experiments on standard benchmarks demonstrate that integrating FlowUDA into existing UDA methods leads to notable performance gains, highlighting its effectiveness in addressing domain shift challenges.&lt;/p&gt;</content:encoded></item><item><title>SpaceFormer: Spatial Position Contextual Semantics Embedding for Multi-View 3D Object Detection</title><link>https://doi.org/10.1109/tits.2025.3639680</link><guid>10.1109/tits.2025.3639680</guid><pubDate>Wed, 10 Dec 2025 18:34:12 +0000</pubDate><dc:creator>Jiaqi Zhao</dc:creator><dc:creator>Huanfeng Hu</dc:creator><dc:creator>Wen-Liang Du</dc:creator><dc:creator>Yong Zhou</dc:creator><dc:creator>Kunyang Sun</dc:creator><dc:creator>Rui Yao</dc:creator><dc:creator>Abdulmotaleb El Saddik</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3639680</prism:doi><description>3D object detection aims to accurately localize and recognize objects in 3D space. It serves as a fundamental task for reliable perception in intelligent transportation systems, enabling the monitoring of diverse traffic participants such as vehicles, pedestrians, cyclists, and public transport. Recently, transformer-based methods have gained significant attention in multi-view 3D object detection due to their strong global reasoning capabilities. However, their limited capacity to model spatial positional information hinders accurate object localization, especially in complex and large-scale scenes. To address this limitation, SpaceFormer is proposed as a novel transformer-based multi-view 3D object detector. Specifically, a Contextual Visual Prompts Learning strategy is proposed to enhance the perception of small and sparse traffic participants by incorporating contextual priors. To further suppress background interference, a Semantics-guided Depth Estimation method is proposed to refine depth representations using high-level semantic information. Furthermore, a Spatial Position Embedding mechanism is proposed to improve the spatial localization capability of the transformer by integrating geometric position and polar spatial embedding. Extensive experiments on the nuScenes benchmark demonstrate that SpaceFormer achieves state-of-the-art performance with 55.5% mAP and 62.9% NDS. These improvements indicate not only methodological advances but also practical benefits for intelligent transportation systems, enhancing safety, reliability, and efficiency in real-world deployments.
Published: 2025-12-10T18:34:12+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Zhao; Huanfeng Hu; Wen-Liang Du; Yong Zhou; Kunyang Sun; Rui Yao; Abdulmotaleb El Saddik&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3639680"&gt;10.1109/tits.2025.3639680&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detection aims to accurately localize and recognize objects in 3D space. It serves as a fundamental task for reliable perception in intelligent transportation systems, enabling the monitoring of diverse traffic participants such as vehicles, pedestrians, cyclists, and public transport. Recently, transformer-based methods have gained significant attention in multi-view 3D object detection due to their strong global reasoning capabilities. However, their limited capacity to model spatial positional information hinders accurate object localization, especially in complex and large-scale scenes. To address this limitation, SpaceFormer is proposed as a novel transformer-based multi-view 3D object detector. Specifically, a Contextual Visual Prompts Learning strategy is proposed to enhance the perception of small and sparse traffic participants by incorporating contextual priors. To further suppress background interference, a Semantics-guided Depth Estimation method is proposed to refine depth representations using high-level semantic information. Furthermore, a Spatial Position Embedding mechanism is proposed to improve the spatial localization capability of the transformer by integrating geometric position and polar spatial embedding. Extensive experiments on the nuScenes benchmark demonstrate that SpaceFormer achieves state-of-the-art performance with 55.5% mAP and 62.9% NDS. These improvements indicate not only methodological advances but also practical benefits for intelligent transportation systems, enhancing safety, reliability, and efficiency in real-world deployments.&lt;/p&gt;</content:encoded></item><item><title>Unified Granularity Controller for Interactive Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3642834</link><guid>10.1109/tpami.2025.3642834</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Yian Zhao</dc:creator><dc:creator>Kehan Li</dc:creator><dc:creator>Pengchong Qiao</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Rongrong Ji</dc:creator><dc:creator>Jie Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642834</prism:doi><description>Interactive Segmentation (IS) segments specific objects or parts by deducing human intent from sparse input prompts. However, the sparse-to-dense mapping is ambiguous, making it challenging for users to obtain segmentations at the desired granularity and causing them to engage in trial-and-error cycles. Although existing multi-granularity IS models (e.g., SAM) alleviate the ambiguity of single-granularity methods by predicting multiple masks simultaneously, this approach has limited scalability and produces redundant results. To address this issue, we introduce a creative granularity-controllable IS paradigm that resolves ambiguity by enabling users to precisely control the segmentation granularity. Specifically, we propose a Unified Granularity Controller (UniGraCo) that supports multi-type optional granularity control signals to pursue unified control over diverse segmentation requirements, effectively overcoming the limitation of single-type control in adapting to different needs, thus boosting the system efficiency and practicality. To mitigate the excessive cost of annotating the multi-granularity masks and the corresponding granularity control signals for training UniGraCo, we construct an automated data engine capable of generating high-quality and granularity-abundant mask-granularity data pairs at low cost. To enable UniGraCo to learn unified granularity controllability in an efficient and stable manner, we further design a granularity-controllable learning strategy. This strategy leverages the generated data pairs to incrementally equip the pre-trained IS model with granularity controllability while preserving its segmentation capability. Extensive experiments on intricate scenarios at both instance and part level demonstrate that our UniGraCo has significant advantages over previous methods, highlighting its potential as a practical interactive tool. Code and model weights are available at https://github.com/Zhao-Yian/UniGraCo.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yian Zhao; Kehan Li; Pengchong Qiao; Chang Liu; Rongrong Ji; Jie Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642834"&gt;10.1109/tpami.2025.3642834&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Interactive Segmentation (IS) segments specific objects or parts by deducing human intent from sparse input prompts. However, the sparse-to-dense mapping is ambiguous, making it challenging for users to obtain segmentations at the desired granularity and causing them to engage in trial-and-error cycles. Although existing multi-granularity IS models (e.g., SAM) alleviate the ambiguity of single-granularity methods by predicting multiple masks simultaneously, this approach has limited scalability and produces redundant results. To address this issue, we introduce a creative granularity-controllable IS paradigm that resolves ambiguity by enabling users to precisely control the segmentation granularity. Specifically, we propose a Unified Granularity Controller (UniGraCo) that supports multi-type optional granularity control signals to pursue unified control over diverse segmentation requirements, effectively overcoming the limitation of single-type control in adapting to different needs, thus boosting the system efficiency and practicality. To mitigate the excessive cost of annotating the multi-granularity masks and the corresponding granularity control signals for training UniGraCo, we construct an automated data engine capable of generating high-quality and granularity-abundant mask-granularity data pairs at low cost. To enable UniGraCo to learn unified granularity controllability in an efficient and stable manner, we further design a granularity-controllable learning strategy. This strategy leverages the generated data pairs to incrementally equip the pre-trained IS model with granularity controllability while preserving its segmentation capability. Extensive experiments on intricate scenarios at both instance and part level demonstrate that our UniGraCo has significant advantages over previous methods, highlighting its potential as a practical interactive tool. Code and model weights are available at https://github.com/Zhao-Yian/UniGraCo.&lt;/p&gt;</content:encoded></item><item><title>Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration</title><link>https://doi.org/10.1109/tpami.2025.3642852</link><guid>10.1109/tpami.2025.3642852</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Gang Wu</dc:creator><dc:creator>Junjun Jiang</dc:creator><dc:creator>Kui Jiang</dc:creator><dc:creator>Xianming Liu</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642852</prism:doi><description>All-in-one image restoration, addressing diverse degradation types with a unified model, presents significant challenges in designing task-aware prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but may discard critical visual information for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a novel framework that fundamentally enhances prompt-task alignment through two complementary innovations: a Sparse Prompt Module (SPM) that efficiently captures degradation-specific features while minimizing redundancy, and a Contrastive Prompt Regularization (CPR) that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL optimizes the critical interaction between prompts and the restoration model itself. Extensive experiments across five comprehensive benchmarks demonstrate that CPL consistently enhances state-of-the-art all-in-one restoration models, achieving significant improvements in both standard multi-task scenarios and challenging composite degradation settings. Our framework establishes new state-of-the-art performance while maintaining parameter efficiency, offering a principled solution for unified image restoration.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gang Wu; Junjun Jiang; Kui Jiang; Xianming Liu; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642852"&gt;10.1109/tpami.2025.3642852&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;All-in-one image restoration, addressing diverse degradation types with a unified model, presents significant challenges in designing task-aware prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but may discard critical visual information for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a novel framework that fundamentally enhances prompt-task alignment through two complementary innovations: a Sparse Prompt Module (SPM) that efficiently captures degradation-specific features while minimizing redundancy, and a Contrastive Prompt Regularization (CPR) that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL optimizes the critical interaction between prompts and the restoration model itself. Extensive experiments across five comprehensive benchmarks demonstrate that CPL consistently enhances state-of-the-art all-in-one restoration models, achieving significant improvements in both standard multi-task scenarios and challenging composite degradation settings. Our framework establishes new state-of-the-art performance while maintaining parameter efficiency, offering a principled solution for unified image restoration.&lt;/p&gt;</content:encoded></item><item><title>Cross-domain Class Context Optimization for Universal Domain Adaptation</title><link>https://doi.org/10.1109/tcsvt.2025.3642710</link><guid>10.1109/tcsvt.2025.3642710</guid><pubDate>Thu, 11 Dec 2025 18:47:52 +0000</pubDate><dc:creator>Bo Zhou</dc:creator><dc:creator>Long Liu</dc:creator><dc:creator>Chenyue Fan</dc:creator><dc:creator>Zhipeng Zhao</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642710</prism:doi><description>Universal Domain Adaptation (UniDA) aims to achieve cross-domain knowledge transfer without label set assumptions. UniDA primarily faces two challenges: domain alignment under label shift and identifying unknown class samples in the target domain. We propose a cross-domain class context optimization method for UniDA to address these two challenges, leveraging a contrastive language-image pre-training model containing learnable prompts. First, we develop a domain context-guided feature augmentation technique, which augments source domain features based on textual features related to the target domain style, improving the consistency of feature distributions between the source domain and target domain. Subsequently, we learn a set of class contexts suitable for the target domain using the augmented source domain features. Furthermore, to improve the ability of the class context to filter unknown class samples, we propose a local known-unknown entropy optimization strategy, which effectively reduces the interference from class-irrelevant semantic information in the images, thereby mitigating erroneous class matching under label shift. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves competitive results compared to advanced UniDA approaches. Additionally, experimental results show that our entropy optimization strategy can serve as a general optimization component for prompt tuning, enhancing the generalization performance of existing methods when applied to downstream tasks with label shift.
Published: 2025-12-11T18:47:52+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Zhou; Long Liu; Chenyue Fan; Zhipeng Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642710"&gt;10.1109/tcsvt.2025.3642710&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Universal Domain Adaptation (UniDA) aims to achieve cross-domain knowledge transfer without label set assumptions. UniDA primarily faces two challenges: domain alignment under label shift and identifying unknown class samples in the target domain. We propose a cross-domain class context optimization method for UniDA to address these two challenges, leveraging a contrastive language-image pre-training model containing learnable prompts. First, we develop a domain context-guided feature augmentation technique, which augments source domain features based on textual features related to the target domain style, improving the consistency of feature distributions between the source domain and target domain. Subsequently, we learn a set of class contexts suitable for the target domain using the augmented source domain features. Furthermore, to improve the ability of the class context to filter unknown class samples, we propose a local known-unknown entropy optimization strategy, which effectively reduces the interference from class-irrelevant semantic information in the images, thereby mitigating erroneous class matching under label shift. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves competitive results compared to advanced UniDA approaches. Additionally, experimental results show that our entropy optimization strategy can serve as a general optimization component for prompt tuning, enhancing the generalization performance of existing methods when applied to downstream tasks with label shift.&lt;/p&gt;</content:encoded></item><item><title>VPT-NSP
                    &lt;sup&gt;2&lt;/sup&gt;
                    ++: Importance-Aware Visual Prompt Tuning in Null Space for Continual Learning</title><link>https://doi.org/10.1109/tpami.2025.3642298</link><guid>10.1109/tpami.2025.3642298</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Yue Lu</dc:creator><dc:creator>De Cheng</dc:creator><dc:creator>Yinghui Xing</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Peng Wang</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642298</prism:doi><description>Continual learning (CL) enables AI models to adapt to evolving environments while mitigating catastrophic forgetting, which is a critical capability for dynamic real-world applications. With the growing popularity of pre-trained Vision Transformer (ViT) models and visual prompt tuning (VPT) technique in CL, this work explores a CL method on top of the ViT-based foundation model, through VPT mechanism with theoretical guarantees. Inspired by the orthogonal projection method, we aim to leverage this approach for VPT to enhance CL performance, particularly in long-term scenarios. However, since the orthogonal projection is originally designed for linear operations in CNNs, applying it to ViTs poses challenges induced by the non-linear self-attention mechanism and the distribution drift within LayerNorm. To address these issues, we deduced two orthogonality conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of maintaining stability. Considering the strict orthogonal constraints can diminish model capacity and reduce plasticity, we further propose an importance-aware orthogonal regularization framework. By applying varying degrees of orthogonal constraints to different parameters based on their importance to old and new tasks, the framework adaptively enhances model capacity and thereby promotes long-sequence CL while improving the stability-plasticity trade-off. To implement the proposed approach, a null-space-based approximation solution is employed to efficiently achieve the prompt gradient orthogonal projection. Extensive experiments on various class-incremental learning benchmarks demonstrate that our method achieves state-of-the-art performance across diverse CL scenarios.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shizhou Zhang; Yue Lu; De Cheng; Yinghui Xing; Nannan Wang; Peng Wang; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642298"&gt;10.1109/tpami.2025.3642298&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Continual learning (CL) enables AI models to adapt to evolving environments while mitigating catastrophic forgetting, which is a critical capability for dynamic real-world applications. With the growing popularity of pre-trained Vision Transformer (ViT) models and visual prompt tuning (VPT) technique in CL, this work explores a CL method on top of the ViT-based foundation model, through VPT mechanism with theoretical guarantees. Inspired by the orthogonal projection method, we aim to leverage this approach for VPT to enhance CL performance, particularly in long-term scenarios. However, since the orthogonal projection is originally designed for linear operations in CNNs, applying it to ViTs poses challenges induced by the non-linear self-attention mechanism and the distribution drift within LayerNorm. To address these issues, we deduced two orthogonality conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of maintaining stability. Considering the strict orthogonal constraints can diminish model capacity and reduce plasticity, we further propose an importance-aware orthogonal regularization framework. By applying varying degrees of orthogonal constraints to different parameters based on their importance to old and new tasks, the framework adaptively enhances model capacity and thereby promotes long-sequence CL while improving the stability-plasticity trade-off. To implement the proposed approach, a null-space-based approximation solution is employed to efficiently achieve the prompt gradient orthogonal projection. Extensive experiments on various class-incremental learning benchmarks demonstrate that our method achieves state-of-the-art performance across diverse CL scenarios.&lt;/p&gt;</content:encoded></item><item><title>Large Visual Language Models Continual Learning with Dynamic Mixture-of-Experts</title><link>https://doi.org/10.1109/tip.2025.3639925</link><guid>10.1109/tip.2025.3639925</guid><pubDate>Wed, 10 Dec 2025 18:34:53 +0000</pubDate><dc:creator>Yizhou Chen</dc:creator><dc:creator>Xihao Huang</dc:creator><dc:creator>Wei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639925</prism:doi><description>In dynamic and evolving application scenarios, the ability of visual language models to continuously learn from new data while preserving historical knowledge is critically important. Existing continual learning methods for large visual language models (LVLMs) often restrict the number of tasks they can handle, causing performance to decline as tasks continue to increase. In this paper, we propose a novel continual learning framework that adapts to the growing number of tasks, enabling visual language models to handle a dynamic range of open-set tasks while overcoming the catastrophic forgetting problem of learning new tasks at the expense of forgetting old ones.Our method builds on a pre-trained CLIP model and incorporates a dynamic mixture-of-experts (MoE) layer, enabling flexible adaptation to a wide range of open-set tasks. We design an elastic expert weight management strategy to effectively mitigate the catastrophic forgetting problem. Furthermore, we optimize the LoRA experts with adaptive ranks to achieve a balanced trade-off between model complexity and representational capacity. Extensive experiments across diverse settings demonstrate that our proposed method significantly reduces the number of tunable parameters while consistently surpassing state-of-the-art methods in new task learning capability and maintaining performance on historical tasks.
Published: 2025-12-10T18:34:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yizhou Chen; Xihao Huang; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639925"&gt;10.1109/tip.2025.3639925&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;In dynamic and evolving application scenarios, the ability of visual language models to continuously learn from new data while preserving historical knowledge is critically important. Existing continual learning methods for large visual language models (LVLMs) often restrict the number of tasks they can handle, causing performance to decline as tasks continue to increase. In this paper, we propose a novel continual learning framework that adapts to the growing number of tasks, enabling visual language models to handle a dynamic range of open-set tasks while overcoming the catastrophic forgetting problem of learning new tasks at the expense of forgetting old ones.Our method builds on a pre-trained CLIP model and incorporates a dynamic mixture-of-experts (MoE) layer, enabling flexible adaptation to a wide range of open-set tasks. We design an elastic expert weight management strategy to effectively mitigate the catastrophic forgetting problem. Furthermore, we optimize the LoRA experts with adaptive ranks to achieve a balanced trade-off between model complexity and representational capacity. Extensive experiments across diverse settings demonstrate that our proposed method significantly reduces the number of tunable parameters while consistently surpassing state-of-the-art methods in new task learning capability and maintaining performance on historical tasks.&lt;/p&gt;</content:encoded></item><item><title>MPRANet: Multi-scale perception and reference attention network for lightweight SAR target recognition</title><link>https://doi.org/10.1016/j.neucom.2025.132310</link><guid>10.1016/j.neucom.2025.132310</guid><pubDate>Wed, 10 Dec 2025 08:03:05 +0000</pubDate><dc:creator>Yonggang Qian</dc:creator><dc:creator>Yinghua Wang</dc:creator><dc:creator>Hongwei Liu</dc:creator><dc:creator>Zelong Wang</dc:creator><dc:creator>Feipeng Yu</dc:creator><dc:creator>Chunhui Qu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132310</prism:doi><description>Deep learning methods have been widely used in Synthetic Aperture Radar Automatic Target Recognition (SAR ATR). However, challenges remain due to limited SAR data and computational constraints on mobile devices, which hinder model training and deployment. In this paper, we propose a Multi-scale Perception and Reference Attention Network (MPRANet) for lightweight SAR ATR, which is a hybrid structure combining convolutional networks and transformers, built upon the ShuffleNetV2 network. Specifically, MPRANet introduces two key improvements compared to the CNN-based ShuffleNetV2. Firstly, we replace the depthwise convolutions (DWConv) in the downsampling and basic units of ShuffleNetV2 with the Multi-scale Parameter-Shared Convolution (MPConv) module. MPConv enables the extraction of multi-scale features of SAR targets with almost no additional parameters, thereby enhancing the network’s feature extraction capabilities. Secondly, we propose a lightweight Reference Attention Transformer (RAformer) to capture global information, addressing the issue of insufficient channel feature interaction in ShuffleNetV2. In RAformer, a Local Linear Mapping Unit (LMU) is designed to perform linear mappings, reducing the introduction of redundant features while ensuring its lightweight and efficient nature. RAformer contains two modules: the Reference Vector Attention (RVA) module, which efficiently models attention relationships, and the Lightweight Feedforward Neural Network (LW-FFN) module, which enhances the network’s ability to capture nonlinear representations. We evaluated the performance of MPRANet using publicly available SAR datasets, including the MSTAR dataset, OpenSARShip dataset, and SAR-AIRcraft-1.0 dataset. The experimental results demonstrate that MPRANet consistently achieves superior recognition performance compared to other lightweight networks of similar complexity.
Published: 2025-12-10T08:03:05+00:00
Venue: Neurocomputing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yonggang Qian; Yinghua Wang; Hongwei Liu; Zelong Wang; Feipeng Yu; Chunhui Qu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132310"&gt;10.1016/j.neucom.2025.132310&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning methods have been widely used in Synthetic Aperture Radar Automatic Target Recognition (SAR ATR). However, challenges remain due to limited SAR data and computational constraints on mobile devices, which hinder model training and deployment. In this paper, we propose a Multi-scale Perception and Reference Attention Network (MPRANet) for lightweight SAR ATR, which is a hybrid structure combining convolutional networks and transformers, built upon the ShuffleNetV2 network. Specifically, MPRANet introduces two key improvements compared to the CNN-based ShuffleNetV2. Firstly, we replace the depthwise convolutions (DWConv) in the downsampling and basic units of ShuffleNetV2 with the Multi-scale Parameter-Shared Convolution (MPConv) module. MPConv enables the extraction of multi-scale features of SAR targets with almost no additional parameters, thereby enhancing the network’s feature extraction capabilities. Secondly, we propose a lightweight Reference Attention Transformer (RAformer) to capture global information, addressing the issue of insufficient channel feature interaction in ShuffleNetV2. In RAformer, a Local Linear Mapping Unit (LMU) is designed to perform linear mappings, reducing the introduction of redundant features while ensuring its lightweight and efficient nature. RAformer contains two modules: the Reference Vector Attention (RVA) module, which efficiently models attention relationships, and the Lightweight Feedforward Neural Network (LW-FFN) module, which enhances the network’s ability to capture nonlinear representations. We evaluated the performance of MPRANet using publicly available SAR datasets, including the MSTAR dataset, OpenSARShip dataset, and SAR-AIRcraft-1.0 dataset. The experimental results demonstrate that MPRANet consistently achieves superior recognition performance compared to other lightweight networks of similar complexity.&lt;/p&gt;</content:encoded></item><item><title>Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement</title><link>https://arxiv.org/abs/2512.07611v1</link><guid>http://arxiv.org/abs/2512.07611v1</guid><pubDate>Mon, 08 Dec 2025 14:58:19 +0000</pubDate><dc:creator>Yongsheng Lian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.
Published: 2025-12-08T14:58:19+00:00
Venue: arXiv
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongsheng Lian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.&lt;/p&gt;</content:encoded></item><item><title>A global-local interaction and conditional consistency constrained diffusion model for SAR-guided optical image cloud removal</title><link>https://doi.org/10.1016/j.jag.2025.105013</link><guid>10.1016/j.jag.2025.105013</guid><pubDate>Thu, 11 Dec 2025 17:57:40 +0000</pubDate><dc:creator>Liwen Cao</dc:creator><dc:creator>Jun Pan</dc:creator><dc:creator>Jiangong Xu</dc:creator><dc:creator>Tao Chen</dc:creator><dc:creator>Qiangqiang Yuan</dc:creator><dc:creator>Jizhang Sang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105013</prism:doi><description>Cloud cover constitutes a formidable obstacle in the field of optical remote sensing image processing, substantially impeding the extraction and utilization of surface information. Synthetic Aperture Radar (SAR) imagery, serving as a complementary informational resource, is capable of furnishing crucial auxiliary data for optical images. In recent years, diffusion-based cloud removal methodologies have made significant progress. Nevertheless, their inherent generative diversity and randomness pose challenges in meeting the realism requirements for cloud removal in optical remote sensing imagery. To address this, this paper presents a SAR-guided optical imagery cloud removal method based on global–local interaction and conditional consistency-constrained diffusion models (GLCdiffcr). Specifically, the method integrates a multi-scale residual self-attention network in the denoising module. This network captures both global and local details of SAR imagery and the captured details provide precise guidance for cloud removal. Additionally, within the reverse diffusion framework, the method directly predicts cloud-free optical images and iterates over multiple steps, reducing errors caused by generative randomness and improving consistency. Meanwhile, in order to enhance the realism of the generated images, the method employs a novel multi-condition consistency-constrained loss function, which combines pixel-level errors with structural similarity measures. Through this loss function, the gap between the generated images and real-world land cover types is further minimized. Experimental results demonstrate that the proposed method outperforms current state-of-the-art methods in both quantitative metrics and visual quality, particularly in complex regions, with higher accuracy and reliability.
Published: 2025-12-11T17:57:40+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liwen Cao; Jun Pan; Jiangong Xu; Tao Chen; Qiangqiang Yuan; Jizhang Sang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105013"&gt;10.1016/j.jag.2025.105013&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Cloud cover constitutes a formidable obstacle in the field of optical remote sensing image processing, substantially impeding the extraction and utilization of surface information. Synthetic Aperture Radar (SAR) imagery, serving as a complementary informational resource, is capable of furnishing crucial auxiliary data for optical images. In recent years, diffusion-based cloud removal methodologies have made significant progress. Nevertheless, their inherent generative diversity and randomness pose challenges in meeting the realism requirements for cloud removal in optical remote sensing imagery. To address this, this paper presents a SAR-guided optical imagery cloud removal method based on global–local interaction and conditional consistency-constrained diffusion models (GLCdiffcr). Specifically, the method integrates a multi-scale residual self-attention network in the denoising module. This network captures both global and local details of SAR imagery and the captured details provide precise guidance for cloud removal. Additionally, within the reverse diffusion framework, the method directly predicts cloud-free optical images and iterates over multiple steps, reducing errors caused by generative randomness and improving consistency. Meanwhile, in order to enhance the realism of the generated images, the method employs a novel multi-condition consistency-constrained loss function, which combines pixel-level errors with structural similarity measures. Through this loss function, the gap between the generated images and real-world land cover types is further minimized. Experimental results demonstrate that the proposed method outperforms current state-of-the-art methods in both quantitative metrics and visual quality, particularly in complex regions, with higher accuracy and reliability.&lt;/p&gt;</content:encoded></item><item><title>Began+: Leveraging bi-temporal SAR-optical data fusion to reconstruct clear-sky satellite imagery under large cloud cover</title><link>https://doi.org/10.1016/j.rse.2025.115171</link><guid>10.1016/j.rse.2025.115171</guid><pubDate>Thu, 11 Dec 2025 17:30:56 +0000</pubDate><dc:creator>Yu Xia</dc:creator><dc:creator>Wei He</dc:creator><dc:creator>Liangpei Zhang</dc:creator><dc:creator>Hongyan Zhang</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115171</prism:doi><description>In recent years, optical remote sensing imagery has played an increasingly vital role in Earth observation, but cloud contamination exists as an inevitable degradation. Combining synthetic aperture radar (SAR) and optical data with machine learning offers a promising solution for reconstructing clear-sky satellite imagery. Nevertheless, several challenges persist, including insufficient attention to large cloud cover, difficulties in restoring temporal changes, and limited practicality of deep models. To address these issues, this paper introduces a novel deep learning-based cloud removal framework, termed Began+, which integrates bi-temporal SAR-optical data to deal with cloudy images with high cover ratios. The Began+ framework comprises two primary components: a deep network and a flexible post-processing step, combining the strengths of data-driven models for restoring change information and traditional gap-filling algorithms for mitigating radiance discrepancies. First, a bi-output enhanced generative adversarial network, abbreviated as Began, is designed for image synthesis, featuring an enhanced channel-wise fusion block (ECFB) and a multi-scale depth-wise convolution residual block (MDRB). By applying the dual-tasking optimization and co-learning strategy, the Began model identifies potential change areas from bi-temporal SAR and pre-temporal optical inputs, guiding the synthesis of target optical images. Second, a range of cloud masking and gap-filling techniques can be optionally employed to effectively reduce radiometric discrepancies between the synthesized images and the cloudy data, ultimately yielding high-quality, clear-sky imagery. To meet the big data requirements of deep learning, we constructed two globally distributed cloud removal datasets, named BiS1L8-CR and BiS1S2-CR. Supported by these datasets, extensive experiments demonstrated that the Began+ framework effectively captures bi-temporal change features, reconstructing precise surface information in both Landsat-8 and Sentinel-2 satellite images under large cloud cover. Compared to the latest solutions and algorithms, our proposed Began+ framework exhibits significant advantages from both qualitative and quantitative perspectives in both simulated and real experiments. Furthermore, without strict constraints on input timing, the Began+ framework enables accurate reconstruction of large-scale dual-sensor imagery under high-ratio cloud cover, effectively restoring changing surfaces and improving the quality of unsupervised vegetation extraction.
Published: 2025-12-11T17:30:56+00:00
Venue: Remote Sensing of Environment
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Xia; Wei He; Liangpei Zhang; Hongyan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115171"&gt;10.1016/j.rse.2025.115171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, optical remote sensing imagery has played an increasingly vital role in Earth observation, but cloud contamination exists as an inevitable degradation. Combining synthetic aperture radar (SAR) and optical data with machine learning offers a promising solution for reconstructing clear-sky satellite imagery. Nevertheless, several challenges persist, including insufficient attention to large cloud cover, difficulties in restoring temporal changes, and limited practicality of deep models. To address these issues, this paper introduces a novel deep learning-based cloud removal framework, termed Began+, which integrates bi-temporal SAR-optical data to deal with cloudy images with high cover ratios. The Began+ framework comprises two primary components: a deep network and a flexible post-processing step, combining the strengths of data-driven models for restoring change information and traditional gap-filling algorithms for mitigating radiance discrepancies. First, a bi-output enhanced generative adversarial network, abbreviated as Began, is designed for image synthesis, featuring an enhanced channel-wise fusion block (ECFB) and a multi-scale depth-wise convolution residual block (MDRB). By applying the dual-tasking optimization and co-learning strategy, the Began model identifies potential change areas from bi-temporal SAR and pre-temporal optical inputs, guiding the synthesis of target optical images. Second, a range of cloud masking and gap-filling techniques can be optionally employed to effectively reduce radiometric discrepancies between the synthesized images and the cloudy data, ultimately yielding high-quality, clear-sky imagery. To meet the big data requirements of deep learning, we constructed two globally distributed cloud removal datasets, named BiS1L8-CR and BiS1S2-CR. Supported by these datasets, extensive experiments demonstrated that the Began+ framework effectively captures bi-temporal change features, reconstructing precise surface information in both Landsat-8 and Sentinel-2 satellite images under large cloud cover. Compared to the latest solutions and algorithms, our proposed Began+ framework exhibits significant advantages from both qualitative and quantitative perspectives in both simulated and real experiments. Furthermore, without strict constraints on input timing, the Began+ framework enables accurate reconstruction of large-scale dual-sensor imagery under high-ratio cloud cover, effectively restoring changing surfaces and improving the quality of unsupervised vegetation extraction.&lt;/p&gt;</content:encoded></item><item><title>A lightweight network for foreign object detection in railway overhead contact lines based on optical imagery</title><link>https://doi.org/10.1016/j.eswa.2025.130718</link><guid>10.1016/j.eswa.2025.130718</guid><pubDate>Wed, 10 Dec 2025 08:05:55 +0000</pubDate><dc:creator>Fengqiang Xu</dc:creator><dc:creator>Li Diao</dc:creator><dc:creator>Haolin Yang</dc:creator><dc:creator>Renxuan Xiong</dc:creator><dc:creator>Jinhao Cao</dc:creator><dc:creator>Yanjuan Wang</dc:creator><dc:creator>Fengqi Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130718</prism:doi><description>Foreign object detection in railway overhead contact lines (ROCL) is vital for train operation safety. However, traditional object detection models often struggle to balance accuracy and efficiency, particularly under complex backgrounds and varying object shapes. To overcome these challenges, we propose a lightweight object detection model, LWOD-DETR, tailored for ROCL foreign object detection. Firstly, a reparameterizable cross-stage partial (RepCSP) module is designed to enhance the model’s feature extraction capability with reduced parameter count and computational cost. Secondly, a cascaded group attention (CGA) mechanism is employed into the encoder to improve the model’s ability to capture and utilize essential image features while suppressing redundant information. Thirdly, an efficient multi-scale feature fusion (EMSFF) module is proposed to improve the detection accuracy of foreign objects across different scales and simultaneously reduce the model’s parameter count. In addition, a gating mechanism based on the HardSigmoid activation function is introduced to optimize the upsampling and downsampling modules. This mechanism enables the model to selectively activate or suppress different feature channels through learning, thereby enhancing the model’s nonlinear representation capability and improving the efficiency of multi-scale feature fusion. Experimental results on two ROCL foreign body datasets demonstrate that our proposed method outperforms the state-of-the-art methods in terms of AP metric (RCPSSFO: 0.607, RailFOD23: 0.841) while maintaining low parameter and GFLOPs (13.57M, 45.8G).
Published: 2025-12-10T08:05:55+00:00
Venue: Expert Systems with Applications
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fengqiang Xu; Li Diao; Haolin Yang; Renxuan Xiong; Jinhao Cao; Yanjuan Wang; Fengqi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130718"&gt;10.1016/j.eswa.2025.130718&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Foreign object detection in railway overhead contact lines (ROCL) is vital for train operation safety. However, traditional object detection models often struggle to balance accuracy and efficiency, particularly under complex backgrounds and varying object shapes. To overcome these challenges, we propose a lightweight object detection model, LWOD-DETR, tailored for ROCL foreign object detection. Firstly, a reparameterizable cross-stage partial (RepCSP) module is designed to enhance the model’s feature extraction capability with reduced parameter count and computational cost. Secondly, a cascaded group attention (CGA) mechanism is employed into the encoder to improve the model’s ability to capture and utilize essential image features while suppressing redundant information. Thirdly, an efficient multi-scale feature fusion (EMSFF) module is proposed to improve the detection accuracy of foreign objects across different scales and simultaneously reduce the model’s parameter count. In addition, a gating mechanism based on the HardSigmoid activation function is introduced to optimize the upsampling and downsampling modules. This mechanism enables the model to selectively activate or suppress different feature channels through learning, thereby enhancing the model’s nonlinear representation capability and improving the efficiency of multi-scale feature fusion. Experimental results on two ROCL foreign body datasets demonstrate that our proposed method outperforms the state-of-the-art methods in terms of AP metric (RCPSSFO: 0.607, RailFOD23: 0.841) while maintaining low parameter and GFLOPs (13.57M, 45.8G).&lt;/p&gt;</content:encoded></item><item><title>Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving</title><link>https://arxiv.org/abs/2512.09296v1</link><guid>http://arxiv.org/abs/2512.09296v1</guid><pubDate>Wed, 10 Dec 2025 03:46:57 +0000</pubDate><dc:creator>Songhan Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.
Published: 2025-12-10T03:46:57+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songhan Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model&amp;#x27;s contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.&lt;/p&gt;</content:encoded></item><item><title>A Unified Experience Replay Framework for Spiking Deep Reinforcement Learning</title><link>https://doi.org/10.1109/tpami.2025.3642900</link><guid>10.1109/tpami.2025.3642900</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Meng Xu</dc:creator><dc:creator>Xinhong Chen</dc:creator><dc:creator>Bingyi Liu</dc:creator><dc:creator>Yi-Rong Lin</dc:creator><dc:creator>Yung-Hui Li</dc:creator><dc:creator>Jianping Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642900</prism:doi><description>Deep Reinforcement Learning (DRL) methods have shown remarkable success in many applications, yet their high energy consumption limits their practicability. Recent studies incorporated energy-efficient Spiking Neural Networks (SNNs) to build Spiking DRL methods and lower energy consumption by setting a shorter simulation duration for SNNs to compute fewer gradients. However, these existing Spiking DRL methods fail to sample sufficient high-quality samples within a fixed-size replay buffer and perform poorly when the simulation duration is small, introducing the challenging tradeoff between energy consumption and model performance. Motivated by such observations, we develop a generic resilient experience replay method that can be seamlessly integrated into existing spiking DRL methods to effectively address the above tradeoff. Specifically, we allow the replay buffer to dynamically expand as the number of training samples increases, thereby accommodating more potentially valuable candidate samples for policy training. Meanwhile, we introduce an adaptive approach to manage the buffer size by determining when to shrink the replay buffer and removing redundant samples automatically. This strategy prevents the buffer from expanding unnecessarily, thereby mitigating the potential negative impact on model performance. Extensive experimental results demonstrate that our approach significantly enhances the performance of five state-of-the-art (SOTA) spiking DRL methods across various simulation durations in sixteen tasks, in terms of return, without compromising their energy efficiency.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Xu; Xinhong Chen; Bingyi Liu; Yi-Rong Lin; Yung-Hui Li; Jianping Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642900"&gt;10.1109/tpami.2025.3642900&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Deep Reinforcement Learning (DRL) methods have shown remarkable success in many applications, yet their high energy consumption limits their practicability. Recent studies incorporated energy-efficient Spiking Neural Networks (SNNs) to build Spiking DRL methods and lower energy consumption by setting a shorter simulation duration for SNNs to compute fewer gradients. However, these existing Spiking DRL methods fail to sample sufficient high-quality samples within a fixed-size replay buffer and perform poorly when the simulation duration is small, introducing the challenging tradeoff between energy consumption and model performance. Motivated by such observations, we develop a generic resilient experience replay method that can be seamlessly integrated into existing spiking DRL methods to effectively address the above tradeoff. Specifically, we allow the replay buffer to dynamically expand as the number of training samples increases, thereby accommodating more potentially valuable candidate samples for policy training. Meanwhile, we introduce an adaptive approach to manage the buffer size by determining when to shrink the replay buffer and removing redundant samples automatically. This strategy prevents the buffer from expanding unnecessarily, thereby mitigating the potential negative impact on model performance. Extensive experimental results demonstrate that our approach significantly enhances the performance of five state-of-the-art (SOTA) spiking DRL methods across various simulation durations in sixteen tasks, in terms of return, without compromising their energy efficiency.&lt;/p&gt;</content:encoded></item><item><title>Self-Rectification Historical Consistency Learning for Coupled Noisy Visible-Infrared Person Re-identification</title><link>https://doi.org/10.1109/tcsvt.2025.3642770</link><guid>10.1109/tcsvt.2025.3642770</guid><pubDate>Thu, 11 Dec 2025 18:47:52 +0000</pubDate><dc:creator>Jiacheng Zhao</dc:creator><dc:creator>Yongxi Li</dc:creator><dc:creator>Changsheng Xu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642770</prism:doi><description>Visible-infrared person re-identification (VI-ReID) retrieves cross-modal identity matches between visible and infrared images, offering significant value for round-the-clock surveillance. Despite recent advances, challenges remain: the task relies heavily on high-quality annotations, and factors such as occlusion, viewpoint variations, and the inherent difficulty of labeling infrared images inevitably introduce noisy annotations (NA) into the dataset during large-scale dataset construction. Moreover, coupled noisy labels in two modalities lead to noisy correspondence (NC), further complicating the learning process. Although prior research has achieved relatively stable results in addressing the NA and NC problem for VI-ReID through noise detection and robust loss functions, they still exhibit certain limitations: 1) Underutilization of training data. Existing methods often discard noisy samples to mitigate their negative impact, overlooking their potential value. 2) Lack of historical relevance. Unstable learning dynamics under noisy labels lead to inconsistent outputs, yet current approaches ignore the valuable historical information embedded in these fluctuations. Focusing on these challenges in VI-ReID, we propose Self-Rectification Historical Consistency Learning (SRHCL) for VI-ReID, which consists of noise detection, self-refined label rectification, and historical consistency learning modules. Firstly, the noise detection module calculates confidence weights for each sample by modeling the model’s loss response, thereby mitigating the adverse impact of noisy samples in subsequent training phases. Secondly, we propose a self-refined label rectification module to rectify noisy labels by reliable historical predictions, progressively collating the training data at fixed intervals. Finally, we introduce cross-modal contrastive learning and early learning regularization based on momentum-updated memories to facilitate historical consistency learning. Extensive exp...
Published: 2025-12-11T18:47:52+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiacheng Zhao; Yongxi Li; Changsheng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642770"&gt;10.1109/tcsvt.2025.3642770&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Visible-infrared person re-identification (VI-ReID) retrieves cross-modal identity matches between visible and infrared images, offering significant value for round-the-clock surveillance. Despite recent advances, challenges remain: the task relies heavily on high-quality annotations, and factors such as occlusion, viewpoint variations, and the inherent difficulty of labeling infrared images inevitably introduce noisy annotations (NA) into the dataset during large-scale dataset construction. Moreover, coupled noisy labels in two modalities lead to noisy correspondence (NC), further complicating the learning process. Although prior research has achieved relatively stable results in addressing the NA and NC problem for VI-ReID through noise detection and robust loss functions, they still exhibit certain limitations: 1) Underutilization of training data. Existing methods often discard noisy samples to mitigate their negative impact, overlooking their potential value. 2) Lack of historical relevance. Unstable learning dynamics under noisy labels lead to inconsistent outputs, yet current approaches ignore the valuable historical information embedded in these fluctuations. Focusing on these challenges in VI-ReID, we propose Self-Rectification Historical Consistency Learning (SRHCL) for VI-ReID, which consists of noise detection, self-refined label rectification, and historical consistency learning modules. Firstly, the noise detection module calculates confidence weights for each sample by modeling the model’s loss response, thereby mitigating the adverse impact of noisy samples in subsequent training phases. Secondly, we propose a self-refined label rectification module to rectify noisy labels by reliable historical predictions, progressively collating the training data at fixed intervals. Finally, we introduce cross-modal contrastive learning and early learning regularization based on momentum-updated memories to facilitate historical consistency learning. Extensive exp...&lt;/p&gt;</content:encoded></item><item><title>Hands-on Evaluation of Visual Transformers for Object Recognition and Detection</title><link>https://arxiv.org/abs/2512.09579v1</link><guid>http://arxiv.org/abs/2512.09579v1</guid><pubDate>Wed, 10 Dec 2025 12:15:48 +0000</pubDate><dc:creator>Dimitrios N. Vlachogiannis</dc:creator><dc:creator>Dimitrios A. Koutsomitropoulos</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.
Published: 2025-12-10T12:15:48+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dimitrios N. Vlachogiannis; Dimitrios A. Koutsomitropoulos&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.&lt;/p&gt;</content:encoded></item><item><title>LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery</title><link>https://arxiv.org/abs/2512.09700v1</link><guid>http://arxiv.org/abs/2512.09700v1</guid><pubDate>Wed, 10 Dec 2025 14:48:58 +0000</pubDate><dc:creator>Seon-Hoon Kim</dc:creator><dc:creator>Hyeji Sim</dc:creator><dc:creator>Youeyun Jung</dc:creator><dc:creator>Ok-Chul Jung</dc:creator><dc:creator>Yerin Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.
Published: 2025-12-10T14:48:58+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seon-Hoon Kim; Hyeji Sim; Youeyun Jung; Ok-Chul Jung; Yerin Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.&lt;/p&gt;</content:encoded></item><item><title>DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection</title><link>https://arxiv.org/abs/2512.07078v1</link><guid>http://arxiv.org/abs/2512.07078v1</guid><pubDate>Mon, 08 Dec 2025 01:25:10 +0000</pubDate><dc:creator>Bo Gao</dc:creator><dc:creator>Jingcheng Tong</dc:creator><dc:creator>Xingsheng Chen</dc:creator><dc:creator>Han Yu</dc:creator><dc:creator>Zichen Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.
Published: 2025-12-08T01:25:10+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Gao; Jingcheng Tong; Xingsheng Chen; Han Yu; Zichen Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.&lt;/p&gt;</content:encoded></item><item><title>Time Series Domain Adaptation Via Latent Invariant Causal Mechanism</title><link>https://doi.org/10.1109/tpami.2025.3642245</link><guid>10.1109/tpami.2025.3642245</guid><pubDate>Wed, 10 Dec 2025 18:32:22 +0000</pubDate><dc:creator>Ruichu Cai</dc:creator><dc:creator>Junxian Huang</dc:creator><dc:creator>Zhenhui Yang</dc:creator><dc:creator>Zijian Li</dc:creator><dc:creator>Emadeldeen Eldele</dc:creator><dc:creator>Min Wu</dc:creator><dc:creator>Fuchun Sun</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642245</prism:doi><description>Time series domain adaptation aims to transfer the complex temporal dependence from the labeled source domain to the unlabeled target domain. Recent advances leverage the stable causal mechanism over observed variables to model the domain-invariant temporal dependence. However, modeling precise causal structures in high-dimensional data, such as videos, remains challenging. Additionally, direct causal edges may not exist among observed variables (e.g., pixels). These limitations hinder the applicability of existing approaches to real-world scenarios. To address these challenges, we find that the high-dimension time series data are generated from the low-dimension latent variables, which motivates us to model the causal mechanisms of the temporal latent process. Based on this intuition, we propose a latent causal mechanism identification framework that guarantees the uniqueness of the reconstructed latent causal structures. Specifically, we first identify latent variables by utilizing sufficient changes in historical information. Moreover, by enforcing the sparsity of the relationships of latent variables, we can achieve identifiable latent causal structures. Built on the theoretical results, we develop the Latent Causality Alignment (LCA) model that leverages variational inference, which incorporates an intra-domain latent sparsity constraint for latent structure reconstruction and an inter-domain latent sparsity constraint for domain-invariant structure reconstruction. Experiment results on eight benchmarks show a general improvement in the domain-adaptive time series classification and forecasting tasks, highlighting the effectiveness of our method in real-world scenarios. Codes are available at https://github.com/DMIRLAB-Group/LCA.
Published: 2025-12-10T18:32:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruichu Cai; Junxian Huang; Zhenhui Yang; Zijian Li; Emadeldeen Eldele; Min Wu; Fuchun Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642245"&gt;10.1109/tpami.2025.3642245&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Time series domain adaptation aims to transfer the complex temporal dependence from the labeled source domain to the unlabeled target domain. Recent advances leverage the stable causal mechanism over observed variables to model the domain-invariant temporal dependence. However, modeling precise causal structures in high-dimensional data, such as videos, remains challenging. Additionally, direct causal edges may not exist among observed variables (e.g., pixels). These limitations hinder the applicability of existing approaches to real-world scenarios. To address these challenges, we find that the high-dimension time series data are generated from the low-dimension latent variables, which motivates us to model the causal mechanisms of the temporal latent process. Based on this intuition, we propose a latent causal mechanism identification framework that guarantees the uniqueness of the reconstructed latent causal structures. Specifically, we first identify latent variables by utilizing sufficient changes in historical information. Moreover, by enforcing the sparsity of the relationships of latent variables, we can achieve identifiable latent causal structures. Built on the theoretical results, we develop the Latent Causality Alignment (LCA) model that leverages variational inference, which incorporates an intra-domain latent sparsity constraint for latent structure reconstruction and an inter-domain latent sparsity constraint for domain-invariant structure reconstruction. Experiment results on eight benchmarks show a general improvement in the domain-adaptive time series classification and forecasting tasks, highlighting the effectiveness of our method in real-world scenarios. Codes are available at https://github.com/DMIRLAB-Group/LCA.&lt;/p&gt;</content:encoded></item></channel></rss>