<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 20 Dec 2025 02:51:35 +0000</lastBuildDate><item><title>The CUR Decomposition of Self-Attention Matrices in Vision Transformers</title><link>https://doi.org/10.1109/tpami.2025.3646452</link><guid>10.1109/tpami.2025.3646452</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Chong Wu</dc:creator><dc:creator>Maolin Che</dc:creator><dc:creator>Hong Yan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646452</prism:doi><description>Transformers have achieved great success in natural language processing and computer vision. The core and basic technique of transformers is the self-attention mechanism. The vanilla self-attention mechanism has quadratic complexity, which limits its applications to vision tasks. Most of the existing linear self-attention mechanisms will sacrifice performance to some extent to reduce complexity. In this paper, we propose a novel linear approximation of the vanilla self-attention mechanism named CURSA to achieve both high performance and low complexity at the same time. CURSA is based on the CUR decomposition to decompose the multiplication of large matrices into the multiplication of several small matrices to achieve almost linear complexity. Experiment results of CURSA in image classification tasks, semantic segmentation tasks, object detection tasks, and long-range arena show that it outperforms state-of-the-art self-attention mechanisms with better data efficiency, faster speed, and higher accuracy.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.835 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chong Wu; Maolin Che; Hong Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646452"&gt;10.1109/tpami.2025.3646452&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.835 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have achieved great success in natural language processing and computer vision. The core and basic technique of transformers is the self-attention mechanism. The vanilla self-attention mechanism has quadratic complexity, which limits its applications to vision tasks. Most of the existing linear self-attention mechanisms will sacrifice performance to some extent to reduce complexity. In this paper, we propose a novel linear approximation of the vanilla self-attention mechanism named CURSA to achieve both high performance and low complexity at the same time. CURSA is based on the CUR decomposition to decompose the multiplication of large matrices into the multiplication of several small matrices to achieve almost linear complexity. Experiment results of CURSA in image classification tasks, semantic segmentation tasks, object detection tasks, and long-range arena show that it outperforms state-of-the-art self-attention mechanisms with better data efficiency, faster speed, and higher accuracy.&lt;/p&gt;</content:encoded></item><item><title>DCCS-Det: Directional Context and Cross-Scale Aware Detector for Infrared Small Target</title><link>https://doi.org/10.1109/tgrs.2025.3646345</link><guid>10.1109/tgrs.2025.3646345</guid><pubDate>Fri, 19 Dec 2025 18:58:35 +0000</pubDate><dc:creator>Shuying Li</dc:creator><dc:creator>Qiang Ma</dc:creator><dc:creator>San Zhang</dc:creator><dc:creator>Chuang Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646345</prism:doi><description>Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. Code is available at https://github.com/ML202010/DCCS-Det.
Published: 2025-12-19T18:58:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.827 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuying Li; Qiang Ma; San Zhang; Chuang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646345"&gt;10.1109/tgrs.2025.3646345&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.827 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is critical for applications like remote sensing and surveillance, which aims to identify small, low-contrast targets against complex backgrounds. However, existing methods often struggle with inadequate joint modeling of local-global features (harming target-background discrimination) or feature redundancy and semantic dilution (degrading target representation quality). To tackle these issues, we propose DCCS-Det (Directional Context and Cross-Scale Aware Detector for Infrared Small Target), a novel detector that incorporates a Dual-stream Saliency Enhancement (DSE) block and a Latent-aware Semantic Extraction and Aggregation (LaSEA) module. The DSE block integrates localized perception with direction-aware context aggregation to help capture long-range spatial dependencies and local details. On this basis, the LaSEA module mitigates feature degradation via cross-scale feature extraction and random pooling sampling strategies, enhancing discriminative features and suppressing noise. Extensive experiments show that DCCS-Det achieves state-of-the-art detection accuracy with competitive efficiency across multiple datasets. Ablation studies further validate the contributions of DSE and LaSEA in improving target perception and feature representation under complex scenarios. Code is available at https://github.com/ML202010/DCCS-Det.&lt;/p&gt;</content:encoded></item><item><title>Controllable Generation with Text-to-Image Diffusion Models: a Survey</title><link>https://doi.org/10.1109/tpami.2025.3646548</link><guid>10.1109/tpami.2025.3646548</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Pu Cao</dc:creator><dc:creator>Feng Zhou</dc:creator><dc:creator>Qing Song</dc:creator><dc:creator>Lu Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646548</prism:doi><description>In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. Additionally, we provide a detailed overview of research in this area, categorizing it from the condition perspective into three directions: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For each category, we analyze the underlying control mechanisms and review representative methods based on their core techniques. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.823 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pu Cao; Feng Zhou; Qing Song; Lu Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646548"&gt;10.1109/tpami.2025.3646548&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.823 (must_read)&lt;/p&gt;
&lt;p&gt;In the rapidly advancing realm of visual generation, diffusion models have revolutionized the landscape, marking a significant shift in capabilities with their impressive text-guided generative functions. However, relying solely on text for conditioning these models does not fully cater to the varied and complex requirements of different applications and scenarios. Acknowledging this shortfall, a variety of studies aim to control pre-trained text-to-image (T2I) models to support novel conditions. In this survey, we undertake a thorough review of the literature on controllable generation with T2I diffusion models, covering both the theoretical foundations and practical advancements in this domain. Our review begins with a brief introduction to the basics of denoising diffusion probabilistic models (DDPMs) and widely used T2I diffusion models. Additionally, we provide a detailed overview of research in this area, categorizing it from the condition perspective into three directions: generation with specific conditions, generation with multiple conditions, and universal controllable generation. For each category, we analyze the underlying control mechanisms and review representative methods based on their core techniques. For an exhaustive list of the controllable generation literature surveyed, please refer to our curated repository at https://github.com/PRIV-Creation/Awesome-Controllable-T2I-Diffusion-Models.&lt;/p&gt;</content:encoded></item><item><title>Two-Stage SAR Image Generation Based on Attribute Feature Decoupling</title><link>https://doi.org/10.1109/lgrs.2025.3645620</link><guid>10.1109/lgrs.2025.3645620</guid><pubDate>Thu, 18 Dec 2025 18:36:05 +0000</pubDate><dc:creator>Rubo Jin</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Jianda Cheng</dc:creator><dc:creator>Hui Fan</dc:creator><dc:creator>Jiyuan Liu</dc:creator><dc:creator>Hongqi Fan</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3645620</prism:doi><description>Training of synthetic aperture radar (SAR) target detection and recognition methods based on deep learning heavily relies on a large amount of data. As one of the significant approaches to address the scarcity of SAR data, SAR image intelligent generation methods have witnessed rapid development. However, these methods often require many data samples for learning and are prone to deviating from the physical scattering characteristics. To address these issues, this paper proposes a two-stage SAR image generation method based on attribute feature decoupling within a generative adversarial network (GAN) architecture. In the first stage, the original SAR target image undergoes feature extraction and reconstruction, yielding generated images highly similar to real images. The attribute features decoupled during this process correlate with the scattering characteristics of SAR target, providing guiding information for generating target images in the second stage. In the second stage, by applying perturbations to specific dimensions of the decoupled features, we can reconstruct target images with altered attributes, achieving diverse data augmentation. Multi-task discrimination based on pixel intensity, authenticity, and feature distance differences enhances the quality of generated images across multiple levels. The decoupled representation-driven generation paradigm simplifies the network’s mapping learning task through task decomposition, diminishing the dependency on the volume of data. The experimental results demonstrate that the generated images possess higher quality and superior application performance, with an improvement of 5.23% in recognition accuracy.
Published: 2025-12-18T18:36:05+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rubo Jin; Wei Wang; Jianda Cheng; Hui Fan; Jiyuan Liu; Hongqi Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3645620"&gt;10.1109/lgrs.2025.3645620&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Training of synthetic aperture radar (SAR) target detection and recognition methods based on deep learning heavily relies on a large amount of data. As one of the significant approaches to address the scarcity of SAR data, SAR image intelligent generation methods have witnessed rapid development. However, these methods often require many data samples for learning and are prone to deviating from the physical scattering characteristics. To address these issues, this paper proposes a two-stage SAR image generation method based on attribute feature decoupling within a generative adversarial network (GAN) architecture. In the first stage, the original SAR target image undergoes feature extraction and reconstruction, yielding generated images highly similar to real images. The attribute features decoupled during this process correlate with the scattering characteristics of SAR target, providing guiding information for generating target images in the second stage. In the second stage, by applying perturbations to specific dimensions of the decoupled features, we can reconstruct target images with altered attributes, achieving diverse data augmentation. Multi-task discrimination based on pixel intensity, authenticity, and feature distance differences enhances the quality of generated images across multiple levels. The decoupled representation-driven generation paradigm simplifies the network’s mapping learning task through task decomposition, diminishing the dependency on the volume of data. The experimental results demonstrate that the generated images possess higher quality and superior application performance, with an improvement of 5.23% in recognition accuracy.&lt;/p&gt;</content:encoded></item><item><title>Context-Aware and Semantic-Guided Adaptive Filtering Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3646255</link><guid>10.1109/tgrs.2025.3646255</guid><pubDate>Fri, 19 Dec 2025 18:58:35 +0000</pubDate><dc:creator>Lingchuan Kong</dc:creator><dc:creator>Bo Yang</dc:creator><dc:creator>Rui Chang</dc:creator><dc:creator>Jun Luo</dc:creator><dc:creator>Huayan Pu</dc:creator><dc:creator>Yangjun Pi</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646255</prism:doi><description>Infrared small target detection (ISTD) is a crucial task to identify tiny targets from infrared images. Although existing hybrid CNN-Transformer methods achieve excellent segmentation performance, they still face some challenges. First, the self-attention mechanism is insensitive to subtle local variations and incurs high computational cost; second, during feature fusion these methods fail to fully exploit the key information contained in shallow features. Consequently, they struggle to distinguish targets from backgrounds efficiently and accurately in scenes where the two are highly similar. To address these issues, this paper proposes CSAFNet to enhances the discriminability of targets and backgrounds. Specifically, we introduce Parallel Self-Awareness Attention (PSAA), which leverages physical priors to capture global context and incorporates wavelet transforms to strengthen local detail, achieving efficient fusion of local and global features. Considering the importance of shallow features for precise localization and fine segmentation, we design cross-semantic adaptive filtering module (CAFM) in feature fusion, which deeply explores key information from shallow features and enhances the relative saliency of target representations. Moreover, we propose the dynamic multi-scale spatial pyramid (DMSSP) module to improve edge precision and enhance segmentation accuracy. Extensive experiments on the two most widely used ISTD datasets, NUAA-SIRST and IRSTD-1K, show that CSAFNet outperforms other state-of-the-art methods. The code is available at https://github.com/LingchuanK/CSAFNet.
Published: 2025-12-19T18:58:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lingchuan Kong; Bo Yang; Rui Chang; Jun Luo; Huayan Pu; Yangjun Pi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646255"&gt;10.1109/tgrs.2025.3646255&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) is a crucial task to identify tiny targets from infrared images. Although existing hybrid CNN-Transformer methods achieve excellent segmentation performance, they still face some challenges. First, the self-attention mechanism is insensitive to subtle local variations and incurs high computational cost; second, during feature fusion these methods fail to fully exploit the key information contained in shallow features. Consequently, they struggle to distinguish targets from backgrounds efficiently and accurately in scenes where the two are highly similar. To address these issues, this paper proposes CSAFNet to enhances the discriminability of targets and backgrounds. Specifically, we introduce Parallel Self-Awareness Attention (PSAA), which leverages physical priors to capture global context and incorporates wavelet transforms to strengthen local detail, achieving efficient fusion of local and global features. Considering the importance of shallow features for precise localization and fine segmentation, we design cross-semantic adaptive filtering module (CAFM) in feature fusion, which deeply explores key information from shallow features and enhances the relative saliency of target representations. Moreover, we propose the dynamic multi-scale spatial pyramid (DMSSP) module to improve edge precision and enhance segmentation accuracy. Extensive experiments on the two most widely used ISTD datasets, NUAA-SIRST and IRSTD-1K, show that CSAFNet outperforms other state-of-the-art methods. The code is available at https://github.com/LingchuanK/CSAFNet.&lt;/p&gt;</content:encoded></item><item><title>Unconstrained Feature Enhancement Text-Guided Few-Shot Remote Sensing Image Object Detector</title><link>https://doi.org/10.1109/tgrs.2025.3645347</link><guid>10.1109/tgrs.2025.3645347</guid><pubDate>Thu, 18 Dec 2025 18:33:19 +0000</pubDate><dc:creator>Xiping Shang</dc:creator><dc:creator>Wei Zhao</dc:creator><dc:creator>Haoxiang Chen</dc:creator><dc:creator>Xudong Fan</dc:creator><dc:creator>Nannan Li</dc:creator><dc:creator>Dongjin Li</dc:creator><dc:creator>Jianwei Lv</dc:creator><dc:creator>Rufei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3645347</prism:doi><description>Few-shot object detection for remote sensing images (RSIs) has received much attention. The main difficulty in few-shot object detection currently lies in the imprecise classification, which can be tackled from three key perspectives: features to be classified (Region of Interest features), reference features (feature prototypes), and classifiers. However, existing methodologies have not comprehensively addressed these aspects. To fill this gap, we propose the Unconstrained Feature Enhancement Text-Guided Few-Shot Remote Sensing Image Object Detector (UFEDet). Initially, we introduce the text-image shared-specific module, which not only focuses on the consistent representation of the same class across different modalities but also emphasizes the unique information of each modality, thereby enhancing the distinctiveness of class feature prototypes. Subsequently, we propose an unconstrained feature enhancement module, which enhances the Region of Interest (RoI) features by leveraging the global class information from feature prototypes, while also increasing the distributional differences between classifiers. Finally, we introduce a gradient-controlled optimizer that modulates the gradient based on the gradient response of base classes, alleviating catastrophic forgetting of base classes during the training of novel classes. To validate the efficacy of our proposed approach, experiments were conducted on the DIOR and NWPU VHR-10 datasets. The experimental results demonstrate that the mAP of both base and novel classes exceeds that of popular few-shot object detectors.
Published: 2025-12-18T18:33:19+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiping Shang; Wei Zhao; Haoxiang Chen; Xudong Fan; Nannan Li; Dongjin Li; Jianwei Lv; Rufei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3645347"&gt;10.1109/tgrs.2025.3645347&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot object detection for remote sensing images (RSIs) has received much attention. The main difficulty in few-shot object detection currently lies in the imprecise classification, which can be tackled from three key perspectives: features to be classified (Region of Interest features), reference features (feature prototypes), and classifiers. However, existing methodologies have not comprehensively addressed these aspects. To fill this gap, we propose the Unconstrained Feature Enhancement Text-Guided Few-Shot Remote Sensing Image Object Detector (UFEDet). Initially, we introduce the text-image shared-specific module, which not only focuses on the consistent representation of the same class across different modalities but also emphasizes the unique information of each modality, thereby enhancing the distinctiveness of class feature prototypes. Subsequently, we propose an unconstrained feature enhancement module, which enhances the Region of Interest (RoI) features by leveraging the global class information from feature prototypes, while also increasing the distributional differences between classifiers. Finally, we introduce a gradient-controlled optimizer that modulates the gradient based on the gradient response of base classes, alleviating catastrophic forgetting of base classes during the training of novel classes. To validate the efficacy of our proposed approach, experiments were conducted on the DIOR and NWPU VHR-10 datasets. The experimental results demonstrate that the mAP of both base and novel classes exceeds that of popular few-shot object detectors.&lt;/p&gt;</content:encoded></item><item><title>Handwritten Text Recognition: A Survey</title><link>https://doi.org/10.1109/tpami.2025.3646002</link><guid>10.1109/tpami.2025.3646002</guid><pubDate>Thu, 18 Dec 2025 18:33:13 +0000</pubDate><dc:creator>Carlos Garrido-Munoz</dc:creator><dc:creator>Antonio Rios-Vila</dc:creator><dc:creator>Jorge Calvo-Zaragoza</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646002</prism:doi><description>Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) up to line-level, encompassing word and line recognition, and (2) beyond line-level, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.
Published: 2025-12-18T18:33:13+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Carlos Garrido-Munoz; Antonio Rios-Vila; Jorge Calvo-Zaragoza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646002"&gt;10.1109/tpami.2025.3646002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Handwritten Text Recognition (HTR) has become an essential field within pattern recognition and machine learning, with applications spanning historical document preservation to modern data entry and accessibility solutions. The complexity of HTR lies in the high variability of handwriting, which makes it challenging to develop robust recognition systems. This survey examines the evolution of HTR models, tracing their progression from early heuristic-based approaches to contemporary state-of-the-art neural models, which leverage deep learning techniques. The scope of the field has also expanded, with models initially capable of recognizing only word-level content progressing to recent end-to-end document-level approaches. Our paper categorizes existing work into two primary levels of recognition: (1) up to line-level, encompassing word and line recognition, and (2) beyond line-level, addressing paragraph- and document-level challenges. We provide a unified framework that examines research methodologies, recent advances in benchmarking, key datasets in the field, and a discussion of the results reported in the literature. Finally, we identify pressing research challenges and outline promising future directions, aiming to equip researchers and practitioners with a roadmap for advancing the field.&lt;/p&gt;</content:encoded></item><item><title>DSwinIR: Rethinking Window-Based Attention for Image Restoration</title><link>https://doi.org/10.1109/tpami.2025.3646016</link><guid>10.1109/tpami.2025.3646016</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Gang Wu</dc:creator><dc:creator>Junjun Jiang</dc:creator><dc:creator>Kui Jiang</dc:creator><dc:creator>Xianming Liu</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646016</prism:doi><description>Image restoration has witnessed significant advancements with the development of deep learning models. Transformer-based models, particularly those using window-based self-attention, have become a dominant force. However, their performance is constrained by the rigid, non-overlapping window partitioning scheme, which leads to insufficient feature interaction across windows and limited receptive fields. This highlights the need for more adaptive and flexible attention mechanisms. In this paper, we propose the Deformable Sliding Window Transformer for Image Restoration (DSwinIR), a new attention mechanism: the Deformable Sliding Window (DSwin) Attention. This mechanism introduces a token-centric and content-aware paradigm that moves beyond the grid and fixed window partition. It comprises two complementary components. First, it replaces the rigid partitioning with a token-centric sliding window paradigm, making it effective at eliminating boundary artifacts. Second, it incorporates a content-aware deformable sampling strategy, which allows the attention mechanism to learn data-dependent offsets and actively shape its receptive field to focus on the most informative image regions. Extensive experiments show that DSwinIR achieves strong results, including stateoftheart performance on several evaluated benchmarks. For instance, in all-in-one image restoration, our DSwinIR surpasses the most recent backbone GridFormer by 0.53 dB on the three-task benchmark and 0.87 dB on the five-task benchmark. The code and pre-trained models are available at https://github.com/Aitical/DSwinIR.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gang Wu; Junjun Jiang; Kui Jiang; Xianming Liu; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646016"&gt;10.1109/tpami.2025.3646016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Image restoration has witnessed significant advancements with the development of deep learning models. Transformer-based models, particularly those using window-based self-attention, have become a dominant force. However, their performance is constrained by the rigid, non-overlapping window partitioning scheme, which leads to insufficient feature interaction across windows and limited receptive fields. This highlights the need for more adaptive and flexible attention mechanisms. In this paper, we propose the Deformable Sliding Window Transformer for Image Restoration (DSwinIR), a new attention mechanism: the Deformable Sliding Window (DSwin) Attention. This mechanism introduces a token-centric and content-aware paradigm that moves beyond the grid and fixed window partition. It comprises two complementary components. First, it replaces the rigid partitioning with a token-centric sliding window paradigm, making it effective at eliminating boundary artifacts. Second, it incorporates a content-aware deformable sampling strategy, which allows the attention mechanism to learn data-dependent offsets and actively shape its receptive field to focus on the most informative image regions. Extensive experiments show that DSwinIR achieves strong results, including stateoftheart performance on several evaluated benchmarks. For instance, in all-in-one image restoration, our DSwinIR surpasses the most recent backbone GridFormer by 0.53 dB on the three-task benchmark and 0.87 dB on the five-task benchmark. The code and pre-trained models are available at https://github.com/Aitical/DSwinIR.&lt;/p&gt;</content:encoded></item><item><title>Efficient Scene Modeling Via Structure-Aware and Region-Prioritized 3D Gaussians</title><link>https://doi.org/10.1109/tpami.2025.3646473</link><guid>10.1109/tpami.2025.3646473</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Guangchi Fang</dc:creator><dc:creator>Bing Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646473</prism:doi><description>Reconstructing 3D scenes with high fidelity and efficiency remains a central pursuit in computer vision and graphics. Recent advances in 3D Gaussian Splatting (3DGS) enable photorealistic rendering with Gaussian primitives, yet the modeling process remains governed predominantly by photometric supervision. This reliance often leads to irregular spatial distribution and indiscriminate primitive adjustments that largely ignore underlying geometric context. In this work, we rethink Gaussian modeling from a geometric standpoint and introduce Mini-Splatting2, an efficient scene modeling framework that couples structure-aware distribution and region-prioritized optimization, driving 3DGS into a geometry-regulated paradigm. The structure-aware distribution enforces spatial regularity through structured reorganization and representation sparsity, ensuring balanced structural coverage for compact organization. The region-prioritized optimization improves training discrimination through geometric saliency and computational selectivity, fostering appropriate structural emergence for fast convergence. These mechanisms alleviate the long-standing tension among representation compactness, convergence acceleration, and rendering fidelity. Extensive experiments demonstrate that Mini-Splatting2 achieves up to 4× fewer Gaussians and 3× faster optimization while maintaining state-of-the-art visual quality, paving the way towards structured and efficient 3D Gaussian modeling.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangchi Fang; Bing Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646473"&gt;10.1109/tpami.2025.3646473&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Reconstructing 3D scenes with high fidelity and efficiency remains a central pursuit in computer vision and graphics. Recent advances in 3D Gaussian Splatting (3DGS) enable photorealistic rendering with Gaussian primitives, yet the modeling process remains governed predominantly by photometric supervision. This reliance often leads to irregular spatial distribution and indiscriminate primitive adjustments that largely ignore underlying geometric context. In this work, we rethink Gaussian modeling from a geometric standpoint and introduce Mini-Splatting2, an efficient scene modeling framework that couples structure-aware distribution and region-prioritized optimization, driving 3DGS into a geometry-regulated paradigm. The structure-aware distribution enforces spatial regularity through structured reorganization and representation sparsity, ensuring balanced structural coverage for compact organization. The region-prioritized optimization improves training discrimination through geometric saliency and computational selectivity, fostering appropriate structural emergence for fast convergence. These mechanisms alleviate the long-standing tension among representation compactness, convergence acceleration, and rendering fidelity. Extensive experiments demonstrate that Mini-Splatting2 achieves up to 4× fewer Gaussians and 3× faster optimization while maintaining state-of-the-art visual quality, paving the way towards structured and efficient 3D Gaussian modeling.&lt;/p&gt;</content:encoded></item><item><title>Few-shot object detection via semantic prompts and classifier decoupling</title><link>https://doi.org/10.1016/j.neunet.2025.108488</link><guid>10.1016/j.neunet.2025.108488</guid><pubDate>Fri, 19 Dec 2025 07:49:43 +0000</pubDate><dc:creator>Baifan Chen</dc:creator><dc:creator>Ruyi Zhu</dc:creator><dc:creator>Yilan Li</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108488</prism:doi><description>Many existing few-shot object detection methods employ two-stage detectors to achieve higher accuracy. Given the limited feature information and the challenges of adapting two-stage object detectors to few-shot learning, this paper proposes a few-shot object detection method based on semantic prompts and classifier decoupling. The key to incorporating textual information into object detectors lies in the effective fusion and alignment of image and text features. This paper introduces a Semantic Prompts module, enhancing the features of few-shot learning while aiding the model in better understanding image content. Leveraging the functionalities of the components of two-stage object detectors and their inter-component interactions, Gradient Scaling is employed to attenuate parameter updates, mitigating negative inter-module influences. To address the inconsistent feature demands between classification and regression branches, a Classifier Decoupling module is utilized to achieve more accurate classification and localization effects. Experimental evaluations on benchmark datasets demonstrate that the proposed method outperforms strong baselines such as DeFRCN by up to 3.15% mAP under 1-shot settings on PASCAL VOC. These improvements indicate enhanced generalization and robustness in low-data regimes.
Published: 2025-12-19T07:49:43+00:00
Venue: Neural Networks
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baifan Chen; Ruyi Zhu; Yilan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108488"&gt;10.1016/j.neunet.2025.108488&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Many existing few-shot object detection methods employ two-stage detectors to achieve higher accuracy. Given the limited feature information and the challenges of adapting two-stage object detectors to few-shot learning, this paper proposes a few-shot object detection method based on semantic prompts and classifier decoupling. The key to incorporating textual information into object detectors lies in the effective fusion and alignment of image and text features. This paper introduces a Semantic Prompts module, enhancing the features of few-shot learning while aiding the model in better understanding image content. Leveraging the functionalities of the components of two-stage object detectors and their inter-component interactions, Gradient Scaling is employed to attenuate parameter updates, mitigating negative inter-module influences. To address the inconsistent feature demands between classification and regression branches, a Classifier Decoupling module is utilized to achieve more accurate classification and localization effects. Experimental evaluations on benchmark datasets demonstrate that the proposed method outperforms strong baselines such as DeFRCN by up to 3.15% mAP under 1-shot settings on PASCAL VOC. These improvements indicate enhanced generalization and robustness in low-data regimes.&lt;/p&gt;</content:encoded></item><item><title>Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution</title><link>https://doi.org/10.1109/tip.2025.3643146</link><guid>10.1109/tip.2025.3643146</guid><pubDate>Thu, 18 Dec 2025 18:35:49 +0000</pubDate><dc:creator>Junbo Qiao</dc:creator><dc:creator>Jincheng Liao</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Yulun Zhang</dc:creator><dc:creator>Yong Guo</dc:creator><dc:creator>Jiao Xie</dc:creator><dc:creator>Jie Hu</dc:creator><dc:creator>Shaohui Lin</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643146</prism:doi><description>Despite Transformers have achieved significant success in low-level vision tasks, they are constrained by computing self-attention with a quadratic complexity and limited-size windows. This limitation results in a lack of global receptive field across the entire image. Recently, State Space Models (SSMs) have gained widespread attention due to their global receptive field and linear complexity with respect to input length. However, integrating SSMs into low-level vision tasks presents two major challenges: (1) Relationship degradation of long-range tokens with a long-range forgetting problem by encoding pixel-by-pixel high-resolution images. (2) Significant redundancy in the existing multi-direction scanning strategy. To this end, we propose Hi-Mamba for image super-resolution (SR) to address these challenges, which unfolds the image with only a single scan. Specifically, the Global Hierarchical Mamba Block (GHMB) enables token interactions across the entire image, providing a global receptive field while leveraging a multi-scale structure to facilitate long-range dependency learning. Additionally, the Direction Alternation Module (DAM) adjusts the scanning patterns of GHMB across different layers to enhance spatial relationship modeling. Extensive experiments demonstrate that our Hi-Mamba achieves 0.2-0.27dB PSNR gains on the Urban100 dataset across different scaling factors compared to the state-of-the-art MambaIRv2 for SR. Moreover, our lightweight Hi-Mamba also outperforms lightweight SRFormer by 0.39dB PSNR for ×2 SR.
Published: 2025-12-18T18:35:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junbo Qiao; Jincheng Liao; Wei Li; Yulun Zhang; Yong Guo; Jiao Xie; Jie Hu; Shaohui Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643146"&gt;10.1109/tip.2025.3643146&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Despite Transformers have achieved significant success in low-level vision tasks, they are constrained by computing self-attention with a quadratic complexity and limited-size windows. This limitation results in a lack of global receptive field across the entire image. Recently, State Space Models (SSMs) have gained widespread attention due to their global receptive field and linear complexity with respect to input length. However, integrating SSMs into low-level vision tasks presents two major challenges: (1) Relationship degradation of long-range tokens with a long-range forgetting problem by encoding pixel-by-pixel high-resolution images. (2) Significant redundancy in the existing multi-direction scanning strategy. To this end, we propose Hi-Mamba for image super-resolution (SR) to address these challenges, which unfolds the image with only a single scan. Specifically, the Global Hierarchical Mamba Block (GHMB) enables token interactions across the entire image, providing a global receptive field while leveraging a multi-scale structure to facilitate long-range dependency learning. Additionally, the Direction Alternation Module (DAM) adjusts the scanning patterns of GHMB across different layers to enhance spatial relationship modeling. Extensive experiments demonstrate that our Hi-Mamba achieves 0.2-0.27dB PSNR gains on the Urban100 dataset across different scaling factors compared to the state-of-the-art MambaIRv2 for SR. Moreover, our lightweight Hi-Mamba also outperforms lightweight SRFormer by 0.39dB PSNR for ×2 SR.&lt;/p&gt;</content:encoded></item><item><title>Noisy Correspondence Rectification in Multimodal Clustering Space for Cross-Modal Matching</title><link>https://doi.org/10.1109/tpami.2025.3646184</link><guid>10.1109/tpami.2025.3646184</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Shuo Yang</dc:creator><dc:creator>Yancheng Long</dc:creator><dc:creator>Yujie Wei</dc:creator><dc:creator>Zeke Xie</dc:creator><dc:creator>Hongxun Yao</dc:creator><dc:creator>Min Xu</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646184</prism:doi><description>As one of the most fundamental techniques in multimodal learning, cross-modal matching aims to project various sensory modalities into a shared feature space. To achieve this, massive and correctly aligned data pairs are required for model training. However, unlike unimodal datasets, multimodal datasets are extremely harder to collect and annotate precisely. As an alternative, the co-occurred data pairs (e.g., image-text pairs) collected from the Internet have been widely exploited in the area. Unfortunately, the cheaply collected dataset unavoidably contains many mismatched data pairs, which have been proven to be harmful to the model's performance. To address this, we propose BiCro++ (Improved Bidirectional Cross-modal Similarity Consistency). This module can be integrated into existing cross-modal matching models, enhancing their robustness against noisy data through self-adaptive soft labels that dynamically reflect the true correspondence of data pairs. The basic idea of BiCro++ is motivated by that taking image-text matching as an example similar images should have similar textual descriptions and vice versa. This bidirectional similarity consistency can be directly translated into soft labels as a self-supervision signal to train the matching model. To further refine soft label quality, BiCro++ first introduces a Diagonal-Dominance Purification process to identify reliable anchor points from noisy dataset as the reference for soft label estimation. Then it employs a Hybrid-level Codebook Alignment mechanism that establishes enhanced consistency in bidirectional cross-modal similarity. The experiments on three popular cross-modal matching datasets show that our method significantly improves the noise-robustness of various matching models, and surpasses the state-of-the-art method by an average of 5.3%, 3.1% and 6.4% in terms of recall, respectively.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuo Yang; Yancheng Long; Yujie Wei; Zeke Xie; Hongxun Yao; Min Xu; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646184"&gt;10.1109/tpami.2025.3646184&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;As one of the most fundamental techniques in multimodal learning, cross-modal matching aims to project various sensory modalities into a shared feature space. To achieve this, massive and correctly aligned data pairs are required for model training. However, unlike unimodal datasets, multimodal datasets are extremely harder to collect and annotate precisely. As an alternative, the co-occurred data pairs (e.g., image-text pairs) collected from the Internet have been widely exploited in the area. Unfortunately, the cheaply collected dataset unavoidably contains many mismatched data pairs, which have been proven to be harmful to the model&amp;#x27;s performance. To address this, we propose BiCro++ (Improved Bidirectional Cross-modal Similarity Consistency). This module can be integrated into existing cross-modal matching models, enhancing their robustness against noisy data through self-adaptive soft labels that dynamically reflect the true correspondence of data pairs. The basic idea of BiCro++ is motivated by that taking image-text matching as an example similar images should have similar textual descriptions and vice versa. This bidirectional similarity consistency can be directly translated into soft labels as a self-supervision signal to train the matching model. To further refine soft label quality, BiCro++ first introduces a Diagonal-Dominance Purification process to identify reliable anchor points from noisy dataset as the reference for soft label estimation. Then it employs a Hybrid-level Codebook Alignment mechanism that establishes enhanced consistency in bidirectional cross-modal similarity. The experiments on three popular cross-modal matching datasets show that our method significantly improves the noise-robustness of various matching models, and surpasses the state-of-the-art method by an average of 5.3%, 3.1% and 6.4% in terms of recall, respectively.&lt;/p&gt;</content:encoded></item><item><title>DBCFS-Net: Dynamic Blocks and Cross-domain Feature Synergy for Remote Sensing Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3645796</link><guid>10.1109/tgrs.2025.3645796</guid><pubDate>Thu, 18 Dec 2025 18:33:19 +0000</pubDate><dc:creator>Qingyao Lin</dc:creator><dc:creator>Yuhui Zheng</dc:creator><dc:creator>Le Sun</dc:creator><dc:creator>Shihao Dong</dc:creator><dc:creator>Rugang Wang</dc:creator><dc:creator>Feng Zhou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3645796</prism:doi><description>The detection of objects in remote sensing images represents a significant sub-task in the domain of computer vision. However, it poses some challenges: mitigating high-frequency fine-grained feature loss, capturing efficient and discriminative object features, and improving separability between complex backgrounds and objects. So we propose a Dynamic Blocks and Cross-domain Feature Synergy(DBCFS-Net) for Remote Sensing Object Detection. Firstly, we propose a Dynamic Block Downsampling Module(DBDM). Its block partitioning strategy and dynamic offsets overcome the limitations of rigid downsampling that cause blurred features, while adaptively focusing on critical target regions to preserve high-frequency fine-grained details. Secondly, a Cross-Domain Aware Feature Enhancement Module Module(CDAFEM) is introduced to enhance the model’s adaptability to scale variations and complex backgrounds by establishing consistency constraints through cross-domain joint modeling, thereby preserving discriminative feature information. Concurrently, it refines object boundaries and edge structures through texture information optimization, reinforcing edge continuity to improve the model’s generalization capability and robustness. Finally, a Mamba-Based Local-Global Feature Co-Promotion Module(MB-LGFCPM) is designed to guide the model to couple local details with global contextual information, constructing a bidirectional ‘detail-semantic’ enhancement mechanism for feature representation. This achieves dynamic interaction and synergistic complementarity optimization between features. Extensive experimental results validate the effectiveness of the proposed method, achieving mAP scores of 87.8%, 96.9% and 96.6% on DIOR, RSOD and NWPU VHR-10 datasets—surpassing state-of-the-art detectors.
Published: 2025-12-18T18:33:19+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingyao Lin; Yuhui Zheng; Le Sun; Shihao Dong; Rugang Wang; Feng Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3645796"&gt;10.1109/tgrs.2025.3645796&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;The detection of objects in remote sensing images represents a significant sub-task in the domain of computer vision. However, it poses some challenges: mitigating high-frequency fine-grained feature loss, capturing efficient and discriminative object features, and improving separability between complex backgrounds and objects. So we propose a Dynamic Blocks and Cross-domain Feature Synergy(DBCFS-Net) for Remote Sensing Object Detection. Firstly, we propose a Dynamic Block Downsampling Module(DBDM). Its block partitioning strategy and dynamic offsets overcome the limitations of rigid downsampling that cause blurred features, while adaptively focusing on critical target regions to preserve high-frequency fine-grained details. Secondly, a Cross-Domain Aware Feature Enhancement Module Module(CDAFEM) is introduced to enhance the model’s adaptability to scale variations and complex backgrounds by establishing consistency constraints through cross-domain joint modeling, thereby preserving discriminative feature information. Concurrently, it refines object boundaries and edge structures through texture information optimization, reinforcing edge continuity to improve the model’s generalization capability and robustness. Finally, a Mamba-Based Local-Global Feature Co-Promotion Module(MB-LGFCPM) is designed to guide the model to couple local details with global contextual information, constructing a bidirectional ‘detail-semantic’ enhancement mechanism for feature representation. This achieves dynamic interaction and synergistic complementarity optimization between features. Extensive experimental results validate the effectiveness of the proposed method, achieving mAP scores of 87.8%, 96.9% and 96.6% on DIOR, RSOD and NWPU VHR-10 datasets—surpassing state-of-the-art detectors.&lt;/p&gt;</content:encoded></item><item><title>Towards High spatial resolution and fine-grained fidelity depth reconstruction of single-photon LiDAR with context-aware spatiotemporal modeling</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.034</link><guid>10.1016/j.isprsjprs.2025.11.034</guid><pubDate>Thu, 18 Dec 2025 16:01:21 +0000</pubDate><dc:creator>Zhenyu Zhang</dc:creator><dc:creator>Yuan Li</dc:creator><dc:creator>Feihu Zhu</dc:creator><dc:creator>Yuechao Ma</dc:creator><dc:creator>Junying Lv</dc:creator><dc:creator>Qian Sun</dc:creator><dc:creator>Lin Li</dc:creator><dc:creator>Wuming Zhang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.034</prism:doi><description>While single-photon LiDAR promises revolutionary depth sensing capabilities, existing deep learning frameworks fundamentally fail to overcome the challenge of high spatial resolution (SR) data processing. To address the amplification of fine geometric details and complex spatiotemporal dependencies in high-SR single-photon data, we adopt a U-Net++ backbone with dense skip connections to preserve high-frequency features. Our encoder cascades two novel modules, integrating attention-driven modulation and convolution to adaptively model intricate patterns without sacrificing detail. We propose a 3D triple local-attention fusion module (3D-TriLAF) to suppress incoherent responses across temporal, spatial, and channel axes. In parallel, an opposite continuous dilation spatial–temporal convolution module (OCDSConv) is designed to extract structured context while preserving transient cues. To alleviate the misalignment and semantic drift between low and high-level features—problems exacerbated by increased resolution—we design a multi-scale fusion mechanism that facilitates consistent geometric modeling across scales. Finally, we propose a hybrid loss combining ordinal regression (OR) loss, structural similarity index measure (SSIM) loss, and bilateral total variation (BTV) loss to jointly enhances peak localization, structural fidelity, and edge-aware smoothness. Extensive experiments on two 128 × 128 SR simulated datasets show that, compared with the best baseline, our framework reduces RMSE and Abs Rel by up to 60.00 % and 31.58 %. On two (200 + )×(200 + ) SR real-world datasets, RMSE and Abs Rel drop by 42.31 % and 39.44 %. These quantitative gains and visual improvements in geometric continuity under complex lighting confirm its suitability for fine-grained high-SR single-photon depth reconstruction.
Published: 2025-12-18T16:01:21+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyu Zhang; Yuan Li; Feihu Zhu; Yuechao Ma; Junying Lv; Qian Sun; Lin Li; Wuming Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.034"&gt;10.1016/j.isprsjprs.2025.11.034&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;While single-photon LiDAR promises revolutionary depth sensing capabilities, existing deep learning frameworks fundamentally fail to overcome the challenge of high spatial resolution (SR) data processing. To address the amplification of fine geometric details and complex spatiotemporal dependencies in high-SR single-photon data, we adopt a U-Net++ backbone with dense skip connections to preserve high-frequency features. Our encoder cascades two novel modules, integrating attention-driven modulation and convolution to adaptively model intricate patterns without sacrificing detail. We propose a 3D triple local-attention fusion module (3D-TriLAF) to suppress incoherent responses across temporal, spatial, and channel axes. In parallel, an opposite continuous dilation spatial–temporal convolution module (OCDSConv) is designed to extract structured context while preserving transient cues. To alleviate the misalignment and semantic drift between low and high-level features—problems exacerbated by increased resolution—we design a multi-scale fusion mechanism that facilitates consistent geometric modeling across scales. Finally, we propose a hybrid loss combining ordinal regression (OR) loss, structural similarity index measure (SSIM) loss, and bilateral total variation (BTV) loss to jointly enhances peak localization, structural fidelity, and edge-aware smoothness. Extensive experiments on two 128 × 128 SR simulated datasets show that, compared with the best baseline, our framework reduces RMSE and Abs Rel by up to 60.00 % and 31.58 %. On two (200 + )×(200 + ) SR real-world datasets, RMSE and Abs Rel drop by 42.31 % and 39.44 %. These quantitative gains and visual improvements in geometric continuity under complex lighting confirm its suitability for fine-grained high-SR single-photon depth reconstruction.&lt;/p&gt;</content:encoded></item><item><title>SD-Fuse: An Image Structure-Driven Model for Multi-Focus Image Fusion</title><link>https://doi.org/10.1016/j.inffus.2025.104058</link><guid>10.1016/j.inffus.2025.104058</guid><pubDate>Fri, 19 Dec 2025 05:59:44 +0000</pubDate><dc:creator>Zeyu Wang</dc:creator><dc:creator>Jiayu Wang</dc:creator><dc:creator>Haiyu Song</dc:creator><dc:creator>Pengjie Wang</dc:creator><dc:creator>Kedi Lyu</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Libo Zhao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104058</prism:doi><description>Multi-focus image fusion (MFIF) aims to generate a fully focused composite from multiple partially focused images. Existing methods often employ complex loss functions or customized network architectures to refine decision map boundaries, overlooking intrinsic structural information. In this study, we empirically uncover an image structure-boundary prior through comprehensive statistical analysis, explicitly demonstrating that boundaries between focused and defocused regions naturally align with prominent structural features of images. Motivated by this structural prior, we propose a structure-driven fusion framework termed SD-Fuse. This framework consists of three complementary components: a global structure-aware branch, a local focus detection branch, and a novel structure-guided filter (SGF). The structure-aware branch first extracts essential structural cues and employs a Transformer module to capture global structural dependencies. Concurrently, the focus detection branch leverages a CNN architecture to generate initial decision maps based on spatial inputs. Crucially, we introduce SGF, inspired by traditional guided filtering methods, to facilitate effective interaction between global and local features. Through optimization within SGF, the refined global structure provided by the Transformer progressively guides the local spatial features, ensuring precise alignment of boundaries and artifact-free decision maps. Extensive qualitative and quantitative experiments demonstrate that our SD-Fuse significantly outperforms existing methods, achieving state-of-the-art performance. We will release code and pretrained weights.
Published: 2025-12-19T05:59:44+00:00
Venue: Information Fusion
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeyu Wang; Jiayu Wang; Haiyu Song; Pengjie Wang; Kedi Lyu; Wei Li; Libo Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104058"&gt;10.1016/j.inffus.2025.104058&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-focus image fusion (MFIF) aims to generate a fully focused composite from multiple partially focused images. Existing methods often employ complex loss functions or customized network architectures to refine decision map boundaries, overlooking intrinsic structural information. In this study, we empirically uncover an image structure-boundary prior through comprehensive statistical analysis, explicitly demonstrating that boundaries between focused and defocused regions naturally align with prominent structural features of images. Motivated by this structural prior, we propose a structure-driven fusion framework termed SD-Fuse. This framework consists of three complementary components: a global structure-aware branch, a local focus detection branch, and a novel structure-guided filter (SGF). The structure-aware branch first extracts essential structural cues and employs a Transformer module to capture global structural dependencies. Concurrently, the focus detection branch leverages a CNN architecture to generate initial decision maps based on spatial inputs. Crucially, we introduce SGF, inspired by traditional guided filtering methods, to facilitate effective interaction between global and local features. Through optimization within SGF, the refined global structure provided by the Transformer progressively guides the local spatial features, ensuring precise alignment of boundaries and artifact-free decision maps. Extensive qualitative and quantitative experiments demonstrate that our SD-Fuse significantly outperforms existing methods, achieving state-of-the-art performance. We will release code and pretrained weights.&lt;/p&gt;</content:encoded></item><item><title>CMAI-Det: Cross-Modal Alignment and Interaction for RGB-T Object Detection in Drone Scenes</title><link>https://doi.org/10.1109/tgrs.2025.3645820</link><guid>10.1109/tgrs.2025.3645820</guid><pubDate>Thu, 18 Dec 2025 18:33:19 +0000</pubDate><dc:creator>Xin Wen</dc:creator><dc:creator>Haixu Yin</dc:creator><dc:creator>Kai Li</dc:creator><dc:creator>Wanying Nie</dc:creator><dc:creator>Jianxun Zhao</dc:creator><dc:creator>Kechen Song</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3645820</prism:doi><description>RGB-T object detection is increasingly applied in drone surveillance, autonomous driving, and intelligent transportation due to its robustness in complex conditions. However, most existing methods assume well-aligned image pairs and often overlook misalignment between modalities, which arises from drone motion, viewpoint shifts, and sensor inconsistencies, thereby hindering detection accuracy. To address this limitation, we present CMAI-Det, a framework designed for object detection in unaligned RGB-T images. The approach employs a dual-stream extractor to strengthen the representational capacity of visible and thermal features. A modality-cooperative alignment module, using the thermal stream as reference, integrates multi-scale deformable convolutions and attention to align visible features. An adaptive fusion scheme is further introduced to balance modality contributions according to their reliability under varying conditions, enhancing feature robustness. Finally, a perception-driven deformable detection head improves discriminability by reinforcing spatial selectivity and structural adaptability, enabling precise modeling of diverse object appearances. Extensive experiments on the DVTOD dataset show that CMAI-Det surpasses 12 state-of-the-art RGB-T detectors in challenging drone scenarios, achieving an mAP of 86.1. It also performs strongly at standard thresholds, with AP50 of 90.0 and AP75 of 82.3, underscoring its robustness and effectiveness in complex detection tasks. The code is available at https://github.com/yinhaixu2000-coder/CMAI-Detection.
Published: 2025-12-18T18:33:19+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Wen; Haixu Yin; Kai Li; Wanying Nie; Jianxun Zhao; Kechen Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3645820"&gt;10.1109/tgrs.2025.3645820&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;RGB-T object detection is increasingly applied in drone surveillance, autonomous driving, and intelligent transportation due to its robustness in complex conditions. However, most existing methods assume well-aligned image pairs and often overlook misalignment between modalities, which arises from drone motion, viewpoint shifts, and sensor inconsistencies, thereby hindering detection accuracy. To address this limitation, we present CMAI-Det, a framework designed for object detection in unaligned RGB-T images. The approach employs a dual-stream extractor to strengthen the representational capacity of visible and thermal features. A modality-cooperative alignment module, using the thermal stream as reference, integrates multi-scale deformable convolutions and attention to align visible features. An adaptive fusion scheme is further introduced to balance modality contributions according to their reliability under varying conditions, enhancing feature robustness. Finally, a perception-driven deformable detection head improves discriminability by reinforcing spatial selectivity and structural adaptability, enabling precise modeling of diverse object appearances. Extensive experiments on the DVTOD dataset show that CMAI-Det surpasses 12 state-of-the-art RGB-T detectors in challenging drone scenarios, achieving an mAP of 86.1. It also performs strongly at standard thresholds, with AP50 of 90.0 and AP75 of 82.3, underscoring its robustness and effectiveness in complex detection tasks. The code is available at https://github.com/yinhaixu2000-coder/CMAI-Detection.&lt;/p&gt;</content:encoded></item><item><title>Real-Time DEtection TRansformer Enhanced by WaveFormer and WS-GD Neck</title><link>https://doi.org/10.1109/lgrs.2025.3646494</link><guid>10.1109/lgrs.2025.3646494</guid><pubDate>Fri, 19 Dec 2025 19:03:04 +0000</pubDate><dc:creator>Litao Kang</dc:creator><dc:creator>Chaoyue Liu</dc:creator><dc:creator>Huaitao Fan</dc:creator><dc:creator>Zhimin Zhang</dc:creator><dc:creator>Zhen Chen</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3646494</prism:doi><description>Deep learning-based methods hold significant potential for synthetic aperture radar (SAR) target detection, but they still face numerous challenges, including difficulty extracting global contextual features for large-scale targets, significant multi-scale issues, and the problem of feature extraction of SAR targets with large aspect ratios, which hinder further performance improvement. To this end, this paper proposes a WaveFormer module, which decomposes the image through wavelet convolution and uses convolution and Transformer to process the frequency domain components they are good at, respectively, to expand the receptive field with low parameter overhead and enhance the target feature extraction ability. To address cross-layer information attenuation during feature fusion, a Gather-and-Distribute(GD) mechanism is introduced to reconstruct the Neck network, enhancing multi-scale feature fusion and detection capabilities. Furthermore, given the large aspect ratio and distinct principal axis orientation of SAR targets, a Weighted Strip-Convolution(WSConv) is proposed to effectively improve detection performance. Experiments on the largest multi-class SAR target detection dataset, SARDet-100K, demonstrate that our method achieves a mean average precision (mAP) of 61.5%, reaching state-of-the-art performance and validating its effectiveness.
Published: 2025-12-19T19:03:04+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Litao Kang; Chaoyue Liu; Huaitao Fan; Zhimin Zhang; Zhen Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3646494"&gt;10.1109/lgrs.2025.3646494&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning-based methods hold significant potential for synthetic aperture radar (SAR) target detection, but they still face numerous challenges, including difficulty extracting global contextual features for large-scale targets, significant multi-scale issues, and the problem of feature extraction of SAR targets with large aspect ratios, which hinder further performance improvement. To this end, this paper proposes a WaveFormer module, which decomposes the image through wavelet convolution and uses convolution and Transformer to process the frequency domain components they are good at, respectively, to expand the receptive field with low parameter overhead and enhance the target feature extraction ability. To address cross-layer information attenuation during feature fusion, a Gather-and-Distribute(GD) mechanism is introduced to reconstruct the Neck network, enhancing multi-scale feature fusion and detection capabilities. Furthermore, given the large aspect ratio and distinct principal axis orientation of SAR targets, a Weighted Strip-Convolution(WSConv) is proposed to effectively improve detection performance. Experiments on the largest multi-class SAR target detection dataset, SARDet-100K, demonstrate that our method achieves a mean average precision (mAP) of 61.5%, reaching state-of-the-art performance and validating its effectiveness.&lt;/p&gt;</content:encoded></item><item><title>Breaking the Multi-Enhancement Bottleneck: Domain-Consistent Quality Enhancement for Compressed Images</title><link>https://doi.org/10.1109/tpami.2025.3646223</link><guid>10.1109/tpami.2025.3646223</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Qunliang Xing</dc:creator><dc:creator>Ce Zheng</dc:creator><dc:creator>Mai Xu</dc:creator><dc:creator>Jing Yang</dc:creator><dc:creator>Shengxi Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646223</prism:doi><description>Quality enhancement methods have been widely integrated into visual communication pipelines to mitigate artifacts in compressed images. Ideally, these quality enhancement methods should perform robustly when applied to images that have already undergone prior enhancement during transmission. We refer to this scenario as multi-enhancement, which generalizes the well-known multi-generation scenario of image compression. Unfortunately, current quality enhancement methods suffer from severe degradation when applied in multi-enhancement. To address this challenge, we propose a novel adaptation method that transforms existing quality enhancement models into domain-consistent ones. Specifically, our method enhances a low-quality compressed image into a high-quality image within the natural domain during the first enhancement, and ensures that subsequent enhancements preserve this quality without further degradation. Extensive experiments validate the effectiveness of our method and show that various existing models can be successfully adapted to maintain both fidelity and perceptual quality in multi-enhancement scenarios.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qunliang Xing; Ce Zheng; Mai Xu; Jing Yang; Shengxi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646223"&gt;10.1109/tpami.2025.3646223&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Quality enhancement methods have been widely integrated into visual communication pipelines to mitigate artifacts in compressed images. Ideally, these quality enhancement methods should perform robustly when applied to images that have already undergone prior enhancement during transmission. We refer to this scenario as multi-enhancement, which generalizes the well-known multi-generation scenario of image compression. Unfortunately, current quality enhancement methods suffer from severe degradation when applied in multi-enhancement. To address this challenge, we propose a novel adaptation method that transforms existing quality enhancement models into domain-consistent ones. Specifically, our method enhances a low-quality compressed image into a high-quality image within the natural domain during the first enhancement, and ensures that subsequent enhancements preserve this quality without further degradation. Extensive experiments validate the effectiveness of our method and show that various existing models can be successfully adapted to maintain both fidelity and perceptual quality in multi-enhancement scenarios.&lt;/p&gt;</content:encoded></item><item><title>RA-MD: An RKHS-based Adaptive Mahalanobis Distance to Enhance Counterfactual Explanations for Neural Networks</title><link>https://doi.org/10.1016/j.inffus.2025.104067</link><guid>10.1016/j.inffus.2025.104067</guid><pubDate>Fri, 19 Dec 2025 05:59:50 +0000</pubDate><dc:creator>Ao Xu</dc:creator><dc:creator>Yukai Zhang</dc:creator><dc:creator>Tieru Wu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104067</prism:doi><description>With the rapid advancement of deep neural networks, their integration into critical decision-making systems has become a significant driver of societal progress, necessitating robust methods for interpretability. Counterfactual explanations (CEs) play a pivotal role in enhancing the transparency of neural network models within eXplainable Artificial Intelligence (XAI). Although extensive research has explored counterfactual explanation generation, efficiently producing minimal and human-interpretable CEs for complex neural architectures remains a persistent challenge. In this paper, we propose a unified RKHS-based Adaptive Mahalanobis Distance (RA-MD) framework for generating CEs in neural networks. The framework first selects the most informative layers using a Kernel Feature Disagreement (KFD) criterion, then captures feature relevance through a Wasserstein-based Divergence Vector Representation (WDVR), and finally employs a two-stage optimization strategy that refines counterfactuals in feature space and reconstructs realistic instances via a generative model. This formulation unifies distributional modeling and interpretability under a single framework, leading to more robust and semantically consistent counterfactual explanations. Extensive experiments on multiple datasets and architectures demonstrate that the proposed RA-MD approach produces counterfactuals with smaller perturbations, higher fidelity, and improved interpretability compared to existing methods.
Published: 2025-12-19T05:59:50+00:00
Venue: Information Fusion
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ao Xu; Yukai Zhang; Tieru Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104067"&gt;10.1016/j.inffus.2025.104067&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancement of deep neural networks, their integration into critical decision-making systems has become a significant driver of societal progress, necessitating robust methods for interpretability. Counterfactual explanations (CEs) play a pivotal role in enhancing the transparency of neural network models within eXplainable Artificial Intelligence (XAI). Although extensive research has explored counterfactual explanation generation, efficiently producing minimal and human-interpretable CEs for complex neural architectures remains a persistent challenge. In this paper, we propose a unified RKHS-based Adaptive Mahalanobis Distance (RA-MD) framework for generating CEs in neural networks. The framework first selects the most informative layers using a Kernel Feature Disagreement (KFD) criterion, then captures feature relevance through a Wasserstein-based Divergence Vector Representation (WDVR), and finally employs a two-stage optimization strategy that refines counterfactuals in feature space and reconstructs realistic instances via a generative model. This formulation unifies distributional modeling and interpretability under a single framework, leading to more robust and semantically consistent counterfactual explanations. Extensive experiments on multiple datasets and architectures demonstrate that the proposed RA-MD approach produces counterfactuals with smaller perturbations, higher fidelity, and improved interpretability compared to existing methods.&lt;/p&gt;</content:encoded></item><item><title>Complexity Control Facilitates Reasoning-Based Compositional Generalization in Transformers</title><link>https://doi.org/10.1109/tpami.2025.3646483</link><guid>10.1109/tpami.2025.3646483</guid><pubDate>Fri, 19 Dec 2025 18:58:30 +0000</pubDate><dc:creator>Zhongwang Zhang</dc:creator><dc:creator>Pengxiao Lin</dc:creator><dc:creator>Zhiwei Wang</dc:creator><dc:creator>Yaoyu Zhang</dc:creator><dc:creator>Zhi-Qin John Xu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646483</prism:doi><description>Transformers have demonstrated impressive capabilities across various tasks, yet their performance on compositional problems remains a subject of debate. In this study, we investigate the internal mechanisms underlying Transformers' behavior in compositional tasks. We find that complexity control strategies—particularly the choice of parameter initialization scale and weight decay—significantly influence whether the model learns primitive-level rules that generalize out-of-distribution (reasoning-based solutions) or relies solely on memorized map pings (memory-based solutions). By applying masking strategies to the model's information circuits and employing multiple complexity metrics, we reveal distinct internal working mechanisms associated with different solution types. Further analysis reveals that reasoning-based solutions exhibit a lower complexity bias, which aligns with the well-studied neuron condensation phenomenon. This lower complexity bias is hypothesized to be the key factor enabling these solutions to learn reasoning rules. We validate these conclusions across multiple real-world datasets, including image generation and natural language processing tasks, confirming the broad applicability of our findings.
Published: 2025-12-19T18:58:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhongwang Zhang; Pengxiao Lin; Zhiwei Wang; Yaoyu Zhang; Zhi-Qin John Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646483"&gt;10.1109/tpami.2025.3646483&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have demonstrated impressive capabilities across various tasks, yet their performance on compositional problems remains a subject of debate. In this study, we investigate the internal mechanisms underlying Transformers&amp;#x27; behavior in compositional tasks. We find that complexity control strategies—particularly the choice of parameter initialization scale and weight decay—significantly influence whether the model learns primitive-level rules that generalize out-of-distribution (reasoning-based solutions) or relies solely on memorized map pings (memory-based solutions). By applying masking strategies to the model&amp;#x27;s information circuits and employing multiple complexity metrics, we reveal distinct internal working mechanisms associated with different solution types. Further analysis reveals that reasoning-based solutions exhibit a lower complexity bias, which aligns with the well-studied neuron condensation phenomenon. This lower complexity bias is hypothesized to be the key factor enabling these solutions to learn reasoning rules. We validate these conclusions across multiple real-world datasets, including image generation and natural language processing tasks, confirming the broad applicability of our findings.&lt;/p&gt;</content:encoded></item><item><title>ViV-ReID: Bidirectional Structural-Aware Spatial-Temporal Graph Networks on Large-Scale Video-Based Vessel Re-Identification Dataset</title><link>https://doi.org/10.1109/tip.2025.3643156</link><guid>10.1109/tip.2025.3643156</guid><pubDate>Thu, 18 Dec 2025 18:35:49 +0000</pubDate><dc:creator>Mingxin Zhang</dc:creator><dc:creator>Fuxiang Feng</dc:creator><dc:creator>Xing Fang</dc:creator><dc:creator>Lin Zhang</dc:creator><dc:creator>Youmei Zhang</dc:creator><dc:creator>Xiaolei Li</dc:creator><dc:creator>Wei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3643156</prism:doi><description>Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.
Published: 2025-12-18T18:35:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingxin Zhang; Fuxiang Feng; Xing Fang; Lin Zhang; Youmei Zhang; Xiaolei Li; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3643156"&gt;10.1109/tip.2025.3643156&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Vessel re-identification (ReID) serves as a foundational task for intelligent maritime transportation systems. To enhance maritime surveillance capabilities, this study investigates video-based vessel ReID, a critical yet underexplored task in intelligent transportation systems. The lack of relevant datasets has limited the progress of Video-based vessel ReID research work. We established ViV-ReID, the first publicly available large-scale video-based vessel ReID dataset, comprising 480 vessel identities captured from 20 cross-port camera views (7,165 tracklets and 1.14 million frames), establishing a benchmark for advancing vessel ReID from image to video processing. Videos offer significantly richer information than single-frame images. The dynamic nature of video often leads to fragmented spatio-temporal features causing disrupted contextual understanding, and to address this problem, we further propose a Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) that explicitly aligns spatio-temporal features using vessel structural priors. Extensive experiments on the ViV-ReID dataset demonstrate that image-based ReID methods often show suboptimal performance when applied to video data. Meanwhile, it is crucial to validate the effectiveness of spatio-temporal information and establish performance benchmarks for different methods. The Bidirectional Structural-Aware Spatial-Temporal Graph Network (Bi-SSTN) significantly outperforms state-of-the-art methods on ViV-ReID, confirming its efficacy in modeling vessel-specific spatio-temporal patterns. Project web page: https://vsislab.github.io/ViV_ReID/.&lt;/p&gt;</content:encoded></item><item><title>4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</title><link>https://arxiv.org/abs/2512.14235v1</link><guid>http://arxiv.org/abs/2512.14235v1</guid><pubDate>Tue, 16 Dec 2025 09:43:05 +0000</pubDate><dc:creator>Jimmie Kwok</dc:creator><dc:creator>Holger Caesar</dc:creator><dc:creator>Andras Palffy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.
Published: 2025-12-16T09:43:05+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jimmie Kwok; Holger Caesar; Andras Palffy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.&lt;/p&gt;</content:encoded></item><item><title>Light CNN-Transformer Dual-Branch Network for Real-Time Semantic Segmentation</title><link>https://doi.org/10.1109/tmm.2025.3645624</link><guid>10.1109/tmm.2025.3645624</guid><pubDate>Thu, 18 Dec 2025 18:34:14 +0000</pubDate><dc:creator>Yongsheng Dong</dc:creator><dc:creator>Siming Jia</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3645624</prism:doi><description>Convolutional Neural Networks (CNN) have widely used in semantic segmentation, and can effectively extract local hierarchical information while being unsatisfactory in extracting global information. By contrast, Transformer is good at extracting long-distance dependencies in semantics while it is time-consuming. In this work, we propose a Light CNN-Transformer Dual-Branch Network (LCTDBNet) for real-time semantic segmentation. It consists of a longer CNN branch to extract local hierarchical information and a shorter Transformer branch to extract global contextual information. The CNN branch uses a lightweight encoder-decoder structure to further extract more local hierarchical information. We propose a Deep Strip Aggregation Pyramid Pooling Module (DSAPPM) to extract contextual and strip information. We further propose a Feature Pooling Refinement Module (FPRM) to optimise the feature representation at different stages. Finally, we propose a CNN-Transformer Fusion Module (CTFM) to fuse the features of two branches. Experimental results demonstrate that our proposed LCTDBNet is effective and achieves satisfactory results. Specifically, the base version of LCTDBNet achieves 80.3% mean intersection over union (mIoU) at 78.6 frames per second (FPS) on Cityscapes, 80.0% mIoU at 137.5 FPS on CamVid and 40.9% mIoU at 253.7 FPS on ADE20 K.
Published: 2025-12-18T18:34:14+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongsheng Dong; Siming Jia; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3645624"&gt;10.1109/tmm.2025.3645624&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNN) have widely used in semantic segmentation, and can effectively extract local hierarchical information while being unsatisfactory in extracting global information. By contrast, Transformer is good at extracting long-distance dependencies in semantics while it is time-consuming. In this work, we propose a Light CNN-Transformer Dual-Branch Network (LCTDBNet) for real-time semantic segmentation. It consists of a longer CNN branch to extract local hierarchical information and a shorter Transformer branch to extract global contextual information. The CNN branch uses a lightweight encoder-decoder structure to further extract more local hierarchical information. We propose a Deep Strip Aggregation Pyramid Pooling Module (DSAPPM) to extract contextual and strip information. We further propose a Feature Pooling Refinement Module (FPRM) to optimise the feature representation at different stages. Finally, we propose a CNN-Transformer Fusion Module (CTFM) to fuse the features of two branches. Experimental results demonstrate that our proposed LCTDBNet is effective and achieves satisfactory results. Specifically, the base version of LCTDBNet achieves 80.3% mean intersection over union (mIoU) at 78.6 frames per second (FPS) on Cityscapes, 80.0% mIoU at 137.5 FPS on CamVid and 40.9% mIoU at 253.7 FPS on ADE20 K.&lt;/p&gt;</content:encoded></item><item><title>Temporal Stereo Matching From Event Cameras Via Joint Learning With Stereoscopic Flow</title><link>https://doi.org/10.1109/tpami.2025.3645734</link><guid>10.1109/tpami.2025.3645734</guid><pubDate>Thu, 18 Dec 2025 18:33:13 +0000</pubDate><dc:creator>Jae-Young Kang</dc:creator><dc:creator>Hoonhee Cho</dc:creator><dc:creator>Kuk-Jin Yoon</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3645734</prism:doi><description>Event cameras are dynamic vision sensors inspired by the biological retina, offering high dynamic range, high temporal resolution, and low power consumption. These qualities allow them to perceive 3D environments even in extreme conditions. Event data is continuously recorded over time, capturing pixel movements in detail. To leverage this temporal density, we introduce a temporal event stereo framework that continuously uses past information. The event stereo matching network is jointly trained with stereoscopic flow, which tracks pixel movements from stereo cameras. Instead of relying on optical flow ground truth, our method trains motion flows using disparity maps. The temporal aggregation of information via stereoscopic flow boosts stereo matching performance, achieving state-of-the-art results on MVSEC, DSEC, M3ED, and EVIMO2 datasets. Our method also demonstrates computational efficiency by stacking past data in a cascading manner.
Published: 2025-12-18T18:33:13+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jae-Young Kang; Hoonhee Cho; Kuk-Jin Yoon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3645734"&gt;10.1109/tpami.2025.3645734&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Event cameras are dynamic vision sensors inspired by the biological retina, offering high dynamic range, high temporal resolution, and low power consumption. These qualities allow them to perceive 3D environments even in extreme conditions. Event data is continuously recorded over time, capturing pixel movements in detail. To leverage this temporal density, we introduce a temporal event stereo framework that continuously uses past information. The event stereo matching network is jointly trained with stereoscopic flow, which tracks pixel movements from stereo cameras. Instead of relying on optical flow ground truth, our method trains motion flows using disparity maps. The temporal aggregation of information via stereoscopic flow boosts stereo matching performance, achieving state-of-the-art results on MVSEC, DSEC, M3ED, and EVIMO2 datasets. Our method also demonstrates computational efficiency by stacking past data in a cascading manner.&lt;/p&gt;</content:encoded></item><item><title>Next-Generation License Plate Detection and Recognition System using YOLOv8</title><link>https://arxiv.org/abs/2512.16826v1</link><guid>http://arxiv.org/abs/2512.16826v1</guid><pubDate>Thu, 18 Dec 2025 18:06:29 +0000</pubDate><dc:creator>Arslan Amin</dc:creator><dc:creator>Rafia Mumtaz</dc:creator><dc:creator>Muhammad Jawad Bashir</dc:creator><dc:creator>Syed Mohammad Hassan Zaidi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/HONET59747.2023.10374756</prism:doi><description>In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.
Published: 2025-12-18T18:06:29+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Arslan Amin; Rafia Mumtaz; Muhammad Jawad Bashir; Syed Mohammad Hassan Zaidi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/HONET59747.2023.10374756"&gt;10.1109/HONET59747.2023.10374756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.&lt;/p&gt;</content:encoded></item><item><title>Lightweight Semantic Feature Extraction Model With Direction Awareness for Aerial Traffic Object Detection</title><link>https://doi.org/10.1109/tits.2025.3642410</link><guid>10.1109/tits.2025.3642410</guid><pubDate>Fri, 19 Dec 2025 19:00:48 +0000</pubDate><dc:creator>Jiaquan Shen</dc:creator><dc:creator>Ningzhong Liu</dc:creator><dc:creator>Han Sun</dc:creator><dc:creator>Shang Wu</dc:creator><dc:creator>Zongzheng Liang</dc:creator><dc:creator>Lulu Han</dc:creator><dc:creator>Yongxin Zhang</dc:creator><dc:creator>Deguang Li</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3642410</prism:doi><description>The detection of traffic objects in aerial scenes holds significant application potential in both military and civilian sectors. However, current aerial traffic object detection techniques based on computer vision face challenges including limited awareness of object direction, a heavy computational burden on the feature extraction backbone network, and inadequate capacity to learn crucial semantic information. In this paper, our focus is on investigating the mechanisms for predicting the directional perception of traffic objects in aerial scenes, achieving backbone network lightness, and exploring methods for extracting key semantic information from objects. Firstly, to tackle the challenge of poor perception of traffic object direction and angle in aerial scenes, we utilize techniques like equivariant vector field convolution, multi-task anchor-free prediction, and adaptive loss to develop a precise mechanism for recognizing and predicting object directions. Secondly, given the presence of small-sized and numerous objects in aerial scenes, we propose the adoption of a lightweight backbone network employing channel stacking to decrease the model’s computational burden. Additionally, we establish a theoretical framework and methodology for optimizing and compressing this backbone network, aimed at enhancing feature extraction and propagation for aerial traffic objects. Furthermore, to address the issue of inadequate learning of key semantic information features, we incorporate saliency attention and multi-scale contextual information to capture the essential semantic characteristics of the objects. We also establish a method for extracting semantic features specifically for aerial traffic objects. The approach presented in this paper broadens the applicability of aerial object detection algorithms and offers novel methodologies and theoretical foundations for object detection in intricate scenarios.
Published: 2025-12-19T19:00:48+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaquan Shen; Ningzhong Liu; Han Sun; Shang Wu; Zongzheng Liang; Lulu Han; Yongxin Zhang; Deguang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3642410"&gt;10.1109/tits.2025.3642410&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;The detection of traffic objects in aerial scenes holds significant application potential in both military and civilian sectors. However, current aerial traffic object detection techniques based on computer vision face challenges including limited awareness of object direction, a heavy computational burden on the feature extraction backbone network, and inadequate capacity to learn crucial semantic information. In this paper, our focus is on investigating the mechanisms for predicting the directional perception of traffic objects in aerial scenes, achieving backbone network lightness, and exploring methods for extracting key semantic information from objects. Firstly, to tackle the challenge of poor perception of traffic object direction and angle in aerial scenes, we utilize techniques like equivariant vector field convolution, multi-task anchor-free prediction, and adaptive loss to develop a precise mechanism for recognizing and predicting object directions. Secondly, given the presence of small-sized and numerous objects in aerial scenes, we propose the adoption of a lightweight backbone network employing channel stacking to decrease the model’s computational burden. Additionally, we establish a theoretical framework and methodology for optimizing and compressing this backbone network, aimed at enhancing feature extraction and propagation for aerial traffic objects. Furthermore, to address the issue of inadequate learning of key semantic information features, we incorporate saliency attention and multi-scale contextual information to capture the essential semantic characteristics of the objects. We also establish a method for extracting semantic features specifically for aerial traffic objects. The approach presented in this paper broadens the applicability of aerial object detection algorithms and offers novel methodologies and theoretical foundations for object detection in intricate scenarios.&lt;/p&gt;</content:encoded></item><item><title>UAV-DETR: Few-parameter DETR for Small Object Detection in High-Altitude UAV Images</title><link>https://doi.org/10.1109/jstars.2025.3645731</link><guid>10.1109/jstars.2025.3645731</guid><pubDate>Thu, 18 Dec 2025 18:33:57 +0000</pubDate><dc:creator>Ningsheng Liao</dc:creator><dc:creator>Yuning Zhang</dc:creator><dc:creator>Zhongliang Yu</dc:creator><dc:creator>Jiangshuai Huang</dc:creator><dc:creator>Mi Zhu</dc:creator><dc:creator>Bo Peng</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3645731</prism:doi><description>In the field of computer vision, DEtection TRansformer (DETR) has received significant recognition for its ability to streamline the design process of object detectors through the concept of set prediction. However, its exceptional performance comes at the cost of a high parameter count and significant computational requirements. Moreover, its ability to detect small objects is compromised, making it less suitable for analyzing high-altitude Unmanned Aerial Vehicle (UAV) images. This paper proposes UAV-DETR, a DETR architecture specifically designed for detecting UAV images captured at high altitudes, which achieves a trade-off between parameter count and precision. UAV-DETR is built in two steps: first, inverted residual structures are used to preserve low-dimensional image features, followed by a carefully designed cascaded linear attention mechanism to mitigate parameter redundancy. Through observation and analysis of the attention diffusion issue in the encoder, a cross-channel dynamic sampling mechanism is proposed, which effectively expands the model's receptive field while maintaining accuracy. In addition, the loss function is redesigned by incorporating the Wasserstein distance, which is insensitive to bounding boxes, in order to significantly enhance the convergence speed of the model. Extensive experimental results on two major benchmarks, i.e. VisDrone and UAVDT, validate the simplicity and efficiency of our model. Specifically, on the VisDrone2021 public test set, UAV-DETR exhibits superior performance with only 14 million parameters compared to YOLOv8 _{m} _{m} , reducing the model's parameter count and complexity by 44% and 10% respectively, while achieving a 16.6% improvement in accuracy, without any data augmentation or post-processing procedures.
Published: 2025-12-18T18:33:57+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ningsheng Liao; Yuning Zhang; Zhongliang Yu; Jiangshuai Huang; Mi Zhu; Bo Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3645731"&gt;10.1109/jstars.2025.3645731&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;In the field of computer vision, DEtection TRansformer (DETR) has received significant recognition for its ability to streamline the design process of object detectors through the concept of set prediction. However, its exceptional performance comes at the cost of a high parameter count and significant computational requirements. Moreover, its ability to detect small objects is compromised, making it less suitable for analyzing high-altitude Unmanned Aerial Vehicle (UAV) images. This paper proposes UAV-DETR, a DETR architecture specifically designed for detecting UAV images captured at high altitudes, which achieves a trade-off between parameter count and precision. UAV-DETR is built in two steps: first, inverted residual structures are used to preserve low-dimensional image features, followed by a carefully designed cascaded linear attention mechanism to mitigate parameter redundancy. Through observation and analysis of the attention diffusion issue in the encoder, a cross-channel dynamic sampling mechanism is proposed, which effectively expands the model&amp;#x27;s receptive field while maintaining accuracy. In addition, the loss function is redesigned by incorporating the Wasserstein distance, which is insensitive to bounding boxes, in order to significantly enhance the convergence speed of the model. Extensive experimental results on two major benchmarks, i.e. VisDrone and UAVDT, validate the simplicity and efficiency of our model. Specifically, on the VisDrone2021 public test set, UAV-DETR exhibits superior performance with only 14 million parameters compared to YOLOv8 _{m} _{m} , reducing the model&amp;#x27;s parameter count and complexity by 44% and 10% respectively, while achieving a 16.6% improvement in accuracy, without any data augmentation or post-processing procedures.&lt;/p&gt;</content:encoded></item><item><title>Multitask Reinforcement Learning with Metadata-Guided Adaptive Routing</title><link>https://doi.org/10.1016/j.inffus.2025.104068</link><guid>10.1016/j.inffus.2025.104068</guid><pubDate>Fri, 19 Dec 2025 05:59:38 +0000</pubDate><dc:creator>Rui Pan</dc:creator><dc:creator>Haoran Luo</dc:creator><dc:creator>Quan Yuan</dc:creator><dc:creator>Guiyang Luo</dc:creator><dc:creator>Jinglin Li</dc:creator><dc:creator>Tiesunlong Shen</dc:creator><dc:creator>Rui Mao</dc:creator><dc:creator>Erik Cambria</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104068</prism:doi><description>Multitask reinforcement learning aims to train a unified policy that generalizes across multiple related tasks, improving sample efficiency and promoting knowledge transfer. However, existing methods often suffer from negative knowledge transfer due to task interference, especially when using hard parameter sharing across tasks with diverse dynamics or goals. Conventional solutions typically adopt shared backbones with task-specific heads, gradient projection methods, or routing-based networks to mitigate conflict. However, many of these methods rely on simplistic task identifiers (e.g., one-hot vectors), lack expressive representations of task semantics, or fail to modulate shared components in a fine-grained, task-specific manner. To overcome these challenges, we propose Meta data-guided A daptive R outing ( MetaAR ), a novel framework that incorporates rich task metadata such as natural language descriptions to generate expressive and interpretable task representations. These representations are injected into a dynamic routing network, which adaptively reconfigures layer-wise computation paths in a shared modular policy network. To enable robust task-specific adaptation, we further introduce a noise-injected Top-K routing mechanism that dynamically selects the most relevant computation paths for each task. By injecting stochasticity during routing, this mechanism promotes exploration and mitigates interference between tasks through sparse, selective information flow. We evaluate MetaAR on the Meta-World benchmark with up to 50 robotic manipulation tasks, where it consistently outperforms strong baselines, achieving 4–8% higher mean success rates than the best-performing methods across the MT10 and MT50 variants.
Published: 2025-12-19T05:59:38+00:00
Venue: Information Fusion
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Pan; Haoran Luo; Quan Yuan; Guiyang Luo; Jinglin Li; Tiesunlong Shen; Rui Mao; Erik Cambria&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104068"&gt;10.1016/j.inffus.2025.104068&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Multitask reinforcement learning aims to train a unified policy that generalizes across multiple related tasks, improving sample efficiency and promoting knowledge transfer. However, existing methods often suffer from negative knowledge transfer due to task interference, especially when using hard parameter sharing across tasks with diverse dynamics or goals. Conventional solutions typically adopt shared backbones with task-specific heads, gradient projection methods, or routing-based networks to mitigate conflict. However, many of these methods rely on simplistic task identifiers (e.g., one-hot vectors), lack expressive representations of task semantics, or fail to modulate shared components in a fine-grained, task-specific manner. To overcome these challenges, we propose Meta data-guided A daptive R outing ( MetaAR ), a novel framework that incorporates rich task metadata such as natural language descriptions to generate expressive and interpretable task representations. These representations are injected into a dynamic routing network, which adaptively reconfigures layer-wise computation paths in a shared modular policy network. To enable robust task-specific adaptation, we further introduce a noise-injected Top-K routing mechanism that dynamically selects the most relevant computation paths for each task. By injecting stochasticity during routing, this mechanism promotes exploration and mitigates interference between tasks through sparse, selective information flow. We evaluate MetaAR on the Meta-World benchmark with up to 50 robotic manipulation tasks, where it consistently outperforms strong baselines, achieving 4–8% higher mean success rates than the best-performing methods across the MT10 and MT50 variants.&lt;/p&gt;</content:encoded></item><item><title>Hunting for the Unknown: Open World Object Detection from a Class-Agnostic Perspective</title><link>https://doi.org/10.1016/j.neunet.2025.108501</link><guid>10.1016/j.neunet.2025.108501</guid><pubDate>Fri, 19 Dec 2025 04:24:19 +0000</pubDate><dc:creator>Jing Wang</dc:creator><dc:creator>Yonghua Cao</dc:creator><dc:creator>Zhanqiang Huo</dc:creator><dc:creator>Yingxu Qiao</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108501</prism:doi><description>The existing Open World Object Detection models rely on pseudo-labeling to annotate unknown objects during training. However, this approach leads to an over-dependence on known objects, thereby weakening the model’s capability to detect unknown objects. To tackle this issue, this paper presents a novel class-agnostic object detection model based on dynamic foreground perception and localization. The model leverages a dynamic foreground perception and localization algorithm that adeptly distinguishes foreground and background regions within images using dynamic detection heads. Additionally, by employing class-agnostic detection that does not rely on specific class information, the model mitigates excessive dependence on known categories and demonstrates improved performance in the detection of unknown objects. The key innovation of the model revolves around three main aspects: the refinement of spatial perception features, the disentanglement of attention features, and dynamic foreground perception and localization. Experimental findings across PASCAL VOC, COCO2017, LVISv1.0, and Objects365 datasets demonstrate that our model maintains high-level detection performance on known objects while surpassing most existing methods in the detection of unknown objects, exhibiting +11 points improvement in U-Recall performance. These results affirm the efficacy and superiority of the proposed detection method detailed in this paper.
Published: 2025-12-19T04:24:19+00:00
Venue: Neural Networks
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Wang; Yonghua Cao; Zhanqiang Huo; Yingxu Qiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108501"&gt;10.1016/j.neunet.2025.108501&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;The existing Open World Object Detection models rely on pseudo-labeling to annotate unknown objects during training. However, this approach leads to an over-dependence on known objects, thereby weakening the model’s capability to detect unknown objects. To tackle this issue, this paper presents a novel class-agnostic object detection model based on dynamic foreground perception and localization. The model leverages a dynamic foreground perception and localization algorithm that adeptly distinguishes foreground and background regions within images using dynamic detection heads. Additionally, by employing class-agnostic detection that does not rely on specific class information, the model mitigates excessive dependence on known categories and demonstrates improved performance in the detection of unknown objects. The key innovation of the model revolves around three main aspects: the refinement of spatial perception features, the disentanglement of attention features, and dynamic foreground perception and localization. Experimental findings across PASCAL VOC, COCO2017, LVISv1.0, and Objects365 datasets demonstrate that our model maintains high-level detection performance on known objects while surpassing most existing methods in the detection of unknown objects, exhibiting +11 points improvement in U-Recall performance. These results affirm the efficacy and superiority of the proposed detection method detailed in this paper.&lt;/p&gt;</content:encoded></item><item><title>Condition-Guided Diffusion for Multi-Modal Pedestrian Trajectory Prediction Incorporating Intention and Interaction Priors</title><link>https://doi.org/10.1109/tpami.2025.3645918</link><guid>10.1109/tpami.2025.3645918</guid><pubDate>Thu, 18 Dec 2025 18:33:13 +0000</pubDate><dc:creator>Yanghong Liu</dc:creator><dc:creator>Xingping Dong</dc:creator><dc:creator>Yutian Lin</dc:creator><dc:creator>Mang Ye</dc:creator><dc:creator>Kaihao Zhang</dc:creator><dc:creator>Bo Du</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3645918</prism:doi><description>Pedestrian behavior exhibits inherent multi-modality, necessitating predictions that balance accuracy and diversity to adapt effectively to various complex scenarios. However, conventional noise addition in diffusion models is often aimless and unguided, leading to redundant noise reduction steps and the generation of uncontrollable samples. To address these issues, we propose a Prior Condition-Guided Diffusion Model (CGD-TraP) for multi-modal pedestrian trajectory prediction. Instead of directly adding Gaussian noise to trajectories at each timestep during the forward process, our approach leverages internal intention and external interaction to guide noise estimation. Specifically, we design two specialized modules to extract and aggregate intention and interaction features. These features are then adaptively fused through a spatial-temporal fusion based on selective state space, which estimates a controllable noisy trajectory distribution. By optimizing the noise addition process in a more controlled and efficient manner, our method ensures that the denoising process is effectively guided, resulting in predictions that are both accurate and diverse. Extensive experiments on the ETH-UCY, SDD, and NBA datasets demonstrate that CGD-TraP surpasses state-of-the-art diffusion-based and other generative methods, achieving superior efficiency, accuracy, and diversity.
Published: 2025-12-18T18:33:13+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanghong Liu; Xingping Dong; Yutian Lin; Mang Ye; Kaihao Zhang; Bo Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3645918"&gt;10.1109/tpami.2025.3645918&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Pedestrian behavior exhibits inherent multi-modality, necessitating predictions that balance accuracy and diversity to adapt effectively to various complex scenarios. However, conventional noise addition in diffusion models is often aimless and unguided, leading to redundant noise reduction steps and the generation of uncontrollable samples. To address these issues, we propose a Prior Condition-Guided Diffusion Model (CGD-TraP) for multi-modal pedestrian trajectory prediction. Instead of directly adding Gaussian noise to trajectories at each timestep during the forward process, our approach leverages internal intention and external interaction to guide noise estimation. Specifically, we design two specialized modules to extract and aggregate intention and interaction features. These features are then adaptively fused through a spatial-temporal fusion based on selective state space, which estimates a controllable noisy trajectory distribution. By optimizing the noise addition process in a more controlled and efficient manner, our method ensures that the denoising process is effectively guided, resulting in predictions that are both accurate and diverse. Extensive experiments on the ETH-UCY, SDD, and NBA datasets demonstrate that CGD-TraP surpasses state-of-the-art diffusion-based and other generative methods, achieving superior efficiency, accuracy, and diversity.&lt;/p&gt;</content:encoded></item></channel></rss>