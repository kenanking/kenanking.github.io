<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 22 Dec 2025 02:46:52 +0000</lastBuildDate><item><title>A YOLO-based Polymerized Head-auxiliary Structures for Target Detection in Remote Sensing Images</title><link>https://doi.org/10.1016/j.patcog.2025.112961</link><guid>10.1016/j.patcog.2025.112961</guid><pubDate>Sun, 21 Dec 2025 06:49:30 +0000</pubDate><dc:creator>Yalu Zhang</dc:creator><dc:creator>Sixiang Quan</dc:creator><dc:creator>Hai Xiao</dc:creator><dc:creator>Jun Liu</dc:creator><dc:creator>Zhenfeng Shao</dc:creator><dc:creator>Zhihui Wang</dc:creator><dc:creator>Yingying Peng</dc:creator><dc:creator>Huali Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112961</prism:doi><description>Target detection tasks are now widely applied in the field of remote sensing. However, remote sensing target detection tasks are confronted with problems such as cluttered backgrounds and large scale variations. To address these issues, this paper proposes a high-precision aggregation head-auxiliary target detector (PHAS-YOLO). PHAS-YOLO includes two innovative plug-and-play modules: the spatial awareness attention module (SAAM) and the convolutional re-calibration multiscale feature fusion module (CRMSFF), as well as the context aggregation bidirectional connection structure (CABi-FPN) and the adaptive auxiliary head structure (AAHS). The proposed modules enable the model to have good spatial feature aggregation capabilities to retain key feature information, incorporate an adaptive weighting mechanism to reduce information loss caused by the fusion of different scales, and refine the features of the images to be detected. A series of experiments were conducted on three public remote sensing target detection datasets, namely DIOR, DOTAv1.0, and HRRSD, to verify the effectiveness and superiority of the proposed method in remote sensing target detection tasks.
Published: 2025-12-21T06:49:30+00:00
Venue: Pattern Recognition
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yalu Zhang; Sixiang Quan; Hai Xiao; Jun Liu; Zhenfeng Shao; Zhihui Wang; Yingying Peng; Huali Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112961"&gt;10.1016/j.patcog.2025.112961&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Target detection tasks are now widely applied in the field of remote sensing. However, remote sensing target detection tasks are confronted with problems such as cluttered backgrounds and large scale variations. To address these issues, this paper proposes a high-precision aggregation head-auxiliary target detector (PHAS-YOLO). PHAS-YOLO includes two innovative plug-and-play modules: the spatial awareness attention module (SAAM) and the convolutional re-calibration multiscale feature fusion module (CRMSFF), as well as the context aggregation bidirectional connection structure (CABi-FPN) and the adaptive auxiliary head structure (AAHS). The proposed modules enable the model to have good spatial feature aggregation capabilities to retain key feature information, incorporate an adaptive weighting mechanism to reduce information loss caused by the fusion of different scales, and refine the features of the images to be detected. A series of experiments were conducted on three public remote sensing target detection datasets, namely DIOR, DOTAv1.0, and HRRSD, to verify the effectiveness and superiority of the proposed method in remote sensing target detection tasks.&lt;/p&gt;</content:encoded></item><item><title>Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing</title><link>https://arxiv.org/abs/2512.17224v1</link><guid>http://arxiv.org/abs/2512.17224v1</guid><pubDate>Fri, 19 Dec 2025 04:21:01 +0000</pubDate><dc:creator>Xuyang Li</dc:creator><dc:creator>Chenyu Li</dc:creator><dc:creator>Danfeng Hong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.
Published: 2025-12-19T04:21:01+00:00
Venue: arXiv
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuyang Li; Chenyu Li; Danfeng Hong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.&lt;/p&gt;</content:encoded></item><item><title>Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection</title><link>https://arxiv.org/abs/2512.17514v1</link><guid>http://arxiv.org/abs/2512.17514v1</guid><pubDate>Fri, 19 Dec 2025 12:30:29 +0000</pubDate><dc:creator>Sairam VCR</dc:creator><dc:creator>Rishabh Lalla</dc:creator><dc:creator>Aveen Dayal</dc:creator><dc:creator>Tejal Kulkarni</dc:creator><dc:creator>Anuj Lalla</dc:creator><dc:creator>Vineeth N Balasubramanian</dc:creator><dc:creator>Muhammad Haris Khan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.
Published: 2025-12-19T12:30:29+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sairam VCR; Rishabh Lalla; Aveen Dayal; Tejal Kulkarni; Anuj Lalla; Vineeth N Balasubramanian; Muhammad Haris Khan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector&amp;#x27;s ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector&amp;#x27;s feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Next-Generation License Plate Detection and Recognition System using YOLOv8</title><link>https://arxiv.org/abs/2512.16826v1</link><guid>http://arxiv.org/abs/2512.16826v1</guid><pubDate>Thu, 18 Dec 2025 18:06:29 +0000</pubDate><dc:creator>Arslan Amin</dc:creator><dc:creator>Rafia Mumtaz</dc:creator><dc:creator>Muhammad Jawad Bashir</dc:creator><dc:creator>Syed Mohammad Hassan Zaidi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/HONET59747.2023.10374756</prism:doi><description>In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.
Published: 2025-12-18T18:06:29+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Arslan Amin; Rafia Mumtaz; Muhammad Jawad Bashir; Syed Mohammad Hassan Zaidi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/HONET59747.2023.10374756"&gt;10.1109/HONET59747.2023.10374756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.&lt;/p&gt;</content:encoded></item><item><title>Dual-stream perception cross-flattening transformer for few-shot surface defect detection</title><link>https://doi.org/10.1016/j.ins.2025.123017</link><guid>10.1016/j.ins.2025.123017</guid><pubDate>Sat, 20 Dec 2025 23:14:32 +0000</pubDate><dc:creator>Yudong Li</dc:creator><dc:creator>Shaoqing Wang</dc:creator><dc:creator>Zihao Jing</dc:creator><dc:creator>Jinghua Zheng</dc:creator><dc:creator>Xiaobo Han</dc:creator><dc:creator>Xiao Zheng</dc:creator><dc:creator>Fuzhen Sun</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2025.123017</prism:doi><description>Few-shot object detection (FSOD) is a promising approach for surface defect detection, addressing challenges like limited annotated data and diverse defect types on irregular surfaces. Convolutional neural networks (CNNs) are the dominant approach for FSOD. However, local receptive fields in CNNs limit the ability to capture global context, and additional feature alignment mechanisms are required to bridge the semantic gap between query and support images. Therefore, we propose a dual-stream perception cross-flattening transformer (DPCFT) framework for few-shot surface defect detection. First, we design an asymmetric cross-flattening attention (ACFA) that captures long-distance dependencies between query and support images at each feature extraction layer. It enhances multi-branch feature interaction while eliminating the need for separate feature alignment and fusion modules. Second, a position perception module (PPM) is presented to enhance the ability to extract directional features from irregular surface defects. Finally, we propose a dual-stream adaptive module (DAM) to enhance the generalization ability for handling diverse surface defect detection tasks. To verify the effectiveness of the proposed framework, we conduct extensive experiments on three surface defect datasets. Experimental results demonstrate that DPCFT achieves better accuracy and generalization ability than other methods across different experimental settings. Code is available at https://github.com/lydcv/DPCFT .
Published: 2025-12-20T23:14:32+00:00
Venue: Information Sciences
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yudong Li; Shaoqing Wang; Zihao Jing; Jinghua Zheng; Xiaobo Han; Xiao Zheng; Fuzhen Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2025.123017"&gt;10.1016/j.ins.2025.123017&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) is a promising approach for surface defect detection, addressing challenges like limited annotated data and diverse defect types on irregular surfaces. Convolutional neural networks (CNNs) are the dominant approach for FSOD. However, local receptive fields in CNNs limit the ability to capture global context, and additional feature alignment mechanisms are required to bridge the semantic gap between query and support images. Therefore, we propose a dual-stream perception cross-flattening transformer (DPCFT) framework for few-shot surface defect detection. First, we design an asymmetric cross-flattening attention (ACFA) that captures long-distance dependencies between query and support images at each feature extraction layer. It enhances multi-branch feature interaction while eliminating the need for separate feature alignment and fusion modules. Second, a position perception module (PPM) is presented to enhance the ability to extract directional features from irregular surface defects. Finally, we propose a dual-stream adaptive module (DAM) to enhance the generalization ability for handling diverse surface defect detection tasks. To verify the effectiveness of the proposed framework, we conduct extensive experiments on three surface defect datasets. Experimental results demonstrate that DPCFT achieves better accuracy and generalization ability than other methods across different experimental settings. Code is available at https://github.com/lydcv/DPCFT .&lt;/p&gt;</content:encoded></item><item><title>StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</title><link>https://arxiv.org/abs/2512.17620v1</link><guid>http://arxiv.org/abs/2512.17620v1</guid><pubDate>Fri, 19 Dec 2025 14:25:46 +0000</pubDate><dc:creator>Di Wu</dc:creator><dc:creator>Feng Yang</dc:creator><dc:creator>Wenhui Zhao</dc:creator><dc:creator>Jinwen Yu</dc:creator><dc:creator>Pan Liao</dc:creator><dc:creator>Benlian Xu</dc:creator><dc:creator>Dingwen Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.
Published: 2025-12-19T14:25:46+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Di Wu; Feng Yang; Wenhui Zhao; Jinwen Yu; Pan Liao; Benlian Xu; Dingwen Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.&lt;/p&gt;</content:encoded></item><item><title>DenseBEV: Transforming BEV Grid Cells into 3D Objects</title><link>https://arxiv.org/abs/2512.16818v1</link><guid>http://arxiv.org/abs/2512.16818v1</guid><pubDate>Thu, 18 Dec 2025 17:59:22 +0000</pubDate><dc:creator>Marius Dähling</dc:creator><dc:creator>Sebastian Krebs</dc:creator><dc:creator>J. Marius Zöllner</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.
Published: 2025-12-18T17:59:22+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Marius Dähling; Sebastian Krebs; J. Marius Zöllner&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;In current research, Bird&amp;#x27;s-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.&lt;/p&gt;</content:encoded></item><item><title>APENet: Task-Aware Adaptation Prototype Evolution Network for Few-shot Semantic Segmentation</title><link>https://doi.org/10.1016/j.eswa.2025.130906</link><guid>10.1016/j.eswa.2025.130906</guid><pubDate>Sat, 20 Dec 2025 16:16:38 +0000</pubDate><dc:creator>Zhaobin Chang</dc:creator><dc:creator>Xiong Gao</dc:creator><dc:creator>Dongliang Chang</dc:creator><dc:creator>Yande Li</dc:creator><dc:creator>Yonggang Lu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130906</prism:doi><description>Few-shot semantic segmentation (FSS) is a challenging computer vision task that aims to predict the masks of unseen classes with only a few labeled samples. Although recent advances have been achieved in FSS based on prototype-based metric approaches, existing methods still face two main challenges. First, previous methods primarily focus on designing a complex interaction mechanism between inter-branch features, neglecting the specific requirements of the query branch. Second, the inappropriate use of query features is very likely to cause semantic ambiguity problems, which hinders the segmentation of unseen objects. To alleviate these problems, we propose a novel task-aware Adaptation Prototype Evolution Network (APENet). Specifically, we design a Support Feature Recombination Module (SFRM), which utilizes the ground truth masks of support images to separate and recombine the features encoded before and after the backbone network. Subsequently, we leverage the Adaptation Prototype Evolution Module (APEM) to perform a reverse segmentation on the original support image, and the support prototypes are separated into a main prototype set and an auxiliary prototype set according to the ground truth mask. Finally, the Query Feature Disentanglement Module (QFDM) is introduced to disentangle the whole query feature using both the text embedding provided by CLIP model and provisionally predicted query pseudo-mask. Meanwhile, we leverage an inter-branch feature alignment strategy to promote the feature interaction and alignment for different branches. Extensive experiments on PASCAL-5 i and COCO-20 i datasets validate the effectiveness of our method. In particular, the APENet is comparable to current classical FSS methods on cross-domain and 2-way segmentation tasks, illustrating the high generalizability. The code is released on https://github.com/GS-Chang-Hn/APENet-fss
Published: 2025-12-20T16:16:38+00:00
Venue: Expert Systems with Applications
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaobin Chang; Xiong Gao; Dongliang Chang; Yande Li; Yonggang Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130906"&gt;10.1016/j.eswa.2025.130906&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot semantic segmentation (FSS) is a challenging computer vision task that aims to predict the masks of unseen classes with only a few labeled samples. Although recent advances have been achieved in FSS based on prototype-based metric approaches, existing methods still face two main challenges. First, previous methods primarily focus on designing a complex interaction mechanism between inter-branch features, neglecting the specific requirements of the query branch. Second, the inappropriate use of query features is very likely to cause semantic ambiguity problems, which hinders the segmentation of unseen objects. To alleviate these problems, we propose a novel task-aware Adaptation Prototype Evolution Network (APENet). Specifically, we design a Support Feature Recombination Module (SFRM), which utilizes the ground truth masks of support images to separate and recombine the features encoded before and after the backbone network. Subsequently, we leverage the Adaptation Prototype Evolution Module (APEM) to perform a reverse segmentation on the original support image, and the support prototypes are separated into a main prototype set and an auxiliary prototype set according to the ground truth mask. Finally, the Query Feature Disentanglement Module (QFDM) is introduced to disentangle the whole query feature using both the text embedding provided by CLIP model and provisionally predicted query pseudo-mask. Meanwhile, we leverage an inter-branch feature alignment strategy to promote the feature interaction and alignment for different branches. Extensive experiments on PASCAL-5 i and COCO-20 i datasets validate the effectiveness of our method. In particular, the APENet is comparable to current classical FSS methods on cross-domain and 2-way segmentation tasks, illustrating the high generalizability. The code is released on https://github.com/GS-Chang-Hn/APENet-fss&lt;/p&gt;</content:encoded></item><item><title>RadarGen: Automotive Radar Point Cloud Generation from Cameras</title><link>https://arxiv.org/abs/2512.17897v1</link><guid>http://arxiv.org/abs/2512.17897v1</guid><pubDate>Fri, 19 Dec 2025 18:57:33 +0000</pubDate><dc:creator>Tomer Borreda</dc:creator><dc:creator>Fangqiang Ding</dc:creator><dc:creator>Sanja Fidler</dc:creator><dc:creator>Shengyu Huang</dc:creator><dc:creator>Or Litany</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.
Published: 2025-12-19T18:57:33+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tomer Borreda; Fangqiang Ding; Sanja Fidler; Shengyu Huang; Or Litany&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird&amp;#x27;s-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.&lt;/p&gt;</content:encoded></item><item><title>FlowDet: Unifying Object Detection and Generative Transport Flows</title><link>https://arxiv.org/abs/2512.16771v1</link><guid>http://arxiv.org/abs/2512.16771v1</guid><pubDate>Thu, 18 Dec 2025 17:03:49 +0000</pubDate><dc:creator>Enis Baty</dc:creator><dc:creator>C. P. Bridges</dc:creator><dc:creator>Simon Hadfield</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.
Published: 2025-12-18T17:03:49+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Enis Baty; C. P. Bridges; Simon Hadfield&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.&lt;/p&gt;</content:encoded></item><item><title>Multi-modal category-aware gating for oriented object detection with single-category experts</title><link>https://doi.org/10.1016/j.neucom.2025.132445</link><guid>10.1016/j.neucom.2025.132445</guid><pubDate>Sat, 20 Dec 2025 07:23:20 +0000</pubDate><dc:creator>Beihang Song</dc:creator><dc:creator>Hongquan Sun</dc:creator><dc:creator>Tong Liu</dc:creator><dc:creator>Kai Zhu</dc:creator><dc:creator>Jun Wan</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132445</prism:doi><description>Oriented object detection is vital for applications such as autonomous driving, maritime rescue, and aerial monitoring. Yet most existing end-to-end detectors exhibit limited generalization and unstable performance under long-tailed category distributions. Conventional models overfit frequent classes while neglecting rare ones, yielding biased predictions. To address these challenges, we introduce a Mixture-of-Experts (MoE) framework for oriented detection. We design a Multi-modal Category-aware Gating (GateNet) that dynamically routes features to category-specific experts, reducing class competition and mitigating long-tail bias. We further propose a single-category data-augmentation strategy that builds a structured corpus of foreground and background instances to enrich feature representations and strengthen rotated bounding-box regression. Extensive experiments on multiple benchmarks show that our approach delivers superior accuracy, robustness, and efficiency, and quickly adapts to novel scenarios with minimal fine-tuning.
Published: 2025-12-20T07:23:20+00:00
Venue: Neurocomputing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Beihang Song; Hongquan Sun; Tong Liu; Kai Zhu; Jun Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132445"&gt;10.1016/j.neucom.2025.132445&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Oriented object detection is vital for applications such as autonomous driving, maritime rescue, and aerial monitoring. Yet most existing end-to-end detectors exhibit limited generalization and unstable performance under long-tailed category distributions. Conventional models overfit frequent classes while neglecting rare ones, yielding biased predictions. To address these challenges, we introduce a Mixture-of-Experts (MoE) framework for oriented detection. We design a Multi-modal Category-aware Gating (GateNet) that dynamically routes features to category-specific experts, reducing class competition and mitigating long-tail bias. We further propose a single-category data-augmentation strategy that builds a structured corpus of foreground and background instances to enrich feature representations and strengthen rotated bounding-box regression. Extensive experiments on multiple benchmarks show that our approach delivers superior accuracy, robustness, and efficiency, and quickly adapts to novel scenarios with minimal fine-tuning.&lt;/p&gt;</content:encoded></item><item><title>Unbiased max–min embedding classification for transductive few-shot learning: Clustering and classification are all you need</title><link>https://doi.org/10.1016/j.neucom.2025.132484</link><guid>10.1016/j.neucom.2025.132484</guid><pubDate>Sat, 20 Dec 2025 07:23:21 +0000</pubDate><dc:creator>Feixiang Liu</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Jungong Han</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132484</prism:doi><description>Convolutional neural networks and supervised learning have achieved strong performance in many applications but are limited by the need for large annotated datasets. Few-shot learning (FSL) addresses this limitation by enabling models to generalize from only a few labeled examples. Transductive few-shot learning (TFSL) extends FSL by leveraging both labeled and unlabeled data, though it faces challenges like the hubness problem. To tackle these issues, we propose Unbiased Max–Min Embedding Classification (UMMEC), a TFSL framework that jointly regularizes the embedding geometry and performs cluster-aware classification. A geometric regularization and spectral fusion (GRSF) module reshapes the embedding space to encourage more balanced neighborhood structures. In addition, UMMEC combines local alignment and global dispersion objectives to form compact same-class clusters and well-separated class prototypes, while a variational Sinkhorn-based classifier refines prototype–sample relationships in a transductive manner. Experiments on TFSL benchmarks show that UMMEC achieves competitive or better performance compared to strong baselines.
Published: 2025-12-20T07:23:21+00:00
Venue: Neurocomputing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feixiang Liu; Yang Liu; Jungong Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132484"&gt;10.1016/j.neucom.2025.132484&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional neural networks and supervised learning have achieved strong performance in many applications but are limited by the need for large annotated datasets. Few-shot learning (FSL) addresses this limitation by enabling models to generalize from only a few labeled examples. Transductive few-shot learning (TFSL) extends FSL by leveraging both labeled and unlabeled data, though it faces challenges like the hubness problem. To tackle these issues, we propose Unbiased Max–Min Embedding Classification (UMMEC), a TFSL framework that jointly regularizes the embedding geometry and performs cluster-aware classification. A geometric regularization and spectral fusion (GRSF) module reshapes the embedding space to encourage more balanced neighborhood structures. In addition, UMMEC combines local alignment and global dispersion objectives to form compact same-class clusters and well-separated class prototypes, while a variational Sinkhorn-based classifier refines prototype–sample relationships in a transductive manner. Experiments on TFSL benchmarks show that UMMEC achieves competitive or better performance compared to strong baselines.&lt;/p&gt;</content:encoded></item><item><title>Cos-UMamba: Optimizing Salient Object Detection with Cosine Scanning and Bias-Corrected Feature Fusion in Optical Remote Sensing Images</title><link>https://doi.org/10.1016/j.eswa.2025.130863</link><guid>10.1016/j.eswa.2025.130863</guid><pubDate>Sat, 20 Dec 2025 00:08:50 +0000</pubDate><dc:creator>Zhen Wang</dc:creator><dc:creator>Fulin He</dc:creator><dc:creator>Nan Xu</dc:creator><dc:creator>Zhuhong You</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130863</prism:doi><description>Salient object detection in optical remote sensing images (ORSI-SOD) is a critical task with wide-ranging applications, including environmental monitoring, urban planning, and disaster management. However, the effective fusion of local and global features remains a fundamental challenge in this field. While existing methods attempt to achieve feature complementarity through architectural innovations, the quadratic complexity of Transformers hinders their scalability, and traditional Mamba architectures suffer from static scanning limitations and lack dynamic adaptability. Moreover, representational bias in heterogeneous feature fusion is frequently overlooked, reducing the reliability of detection outcomes. To address these challenges, we propose Cos-UMamba, a novel hybrid framework that integrates bias correction mechanisms with a dynamic omni-directional cosine scanning strategy. This approach enables global long-range modeling of complex topological structures while effectively mitigating feature fusion bias through a K-nearest neighbor (KNN)-based graph construction. By eliminating interference from non-salient regions, the proposed model significantly enhances feature representation. Extensive evaluations conducted on standard ORSI-SOD datasets, including ORSSD, EORSSD, and ORSI-4199, demonstrate the superior performance of Cos-UMamba across multiple metrics such as mean absolute error (MAE) and F-measure. These results validate its capability to advance the accuracy and robustness of salient object detection in diverse remote sensing scenarios, offering a robust tool for tackling real-world challenges in the field. The source code and dataset will be available on https://github.com/darkseid-arch/Cos-UMamba .
Published: 2025-12-20T00:08:50+00:00
Venue: Expert Systems with Applications
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhen Wang; Fulin He; Nan Xu; Zhuhong You&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130863"&gt;10.1016/j.eswa.2025.130863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Salient object detection in optical remote sensing images (ORSI-SOD) is a critical task with wide-ranging applications, including environmental monitoring, urban planning, and disaster management. However, the effective fusion of local and global features remains a fundamental challenge in this field. While existing methods attempt to achieve feature complementarity through architectural innovations, the quadratic complexity of Transformers hinders their scalability, and traditional Mamba architectures suffer from static scanning limitations and lack dynamic adaptability. Moreover, representational bias in heterogeneous feature fusion is frequently overlooked, reducing the reliability of detection outcomes. To address these challenges, we propose Cos-UMamba, a novel hybrid framework that integrates bias correction mechanisms with a dynamic omni-directional cosine scanning strategy. This approach enables global long-range modeling of complex topological structures while effectively mitigating feature fusion bias through a K-nearest neighbor (KNN)-based graph construction. By eliminating interference from non-salient regions, the proposed model significantly enhances feature representation. Extensive evaluations conducted on standard ORSI-SOD datasets, including ORSSD, EORSSD, and ORSI-4199, demonstrate the superior performance of Cos-UMamba across multiple metrics such as mean absolute error (MAE) and F-measure. These results validate its capability to advance the accuracy and robustness of salient object detection in diverse remote sensing scenarios, offering a robust tool for tackling real-world challenges in the field. The source code and dataset will be available on https://github.com/darkseid-arch/Cos-UMamba .&lt;/p&gt;</content:encoded></item><item><title>DVGT: Driving Visual Geometry Transformer</title><link>https://arxiv.org/abs/2512.16919v1</link><guid>http://arxiv.org/abs/2512.16919v1</guid><pubDate>Thu, 18 Dec 2025 18:59:57 +0000</pubDate><dc:creator>Sicheng Zuo</dc:creator><dc:creator>Zixun Xie</dc:creator><dc:creator>Wenzhao Zheng</dc:creator><dc:creator>Shaoqing Xu</dc:creator><dc:creator>Fang Li</dc:creator><dc:creator>Shengyin Jiang</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Zhi-Xin Yang</dc:creator><dc:creator>Jiwen Lu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.
Published: 2025-12-18T18:59:57+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sicheng Zuo; Zixun Xie; Wenzhao Zheng; Shaoqing Xu; Fang Li; Shengyin Jiang; Long Chen; Zhi-Xin Yang; Jiwen Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.&lt;/p&gt;</content:encoded></item><item><title>YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images</title><link>https://arxiv.org/abs/2512.16493v1</link><guid>http://arxiv.org/abs/2512.16493v1</guid><pubDate>Thu, 18 Dec 2025 13:00:05 +0000</pubDate><dc:creator>Huma Hafeez</dc:creator><dc:creator>Matthew Garratt</dc:creator><dc:creator>Jo Plested</dc:creator><dc:creator>Sankaran Iyer</dc:creator><dc:creator>Arcot Sowmya</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.
Published: 2025-12-18T13:00:05+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huma Hafeez; Matthew Garratt; Jo Plested; Sankaran Iyer; Arcot Sowmya&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.&lt;/p&gt;</content:encoded></item><item><title>DAVE: A VLM Vision Encoder for Document Understanding and Web Agents</title><link>https://arxiv.org/abs/2512.17221v1</link><guid>http://arxiv.org/abs/2512.17221v1</guid><pubDate>Fri, 19 Dec 2025 04:09:24 +0000</pubDate><dc:creator>Brandon Huang</dc:creator><dc:creator>Hang Hua</dc:creator><dc:creator>Zhuoran Yu</dc:creator><dc:creator>Trevor Darrell</dc:creator><dc:creator>Rogerio Feris</dc:creator><dc:creator>Roei Herzig</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder's alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.
Published: 2025-12-19T04:09:24+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Brandon Huang; Hang Hua; Zhuoran Yu; Trevor Darrell; Rogerio Feris; Roei Herzig&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder&amp;#x27;s alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.&lt;/p&gt;</content:encoded></item><item><title>A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs</title><link>https://arxiv.org/abs/2512.17319v1</link><guid>http://arxiv.org/abs/2512.17319v1</guid><pubDate>Fri, 19 Dec 2025 08:07:51 +0000</pubDate><dc:creator>Yunkai Dang</dc:creator><dc:creator>Meiyi Zhu</dc:creator><dc:creator>Donghao Wang</dc:creator><dc:creator>Yizhuo Zhang</dc:creator><dc:creator>Jiacheng Yang</dc:creator><dc:creator>Qi Fan</dc:creator><dc:creator>Yuekun Yang</dc:creator><dc:creator>Wenbin Li</dc:creator><dc:creator>Feng Miao</dc:creator><dc:creator>Yang Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR
Published: 2025-12-19T08:07:51+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunkai Dang; Meiyi Zhu; Donghao Wang; Yizhuo Zhang; Jiacheng Yang; Qi Fan; Yuekun Yang; Wenbin Li; Feng Miao; Yang Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR&lt;/p&gt;</content:encoded></item><item><title>Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks</title><link>https://arxiv.org/abs/2512.16586v1</link><guid>http://arxiv.org/abs/2512.16586v1</guid><pubDate>Thu, 18 Dec 2025 14:32:06 +0000</pubDate><dc:creator>Shaohua Wu</dc:creator><dc:creator>Tong Yu</dc:creator><dc:creator>Shenling Wang</dc:creator><dc:creator>Xudong Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.
Published: 2025-12-18T14:32:06+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shaohua Wu; Tong Yu; Shenling Wang; Xudong Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model&amp;#x27;s ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.&lt;/p&gt;</content:encoded></item><item><title>MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval</title><link>https://arxiv.org/abs/2512.16294v1</link><guid>http://arxiv.org/abs/2512.16294v1</guid><pubDate>Thu, 18 Dec 2025 08:29:27 +0000</pubDate><dc:creator>Amna Amir</dc:creator><dc:creator>Erchan Aptoula</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.
Published: 2025-12-18T08:29:27+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Amna Amir; Erchan Aptoula&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</title><link>https://arxiv.org/abs/2512.16913v1</link><guid>http://arxiv.org/abs/2512.16913v1</guid><pubDate>Thu, 18 Dec 2025 18:59:29 +0000</pubDate><dc:creator>Xin Lin</dc:creator><dc:creator>Meixi Song</dc:creator><dc:creator>Dizhe Zhang</dc:creator><dc:creator>Wenxuan Lu</dc:creator><dc:creator>Haodong Li</dc:creator><dc:creator>Bo Du</dc:creator><dc:creator>Ming-Hsuan Yang</dc:creator><dc:creator>Truong Nguyen</dc:creator><dc:creator>Lu Qi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}
Published: 2025-12-18T18:59:29+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Lin; Meixi Song; Dizhe Zhang; Wenxuan Lu; Haodong Li; Bo Du; Ming-Hsuan Yang; Truong Nguyen; Lu Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}&lt;/p&gt;</content:encoded></item><item><title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title><link>https://arxiv.org/abs/2512.17908v1</link><guid>http://arxiv.org/abs/2512.17908v1</guid><pubDate>Fri, 19 Dec 2025 18:59:56 +0000</pubDate><dc:creator>Ananta R. Bhattarai</dc:creator><dc:creator>Helge Rhodin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.
Published: 2025-12-19T18:59:56+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ananta R. Bhattarai; Helge Rhodin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.&lt;/p&gt;</content:encoded></item><item><title>Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</title><link>https://arxiv.org/abs/2512.17160v1</link><guid>http://arxiv.org/abs/2512.17160v1</guid><pubDate>Fri, 19 Dec 2025 01:39:43 +0000</pubDate><dc:creator>Dianxing Shi</dc:creator><dc:creator>Dingjie Fu</dc:creator><dc:creator>Yuqiao Liu</dc:creator><dc:creator>Jun Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)" framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.
Published: 2025-12-19T01:39:43+00:00
Venue: arXiv
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dianxing Shi; Dingjie Fu; Yuqiao Liu; Jun Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)&amp;quot; framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.&lt;/p&gt;</content:encoded></item><item><title>MMP-YOLO: A multi-branch defect detection model based on rich gradient information</title><link>https://doi.org/10.1016/j.neucom.2025.132454</link><guid>10.1016/j.neucom.2025.132454</guid><pubDate>Sat, 20 Dec 2025 07:23:19 +0000</pubDate><dc:creator>Zhenyu Wang</dc:creator><dc:creator>Weisheng Li</dc:creator><dc:creator>Shaoze Wang</dc:creator><dc:creator>Shiqiang Liu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132454</prism:doi><description>In the manufacturing process, the diversity of products and the complexity of the production environment pose severe challenges to the detection of small defects, which can lead to serious missed detections and false positives. To address these issues, this paper proposes a multi-branch defect detection model based on rich gradient information (MMP-YOLO), which significantly improves the performance of detecting defective objects. Specifically, we design three innovative modules integrated into MMP-YOLO. (1) The Multi-Level Gradient Lightweight Deep Network (MGLD) module processes multi-gradient information through a deep network integrated with large kernel convolution, ensuring accurate transmission of original input information and efficient feature extraction of small objects. (2) The Multi-Scale Function Complementary Upsampling (MFCU) module exploits the complementarity between high-resolution and low-resolution features and introduces transposed convolution and dilated convolution to reduce information loss further. (3) The Parallel Task-Related Feature Selection (PTFS) module selectively suppresses background interference through a combination of global and local information. Extensive experiments on multiple datasets demonstrate that MMP-YOLO outperforms other state-of-the-art methods in reducing information loss and minimizing background noise interference.
Published: 2025-12-20T07:23:19+00:00
Venue: Neurocomputing
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyu Wang; Weisheng Li; Shaoze Wang; Shiqiang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132454"&gt;10.1016/j.neucom.2025.132454&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;In the manufacturing process, the diversity of products and the complexity of the production environment pose severe challenges to the detection of small defects, which can lead to serious missed detections and false positives. To address these issues, this paper proposes a multi-branch defect detection model based on rich gradient information (MMP-YOLO), which significantly improves the performance of detecting defective objects. Specifically, we design three innovative modules integrated into MMP-YOLO. (1) The Multi-Level Gradient Lightweight Deep Network (MGLD) module processes multi-gradient information through a deep network integrated with large kernel convolution, ensuring accurate transmission of original input information and efficient feature extraction of small objects. (2) The Multi-Scale Function Complementary Upsampling (MFCU) module exploits the complementarity between high-resolution and low-resolution features and introduces transposed convolution and dilated convolution to reduce information loss further. (3) The Parallel Task-Related Feature Selection (PTFS) module selectively suppresses background interference through a combination of global and local information. Extensive experiments on multiple datasets demonstrate that MMP-YOLO outperforms other state-of-the-art methods in reducing information loss and minimizing background noise interference.&lt;/p&gt;</content:encoded></item><item><title>C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation</title><link>https://arxiv.org/abs/2512.16164v1</link><guid>http://arxiv.org/abs/2512.16164v1</guid><pubDate>Thu, 18 Dec 2025 04:30:53 +0000</pubDate><dc:creator>Chao Li</dc:creator><dc:creator>Dasha Hu</dc:creator><dc:creator>Chengyang Li</dc:creator><dc:creator>Yuming Jiang</dc:creator><dc:creator>Yuncheng Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.
Published: 2025-12-18T04:30:53+00:00
Venue: arXiv
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Li; Dasha Hu; Chengyang Li; Yuming Jiang; Yuncheng Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.&lt;/p&gt;</content:encoded></item><item><title>A History-Aware Framework with Multi-Scale Hybrid Attention for Robust Visual Object Tracking</title><link>https://doi.org/10.1016/j.eswa.2025.130914</link><guid>10.1016/j.eswa.2025.130914</guid><pubDate>Sat, 20 Dec 2025 00:09:07 +0000</pubDate><dc:creator>Xiaomei Gong</dc:creator><dc:creator>Yanli Liu</dc:creator><dc:creator>Guanyu Xing</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130914</prism:doi><description>Despite the remarkable progress achieved in visual object tracking, existing approaches still struggle when facing occlusions caused by visually similar distractors, which often lead to ambiguous target representations and tracking drift. To address this challenge, we propose a History-Aware Feature Enhancement (HAFE) framework, a lightweight and plug-and-play module that can be seamlessly integrated into a wide range of tracking systems. HAFE leverages the strong feature representation capabilities of modern backbones while introducing minimal computational overhead. Its core contribution lies in a historical feature modeling strategy that selectively propagates and adaptively aligns target information across frames, thereby constructing a stable temporal reference that reinforces current-frame representations. By embedding reliable historical cues into the tracking process, HAFE enhances a tracker’s ability to distinguish the target from visually similar distractors and maintain robustness under severe occlusions . Extensive experiments on five challenging benchmarks with three representative transformer-based trackers, including DropTrack, GRM, and ROMTrack,demonstrate consistent and significant performance gains. In particular, integrating HAFE into the high-performance DropTrack tracker improves AUC on LaSOT by 1.0% and AO on GOT-10k by 2.0%, while simultaneously reducing training parameters by 51% and training iterations by 67%. These results highlight HAFE’s potential as a practical and efficient enhancement module for real-world intelligent visual tracking systems.
Published: 2025-12-20T00:09:07+00:00
Venue: Expert Systems with Applications
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaomei Gong; Yanli Liu; Guanyu Xing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130914"&gt;10.1016/j.eswa.2025.130914&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Despite the remarkable progress achieved in visual object tracking, existing approaches still struggle when facing occlusions caused by visually similar distractors, which often lead to ambiguous target representations and tracking drift. To address this challenge, we propose a History-Aware Feature Enhancement (HAFE) framework, a lightweight and plug-and-play module that can be seamlessly integrated into a wide range of tracking systems. HAFE leverages the strong feature representation capabilities of modern backbones while introducing minimal computational overhead. Its core contribution lies in a historical feature modeling strategy that selectively propagates and adaptively aligns target information across frames, thereby constructing a stable temporal reference that reinforces current-frame representations. By embedding reliable historical cues into the tracking process, HAFE enhances a tracker’s ability to distinguish the target from visually similar distractors and maintain robustness under severe occlusions . Extensive experiments on five challenging benchmarks with three representative transformer-based trackers, including DropTrack, GRM, and ROMTrack,demonstrate consistent and significant performance gains. In particular, integrating HAFE into the high-performance DropTrack tracker improves AUC on LaSOT by 1.0% and AO on GOT-10k by 2.0%, while simultaneously reducing training parameters by 51% and training iterations by 67%. These results highlight HAFE’s potential as a practical and efficient enhancement module for real-world intelligent visual tracking systems.&lt;/p&gt;</content:encoded></item><item><title>Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection</title><link>https://arxiv.org/abs/2512.17350v1</link><guid>http://arxiv.org/abs/2512.17350v1</guid><pubDate>Fri, 19 Dec 2025 08:47:09 +0000</pubDate><dc:creator>Chenming Zhou</dc:creator><dc:creator>Jiaan Wang</dc:creator><dc:creator>Yu Li</dc:creator><dc:creator>Lei Li</dc:creator><dc:creator>Juan Cao</dc:creator><dc:creator>Sheng Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.
Published: 2025-12-19T08:47:09+00:00
Venue: arXiv
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenming Zhou; Jiaan Wang; Yu Li; Lei Li; Juan Cao; Sheng Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.&lt;/p&gt;</content:encoded></item><item><title>PMDNet: Progressive modulation network with global-local representations for single image deraining</title><link>https://doi.org/10.1016/j.eswa.2025.130910</link><guid>10.1016/j.eswa.2025.130910</guid><pubDate>Sun, 21 Dec 2025 15:36:42 +0000</pubDate><dc:creator>Yihao Ni</dc:creator><dc:creator>Shan Gai</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130910</prism:doi><description>Images captured under adverse weather conditions such as rainfall suffer from severe quality degradation, which subsequently impacts the performance of numerous vision-oriented systems. As a potential remedy, we propose an advanced progressive modulation network, named PMDNet, for single image deraining. The proposed method attains exceptional rain removal performance through three pivotal designs: 1) a dual-branch framework is employed to jointly optimize rain residuals and background images, which exploits degradation priors by modulating rain-free features with rain features; 2) the integration of Transformer and convolutional neural network (CNN) paradigms allows the model to combine their complementary strengths and to balance both global and local representations; 3) a novel sandwich-shaped Transformer architecture (i.e., placing self-attention between two feed-forward networks) and dilated convolutions with varying dilation factors are introduced to respectively enhance the effectiveness of self-attention and convolutional attention mechanisms, thereby facilitating more refined rain feature extraction and rain-free feature modulation. Extensive experiments conducted on synthetic rain streak/rain-fog/raindrop datasets, real rain samples, snowy scenes, as well as low-light conditions demonstrate the superiority and extensibility of our proposed method. The source code is available at https://github.com/N-yh/PMDNet .
Published: 2025-12-21T15:36:42+00:00
Venue: Expert Systems with Applications
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihao Ni; Shan Gai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130910"&gt;10.1016/j.eswa.2025.130910&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;Images captured under adverse weather conditions such as rainfall suffer from severe quality degradation, which subsequently impacts the performance of numerous vision-oriented systems. As a potential remedy, we propose an advanced progressive modulation network, named PMDNet, for single image deraining. The proposed method attains exceptional rain removal performance through three pivotal designs: 1) a dual-branch framework is employed to jointly optimize rain residuals and background images, which exploits degradation priors by modulating rain-free features with rain features; 2) the integration of Transformer and convolutional neural network (CNN) paradigms allows the model to combine their complementary strengths and to balance both global and local representations; 3) a novel sandwich-shaped Transformer architecture (i.e., placing self-attention between two feed-forward networks) and dilated convolutions with varying dilation factors are introduced to respectively enhance the effectiveness of self-attention and convolutional attention mechanisms, thereby facilitating more refined rain feature extraction and rain-free feature modulation. Extensive experiments conducted on synthetic rain streak/rain-fog/raindrop datasets, real rain samples, snowy scenes, as well as low-light conditions demonstrate the superiority and extensibility of our proposed method. The source code is available at https://github.com/N-yh/PMDNet .&lt;/p&gt;</content:encoded></item><item><title>PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation</title><link>https://arxiv.org/abs/2512.16494v1</link><guid>http://arxiv.org/abs/2512.16494v1</guid><pubDate>Thu, 18 Dec 2025 13:01:36 +0000</pubDate><dc:creator>Mengyuan Liu</dc:creator><dc:creator>Jiajie Liu</dc:creator><dc:creator>Jinyan Zhang</dc:creator><dc:creator>Wenhao Li</dc:creator><dc:creator>Junsong Yuan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.
Published: 2025-12-18T13:01:36+00:00
Venue: arXiv
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengyuan Liu; Jiajie Liu; Jinyan Zhang; Wenhao Li; Junsong Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.&lt;/p&gt;</content:encoded></item><item><title>ST-Imputer: Multivariate Dependency-aware Diffusion Network with Physics Guidance for Spatiotemporal Imputation</title><link>https://doi.org/10.1016/j.inffus.2025.104084</link><guid>10.1016/j.inffus.2025.104084</guid><pubDate>Sat, 20 Dec 2025 23:22:45 +0000</pubDate><dc:creator>Xingyu Zhao</dc:creator><dc:creator>Jianpeng Qi</dc:creator><dc:creator>Bin Lu</dc:creator><dc:creator>Lei Zhou</dc:creator><dc:creator>Lei Cao</dc:creator><dc:creator>Junyu Dong</dc:creator><dc:creator>Yanwei Yu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104084</prism:doi><description>Data preparation is crucial for achieving optimal results in deep learning. Unfortunately, missing values are common when preparing large-scale spatiotemporal databases. Most existing imputation methods primarily focus on exploring the spatiotemporal correlations of single-source data; however, high missing rates in single-source data result in sparse distributions. Furthermore, existing methods typically focus on shallow correlations at a single scale, limiting the ability of imputation models to effectively leverage multi-scale spatial features. To tackle these challenges, we propose a multivariate dependency-aware spatiotemporal imputation model, named ST-Imputer. Specifically, we introduce multi-source context data to provide sufficient correlation features for target data ( i.e ., data that needs imputation), alleviating the issue of insufficient available features caused by high missing rates in single-source data. By applying a multi-variate spatiotemporal dependency extraction module, ST-Imputer captures potential associations between different spatial scales. Subsequently, the noise prediction module utilizes the learned dual-view features to formulate the spatiotemporal transmission module, thereby reducing weight errors caused by excessive noise. Finally, physical constraints are applied to prevent unrealistic predictions. Extensive experiments on three large-scale datasets demonstrate the significant superiority of ST-Imputer, achieving up to a 13.07% improvement in RMSE. The code of our model is available at https://github.com/Lion1a/ST-Imputer .
Published: 2025-12-20T23:22:45+00:00
Venue: Information Fusion
Score: 0.769 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyu Zhao; Jianpeng Qi; Bin Lu; Lei Zhou; Lei Cao; Junyu Dong; Yanwei Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104084"&gt;10.1016/j.inffus.2025.104084&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (consider)&lt;/p&gt;
&lt;p&gt;Data preparation is crucial for achieving optimal results in deep learning. Unfortunately, missing values are common when preparing large-scale spatiotemporal databases. Most existing imputation methods primarily focus on exploring the spatiotemporal correlations of single-source data; however, high missing rates in single-source data result in sparse distributions. Furthermore, existing methods typically focus on shallow correlations at a single scale, limiting the ability of imputation models to effectively leverage multi-scale spatial features. To tackle these challenges, we propose a multivariate dependency-aware spatiotemporal imputation model, named ST-Imputer. Specifically, we introduce multi-source context data to provide sufficient correlation features for target data ( i.e ., data that needs imputation), alleviating the issue of insufficient available features caused by high missing rates in single-source data. By applying a multi-variate spatiotemporal dependency extraction module, ST-Imputer captures potential associations between different spatial scales. Subsequently, the noise prediction module utilizes the learned dual-view features to formulate the spatiotemporal transmission module, thereby reducing weight errors caused by excessive noise. Finally, physical constraints are applied to prevent unrealistic predictions. Extensive experiments on three large-scale datasets demonstrate the significant superiority of ST-Imputer, achieving up to a 13.07% improvement in RMSE. The code of our model is available at https://github.com/Lion1a/ST-Imputer .&lt;/p&gt;</content:encoded></item><item><title>LAPX: Lightweight Hourglass Network with Global Context</title><link>https://arxiv.org/abs/2512.16089v1</link><guid>http://arxiv.org/abs/2512.16089v1</guid><pubDate>Thu, 18 Dec 2025 02:04:36 +0000</pubDate><dc:creator>Haopeng Zhao</dc:creator><dc:creator>Marsha Mariya Kappan</dc:creator><dc:creator>Mahdi Bamdad</dc:creator><dc:creator>Francisco Cruz</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.
Published: 2025-12-18T02:04:36+00:00
Venue: arXiv
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haopeng Zhao; Marsha Mariya Kappan; Mahdi Bamdad; Francisco Cruz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.&lt;/p&gt;</content:encoded></item></channel></rss>