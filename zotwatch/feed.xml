<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 01 Jan 2026 02:56:23 +0000</lastBuildDate><item><title>On the Transferability and Discriminability of Representation Learning in Unsupervised Domain Adaptation</title><link>https://doi.org/10.1109/tpami.2025.3649294</link><guid>10.1109/tpami.2025.3649294</guid><pubDate>Tue, 30 Dec 2025 18:37:35 +0000</pubDate><dc:creator>Wenwen Qiang</dc:creator><dc:creator>Ziyin Gu</dc:creator><dc:creator>Lingyu Si</dc:creator><dc:creator>Jiangmeng Li</dc:creator><dc:creator>Changwen Zheng</dc:creator><dc:creator>Fuchun Sun</dc:creator><dc:creator>Hui Xiong</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649294</prism:doi><description>In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical–practical gap, we defined “good representation learning” as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.
Published: 2025-12-30T18:37:35+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenwen Qiang; Ziyin Gu; Lingyu Si; Jiangmeng Li; Changwen Zheng; Fuchun Sun; Hui Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649294"&gt;10.1109/tpami.2025.3649294&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical–practical gap, we defined “good representation learning” as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.&lt;/p&gt;</content:encoded></item><item><title>FocusPatch AD: Few-Shot Multi-Class Anomaly Detection with Unified Keywords Patch Prompts</title><link>https://doi.org/10.1109/tip.2025.3646861</link><guid>10.1109/tip.2025.3646861</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Xicheng Ding</dc:creator><dc:creator>Xiaofan Li</dc:creator><dc:creator>Mingang Chen</dc:creator><dc:creator>Jingyu Gong</dc:creator><dc:creator>Yuan Xie</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646861</prism:doi><description>Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xicheng Ding; Xiaofan Li; Mingang Chen; Jingyu Gong; Yuan Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646861"&gt;10.1109/tip.2025.3646861&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.&lt;/p&gt;</content:encoded></item><item><title>SAR-UMSDN: The Unsupervised Multimodal Ship Detection Network Based on SAR Images</title><link>https://doi.org/10.1109/jstars.2025.3649747</link><guid>10.1109/jstars.2025.3649747</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Junpeng Ai</dc:creator><dc:creator>Liang Luo</dc:creator><dc:creator>Shijie Wang</dc:creator><dc:creator>Liandong Hao</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649747</prism:doi><description>In response to the problems faced by synthetic aperture radar (SAR) images in detecting ships in complex near-shore scenarios, such as low signal-to-noise ratio of small targets and limited feature representation capacity of a single SAR mode, and also due to the fact that real optical images in near-shore areas are often affected by conditions like clouds and day/night, and thus difficult to be real-time registered with SAR images. We propose an unsupervised multimodal SAR image ship detection network (SAR-UMSDN) with its performance enhanced by deriving multimodal inputs from SAR data. SAR-UMSDN consists of a self-supervised SAR image enhancement network (URSIEN), SAR colorization module based on generative adversarial networks (cGAN4ColSAR), and multimodal detection model. URSIEN performs unlabeled illumination–reflection decoupling and detail restoration to enhance the original SAR image, while cGAN4ColSAR maps the single-channel SAR image onto a pseudo-color image with rich optical textures. We use images of these two modalities as inputs to the multimodal SAR-UMSDN for ablation tests, model comparisons, and other evaluations. URSIEN achieves a peak signal-to-noise ratio of 28.63 dB and inference speed of 0.0017 s for a model size of 0.4 million parameters, improving most indicators compared with other image enhancement models. After applying URSIEN, the mean average precision at 0.5 (mAP@0.5) of SAR-UMSDN is 0.4% higher than that of the baseline model, and after using cGAN4ColSAR, mAP@0.5 is 1.9% higher than that of the baseline model. The combined effect of the two components improves the SAR-UMSDN mAP@0.5 to 93.5%. SAR-UMSDN outperforms 10 baseline models and 7 state-of-the-art models, with its mAP@0.5–0.95 (67.4%) being superior to that of traditional single-modal and mainstream baseline models, showing improvements of 1.4–2.7%. The generalization experiments conducted on six public ship datasets have shown that SAR-UMSDN has a significant advantage in ship...
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junpeng Ai; Liang Luo; Shijie Wang; Liandong Hao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649747"&gt;10.1109/jstars.2025.3649747&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;In response to the problems faced by synthetic aperture radar (SAR) images in detecting ships in complex near-shore scenarios, such as low signal-to-noise ratio of small targets and limited feature representation capacity of a single SAR mode, and also due to the fact that real optical images in near-shore areas are often affected by conditions like clouds and day/night, and thus difficult to be real-time registered with SAR images. We propose an unsupervised multimodal SAR image ship detection network (SAR-UMSDN) with its performance enhanced by deriving multimodal inputs from SAR data. SAR-UMSDN consists of a self-supervised SAR image enhancement network (URSIEN), SAR colorization module based on generative adversarial networks (cGAN4ColSAR), and multimodal detection model. URSIEN performs unlabeled illumination–reflection decoupling and detail restoration to enhance the original SAR image, while cGAN4ColSAR maps the single-channel SAR image onto a pseudo-color image with rich optical textures. We use images of these two modalities as inputs to the multimodal SAR-UMSDN for ablation tests, model comparisons, and other evaluations. URSIEN achieves a peak signal-to-noise ratio of 28.63 dB and inference speed of 0.0017 s for a model size of 0.4 million parameters, improving most indicators compared with other image enhancement models. After applying URSIEN, the mean average precision at 0.5 (mAP@0.5) of SAR-UMSDN is 0.4% higher than that of the baseline model, and after using cGAN4ColSAR, mAP@0.5 is 1.9% higher than that of the baseline model. The combined effect of the two components improves the SAR-UMSDN mAP@0.5 to 93.5%. SAR-UMSDN outperforms 10 baseline models and 7 state-of-the-art models, with its mAP@0.5–0.95 (67.4%) being superior to that of traditional single-modal and mainstream baseline models, showing improvements of 1.4–2.7%. The generalization experiments conducted on six public ship datasets have shown that SAR-UMSDN has a significant advantage in ship...&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Fine-Grained Classification with Foreground-Aware Kernelized Feature Reconstruction Network</title><link>https://doi.org/10.1109/tip.2025.3646940</link><guid>10.1109/tip.2025.3646940</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Yangfan Li</dc:creator><dc:creator>Wei Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646940</prism:doi><description>Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yangfan Li; Wei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646940"&gt;10.1109/tip.2025.3646940&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.&lt;/p&gt;</content:encoded></item><item><title>Exploring Syn-to-Real Domain Adaptation for Military Target Detection</title><link>https://arxiv.org/abs/2512.23208v1</link><guid>http://arxiv.org/abs/2512.23208v1</guid><pubDate>Mon, 29 Dec 2025 05:05:41 +0000</pubDate><dc:creator>Jongoh Jeong</dc:creator><dc:creator>Youngjin Oh</dc:creator><dc:creator>Gyeongrae Nam</dc:creator><dc:creator>Jeongeun Lee</dc:creator><dc:creator>Kuk-Jin Yoon</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.
Published: 2025-12-29T05:05:41+00:00
Venue: arXiv
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jongoh Jeong; Youngjin Oh; Gyeongrae Nam; Jeongeun Lee; Kuk-Jin Yoon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.&lt;/p&gt;</content:encoded></item><item><title>Harnessing the power of single-cell large language models with parameter-efficient fine-tuning using scPEFT</title><link>https://doi.org/10.1038/s42256-025-01170-z</link><guid>10.1038/s42256-025-01170-z</guid><pubDate>Wed, 31 Dec 2025 10:02:00 +0000</pubDate><dc:creator>Fei He</dc:creator><dc:creator>Ruixin Fei</dc:creator><dc:creator>Jordan E. Krull</dc:creator><dc:creator>Yang Yu</dc:creator><dc:creator>Xinyu Zhang</dc:creator><dc:creator>Xianyu Wang</dc:creator><dc:creator>Hao Cheng</dc:creator><dc:creator>Mingyue Gao</dc:creator><dc:creator>Li Su</dc:creator><dc:creator>Yibo Chen</dc:creator><dc:creator>Jinpu Li</dc:creator><dc:creator>Baichuan Jin</dc:creator><dc:creator>Yuzhou Chang</dc:creator><dc:creator>Anjun Ma</dc:creator><dc:creator>Qin Ma</dc:creator><dc:creator>Dong Xu</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01170-z</prism:doi><description>Single-cell large language models (scLLMs) capture essential biological insights from vast single-cell atlases but struggle in out-of-context applications, where zero-shot predictions can be unreliable. To address this, here we introduce a single-cell parameter-efficient fine-tuning (scPEFT) framework that integrates learnable, low-dimensional adapters into scLLMs. By freezing the backbone model and updating only the adapter parameters, scPEFT efficiently adapts to specific tasks using limited custom data. This approach mitigates catastrophic forgetting, reduces parameter tuning by over 96% and decreases GPU memory usage by more than half, thus substantially enhancing the accessibility of scLLMs for resource-constrained researchers. When validated across diverse datasets, scPEFT outperformed zero-shot models and traditional fine-tuning in disease-specific, cross-species and undercharacterized cell population tasks. Its attention-mechanism analysis identified COVID-related genes associated with specific cell states and uncovered unique blood cell subpopulations, demonstrating the capacity of scPEFT for condition-specific interpretations. These findings position scPEFT as an efficient solution for enhancing the utility of scLLMs in general single-cell analyses. He et al. present a parameter-efficient fine-tuning method for single-cell language models that improves performance on unseen diseases, treatments and cell types.
Published: 2025-12-31T10:02:00+00:00
Venue: Nature Machine Intelligence
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fei He; Ruixin Fei; Jordan E. Krull; Yang Yu; Xinyu Zhang; Xianyu Wang; Hao Cheng; Mingyue Gao; Li Su; Yibo Chen; Jinpu Li; Baichuan Jin; Yuzhou Chang; Anjun Ma; Qin Ma; Dong Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01170-z"&gt;10.1038/s42256-025-01170-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Single-cell large language models (scLLMs) capture essential biological insights from vast single-cell atlases but struggle in out-of-context applications, where zero-shot predictions can be unreliable. To address this, here we introduce a single-cell parameter-efficient fine-tuning (scPEFT) framework that integrates learnable, low-dimensional adapters into scLLMs. By freezing the backbone model and updating only the adapter parameters, scPEFT efficiently adapts to specific tasks using limited custom data. This approach mitigates catastrophic forgetting, reduces parameter tuning by over 96% and decreases GPU memory usage by more than half, thus substantially enhancing the accessibility of scLLMs for resource-constrained researchers. When validated across diverse datasets, scPEFT outperformed zero-shot models and traditional fine-tuning in disease-specific, cross-species and undercharacterized cell population tasks. Its attention-mechanism analysis identified COVID-related genes associated with specific cell states and uncovered unique blood cell subpopulations, demonstrating the capacity of scPEFT for condition-specific interpretations. These findings position scPEFT as an efficient solution for enhancing the utility of scLLMs in general single-cell analyses. He et al. present a parameter-efficient fine-tuning method for single-cell language models that improves performance on unseen diseases, treatments and cell types.&lt;/p&gt;</content:encoded></item><item><title>Learning intermediate physical states for inverse metasurface design</title><link>https://doi.org/10.1038/s42256-025-01167-8</link><guid>10.1038/s42256-025-01167-8</guid><pubDate>Wed, 31 Dec 2025 16:02:04 +0000</pubDate><dc:creator>Chun-Teh Chen</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01167-8</prism:doi><description>Deep generative models that learn intermediate surface-current maps, rather than layouts directly, offer a more stable route to inverse design of tunable and stacked metasurfaces.
Published: 2025-12-31T16:02:04+00:00
Venue: Nature Machine Intelligence
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chun-Teh Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01167-8"&gt;10.1038/s42256-025-01167-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Deep generative models that learn intermediate surface-current maps, rather than layouts directly, offer a more stable route to inverse design of tunable and stacked metasurfaces.&lt;/p&gt;</content:encoded></item><item><title>MSCK-Net: Multiscale Chinese Knot Convolutional Network for Dim and Small Infrared Ship Detection</title><link>https://doi.org/10.1109/tgrs.2025.3649839</link><guid>10.1109/tgrs.2025.3649839</guid><pubDate>Wed, 31 Dec 2025 18:44:00 +0000</pubDate><dc:creator>Yuhao Lin</dc:creator><dc:creator>Dongliang Peng</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Lingjie Jiang</dc:creator><dc:creator>Haewoon Nam</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649839</prism:doi><description>Remote sensing infrared ship detection is crucial for maritime safety and traffic management in civil-military integrated applications. However, the detection accuracy and robustness of current methods are still limited by dataset constraints, including small scale, narrow target size distribution, sparse targets, and high scenario specificity. To address these problems, we integrate publicly available datasets to construct IRShip—a relatively large-scale infrared ship detection dataset comprising 27,138 images, significantly improving data scale and diversity. We further design a copy-poisson blend (CP-PB) offline data augmentation approach and introduce a dense one-to-one (Dense-O2O) online augmentation strategy, which improve adaptability to complex backgrounds and scale variations, mitigate problems from sparse targets, and improve dataset utility and model robustness. We propose MSCK-Net, a multiscale chinese knot convolutional network tailored for detecting dim and small infrared ship targets (DSIRST). Specifically, we propose a novel Chinese Knot Convolution (CKConv) that better aligns with both the local features of small targets and the morphological characteristics of ship targets, thereby significantly enhancing low-level feature representation. Built with CKConv, the multiscale knot block (MSK-Block) and Stem-ck modules enhance deep feature transmission efficiency and global modeling of DSIRST, leading to notable gains in detection performance. Extensive experiments on IRShip, NUDT-SIRST-Sea, ISDD, IRSDSS, and Maritime-sirst demonstrate that MSCK-Net-M achieves state-of-the-art performance, with AP50, AP75, and AP of 82.5%, 53.1%, and 50.9% on IRShip, significantly outperforming the existing 20 general object detectors and 6 infrared target detectors. Generalization experiments on four sub-datasets further verify the effectiveness and robustness of our proposed method. The code and dataset are available at: https://github.com/sjxxrh/MSCK-Net.
Published: 2025-12-31T18:44:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhao Lin; Dongliang Peng; Liang Wang; Lingjie Jiang; Haewoon Nam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649839"&gt;10.1109/tgrs.2025.3649839&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing infrared ship detection is crucial for maritime safety and traffic management in civil-military integrated applications. However, the detection accuracy and robustness of current methods are still limited by dataset constraints, including small scale, narrow target size distribution, sparse targets, and high scenario specificity. To address these problems, we integrate publicly available datasets to construct IRShip—a relatively large-scale infrared ship detection dataset comprising 27,138 images, significantly improving data scale and diversity. We further design a copy-poisson blend (CP-PB) offline data augmentation approach and introduce a dense one-to-one (Dense-O2O) online augmentation strategy, which improve adaptability to complex backgrounds and scale variations, mitigate problems from sparse targets, and improve dataset utility and model robustness. We propose MSCK-Net, a multiscale chinese knot convolutional network tailored for detecting dim and small infrared ship targets (DSIRST). Specifically, we propose a novel Chinese Knot Convolution (CKConv) that better aligns with both the local features of small targets and the morphological characteristics of ship targets, thereby significantly enhancing low-level feature representation. Built with CKConv, the multiscale knot block (MSK-Block) and Stem-ck modules enhance deep feature transmission efficiency and global modeling of DSIRST, leading to notable gains in detection performance. Extensive experiments on IRShip, NUDT-SIRST-Sea, ISDD, IRSDSS, and Maritime-sirst demonstrate that MSCK-Net-M achieves state-of-the-art performance, with AP50, AP75, and AP of 82.5%, 53.1%, and 50.9% on IRShip, significantly outperforming the existing 20 general object detectors and 6 infrared target detectors. Generalization experiments on four sub-datasets further verify the effectiveness and robustness of our proposed method. The code and dataset are available at: https://github.com/sjxxrh/MSCK-Net.&lt;/p&gt;</content:encoded></item><item><title>Cross-modality Feature Aggregation for Cross-domain Point Cloud Representation Learning</title><link>https://doi.org/10.1109/tip.2025.3646890</link><guid>10.1109/tip.2025.3646890</guid><pubDate>Wed, 31 Dec 2025 18:46:40 +0000</pubDate><dc:creator>Guoqing Wang</dc:creator><dc:creator>Chao Ma</dc:creator><dc:creator>Xiaokang Yang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646890</prism:doi><description>Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.
Published: 2025-12-31T18:46:40+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guoqing Wang; Chao Ma; Xiaokang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646890"&gt;10.1109/tip.2025.3646890&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Existing methods for learning 3D point cloud representation often use a single dataset-specific training and testing approach, leading to performance drops due to significant domain shifts between training and testing data. While recent crossdomain methods have made promising progress, the lack of inherent semantic information in point clouds makes models prone to overfitting specific datasets. As such, we introduce 3D-CFA, a simple yet effective cross-modality feature aggregation method for cross-domain 3D point cloud representation learning. 3D-CFA aggregates the geometry tokens with semantic tokens derived from multi-view images, which are projected from the point cloud, thus generating more transferable features for cross-domain 3D point cloud representation learning. Specifically, 3D-CFA consists of two main components: a cross-modality feature aggregation module and an elastic domain alignment module. The cross-modality feature aggregation module converts unordered points into multi-view images using the modality transformation module. Then, the geometry tokens and semantic tokens extracted from the geometry encoder and semantic encoder are fed into the cross-modal projector to get the transferable 3D tokens. A key insight of this design is that the semantic tokens can serve as a bridge between the 3D point cloud model and the 2D foundation model, greatly promoting the generalization of cross-domain models facing the severe domain shift. Finally, the elastic domain alignment module learns the hierarchical domain-invariant features of different training domains for either domain adaptation or domain generalization protocols. 3D-CFA finds a better way to transfer the knowledge of the 2D foundation model pre-trained at scale, meanwhile only introducing a few extra trainable parameters. Comprehensive experiments on several cross-domain point cloud benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>DARFNet: A Divergence-Aware Reciprocal Fusion Network for Multispectral Feature Alignment and Fusion</title><link>https://doi.org/10.1109/jstars.2025.3647819</link><guid>10.1109/jstars.2025.3647819</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Junyu Huang</dc:creator><dc:creator>Jiawei Chen</dc:creator><dc:creator>Renbo Luo</dc:creator><dc:creator>Yongan Lu</dc:creator><dc:creator>Jinxin Yang</dc:creator><dc:creator>Zhifeng Wu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647819</prism:doi><description>Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junyu Huang; Jiawei Chen; Renbo Luo; Yongan Lu; Jinxin Yang; Zhifeng Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647819"&gt;10.1109/jstars.2025.3647819&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Robust detection of small objects in remote sensing imagery remains a significant challenge due to complex backgrounds, scale variation, and modality inconsistency. In this article, we propose DARFNet, a novel multispectral detection framework that effectively integrates RGB and infrared information for accurate small object localization. DARFNet employs a dual-branch architecture with a dynamic attention-based fusion mechanism to adaptively enhance complementary features. In addition, we incorporate lightweight yet expressive modules–ODConv and ConvNeXtBlock–to boost detection performance while maintaining computational efficiency. Extensive experiments on three widely-used benchmarks, including VEDAI, NWPU, and DroneVehicle, demonstrate that DARFNet outperforms state-of-the-art methods in both accuracy and efficiency. Notably, our model shows superior performance in detecting small and densely distributed targets under complex remote sensing conditions.&lt;/p&gt;</content:encoded></item><item><title>Multispectral Remote Sensing Object Detection via Selective Cross-modal Interaction and Aggregation</title><link>https://doi.org/10.1016/j.neunet.2025.108533</link><guid>10.1016/j.neunet.2025.108533</guid><pubDate>Wed, 31 Dec 2025 00:15:06 +0000</pubDate><dc:creator>Minghao Cui</dc:creator><dc:creator>Jing Nie</dc:creator><dc:creator>Hanqing Sun</dc:creator><dc:creator>Jin Xie</dc:creator><dc:creator>Jiale Cao</dc:creator><dc:creator>Yanwei Pang</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108533</prism:doi><description>Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.
Published: 2025-12-31T00:15:06+00:00
Venue: Neural Networks
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minghao Cui; Jing Nie; Hanqing Sun; Jin Xie; Jiale Cao; Yanwei Pang; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108533"&gt;10.1016/j.neunet.2025.108533&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Multispectral remote sensing object detection plays a vital role in a wide range of geoscience and remote sensing applications, such as environmental monitoring and disaster monitoring, by leveraging complementary information from RGB and infrared modalities. The performance of such systems heavily relies on the effective fusion of information across these modalities. A key challenge lies in capturing meaningful cross-modal long-range dependencies that aid object localization and identification, while simultaneously suppressing noise and irrelevant information during feature fusion to enhance the discriminative quality of the fused representations. To address these challenges, we propose a novel framework, termed Selective cross-modal Interaction and Aggregation (SIA), which comprises two key components: the Selective Cross-modal Interaction (SCI) module and the Selective Feature Aggregation (SFA) module. The SCI module addresses the inefficiency of traditional cross-modal attention mechanisms by selectively prioritizing the most informative long-range dependencies. This significantly reduces computational costs while maintaining high detection accuracy. The SFA module utilizes a gating mechanism to effectively filter out noise and redundant information introduced by equal-weight fusion, thereby yielding more discriminative feature representations. Comprehensive experiments are conducted on the challenging multispectral remote sensing object detection benchmark DroneVehicle, as well as two additional multispectral urban object detection datasets, M 3 FD and LLVIP. The proposed approach consistently achieves superior detection accuracy across all datasets. Notably, on the DroneVehicle test set, our method outperforms the recently introduced C 2 Former by 2.8% mAP@0.5, while incurring lower computational cost.&lt;/p&gt;</content:encoded></item><item><title>YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection</title><link>https://arxiv.org/abs/2512.23273v2</link><guid>http://arxiv.org/abs/2512.23273v2</guid><pubDate>Mon, 29 Dec 2025 07:54:49 +0000</pubDate><dc:creator>Xu Lin</dc:creator><dc:creator>Jinlong Peng</dc:creator><dc:creator>Zhenye Gan</dc:creator><dc:creator>Jiawen Zhu</dc:creator><dc:creator>Jun Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.
Published: 2025-12-29T07:54:49+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xu Lin; Jinlong Peng; Zhenye Gan; Jiawen Zhu; Jun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.&lt;/p&gt;</content:encoded></item><item><title>Cross-Modality Fusion of Visible Light, Infrared, and SAR Images Under Few-Shot Conditions for Target Recognition</title><link>https://doi.org/10.1109/jstars.2025.3649648</link><guid>10.1109/jstars.2025.3649648</guid><pubDate>Wed, 31 Dec 2025 18:44:28 +0000</pubDate><dc:creator>Chao Li</dc:creator><dc:creator>Jiacheng Ni</dc:creator><dc:creator>Ying Luo</dc:creator><dc:creator>Dan Wang</dc:creator><dc:creator>Qun Zhang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649648</prism:doi><description>In recent years, the advancement of remote sensing technology has driven the widespread application of multi-modal image fusion recognition technology. By integrating data from different sensors, this technology achieves target information complementarity and improves recognition accuracy. However, it still faces challenges including data heterogeneity and feature redundancy, particularly under few-shot conditions. To address this issue, this paper proposes a multi-source heterogeneous image fusion recognition (MHIF) method for few-shot scenarios. First, we propose a cross-modal sampling (CMS) module with a maximum traversal (MT) method to efficiently generate diverse training combinations, thereby expanding the effective sample space. Next, we design an image quality assessment (IQA) module that adaptively weights features from different modalities, optimizing fusion by emphasizing high-quality information. Furthermore, we propose an intra-modal bidirectional guided cross-attention (IBGC) module to mutually enhance the base and modality-specific features, effectively preserving critical details within each modality. Finally, a stepwise fusion strategy progressively integrates these refined features, effectively reducing redundancy and modal interference. Experimental results on few-shot datasets demonstrate that the proposed method exhibits significant advantages in few-shot multi-modal image fusion recognition tasks.
Published: 2025-12-31T18:44:28+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Li; Jiacheng Ni; Ying Luo; Dan Wang; Qun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649648"&gt;10.1109/jstars.2025.3649648&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, the advancement of remote sensing technology has driven the widespread application of multi-modal image fusion recognition technology. By integrating data from different sensors, this technology achieves target information complementarity and improves recognition accuracy. However, it still faces challenges including data heterogeneity and feature redundancy, particularly under few-shot conditions. To address this issue, this paper proposes a multi-source heterogeneous image fusion recognition (MHIF) method for few-shot scenarios. First, we propose a cross-modal sampling (CMS) module with a maximum traversal (MT) method to efficiently generate diverse training combinations, thereby expanding the effective sample space. Next, we design an image quality assessment (IQA) module that adaptively weights features from different modalities, optimizing fusion by emphasizing high-quality information. Furthermore, we propose an intra-modal bidirectional guided cross-attention (IBGC) module to mutually enhance the base and modality-specific features, effectively preserving critical details within each modality. Finally, a stepwise fusion strategy progressively integrates these refined features, effectively reducing redundancy and modal interference. Experimental results on few-shot datasets demonstrate that the proposed method exhibits significant advantages in few-shot multi-modal image fusion recognition tasks.&lt;/p&gt;</content:encoded></item><item><title>LNet: Lightweight Network for Driver Attention Estimation via Scene and Gaze Consistency</title><link>https://doi.org/10.1109/tip.2025.3646893</link><guid>10.1109/tip.2025.3646893</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Daosong Hu</dc:creator><dc:creator>Xi Li</dc:creator><dc:creator>Mingyue Cui</dc:creator><dc:creator>Kai Huang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646893</prism:doi><description>In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daosong Hu; Xi Li; Mingyue Cui; Kai Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646893"&gt;10.1109/tip.2025.3646893&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.&lt;/p&gt;</content:encoded></item><item><title>YOLO-CMFM: A Visible-SAR Multimodal Object Detection Method Based on Edge-Guided and Gated Cross-Attention Fusion</title><link>https://doi.org/10.3390/rs18010136</link><guid>10.3390/rs18010136</guid><pubDate>Wed, 31 Dec 2025 14:08:11 +0000</pubDate><dc:creator>Xuyang Zhao</dc:creator><dc:creator>Lijun Zhao</dc:creator><dc:creator>Keli Shi</dc:creator><dc:creator>Ruotian Ren</dc:creator><dc:creator>Zheng Zhang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010136</prism:doi><description>To address the challenges of cross-modal feature misalignment and ineffective information fusion caused by the inherent differences in imaging mechanisms, noise statistics, and semantic representations between visible and synthetic aperture radar (SAR) imagery, this paper proposes a multimodal remote sensing object detection method, namely YOLO-CMFM. Built upon the Ultralytics YOLOv11 framework, the proposed approach designs a Cross-Modal Fusion Module (CMFM) that systematically enhances detection accuracy and robustness from the perspectives of modality alignment, feature interaction, and adaptive fusion. Specifically, (1) a Learnable Edge-Guided Attention (LEGA) module is constructed, which leverages a learnable Gaussian saliency prior to achieve edge-oriented cross-modal alignment, effectively mitigating edge-structure mismatches across modalities; (2) a Bidirectional Cross-Attention (BCA) module is developed to enable deep semantic interaction and global contextual aggregation; (3) a Context-Guided Gating (CGG) module is designed to dynamically generate complementary weights based on multimodal source features and global contextual information, thereby achieving adaptive fusion across modalities. Extensive experiments conducted on the OGSOD 1.0 dataset demonstrate that the proposed YOLO-CMFM achieves an mAP@50 of 96.2% and an mAP@50:95 of 75.1%. While maintaining competitive performance comparable to mainstream approaches at lower IoU thresholds, the proposed method significantly outperforms existing counterparts at high IoU thresholds, highlighting its superior capability in precise object localization. Also, the experimental results on the OSPRC dataset demonstrate that the proposed method can consistently achieve stable gains under different kinds of imaging conditions, including diverse SAR polarizations, spatial resolutions, and cloud occlusion conditions. Moreover, the CMFM can be flexibly integrated into different detection frameworks, which further validates its strong generalization and transferability in multimodal remote sensing object detection tasks.
Published: 2025-12-31T14:08:11+00:00
Venue: Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuyang Zhao; Lijun Zhao; Keli Shi; Ruotian Ren; Zheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010136"&gt;10.3390/rs18010136&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;To address the challenges of cross-modal feature misalignment and ineffective information fusion caused by the inherent differences in imaging mechanisms, noise statistics, and semantic representations between visible and synthetic aperture radar (SAR) imagery, this paper proposes a multimodal remote sensing object detection method, namely YOLO-CMFM. Built upon the Ultralytics YOLOv11 framework, the proposed approach designs a Cross-Modal Fusion Module (CMFM) that systematically enhances detection accuracy and robustness from the perspectives of modality alignment, feature interaction, and adaptive fusion. Specifically, (1) a Learnable Edge-Guided Attention (LEGA) module is constructed, which leverages a learnable Gaussian saliency prior to achieve edge-oriented cross-modal alignment, effectively mitigating edge-structure mismatches across modalities; (2) a Bidirectional Cross-Attention (BCA) module is developed to enable deep semantic interaction and global contextual aggregation; (3) a Context-Guided Gating (CGG) module is designed to dynamically generate complementary weights based on multimodal source features and global contextual information, thereby achieving adaptive fusion across modalities. Extensive experiments conducted on the OGSOD 1.0 dataset demonstrate that the proposed YOLO-CMFM achieves an mAP@50 of 96.2% and an mAP@50:95 of 75.1%. While maintaining competitive performance comparable to mainstream approaches at lower IoU thresholds, the proposed method significantly outperforms existing counterparts at high IoU thresholds, highlighting its superior capability in precise object localization. Also, the experimental results on the OSPRC dataset demonstrate that the proposed method can consistently achieve stable gains under different kinds of imaging conditions, including diverse SAR polarizations, spatial resolutions, and cloud occlusion conditions. Moreover, the CMFM can be flexibly integrated into different detection frameworks, which further validates its strong generalization and transferability in multimodal remote sensing object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>Current-diffusion model for metasurface structure discoveries with spatial-frequency dynamics</title><link>https://doi.org/10.1038/s42256-025-01162-z</link><guid>10.1038/s42256-025-01162-z</guid><pubDate>Wed, 31 Dec 2025 16:02:03 +0000</pubDate><dc:creator>Erji Li</dc:creator><dc:creator>Yusong Wang</dc:creator><dc:creator>Lei Jin</dc:creator><dc:creator>Zheng Zong</dc:creator><dc:creator>Enze Zhu</dc:creator><dc:creator>Bao Wang</dc:creator><dc:creator>Qian Wang</dc:creator><dc:creator>Zongyin Yang</dc:creator><dc:creator>Wen-Yan Yin</dc:creator><dc:creator>Zhun Wei</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01162-z</prism:doi><description>In AI-driven metamaterials discovery, designing metasurfaces requires extrapolation to unexplored performance regimes to discover new structures. Here we introduce MetaAI, a physics-aware current-diffusion framework that synergizes spatial topologies and frequency-domain responses to discover non-intuitive metasurface architectures. Unlike conventional inverse design constrained by predefined specifications, MetaAI operates as a performance synthesizer by generating electrical current distributions that bridge electromagnetic performance and metasurface structures. This enables both in-distribution and out-of-distribution targets with diverse topologies. The core innovation of the proposed framework lies in its dual-domain diffusion module, which directly correlates meta-atom current mechanisms with electromagnetic behaviours to enable the discovery of structures with 17.2% wider operational bandwidths. We validate MetaAI across single-layer, multilayer and dynamically tunable metasurfaces, demonstrating out-of-distribution generalization across full-wave simulations and experimental prototypes. Metasurface design driven by AI faces challenges, such as extrapolation to unexplored performance regimes. MetaAI, a physics-aware current-diffusion framework, is introduced to advance metamaterial discovery from interpolation to extrapolation.
Published: 2025-12-31T16:02:03+00:00
Venue: Nature Machine Intelligence
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Erji Li; Yusong Wang; Lei Jin; Zheng Zong; Enze Zhu; Bao Wang; Qian Wang; Zongyin Yang; Wen-Yan Yin; Zhun Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01162-z"&gt;10.1038/s42256-025-01162-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;In AI-driven metamaterials discovery, designing metasurfaces requires extrapolation to unexplored performance regimes to discover new structures. Here we introduce MetaAI, a physics-aware current-diffusion framework that synergizes spatial topologies and frequency-domain responses to discover non-intuitive metasurface architectures. Unlike conventional inverse design constrained by predefined specifications, MetaAI operates as a performance synthesizer by generating electrical current distributions that bridge electromagnetic performance and metasurface structures. This enables both in-distribution and out-of-distribution targets with diverse topologies. The core innovation of the proposed framework lies in its dual-domain diffusion module, which directly correlates meta-atom current mechanisms with electromagnetic behaviours to enable the discovery of structures with 17.2% wider operational bandwidths. We validate MetaAI across single-layer, multilayer and dynamically tunable metasurfaces, demonstrating out-of-distribution generalization across full-wave simulations and experimental prototypes. Metasurface design driven by AI faces challenges, such as extrapolation to unexplored performance regimes. MetaAI, a physics-aware current-diffusion framework, is introduced to advance metamaterial discovery from interpolation to extrapolation.&lt;/p&gt;</content:encoded></item><item><title>Text-Assisted Multi-Modal Adaptive Registration and Fusion Classification Network</title><link>https://doi.org/10.1109/tgrs.2025.3649754</link><guid>10.1109/tgrs.2025.3649754</guid><pubDate>Tue, 30 Dec 2025 18:37:56 +0000</pubDate><dc:creator>Yufei He</dc:creator><dc:creator>Bobo Xi</dc:creator><dc:creator>Guocheng Li</dc:creator><dc:creator>Tie Zheng</dc:creator><dc:creator>Yunsong Li</dc:creator><dc:creator>Changbin Xue</dc:creator><dc:creator>Ming Shen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649754</prism:doi><description>Due to inherent sensor discrepancies, hyperspectral image (HSI) and LiDAR data in fusion classification tasks often suffer from spatial misalignment and limited high-quality annotations, which severely constrain classification performance. While some recent methods attempt to address these issues by introducing semantic alignment strategies or contrastive learning (CL), challenges such as inconsistent cross-modal representations and limited semantic generalization still persist. To address these issues, we propose a novel text-assisted adaptive registration and fusion classification network (TARCNet). First, we develop a feature adaptive alignment (FAA) module, which adaptively adjusts LiDAR features to alleviate semantic inconsistency under misregistration. Second, we introduce a text-assisted contrastive learning (TCL) module, which leverages linguistic priors to strengthen cross-modal consistency and improve the discriminability of the learned representations. Third, we incorporate a multi-loss joint optimization (MLO) module to ensure consistent and stable optimization across heterogeneous modalities. Extensive experiments conducted on three HSI-LiDAR datasets with misregistration and limited annotations demonstrate that our method outperforms several state-of-the-art approaches, validating its effectiveness and generalization capability.
Published: 2025-12-30T18:37:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yufei He; Bobo Xi; Guocheng Li; Tie Zheng; Yunsong Li; Changbin Xue; Ming Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649754"&gt;10.1109/tgrs.2025.3649754&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Due to inherent sensor discrepancies, hyperspectral image (HSI) and LiDAR data in fusion classification tasks often suffer from spatial misalignment and limited high-quality annotations, which severely constrain classification performance. While some recent methods attempt to address these issues by introducing semantic alignment strategies or contrastive learning (CL), challenges such as inconsistent cross-modal representations and limited semantic generalization still persist. To address these issues, we propose a novel text-assisted adaptive registration and fusion classification network (TARCNet). First, we develop a feature adaptive alignment (FAA) module, which adaptively adjusts LiDAR features to alleviate semantic inconsistency under misregistration. Second, we introduce a text-assisted contrastive learning (TCL) module, which leverages linguistic priors to strengthen cross-modal consistency and improve the discriminability of the learned representations. Third, we incorporate a multi-loss joint optimization (MLO) module to ensure consistent and stable optimization across heterogeneous modalities. Extensive experiments conducted on three HSI-LiDAR datasets with misregistration and limited annotations demonstrate that our method outperforms several state-of-the-art approaches, validating its effectiveness and generalization capability.&lt;/p&gt;</content:encoded></item><item><title>FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing</title><link>https://arxiv.org/abs/2512.24022v1</link><guid>http://arxiv.org/abs/2512.24022v1</guid><pubDate>Tue, 30 Dec 2025 06:48:07 +0000</pubDate><dc:creator>Yunkai Dang</dc:creator><dc:creator>Donghao Wang</dc:creator><dc:creator>Jiacheng Yang</dc:creator><dc:creator>Yifan Jiang</dc:creator><dc:creator>Meiyi Zhu</dc:creator><dc:creator>Yuekun Yang</dc:creator><dc:creator>Cong Wang</dc:creator><dc:creator>Qi Fan</dc:creator><dc:creator>Wenbin Li</dc:creator><dc:creator>Yang Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.
Published: 2025-12-30T06:48:07+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunkai Dang; Donghao Wang; Jiacheng Yang; Yifan Jiang; Meiyi Zhu; Yuekun Yang; Cong Wang; Qi Fan; Wenbin Li; Yang Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision-language models (VLMs) exhibit strong performance across various tasks. However, these VLMs encounter significant challenges when applied to the remote sensing domain due to the inherent differences between remote sensing images and natural images. Existing remote sensing VLMs often fail to extract fine-grained visual features and suffer from visual forgetting during deep language processing. To address this, we introduce MF-RSVLM, a Multi-Feature Fusion Remote Sensing Vision--Language Model that effectively extracts and fuses visual features for RS understanding. MF-RSVLM learns multi-scale visual representations and combines global context with local details, improving the capture of small and complex structures in RS scenes. A recurrent visual feature injection scheme ensures the language model remains grounded in visual evidence and reduces visual forgetting during generation. Extensive experiments on diverse RS benchmarks show that MF-RSVLM achieves state-of-the-art or highly competitive performance across remote sensing classification, image captioning, and VQA tasks. Our code is publicly available at https://github.com/Yunkaidang/RSVLM.&lt;/p&gt;</content:encoded></item><item><title>Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects</title><link>https://arxiv.org/abs/2512.22949v1</link><guid>http://arxiv.org/abs/2512.22949v1</guid><pubDate>Sun, 28 Dec 2025 14:27:55 +0000</pubDate><dc:creator>Zhicheng Zhao</dc:creator><dc:creator>Xuanang Fan</dc:creator><dc:creator>Lingma Sun</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.
Published: 2025-12-28T14:27:55+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhicheng Zhao; Xuanang Fan; Lingma Sun; Chenglong Li; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.&lt;/p&gt;</content:encoded></item><item><title>Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations</title><link>https://doi.org/10.1016/j.inffus.2025.104107</link><guid>10.1016/j.inffus.2025.104107</guid><pubDate>Tue, 30 Dec 2025 17:02:53 +0000</pubDate><dc:creator>Yibo Cui</dc:creator><dc:creator>Liang Xie</dc:creator><dc:creator>Yu Zhao</dc:creator><dc:creator>Jiawei Sun</dc:creator><dc:creator>Erwei Yin</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104107</prism:doi><description>Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.
Published: 2025-12-30T17:02:53+00:00
Venue: Information Fusion
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yibo Cui; Liang Xie; Yu Zhao; Jiawei Sun; Erwei Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104107"&gt;10.1016/j.inffus.2025.104107&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.&lt;/p&gt;</content:encoded></item><item><title>Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection</title><link>https://arxiv.org/abs/2512.22972v1</link><guid>http://arxiv.org/abs/2512.22972v1</guid><pubDate>Sun, 28 Dec 2025 15:32:17 +0000</pubDate><dc:creator>Runwei Guan</dc:creator><dc:creator>Jianan Liu</dc:creator><dc:creator>Shaofeng Liang</dc:creator><dc:creator>Fangqiang Ding</dc:creator><dc:creator>Shanliang Yao</dc:creator><dc:creator>Xiaokai Bai</dc:creator><dc:creator>Daizong Liu</dc:creator><dc:creator>Tao Huang</dc:creator><dc:creator>Guoqiang Mao</dc:creator><dc:creator>Hui Xiong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.
Published: 2025-12-28T15:32:17+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runwei Guan; Jianan Liu; Shaofeng Liang; Fangqiang Ding; Shanliang Yao; Xiaokai Bai; Daizong Liu; Tao Huang; Guoqiang Mao; Hui Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.&lt;/p&gt;</content:encoded></item><item><title>Dimensional Compensation for Small-Sample and Small-Size Insulator Burn Mark via RGB-Point Cloud Fusion in Power Grid Inspection</title><link>https://doi.org/10.1016/j.inffus.2025.104105</link><guid>10.1016/j.inffus.2025.104105</guid><pubDate>Wed, 31 Dec 2025 00:19:21 +0000</pubDate><dc:creator>Junqiu Tang</dc:creator><dc:creator>Zhikang Yuan</dc:creator><dc:creator>Zixiang Wei</dc:creator><dc:creator>Shuojie Gao</dc:creator><dc:creator>Changyong Shen</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104105</prism:doi><description>To address the challenge of scarce burn mark samples in power infrastructure inspection, we introduce the Insulator Burn Mark RGB-Point Cloud (IBMR) dataset, the first publicly available benchmark featuring RGB-point clouds with pixel-level annotations for both insulators and burn marks. To tackle the critical issue of severe class imbalance caused by the vast number of background points and the small size of burn marks, we propose a novel two-stage RGB-point cloud segmentation framework. This framework integrates DCCU-Sampling, an innovative downsampling algorithm that effectively suppresses background points while preserving critical structures of the targets, and BB-Backtracking, a geometric recovery method that reconstructs fine-grained burn mark details lost during downsampling process. Experimental results validate the framework’s effectiveness, achieving 81.21% mIoU with 32 training samples and 68.37% mIoU with only 14 samples. The dataset is publicly available at https://huggingface.co/datasets/Junqiu-Tang/IBMR .
Published: 2025-12-31T00:19:21+00:00
Venue: Information Fusion
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junqiu Tang; Zhikang Yuan; Zixiang Wei; Shuojie Gao; Changyong Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104105"&gt;10.1016/j.inffus.2025.104105&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;To address the challenge of scarce burn mark samples in power infrastructure inspection, we introduce the Insulator Burn Mark RGB-Point Cloud (IBMR) dataset, the first publicly available benchmark featuring RGB-point clouds with pixel-level annotations for both insulators and burn marks. To tackle the critical issue of severe class imbalance caused by the vast number of background points and the small size of burn marks, we propose a novel two-stage RGB-point cloud segmentation framework. This framework integrates DCCU-Sampling, an innovative downsampling algorithm that effectively suppresses background points while preserving critical structures of the targets, and BB-Backtracking, a geometric recovery method that reconstructs fine-grained burn mark details lost during downsampling process. Experimental results validate the framework’s effectiveness, achieving 81.21% mIoU with 32 training samples and 68.37% mIoU with only 14 samples. The dataset is publicly available at https://huggingface.co/datasets/Junqiu-Tang/IBMR .&lt;/p&gt;</content:encoded></item><item><title>SDE Diffusion Models for SAR Image Active Jamming Suppression with Pseudo-Paired SAR images</title><link>https://doi.org/10.1109/jstars.2025.3649264</link><guid>10.1109/jstars.2025.3649264</guid><pubDate>Tue, 30 Dec 2025 18:38:21 +0000</pubDate><dc:creator>Xunhao Lin</dc:creator><dc:creator>Dawei Ren</dc:creator><dc:creator>Ping Lang</dc:creator><dc:creator>Huizhang Yang</dc:creator><dc:creator>Junjun Yin</dc:creator><dc:creator>Jian Yang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649264</prism:doi><description>Synthetic Aperture Radar (SAR) imaging is susceptible to various types of jamming, which can severely degrade image quality and hinder downstream tasks. To address this issue, this paper proposes a jamming suppression method through a stochastic differential equation (SDE) based diffusion model trained on pseudo-paired SAR images. Firstly, candidate jamming regions are identified in suppression jamming SAR images through energy concentration and low-rank characteristics. Then, pseudo-paired SAR images representing low and high jamming states are constructed by combining these candidate regions with the original SAR images (referred to as clean images in the following text). Lastly, a diffusion model, with images evolving from the low jamming state to the high jamming state during the forward process and allowing the reverse process to effectively reconstruct clean images from heavily corrupted inputs, is trained to learn the transition between states. This yields a network capable of progressively suppressing jamming and recovering the clean images. Experiments on simulated SAR images with multiple active suppression jamming types and practical Sentinel-1 datasets demonstrate that the proposed method adapts well to diverse jamming types and intensity levels, exhibiting notable effectiveness, robustness, and practical applicability. The training strategy eliminates the need for prior knowledge of suppression jamming patterns and the availability of real paired SAR images, making it especially suitable for complex real-world scenarios where jamming characteristics are difficult to characterize.
Published: 2025-12-30T18:38:21+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xunhao Lin; Dawei Ren; Ping Lang; Huizhang Yang; Junjun Yin; Jian Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649264"&gt;10.1109/jstars.2025.3649264&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) imaging is susceptible to various types of jamming, which can severely degrade image quality and hinder downstream tasks. To address this issue, this paper proposes a jamming suppression method through a stochastic differential equation (SDE) based diffusion model trained on pseudo-paired SAR images. Firstly, candidate jamming regions are identified in suppression jamming SAR images through energy concentration and low-rank characteristics. Then, pseudo-paired SAR images representing low and high jamming states are constructed by combining these candidate regions with the original SAR images (referred to as clean images in the following text). Lastly, a diffusion model, with images evolving from the low jamming state to the high jamming state during the forward process and allowing the reverse process to effectively reconstruct clean images from heavily corrupted inputs, is trained to learn the transition between states. This yields a network capable of progressively suppressing jamming and recovering the clean images. Experiments on simulated SAR images with multiple active suppression jamming types and practical Sentinel-1 datasets demonstrate that the proposed method adapts well to diverse jamming types and intensity levels, exhibiting notable effectiveness, robustness, and practical applicability. The training strategy eliminates the need for prior knowledge of suppression jamming patterns and the availability of real paired SAR images, making it especially suitable for complex real-world scenarios where jamming characteristics are difficult to characterize.&lt;/p&gt;</content:encoded></item><item><title>Text Augmentation for Vision: Modality-Preference Aware Few-Shot Learning</title><link>https://doi.org/10.1016/j.knosys.2025.115122</link><guid>10.1016/j.knosys.2025.115122</guid><pubDate>Wed, 31 Dec 2025 23:34:11 +0000</pubDate><dc:creator>Zehua Hao</dc:creator><dc:creator>Fang Liu</dc:creator><dc:creator>Shuo Li</dc:creator><dc:creator>Yaoyang Du</dc:creator><dc:creator>Jiahao Wang</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Xinyan Huang</dc:creator><dc:creator>Licheng Jiao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115122</prism:doi><description>Recent advances in vision-language models such as CLIP show great potential for few-shot learning, but their performance declines under extreme low-shot scenarios due to limited supervision and the suboptimality of enforcing a unified optimization objective across heterogeneous modalities. To address this, we propose a modality-preference aware framework with textual augmentation to enhance vision-centric few-shot image classification. By treating text descriptions as auxiliary training samples, our method enables effective and scalable augmentation without generating synthetic images. We introduce a training strategy called Alternating-Modality Supervision (AMS), where vision and text samples alternately supervise a shared classifier to mitigate gradient conflicts. Crucially, we identify a Modality-Preference Phenomenon grounded in distinct feature geometries, where high-dimensional visual features favor cross-entropy (CE) for discrimination, while semantic textual features prefer mean squared error (MSE) for manifold alignment. Based on this, we propose Modality-Preference Loss Assignment (MPLA), which aligns each modality with its preferred objective and improves optimization stability. Extensive experiments on diverse datasets and backbone architectures confirm that combining MPLA with AMS improves few-shot performance and demonstrates strong generalizability.
Published: 2025-12-31T23:34:11+00:00
Venue: Knowledge-Based Systems
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zehua Hao; Fang Liu; Shuo Li; Yaoyang Du; Jiahao Wang; Hao Wang; Xinyan Huang; Licheng Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115122"&gt;10.1016/j.knosys.2025.115122&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in vision-language models such as CLIP show great potential for few-shot learning, but their performance declines under extreme low-shot scenarios due to limited supervision and the suboptimality of enforcing a unified optimization objective across heterogeneous modalities. To address this, we propose a modality-preference aware framework with textual augmentation to enhance vision-centric few-shot image classification. By treating text descriptions as auxiliary training samples, our method enables effective and scalable augmentation without generating synthetic images. We introduce a training strategy called Alternating-Modality Supervision (AMS), where vision and text samples alternately supervise a shared classifier to mitigate gradient conflicts. Crucially, we identify a Modality-Preference Phenomenon grounded in distinct feature geometries, where high-dimensional visual features favor cross-entropy (CE) for discrimination, while semantic textual features prefer mean squared error (MSE) for manifold alignment. Based on this, we propose Modality-Preference Loss Assignment (MPLA), which aligns each modality with its preferred objective and improves optimization stability. Extensive experiments on diverse datasets and backbone architectures confirm that combining MPLA with AMS improves few-shot performance and demonstrates strong generalizability.&lt;/p&gt;</content:encoded></item><item><title>Author Correction: Scalable and robust DNA-based storage via coding theory and deep learning</title><link>https://doi.org/10.1038/s42256-025-01175-8</link><guid>10.1038/s42256-025-01175-8</guid><pubDate>Tue, 30 Dec 2025 07:18:03 +0000</pubDate><dc:creator>Daniella Bar-Lev</dc:creator><dc:creator>Itai Orr</dc:creator><dc:creator>Omer Sabary</dc:creator><dc:creator>Tuvi Etzion</dc:creator><dc:creator>Eitan Yaakobi</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01175-8</prism:doi><description>Correction to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01003-z , published online 21 February 2025.
Published: 2025-12-30T07:18:03+00:00
Venue: Nature Machine Intelligence
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daniella Bar-Lev; Itai Orr; Omer Sabary; Tuvi Etzion; Eitan Yaakobi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01175-8"&gt;10.1038/s42256-025-01175-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Correction to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01003-z , published online 21 February 2025.&lt;/p&gt;</content:encoded></item><item><title>Multi-source Heterogeneous Domain Adaptation with Dual-adversarial Feature Alignment</title><link>https://doi.org/10.1016/j.eswa.2025.131060</link><guid>10.1016/j.eswa.2025.131060</guid><pubDate>Wed, 31 Dec 2025 08:06:01 +0000</pubDate><dc:creator>Yun Zhang</dc:creator><dc:creator>Lei Song</dc:creator><dc:creator>Haitian Sun</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131060</prism:doi><description>Heterogeneous domain adaptation (HeDA) aims to leverage knowledge from source data to learn a robust classifier for a heterogeneous target domain, where only a few labeled target samples are available. Real-world applications frequently involve scenarios where the source data are drawn from multiple heterogeneous source domains, termed multi-source heterogeneous domain adaptation (MHeDA). Many existing studies on MHeDA focus on minimizing the distribution divergence between each pair of source and target domains to extract domain-invariant feature representations. However, the discrepancy between labeled and unlabeled target data caused by selection bias has been overlooked, leading to unsatisfactory transfer performance. Furthermore, the discriminability of target representations is not fully strengthened, which limits the generalization ability of the trained model. In this paper, we propose a dual-adversarial feature alignment (DAFA) framework for MHeDA that performs both domain-level and category-level adversarial learning to address these challenges. Specifically, DAFA aligns the domain-level distributions of target and multiple source domains through adversarial learning. The category-level distribution adaptation is achieved through alternately minimizing and maximizing the prediction uncertainty of target domain. Compared with previous works, DAFA not only minimizes the distribution divergence between the target and multiple source domains but also reduces intra-domain discrepancy within the target domain. Experiments on various text-to-text, image-to-image, and image-to-text heterogeneous transfer tasks demonstrate that the proposed DAFA significantly outperforms state-of-the-art MHeDA methods.
Published: 2025-12-31T08:06:01+00:00
Venue: Expert Systems with Applications
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yun Zhang; Lei Song; Haitian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131060"&gt;10.1016/j.eswa.2025.131060&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Heterogeneous domain adaptation (HeDA) aims to leverage knowledge from source data to learn a robust classifier for a heterogeneous target domain, where only a few labeled target samples are available. Real-world applications frequently involve scenarios where the source data are drawn from multiple heterogeneous source domains, termed multi-source heterogeneous domain adaptation (MHeDA). Many existing studies on MHeDA focus on minimizing the distribution divergence between each pair of source and target domains to extract domain-invariant feature representations. However, the discrepancy between labeled and unlabeled target data caused by selection bias has been overlooked, leading to unsatisfactory transfer performance. Furthermore, the discriminability of target representations is not fully strengthened, which limits the generalization ability of the trained model. In this paper, we propose a dual-adversarial feature alignment (DAFA) framework for MHeDA that performs both domain-level and category-level adversarial learning to address these challenges. Specifically, DAFA aligns the domain-level distributions of target and multiple source domains through adversarial learning. The category-level distribution adaptation is achieved through alternately minimizing and maximizing the prediction uncertainty of target domain. Compared with previous works, DAFA not only minimizes the distribution divergence between the target and multiple source domains but also reduces intra-domain discrepancy within the target domain. Experiments on various text-to-text, image-to-image, and image-to-text heterogeneous transfer tasks demonstrate that the proposed DAFA significantly outperforms state-of-the-art MHeDA methods.&lt;/p&gt;</content:encoded></item><item><title>Bridging the Sim-to-Real Gap in RF Localization with Large-Scale Synthetic Pretraining</title><link>https://doi.org/10.1016/j.inffus.2025.104104</link><guid>10.1016/j.inffus.2025.104104</guid><pubDate>Wed, 31 Dec 2025 00:19:29 +0000</pubDate><dc:creator>Armen Manukyan</dc:creator><dc:creator>Rafayel Mkrtchyan</dc:creator><dc:creator>Ararat Saribekyan</dc:creator><dc:creator>Theofanis P. Raptis</dc:creator><dc:creator>Hrant Khachatrian</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104104</prism:doi><description>Radio frequency (RF) fingerprinting is a promising localization technique for GPS-denied environments, yet it tends to suffer from a fundamental limitation: Poor generalization to previously unmapped areas. Traditional methods such as k -nearest neighbors ( k -NN) perform well where data is available but may fail on unseen streets, limiting real-world deployment. Deep learning (DL) offers potential remedies by learning spatial-RF patterns that generalize, but requires far more training data than what simple real-world measurement campaigns can provide. In this paper, we investigate whether synthetic data can bridge this generalization gap. Using (i) a real-world dataset from Rome and (ii) NVIDIA’s open-source ray-tracing simulator Sionna, we generate synthetic datasets under varying realism and scale conditions. Specifically, we use Dataset A containing real-world measurements with real base stations (BS) and real signals, and create Dataset B using real BS locations but simulated signals, Dataset C with both simulated BS locations and signals, and Dataset B’ which represents an optimized version of Dataset B where BS parameters are calibrated via Gaussian Process to maximize signal correlation with Dataset A. Our evaluation reveals a pronounced sim-to-real gap: Models achieving 25m error on synthetic data degrade to 184m on real data. Nonetheless, pretraining on synthetic data reduces real-world localization error from 323m to 162m; a 50% improvement over real-only training. Notably, simulation fidelity proves more important than scale: A smaller calibrated dataset (53K samples) outperforms a larger uncalibrated one (274K samples). To further evaluate the generalization capabilities of the models, we conduct experiments on an unseen geographical region using a real-world dataset from Oslo. In the zero-shot setting, the models achieve a root mean square error (RMSE) of 132.2m on the entire dataset, and 61.5m on unseen streets after fine-tuning on Oslo data. While challenges remain before meeting more practical localization accuracy, this work provides a systematic study in the field of wireless communication of synthetic-to-real transfer in RF localization and highlights the value of simulation-aware pretraining for generalizing DL models to real-world scenarios.
Published: 2025-12-31T00:19:29+00:00
Venue: Information Fusion
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Armen Manukyan; Rafayel Mkrtchyan; Ararat Saribekyan; Theofanis P. Raptis; Hrant Khachatrian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104104"&gt;10.1016/j.inffus.2025.104104&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Radio frequency (RF) fingerprinting is a promising localization technique for GPS-denied environments, yet it tends to suffer from a fundamental limitation: Poor generalization to previously unmapped areas. Traditional methods such as k -nearest neighbors ( k -NN) perform well where data is available but may fail on unseen streets, limiting real-world deployment. Deep learning (DL) offers potential remedies by learning spatial-RF patterns that generalize, but requires far more training data than what simple real-world measurement campaigns can provide. In this paper, we investigate whether synthetic data can bridge this generalization gap. Using (i) a real-world dataset from Rome and (ii) NVIDIA’s open-source ray-tracing simulator Sionna, we generate synthetic datasets under varying realism and scale conditions. Specifically, we use Dataset A containing real-world measurements with real base stations (BS) and real signals, and create Dataset B using real BS locations but simulated signals, Dataset C with both simulated BS locations and signals, and Dataset B’ which represents an optimized version of Dataset B where BS parameters are calibrated via Gaussian Process to maximize signal correlation with Dataset A. Our evaluation reveals a pronounced sim-to-real gap: Models achieving 25m error on synthetic data degrade to 184m on real data. Nonetheless, pretraining on synthetic data reduces real-world localization error from 323m to 162m; a 50% improvement over real-only training. Notably, simulation fidelity proves more important than scale: A smaller calibrated dataset (53K samples) outperforms a larger uncalibrated one (274K samples). To further evaluate the generalization capabilities of the models, we conduct experiments on an unseen geographical region using a real-world dataset from Oslo. In the zero-shot setting, the models achieve a root mean square error (RMSE) of 132.2m on the entire dataset, and 61.5m on unseen streets after fine-tuning on Oslo data. While challenges remain before meeting more practical localization accuracy, this work provides a systematic study in the field of wireless communication of synthetic-to-real transfer in RF localization and highlights the value of simulation-aware pretraining for generalizing DL models to real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>Infrared and Visible Image Fusion via Iterative Feature Decomposition and Deep Balanced Fusion</title><link>https://doi.org/10.1016/j.patcog.2025.113022</link><guid>10.1016/j.patcog.2025.113022</guid><pubDate>Wed, 31 Dec 2025 07:55:41 +0000</pubDate><dc:creator>Wei Li</dc:creator><dc:creator>Baojia Li</dc:creator><dc:creator>Haiyu Song</dc:creator><dc:creator>Pengjie Wang</dc:creator><dc:creator>Zeyu Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113022</prism:doi><description>Infrared and visible image fusion (IVF) combines thermal cues from infrared images with structural details from visible images, improving perception in complex conditions. Current fusion methods typically follow a decomposition-fusion-reconstruction paradigm, but face two major limitations. First, one-shot decomposition strategy inherently assumes equal complexity and importance of information across modalities, ignoring structural differences. Second, redundant or conflicting information is typically unhandled in high-frequency fusion, resulting in structural degradation and detail loss. To address these issues, we propose a novel IVF framework based on the Adaptive Iterative Feature Decomposition module (AIFD) and the Deep-Balanced High-Frequency Fusion module (DBHF). AIFD dynamically adjusts the number of decomposition iterations for infrared and visible images independently, guided by global semantic similarity and channel-wise cosine similarity. DBHF recursively integrates high-frequency features and uses a discriminative network as a learnable convergence criterion, effectively suppressing redundant or conflicting information. Extensive experiments on public benchmarks demonstrate that our method achieves state-of-the-art (SOTA) performance, with improvements of up to 12.3% in SF, 8.9% in AG on RoadScene, and 1.7% in VIF on M 3 FD.
Published: 2025-12-31T07:55:41+00:00
Venue: Pattern Recognition
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Li; Baojia Li; Haiyu Song; Pengjie Wang; Zeyu Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113022"&gt;10.1016/j.patcog.2025.113022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared and visible image fusion (IVF) combines thermal cues from infrared images with structural details from visible images, improving perception in complex conditions. Current fusion methods typically follow a decomposition-fusion-reconstruction paradigm, but face two major limitations. First, one-shot decomposition strategy inherently assumes equal complexity and importance of information across modalities, ignoring structural differences. Second, redundant or conflicting information is typically unhandled in high-frequency fusion, resulting in structural degradation and detail loss. To address these issues, we propose a novel IVF framework based on the Adaptive Iterative Feature Decomposition module (AIFD) and the Deep-Balanced High-Frequency Fusion module (DBHF). AIFD dynamically adjusts the number of decomposition iterations for infrared and visible images independently, guided by global semantic similarity and channel-wise cosine similarity. DBHF recursively integrates high-frequency features and uses a discriminative network as a learnable convergence criterion, effectively suppressing redundant or conflicting information. Extensive experiments on public benchmarks demonstrate that our method achieves state-of-the-art (SOTA) performance, with improvements of up to 12.3% in SF, 8.9% in AG on RoadScene, and 1.7% in VIF on M 3 FD.&lt;/p&gt;</content:encoded></item><item><title>GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</title><link>https://arxiv.org/abs/2512.23176v1</link><guid>http://arxiv.org/abs/2512.23176v1</guid><pubDate>Mon, 29 Dec 2025 03:34:39 +0000</pubDate><dc:creator>Yi Zhang</dc:creator><dc:creator>Yi Wang</dc:creator><dc:creator>Lei Yao</dc:creator><dc:creator>Lap-Pui Chau</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).
Published: 2025-12-29T03:34:39+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Zhang; Yi Wang; Lei Yao; Lap-Pui Chau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).&lt;/p&gt;</content:encoded></item><item><title>SEPEN-YOLO: Detection of Low-altitude Small Objects Based on Shallow Features</title><link>https://doi.org/10.1109/taes.2025.3649605</link><guid>10.1109/taes.2025.3649605</guid><pubDate>Wed, 31 Dec 2025 18:46:30 +0000</pubDate><dc:creator>Yulai Li</dc:creator><dc:creator>Weibin Shi</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3649605</prism:doi><description>With the rapid proliferation of Unmanned Aerial Vehicle (UAV) applications, the ensuing airspace security surveillance demands reveal that existing detection methods face dual challenges in both accuracy and efficiency for real-time recognition of low-altitude small objects. This study proposes an enhanced SFPEH-YOLO algorithm based on YOLOv11, which designs a Hybrid Re-parameterization Block (HRB) to replace the C3k2 module for strengthened feature representation, employs Haar wavelet-based downsampling (HWD) to achieve efficient spatial-frequency domain conversion while preserving critical features, and reconstructs the neck network using scale sequence feature fusion (SSFF). Evaluated on a self-built low-altitude small object dataset, the model achieves a 20.5% reduction in parameters while improving mAP@0.5 by 10.3% and mAP@0.5:0.95 by 11.3%. Validation on the VisDrone2019 test set demonstrates a significant 14.2% enhancement in mAP@0.5 for small object detection. Experimental results confirm the effectiveness of the proposed method in computational efficiency and detection accuracy.
Published: 2025-12-31T18:46:30+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yulai Li; Weibin Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3649605"&gt;10.1109/taes.2025.3649605&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid proliferation of Unmanned Aerial Vehicle (UAV) applications, the ensuing airspace security surveillance demands reveal that existing detection methods face dual challenges in both accuracy and efficiency for real-time recognition of low-altitude small objects. This study proposes an enhanced SFPEH-YOLO algorithm based on YOLOv11, which designs a Hybrid Re-parameterization Block (HRB) to replace the C3k2 module for strengthened feature representation, employs Haar wavelet-based downsampling (HWD) to achieve efficient spatial-frequency domain conversion while preserving critical features, and reconstructs the neck network using scale sequence feature fusion (SSFF). Evaluated on a self-built low-altitude small object dataset, the model achieves a 20.5% reduction in parameters while improving mAP@0.5 by 10.3% and mAP@0.5:0.95 by 11.3%. Validation on the VisDrone2019 test set demonstrates a significant 14.2% enhancement in mAP@0.5 for small object detection. Experimental results confirm the effectiveness of the proposed method in computational efficiency and detection accuracy.&lt;/p&gt;</content:encoded></item></channel></rss>