<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 13 Dec 2025 16:31:46 +0000</lastBuildDate><item><title>RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for Universal Remote Sensing Image Interpretation</title><link>https://doi.org/10.1109/tpami.2025.3643453</link><guid>10.1109/tpami.2025.3643453</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Hanbo Bi</dc:creator><dc:creator>Yingchao Feng</dc:creator><dc:creator>Boyuan Tong</dc:creator><dc:creator>Mengyu Wang</dc:creator><dc:creator>Haichen Yu</dc:creator><dc:creator>Yongqiang Mao</dc:creator><dc:creator>Hao Chang</dc:creator><dc:creator>Wenhui Diao</dc:creator><dc:creator>Peijin Wang</dc:creator><dc:creator>Yue Yu</dc:creator><dc:creator>Hanyang Peng</dc:creator><dc:creator>Yehong Zhang</dc:creator><dc:creator>Kun Fu</dc:creator><dc:creator>Xian Sun</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643453</prism:doi><description>The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.851 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanbo Bi; Yingchao Feng; Boyuan Tong; Mengyu Wang; Haichen Yu; Yongqiang Mao; Hao Chang; Wenhui Diao; Peijin Wang; Yue Yu; Hanyang Peng; Yehong Zhang; Kun Fu; Xian Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643453"&gt;10.1109/tpami.2025.3643453&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.851 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.&lt;/p&gt;</content:encoded></item><item><title>A real-time surface defect detection model based on adaptive feature information selection and fusion</title><link>https://doi.org/10.1016/j.inffus.2025.104041</link><guid>10.1016/j.inffus.2025.104041</guid><pubDate>Fri, 12 Dec 2025 00:32:09 +0000</pubDate><dc:creator>Li-Juan Liu</dc:creator><dc:creator>Shao-Qi Sun</dc:creator><dc:creator>Hamid Reza Karimi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104041</prism:doi><description>In contemporary computer vision, You Only Look Once (YOLO) has become a benchmark for object detection, widely used in domains from intelligent manufacturing—such as industrial quality control and automated inspection—to real-time video surveillance. For example, detecting surface defects on steel products or electronic components in production lines relies on such algorithms to maintain high quality and safety. Despite YOLO’s excellent speed and accuracy in many tasks, it still faces difficulties in certain challenging conditions, notably high dynamic range scenes, complex backgrounds, and the detection of small or subtle objects. These conditions are common in practice—for instance, on shiny metal surfaces with uneven lighting or in busy surveillance scenes—where conventional YOLO models struggle to capture fine details reliably. To overcome these limitations, we propose an improved YOLO-based framework featuring a novel Dynamic Cross-Scale Feature Fusion Module (Dy-CCFM) and a Dual-path Downsampling Convolution Module (DDConv). These modules enhance multi-scale feature representation and preserve detail under extreme lighting and background clutter, which is crucial for monitoring in complex environments. Additionally, we employ the Minimum Point Distance Intersection over Union (MPDIoU) as an optimized loss function for bounding box regression, significantly improving the localization of small objects. Thanks to these innovations, the model achieves a mean Average Precision (mAP) of 75.1% on the challenging Northeastern University surface defect (NEU-DET) dataset, while the smallest variant is only 1.6M in size. Compared to YOLOv8, our approach improves mAP by 2.1% while also delivering higher inference speed (FPS), and it surpasses the Detection Transformer (DETR) by 5.0% mAP. The model further demonstrates excellent generalization on the Google Cloud 10 Defect Detection (GC10-DET) dataset. This enhanced detection algorithm not only improves performance but also offers significant practical value in intelligent manufacturing and automated inspection systems, intelligent video surveillance, and autonomous vehicles, where reliable real-time detection of small defects or targets is critical.
Published: 2025-12-12T00:32:09+00:00
Venue: Information Fusion
Score: 0.845 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li-Juan Liu; Shao-Qi Sun; Hamid Reza Karimi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104041"&gt;10.1016/j.inffus.2025.104041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.845 (must_read)&lt;/p&gt;
&lt;p&gt;In contemporary computer vision, You Only Look Once (YOLO) has become a benchmark for object detection, widely used in domains from intelligent manufacturing—such as industrial quality control and automated inspection—to real-time video surveillance. For example, detecting surface defects on steel products or electronic components in production lines relies on such algorithms to maintain high quality and safety. Despite YOLO’s excellent speed and accuracy in many tasks, it still faces difficulties in certain challenging conditions, notably high dynamic range scenes, complex backgrounds, and the detection of small or subtle objects. These conditions are common in practice—for instance, on shiny metal surfaces with uneven lighting or in busy surveillance scenes—where conventional YOLO models struggle to capture fine details reliably. To overcome these limitations, we propose an improved YOLO-based framework featuring a novel Dynamic Cross-Scale Feature Fusion Module (Dy-CCFM) and a Dual-path Downsampling Convolution Module (DDConv). These modules enhance multi-scale feature representation and preserve detail under extreme lighting and background clutter, which is crucial for monitoring in complex environments. Additionally, we employ the Minimum Point Distance Intersection over Union (MPDIoU) as an optimized loss function for bounding box regression, significantly improving the localization of small objects. Thanks to these innovations, the model achieves a mean Average Precision (mAP) of 75.1% on the challenging Northeastern University surface defect (NEU-DET) dataset, while the smallest variant is only 1.6M in size. Compared to YOLOv8, our approach improves mAP by 2.1% while also delivering higher inference speed (FPS), and it surpasses the Detection Transformer (DETR) by 5.0% mAP. The model further demonstrates excellent generalization on the Google Cloud 10 Defect Detection (GC10-DET) dataset. This enhanced detection algorithm not only improves performance but also offers significant practical value in intelligent manufacturing and automated inspection systems, intelligent video surveillance, and autonomous vehicles, where reliable real-time detection of small defects or targets is critical.&lt;/p&gt;</content:encoded></item><item><title>Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach</title><link>https://doi.org/10.1109/tpami.2025.3642842</link><guid>10.1109/tpami.2025.3642842</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Jiayang Li</dc:creator><dc:creator>Chengjie Jiang</dc:creator><dc:creator>Junjun Jiang</dc:creator><dc:creator>Pengwei Liang</dc:creator><dc:creator>Jiayi Ma</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642842</prism:doi><description>Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, an instruction-driven Diffusion Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayang Li; Chengjie Jiang; Junjun Jiang; Pengwei Liang; Jiayi Ma; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642842"&gt;10.1109/tpami.2025.3642842&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, an instruction-driven Diffusion Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.&lt;/p&gt;</content:encoded></item><item><title>S2AFormer: Strip Self-Attention for Efficient Vision Transformer</title><link>https://doi.org/10.1109/tip.2025.3639919</link><guid>10.1109/tip.2025.3639919</guid><pubDate>Thu, 11 Dec 2025 18:48:12 +0000</pubDate><dc:creator>Guoan Xu</dc:creator><dc:creator>Wenfeng Huang</dc:creator><dc:creator>Wenjing Jia</dc:creator><dc:creator>Jiamao Li</dc:creator><dc:creator>Guangwei Gao</dc:creator><dc:creator>Guo-Jun Qi</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3639919</prism:doi><description>The Vision Transformer (ViT) has achieved remarkable success in computer vision due to its powerful token mixer, which effectively captures global dependencies among all tokens. However, the quadratic complexity of standard self-attention with respect to the number of tokens severely hampers its computational efficiency in practical deployment. Although recent hybrid approaches have sought to combine the strengths of convolutions and self-attention to improve the performance–efficiency trade-off, the costly pairwise token interactions and heavy matrix operations in conventional self-attention remain a critical bottleneck. To overcome this limitation, we introduce S2AFormer, an efficient Vision Transformer architecture built around a novel Strip Self-Attention (SSA) mechanism. Our design incorporates lightweight yet effective Hybrid Perception Blocks (HPBs) that seamlessly fuse the local inductive biases of CNNs with the global modeling capability of Transformer-style attention. The core innovation of SSA lies in simultaneously reducing the spatial resolution of the key (K) and value (V) tensors while compressing the channel dimension of the query (Q) and key (K) tensors. This joint spatial-and-channel compression dramatically lowers computational cost without sacrificing representational power, achieving an excellent balance between accuracy and efficiency. We extensively evaluate S2AFormer on a wide range of vision tasks, including image classification (ImageNet-1K), semantic segmentation (ADE20K), and object detection/instance segmentation (COCO). Experimental results consistently show that S2AFormer delivers substantial accuracy improvements together with superior inference speed and throughput across both GPU and non-GPU platforms, establishing it as a highly competitive solution in the landscape of efficient Vision Transformers.
Published: 2025-12-11T18:48:12+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guoan Xu; Wenfeng Huang; Wenjing Jia; Jiamao Li; Guangwei Gao; Guo-Jun Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3639919"&gt;10.1109/tip.2025.3639919&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;The Vision Transformer (ViT) has achieved remarkable success in computer vision due to its powerful token mixer, which effectively captures global dependencies among all tokens. However, the quadratic complexity of standard self-attention with respect to the number of tokens severely hampers its computational efficiency in practical deployment. Although recent hybrid approaches have sought to combine the strengths of convolutions and self-attention to improve the performance–efficiency trade-off, the costly pairwise token interactions and heavy matrix operations in conventional self-attention remain a critical bottleneck. To overcome this limitation, we introduce S2AFormer, an efficient Vision Transformer architecture built around a novel Strip Self-Attention (SSA) mechanism. Our design incorporates lightweight yet effective Hybrid Perception Blocks (HPBs) that seamlessly fuse the local inductive biases of CNNs with the global modeling capability of Transformer-style attention. The core innovation of SSA lies in simultaneously reducing the spatial resolution of the key (K) and value (V) tensors while compressing the channel dimension of the query (Q) and key (K) tensors. This joint spatial-and-channel compression dramatically lowers computational cost without sacrificing representational power, achieving an excellent balance between accuracy and efficiency. We extensively evaluate S2AFormer on a wide range of vision tasks, including image classification (ImageNet-1K), semantic segmentation (ADE20K), and object detection/instance segmentation (COCO). Experimental results consistently show that S2AFormer delivers substantial accuracy improvements together with superior inference speed and throughput across both GPU and non-GPU platforms, establishing it as a highly competitive solution in the landscape of efficient Vision Transformers.&lt;/p&gt;</content:encoded></item><item><title>Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation</title><link>https://doi.org/10.1109/tpami.2025.3642821</link><guid>10.1109/tpami.2025.3642821</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Jinxing Zhou</dc:creator><dc:creator>Zhihui Li</dc:creator><dc:creator>Yongqiang Yu</dc:creator><dc:creator>Yanghao Zhou</dc:creator><dc:creator>Ruohao Guo</dc:creator><dc:creator>Guangyao Li</dc:creator><dc:creator>Yuxin Mao</dc:creator><dc:creator>Mingfei Han</dc:creator><dc:creator>Xiaojun Chang</dc:creator><dc:creator>Meng Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642821</prism:doi><description>Mainstream research in audio-visual learning has focused on designing task-specific expert models, primarily implemented through sophisticated multimodal fusion approaches. Recently, a few efforts have aimed to develop more task-independent or universal audiovisual embedding networks, encoding advanced representations for use in various audiovisual downstream tasks. This is typically achieved by fine-tuning large pretrained transformers, such as Swin-V2-L and HTS-AT, in a parameter-efficient manner through techniques such as tuning only a few adapter layers inserted into the pretrained transformer backbone. Although these methods are parameter-efficient, they suffer from significant training memory consumption due to gradient backpropagation through the deep transformer backbones, which limits accessibility for researchers with constrained computational resources. In this paper, we present Meta-Token Learning (Mettle), a simple and memory-efficient method for adapting large-scale pretrained transformer models to downstream audio-visual tasks. Instead of sequentially modifying the output feature distribution of the transformer backbone, Mettle utilizes a lightweight Layer-Centric Distillation (LCD) module to distill in parallel the intact audio or visual features embedded by each transformer layer into compact meta-tokens. This distillation process considers both pretrained knowledge preservation and task-specific adaptation. The obtained meta-tokens can be directly applied to classification tasks, such as audio-visual event localization and audio-visual video parsing. To further support fine-grained segmentation tasks, such as audio-visual segmentation, we introduce a Meta-Token Injection (MTI) module, which utilizes the audio and visual meta-tokens distilled from the top transformer layer to guide feature adaptation in earlier layers. Extensive experiments on multiple audiovisual benchmarks demonstrate that our method significantly reduces memory usage and training...
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinxing Zhou; Zhihui Li; Yongqiang Yu; Yanghao Zhou; Ruohao Guo; Guangyao Li; Yuxin Mao; Mingfei Han; Xiaojun Chang; Meng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642821"&gt;10.1109/tpami.2025.3642821&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Mainstream research in audio-visual learning has focused on designing task-specific expert models, primarily implemented through sophisticated multimodal fusion approaches. Recently, a few efforts have aimed to develop more task-independent or universal audiovisual embedding networks, encoding advanced representations for use in various audiovisual downstream tasks. This is typically achieved by fine-tuning large pretrained transformers, such as Swin-V2-L and HTS-AT, in a parameter-efficient manner through techniques such as tuning only a few adapter layers inserted into the pretrained transformer backbone. Although these methods are parameter-efficient, they suffer from significant training memory consumption due to gradient backpropagation through the deep transformer backbones, which limits accessibility for researchers with constrained computational resources. In this paper, we present Meta-Token Learning (Mettle), a simple and memory-efficient method for adapting large-scale pretrained transformer models to downstream audio-visual tasks. Instead of sequentially modifying the output feature distribution of the transformer backbone, Mettle utilizes a lightweight Layer-Centric Distillation (LCD) module to distill in parallel the intact audio or visual features embedded by each transformer layer into compact meta-tokens. This distillation process considers both pretrained knowledge preservation and task-specific adaptation. The obtained meta-tokens can be directly applied to classification tasks, such as audio-visual event localization and audio-visual video parsing. To further support fine-grained segmentation tasks, such as audio-visual segmentation, we introduce a Meta-Token Injection (MTI) module, which utilizes the audio and visual meta-tokens distilled from the top transformer layer to guide feature adaptation in earlier layers. Extensive experiments on multiple audiovisual benchmarks demonstrate that our method significantly reduces memory usage and training...&lt;/p&gt;</content:encoded></item><item><title>Gradient-Guided Learning Network for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2512.09497v1</link><guid>http://arxiv.org/abs/2512.09497v1</guid><pubDate>Wed, 10 Dec 2025 10:21:08 +0000</pubDate><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Chuang Yu</dc:creator><dc:creator>Zelin Shi</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Yingdi Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/LGRS.2023.3308783</prism:doi><description>Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net
Published: 2025-12-10T10:21:08+00:00
Venue: arXiv
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinmiao Zhao; Chuang Yu; Zelin Shi; Yunpeng Liu; Yingdi Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/LGRS.2023.3308783"&gt;10.1109/LGRS.2023.3308783&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, infrared small target detection has attracted extensive attention. However, due to the small size and the lack of intrinsic features of infrared small targets, the existing methods generally have the problem of inaccurate edge positioning and the target is easily submerged by the background. Therefore, we propose an innovative gradient-guided learning network (GGL-Net). Specifically, we are the first to explore the introduction of gradient magnitude images into the deep learning-based infrared small target detection method, which is conducive to emphasizing the edge details and alleviating the problem of inaccurate edge positioning of small targets. On this basis, we propose a novel dual-branch feature extraction network that utilizes the proposed gradient supplementary module (GSM) to encode raw gradient information into deeper network layers and embeds attention mechanisms reasonably to enhance feature extraction ability. In addition, we construct a two-way guidance fusion module (TGFM), which fully considers the characteristics of feature maps at different levels. It can facilitate the effective fusion of multi-scale feature maps and extract richer semantic information and detailed information through reasonable two-way guidance. Extensive experiments prove that GGL-Net has achieves state-of-the-art results on the public real NUAA-SIRST dataset and the public synthetic NUDT-SIRST dataset. Our code has been integrated into https://github.com/YuChuang1205/MSDA-Net&lt;/p&gt;</content:encoded></item><item><title>Breaking Barriers, Localizing Saliency: A Large-scale Benchmark and Baseline for Condition-Constrained Salient Object Detection</title><link>https://doi.org/10.1109/tpami.2025.3642893</link><guid>10.1109/tpami.2025.3642893</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Runmin Cong</dc:creator><dc:creator>Zhiyang Chen</dc:creator><dc:creator>Hao Fang</dc:creator><dc:creator>Sam Kwong</dc:creator><dc:creator>Wei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642893</prism:doi><description>Salient Object Detection (SOD) aims to identify and segment the most prominent objects in an image. In real open environments, intelligent systems often encounter complex and challenging scenes, such as low-light, rain, snow, etc., which we call constrained conditions. These real situations pose more severe challenges to existing SOD models. However, there is no comprehensive and in-depth exploration of this field at both the data and model levels, and most of them focus on ideal situations or a single condition. To bridge this gap, we launch a new task, Condition-Constrained Salient Object Detection (CSOD), aimed at robustly and accurately locating salient objects in constrained environments. On the one hand, to compensate for the lack of datasets, we construct the first large-scale condition-constrained salient object detection dataset CSOD10K, comprising 10,000 pixel-level annotated images and over 100 categories of salient objects. This dataset is oriented towards the real environment and includes 8 real-world constrained scenes under 3 main constraint types, making it extremely challenging. On the other hand, we abandon the paradigm of “restoration before detection” and instead introduce a unified end-to-end framework CSSAM that fully explores scene attributes, eliminating the need for additional ground-truth restored images and reducing computational overhead. Specifically, we design a Scene Prior-Guided Adapter (SPGA), which injects scene priors to enable the foundation model to better adapt to downstream constrained scenes. To automatically decode salient objects, we propose a Hybrid Prompt Decoding Strategy (HPDS), which can effectively integrate multiple types of prompts to achieve adaptation to the SOD task. Extensive experiments show that our model significantly outperforms state-of-the-art methods on both the CSOD10K dataset and existing standard SOD benchmarks.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runmin Cong; Zhiyang Chen; Hao Fang; Sam Kwong; Wei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642893"&gt;10.1109/tpami.2025.3642893&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Salient Object Detection (SOD) aims to identify and segment the most prominent objects in an image. In real open environments, intelligent systems often encounter complex and challenging scenes, such as low-light, rain, snow, etc., which we call constrained conditions. These real situations pose more severe challenges to existing SOD models. However, there is no comprehensive and in-depth exploration of this field at both the data and model levels, and most of them focus on ideal situations or a single condition. To bridge this gap, we launch a new task, Condition-Constrained Salient Object Detection (CSOD), aimed at robustly and accurately locating salient objects in constrained environments. On the one hand, to compensate for the lack of datasets, we construct the first large-scale condition-constrained salient object detection dataset CSOD10K, comprising 10,000 pixel-level annotated images and over 100 categories of salient objects. This dataset is oriented towards the real environment and includes 8 real-world constrained scenes under 3 main constraint types, making it extremely challenging. On the other hand, we abandon the paradigm of “restoration before detection” and instead introduce a unified end-to-end framework CSSAM that fully explores scene attributes, eliminating the need for additional ground-truth restored images and reducing computational overhead. Specifically, we design a Scene Prior-Guided Adapter (SPGA), which injects scene priors to enable the foundation model to better adapt to downstream constrained scenes. To automatically decode salient objects, we propose a Hybrid Prompt Decoding Strategy (HPDS), which can effectively integrate multiple types of prompts to achieve adaptation to the SOD task. Extensive experiments show that our model significantly outperforms state-of-the-art methods on both the CSOD10K dataset and existing standard SOD benchmarks.&lt;/p&gt;</content:encoded></item><item><title>From Point to Flow: Enhancing Unsupervised Domain Adaptation with Flow Classification</title><link>https://doi.org/10.1109/tcsvt.2025.3642573</link><guid>10.1109/tcsvt.2025.3642573</guid><pubDate>Thu, 11 Dec 2025 18:47:52 +0000</pubDate><dc:creator>Lihua Zhou</dc:creator><dc:creator>Mao Ye</dc:creator><dc:creator>Nianxin Li</dc:creator><dc:creator>Song Tang</dc:creator><dc:creator>Xu-Qian Fan</dc:creator><dc:creator>Lei Deng</dc:creator><dc:creator>Zhen Lei</dc:creator><dc:creator>Xiatian Zhu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642573</prism:doi><description>Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Existing methods, whether based on distribution matching or self-supervised learning, often focus solely on classifying individual source samples, potentially overlooking discriminative information. To address this limitation, we propose FlowUDA, a novel plugin method that enhances existing UDA frameworks by constructing semantically invariant flows from individual source samples to corresponding target samples, forming cross-domain trajectories. By leveraging a diffusion network guided by ordinary differential equations, FlowUDA ensures these flows preserve the topological structure of the source domain, maintaining their distinguishability. Our method then classifies these flows by sampling points along them and transferring labels from source samples, effectively capturing spatial relationships between domains. In essence, FlowUDA transforms the traditional point-based classification on individual source samples into flow-based classification on flows, allowing the model to learn richer, more discriminative features that bridge the gap between source and target domains. Extensive experiments on standard benchmarks demonstrate that integrating FlowUDA into existing UDA methods leads to notable performance gains, highlighting its effectiveness in addressing domain shift challenges.
Published: 2025-12-11T18:47:52+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihua Zhou; Mao Ye; Nianxin Li; Song Tang; Xu-Qian Fan; Lei Deng; Zhen Lei; Xiatian Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642573"&gt;10.1109/tcsvt.2025.3642573&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Existing methods, whether based on distribution matching or self-supervised learning, often focus solely on classifying individual source samples, potentially overlooking discriminative information. To address this limitation, we propose FlowUDA, a novel plugin method that enhances existing UDA frameworks by constructing semantically invariant flows from individual source samples to corresponding target samples, forming cross-domain trajectories. By leveraging a diffusion network guided by ordinary differential equations, FlowUDA ensures these flows preserve the topological structure of the source domain, maintaining their distinguishability. Our method then classifies these flows by sampling points along them and transferring labels from source samples, effectively capturing spatial relationships between domains. In essence, FlowUDA transforms the traditional point-based classification on individual source samples into flow-based classification on flows, allowing the model to learn richer, more discriminative features that bridge the gap between source and target domains. Extensive experiments on standard benchmarks demonstrate that integrating FlowUDA into existing UDA methods leads to notable performance gains, highlighting its effectiveness in addressing domain shift challenges.&lt;/p&gt;</content:encoded></item><item><title>Generating Any Changes in the Noise Domain</title><link>https://doi.org/10.1109/tpami.2025.3643733</link><guid>10.1109/tpami.2025.3643733</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Qiang Liu</dc:creator><dc:creator>Yang Kuang</dc:creator><dc:creator>Jun Yue</dc:creator><dc:creator>Pedram Ghamisi</dc:creator><dc:creator>Weiying Xie</dc:creator><dc:creator>Leyuan Fang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643733</prism:doi><description>Change detection is essential in Earth observation, yet current models heavily rely on large-scale annotated datasets. Generative models offer a promising alternative by synthesizing training data, but generating temporally coherent image pairs with realistic, semantically meaningful changes remains a significant challenge. Existing approaches typically simulate changes by generating pre- and post-change label maps using either heuristic rules (e.g., copy-pasting) or text prompts. However, the former offers limited change diversity, while the latter often fails to maintain spatial consistency between image pairs. We observe that the noise space of diffusion models encodes strong generative capacity and spatial controllability: localized perturbations in the noise can yield meaningful, interpretable changes in corresponding image regions. Motivated by this, we propose Noise2Change, a framework for simulating change directly in the noise domain. The key idea is to manipulate the semantic composition of the initial noise sampled from the noise domain, such that the diffusion process generates structurally consistent pre- and post-change images reflecting realistic transformations. Since the unperturbed noise is shared between both images, the resulting pairs exhibit strong temporal alignment and semantic coherence, effectively addressing the trade-off between realism and consistency. Concretely, we employ a discrete diffusion model to extract high-level semantics from the initial noise. Guided by these semantics, we introduce a change simulation strategy that optimizes the noise to encode intended changes. The modified noise is then used to drive the diffusion process, yielding pre- and post-change label maps with natural structural transitions. These maps are passed through a unified framework for image generation and label refinement, producing highly aligned image-label pairs. Our framework supports diverse change types across a wide range of scenarios. Extensive ex...
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiang Liu; Yang Kuang; Jun Yue; Pedram Ghamisi; Weiying Xie; Leyuan Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643733"&gt;10.1109/tpami.2025.3643733&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Change detection is essential in Earth observation, yet current models heavily rely on large-scale annotated datasets. Generative models offer a promising alternative by synthesizing training data, but generating temporally coherent image pairs with realistic, semantically meaningful changes remains a significant challenge. Existing approaches typically simulate changes by generating pre- and post-change label maps using either heuristic rules (e.g., copy-pasting) or text prompts. However, the former offers limited change diversity, while the latter often fails to maintain spatial consistency between image pairs. We observe that the noise space of diffusion models encodes strong generative capacity and spatial controllability: localized perturbations in the noise can yield meaningful, interpretable changes in corresponding image regions. Motivated by this, we propose Noise2Change, a framework for simulating change directly in the noise domain. The key idea is to manipulate the semantic composition of the initial noise sampled from the noise domain, such that the diffusion process generates structurally consistent pre- and post-change images reflecting realistic transformations. Since the unperturbed noise is shared between both images, the resulting pairs exhibit strong temporal alignment and semantic coherence, effectively addressing the trade-off between realism and consistency. Concretely, we employ a discrete diffusion model to extract high-level semantics from the initial noise. Guided by these semantics, we introduce a change simulation strategy that optimizes the noise to encode intended changes. The modified noise is then used to drive the diffusion process, yielding pre- and post-change label maps with natural structural transitions. These maps are passed through a unified framework for image generation and label refinement, producing highly aligned image-label pairs. Our framework supports diverse change types across a wide range of scenarios. Extensive ex...&lt;/p&gt;</content:encoded></item><item><title>Unified Granularity Controller for Interactive Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3642834</link><guid>10.1109/tpami.2025.3642834</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Yian Zhao</dc:creator><dc:creator>Kehan Li</dc:creator><dc:creator>Pengchong Qiao</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Rongrong Ji</dc:creator><dc:creator>Jie Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642834</prism:doi><description>Interactive Segmentation (IS) segments specific objects or parts by deducing human intent from sparse input prompts. However, the sparse-to-dense mapping is ambiguous, making it challenging for users to obtain segmentations at the desired granularity and causing them to engage in trial-and-error cycles. Although existing multi-granularity IS models (e.g., SAM) alleviate the ambiguity of single-granularity methods by predicting multiple masks simultaneously, this approach has limited scalability and produces redundant results. To address this issue, we introduce a creative granularity-controllable IS paradigm that resolves ambiguity by enabling users to precisely control the segmentation granularity. Specifically, we propose a Unified Granularity Controller (UniGraCo) that supports multi-type optional granularity control signals to pursue unified control over diverse segmentation requirements, effectively overcoming the limitation of single-type control in adapting to different needs, thus boosting the system efficiency and practicality. To mitigate the excessive cost of annotating the multi-granularity masks and the corresponding granularity control signals for training UniGraCo, we construct an automated data engine capable of generating high-quality and granularity-abundant mask-granularity data pairs at low cost. To enable UniGraCo to learn unified granularity controllability in an efficient and stable manner, we further design a granularity-controllable learning strategy. This strategy leverages the generated data pairs to incrementally equip the pre-trained IS model with granularity controllability while preserving its segmentation capability. Extensive experiments on intricate scenarios at both instance and part level demonstrate that our UniGraCo has significant advantages over previous methods, highlighting its potential as a practical interactive tool. Code and model weights are available at https://github.com/Zhao-Yian/UniGraCo.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yian Zhao; Kehan Li; Pengchong Qiao; Chang Liu; Rongrong Ji; Jie Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642834"&gt;10.1109/tpami.2025.3642834&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Interactive Segmentation (IS) segments specific objects or parts by deducing human intent from sparse input prompts. However, the sparse-to-dense mapping is ambiguous, making it challenging for users to obtain segmentations at the desired granularity and causing them to engage in trial-and-error cycles. Although existing multi-granularity IS models (e.g., SAM) alleviate the ambiguity of single-granularity methods by predicting multiple masks simultaneously, this approach has limited scalability and produces redundant results. To address this issue, we introduce a creative granularity-controllable IS paradigm that resolves ambiguity by enabling users to precisely control the segmentation granularity. Specifically, we propose a Unified Granularity Controller (UniGraCo) that supports multi-type optional granularity control signals to pursue unified control over diverse segmentation requirements, effectively overcoming the limitation of single-type control in adapting to different needs, thus boosting the system efficiency and practicality. To mitigate the excessive cost of annotating the multi-granularity masks and the corresponding granularity control signals for training UniGraCo, we construct an automated data engine capable of generating high-quality and granularity-abundant mask-granularity data pairs at low cost. To enable UniGraCo to learn unified granularity controllability in an efficient and stable manner, we further design a granularity-controllable learning strategy. This strategy leverages the generated data pairs to incrementally equip the pre-trained IS model with granularity controllability while preserving its segmentation capability. Extensive experiments on intricate scenarios at both instance and part level demonstrate that our UniGraCo has significant advantages over previous methods, highlighting its potential as a practical interactive tool. Code and model weights are available at https://github.com/Zhao-Yian/UniGraCo.&lt;/p&gt;</content:encoded></item><item><title>Beyond Degradation Redundancy: Contrastive Prompt Learning for All-in-One Image Restoration</title><link>https://doi.org/10.1109/tpami.2025.3642852</link><guid>10.1109/tpami.2025.3642852</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Gang Wu</dc:creator><dc:creator>Junjun Jiang</dc:creator><dc:creator>Kui Jiang</dc:creator><dc:creator>Xianming Liu</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642852</prism:doi><description>All-in-one image restoration, addressing diverse degradation types with a unified model, presents significant challenges in designing task-aware prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but may discard critical visual information for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a novel framework that fundamentally enhances prompt-task alignment through two complementary innovations: a Sparse Prompt Module (SPM) that efficiently captures degradation-specific features while minimizing redundancy, and a Contrastive Prompt Regularization (CPR) that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL optimizes the critical interaction between prompts and the restoration model itself. Extensive experiments across five comprehensive benchmarks demonstrate that CPL consistently enhances state-of-the-art all-in-one restoration models, achieving significant improvements in both standard multi-task scenarios and challenging composite degradation settings. Our framework establishes new state-of-the-art performance while maintaining parameter efficiency, offering a principled solution for unified image restoration.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gang Wu; Junjun Jiang; Kui Jiang; Xianming Liu; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642852"&gt;10.1109/tpami.2025.3642852&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;All-in-one image restoration, addressing diverse degradation types with a unified model, presents significant challenges in designing task-aware prompts that effectively guide restoration across multiple degradation scenarios. While adaptive prompt learning enables end-to-end optimization, it often yields overlapping or redundant task representations. Conversely, explicit prompts derived from pretrained classifiers enhance discriminability but may discard critical visual information for reconstruction. To address these limitations, we introduce Contrastive Prompt Learning (CPL), a novel framework that fundamentally enhances prompt-task alignment through two complementary innovations: a Sparse Prompt Module (SPM) that efficiently captures degradation-specific features while minimizing redundancy, and a Contrastive Prompt Regularization (CPR) that explicitly strengthens task boundaries by incorporating negative prompt samples across different degradation types. Unlike previous approaches that focus primarily on degradation classification, CPL optimizes the critical interaction between prompts and the restoration model itself. Extensive experiments across five comprehensive benchmarks demonstrate that CPL consistently enhances state-of-the-art all-in-one restoration models, achieving significant improvements in both standard multi-task scenarios and challenging composite degradation settings. Our framework establishes new state-of-the-art performance while maintaining parameter efficiency, offering a principled solution for unified image restoration.&lt;/p&gt;</content:encoded></item><item><title>Cross-domain Class Context Optimization for Universal Domain Adaptation</title><link>https://doi.org/10.1109/tcsvt.2025.3642710</link><guid>10.1109/tcsvt.2025.3642710</guid><pubDate>Thu, 11 Dec 2025 18:47:52 +0000</pubDate><dc:creator>Bo Zhou</dc:creator><dc:creator>Long Liu</dc:creator><dc:creator>Chenyue Fan</dc:creator><dc:creator>Zhipeng Zhao</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642710</prism:doi><description>Universal Domain Adaptation (UniDA) aims to achieve cross-domain knowledge transfer without label set assumptions. UniDA primarily faces two challenges: domain alignment under label shift and identifying unknown class samples in the target domain. We propose a cross-domain class context optimization method for UniDA to address these two challenges, leveraging a contrastive language-image pre-training model containing learnable prompts. First, we develop a domain context-guided feature augmentation technique, which augments source domain features based on textual features related to the target domain style, improving the consistency of feature distributions between the source domain and target domain. Subsequently, we learn a set of class contexts suitable for the target domain using the augmented source domain features. Furthermore, to improve the ability of the class context to filter unknown class samples, we propose a local known-unknown entropy optimization strategy, which effectively reduces the interference from class-irrelevant semantic information in the images, thereby mitigating erroneous class matching under label shift. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves competitive results compared to advanced UniDA approaches. Additionally, experimental results show that our entropy optimization strategy can serve as a general optimization component for prompt tuning, enhancing the generalization performance of existing methods when applied to downstream tasks with label shift.
Published: 2025-12-11T18:47:52+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Zhou; Long Liu; Chenyue Fan; Zhipeng Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642710"&gt;10.1109/tcsvt.2025.3642710&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Universal Domain Adaptation (UniDA) aims to achieve cross-domain knowledge transfer without label set assumptions. UniDA primarily faces two challenges: domain alignment under label shift and identifying unknown class samples in the target domain. We propose a cross-domain class context optimization method for UniDA to address these two challenges, leveraging a contrastive language-image pre-training model containing learnable prompts. First, we develop a domain context-guided feature augmentation technique, which augments source domain features based on textual features related to the target domain style, improving the consistency of feature distributions between the source domain and target domain. Subsequently, we learn a set of class contexts suitable for the target domain using the augmented source domain features. Furthermore, to improve the ability of the class context to filter unknown class samples, we propose a local known-unknown entropy optimization strategy, which effectively reduces the interference from class-irrelevant semantic information in the images, thereby mitigating erroneous class matching under label shift. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves competitive results compared to advanced UniDA approaches. Additionally, experimental results show that our entropy optimization strategy can serve as a general optimization component for prompt tuning, enhancing the generalization performance of existing methods when applied to downstream tasks with label shift.&lt;/p&gt;</content:encoded></item><item><title>VPT-NSP
                    &lt;sup&gt;2&lt;/sup&gt;
                    ++: Importance-Aware Visual Prompt Tuning in Null Space for Continual Learning</title><link>https://doi.org/10.1109/tpami.2025.3642298</link><guid>10.1109/tpami.2025.3642298</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Yue Lu</dc:creator><dc:creator>De Cheng</dc:creator><dc:creator>Yinghui Xing</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Peng Wang</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642298</prism:doi><description>Continual learning (CL) enables AI models to adapt to evolving environments while mitigating catastrophic forgetting, which is a critical capability for dynamic real-world applications. With the growing popularity of pre-trained Vision Transformer (ViT) models and visual prompt tuning (VPT) technique in CL, this work explores a CL method on top of the ViT-based foundation model, through VPT mechanism with theoretical guarantees. Inspired by the orthogonal projection method, we aim to leverage this approach for VPT to enhance CL performance, particularly in long-term scenarios. However, since the orthogonal projection is originally designed for linear operations in CNNs, applying it to ViTs poses challenges induced by the non-linear self-attention mechanism and the distribution drift within LayerNorm. To address these issues, we deduced two orthogonality conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of maintaining stability. Considering the strict orthogonal constraints can diminish model capacity and reduce plasticity, we further propose an importance-aware orthogonal regularization framework. By applying varying degrees of orthogonal constraints to different parameters based on their importance to old and new tasks, the framework adaptively enhances model capacity and thereby promotes long-sequence CL while improving the stability-plasticity trade-off. To implement the proposed approach, a null-space-based approximation solution is employed to efficiently achieve the prompt gradient orthogonal projection. Extensive experiments on various class-incremental learning benchmarks demonstrate that our method achieves state-of-the-art performance across diverse CL scenarios.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shizhou Zhang; Yue Lu; De Cheng; Yinghui Xing; Nannan Wang; Peng Wang; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642298"&gt;10.1109/tpami.2025.3642298&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Continual learning (CL) enables AI models to adapt to evolving environments while mitigating catastrophic forgetting, which is a critical capability for dynamic real-world applications. With the growing popularity of pre-trained Vision Transformer (ViT) models and visual prompt tuning (VPT) technique in CL, this work explores a CL method on top of the ViT-based foundation model, through VPT mechanism with theoretical guarantees. Inspired by the orthogonal projection method, we aim to leverage this approach for VPT to enhance CL performance, particularly in long-term scenarios. However, since the orthogonal projection is originally designed for linear operations in CNNs, applying it to ViTs poses challenges induced by the non-linear self-attention mechanism and the distribution drift within LayerNorm. To address these issues, we deduced two orthogonality conditions to achieve the prompt gradient orthogonal projection, which provide a theoretical guarantee of maintaining stability. Considering the strict orthogonal constraints can diminish model capacity and reduce plasticity, we further propose an importance-aware orthogonal regularization framework. By applying varying degrees of orthogonal constraints to different parameters based on their importance to old and new tasks, the framework adaptively enhances model capacity and thereby promotes long-sequence CL while improving the stability-plasticity trade-off. To implement the proposed approach, a null-space-based approximation solution is employed to efficiently achieve the prompt gradient orthogonal projection. Extensive experiments on various class-incremental learning benchmarks demonstrate that our method achieves state-of-the-art performance across diverse CL scenarios.&lt;/p&gt;</content:encoded></item><item><title>SSP-SAM: SAM with Semantic-Spatial Prompt for Referring Expression Segmentation</title><link>https://doi.org/10.1109/tcsvt.2025.3643649</link><guid>10.1109/tcsvt.2025.3643649</guid><pubDate>Fri, 12 Dec 2025 18:37:50 +0000</pubDate><dc:creator>Wei Tang</dc:creator><dc:creator>Xuejing Liu</dc:creator><dc:creator>Yanpeng Sun</dc:creator><dc:creator>Zechao Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643649</prism:doi><description>The Segment Anything Model (SAM) excels at general image segmentation but has limited ability to understand natural language, which restricts its direct application in Referring Expression Segmentation (RES). Toward this end, we propose SSP-SAM, a framework that fully utilizes SAM’s segmentation capabilities by integrating a Semantic-Spatial Prompt (SSP) encoder. Specifically, we incorporate both visual and linguistic attention adapters into the SSP encoder, which highlight salient objects within the visual features and discriminative phrases within the linguistic features. This design enhances the referent representation for the prompt generator, resulting in high-quality SSPs that enable SAM to generate precise masks guided by language. Although not specifically designed for Generalized RES (GRES), where the referent may correspond to zero, one, or multiple objects, SSP-SAM naturally supports this more flexible setting without additional modifications. Extensive experiments on widely used RES and GRES benchmarks confirm the superiority of our method. Notably, our approach generates segmentation masks of high quality, achieving strong precision even at strict thresholds such as Pr@0.9. Further evaluation on the PhraseCut dataset demonstrates improved performance in open-vocabulary scenarios compared to existing state-of-the-art RES methods. The code and checkpoints are available at: https://github.com/WayneTomas/SSP-SAM.
Published: 2025-12-12T18:37:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Tang; Xuejing Liu; Yanpeng Sun; Zechao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643649"&gt;10.1109/tcsvt.2025.3643649&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model (SAM) excels at general image segmentation but has limited ability to understand natural language, which restricts its direct application in Referring Expression Segmentation (RES). Toward this end, we propose SSP-SAM, a framework that fully utilizes SAM’s segmentation capabilities by integrating a Semantic-Spatial Prompt (SSP) encoder. Specifically, we incorporate both visual and linguistic attention adapters into the SSP encoder, which highlight salient objects within the visual features and discriminative phrases within the linguistic features. This design enhances the referent representation for the prompt generator, resulting in high-quality SSPs that enable SAM to generate precise masks guided by language. Although not specifically designed for Generalized RES (GRES), where the referent may correspond to zero, one, or multiple objects, SSP-SAM naturally supports this more flexible setting without additional modifications. Extensive experiments on widely used RES and GRES benchmarks confirm the superiority of our method. Notably, our approach generates segmentation masks of high quality, achieving strong precision even at strict thresholds such as Pr@0.9. Further evaluation on the PhraseCut dataset demonstrates improved performance in open-vocabulary scenarios compared to existing state-of-the-art RES methods. The code and checkpoints are available at: https://github.com/WayneTomas/SSP-SAM.&lt;/p&gt;</content:encoded></item><item><title>Developing Evolving Adaptability in Biological Intelligence: A Novel Biologically-Inspired Continual Learning Model for Video Saliency Prediction</title><link>https://doi.org/10.1109/tpami.2025.3643517</link><guid>10.1109/tpami.2025.3643517</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Dandan Zhu</dc:creator><dc:creator>Kaiwei Zhang</dc:creator><dc:creator>Kun Zhu</dc:creator><dc:creator>Nana Zhang</dc:creator><dc:creator>Xiongkuo Min</dc:creator><dc:creator>Guangtao Zhai</dc:creator><dc:creator>Xiaokang Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643517</prism:doi><description>In the era of deep learning, video saliency prediction task still remains major challenge due to the issue of catastrophic forgetting during feature learning. Most prior works commonly employ generative replay strategies to generate pseudo-samples from previous tasks, enabling them to recall the data distribution. However, scaling up generative replay to accommodate class-incremental and task-incremental settings poses challenges, as generated data with low quality can severely deteriorate performance. Additionally, existing advances mainly focus on preserving memory stability to alleviate catastrophic forgetting, but they remain difficult to flexibly adapt to incremental changes in dynamic scenes. To achieve a better balance between memory stability and learning plasticity, we propose a novel biologically-inspired continual learning (BICL) model tailored to effectively predict human attention in dynamic scenes while mitigate catastrophic forgetting. In particular, inspired by the function of the hippocampus in the human neural system, we elaborately design a visual saliency memory bank module to explicitly store and retrieve representative features from previous tasks. Furthermore, drawing inspiration from the Drosophila γ \gamma MB system, we propose an active forgetting strategy equipped with multiple parallel adaptive learner modules, which can appropriately attenuate old memories in parameter distribution to enhance learning plasticity to adapt to new tasks, and accordingly to ensure compatibility among multiple learners. Notably, without compromising the performance of old tasks, our proposed model can achieve a better trade-off between memory stability and learning plasticity. Through extensive experiments on several benchmark datasets, our model not only enhances performance in task-incremental settings, but also potentially provides deep insights into neurological adaptive mechanisms.
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dandan Zhu; Kaiwei Zhang; Kun Zhu; Nana Zhang; Xiongkuo Min; Guangtao Zhai; Xiaokang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643517"&gt;10.1109/tpami.2025.3643517&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;In the era of deep learning, video saliency prediction task still remains major challenge due to the issue of catastrophic forgetting during feature learning. Most prior works commonly employ generative replay strategies to generate pseudo-samples from previous tasks, enabling them to recall the data distribution. However, scaling up generative replay to accommodate class-incremental and task-incremental settings poses challenges, as generated data with low quality can severely deteriorate performance. Additionally, existing advances mainly focus on preserving memory stability to alleviate catastrophic forgetting, but they remain difficult to flexibly adapt to incremental changes in dynamic scenes. To achieve a better balance between memory stability and learning plasticity, we propose a novel biologically-inspired continual learning (BICL) model tailored to effectively predict human attention in dynamic scenes while mitigate catastrophic forgetting. In particular, inspired by the function of the hippocampus in the human neural system, we elaborately design a visual saliency memory bank module to explicitly store and retrieve representative features from previous tasks. Furthermore, drawing inspiration from the Drosophila γ \gamma MB system, we propose an active forgetting strategy equipped with multiple parallel adaptive learner modules, which can appropriately attenuate old memories in parameter distribution to enhance learning plasticity to adapt to new tasks, and accordingly to ensure compatibility among multiple learners. Notably, without compromising the performance of old tasks, our proposed model can achieve a better trade-off between memory stability and learning plasticity. Through extensive experiments on several benchmark datasets, our model not only enhances performance in task-incremental settings, but also potentially provides deep insights into neurological adaptive mechanisms.&lt;/p&gt;</content:encoded></item><item><title>MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning</title><link>https://doi.org/10.1109/tcsvt.2025.3643469</link><guid>10.1109/tcsvt.2025.3643469</guid><pubDate>Fri, 12 Dec 2025 18:37:50 +0000</pubDate><dc:creator>Xiangyu Zhao</dc:creator><dc:creator>Xiangtai Li</dc:creator><dc:creator>Haodong Duan</dc:creator><dc:creator>Haian Huang</dc:creator><dc:creator>Yining Li</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Hua Yang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643469</prism:doi><description>Multi-modal large language models (MLLMs) have made significant strides in various visual understanding tasks. However, the majority of these models are constrained to process low-resolution images, which limits their effectiveness in perception tasks that necessitate detailed visual information. In our study, we present MG-LLaVA, an innovative MLLM that enhances the model’s visual processing capabilities by incorporating a multi-granularity vision flow, which includes low-resolution, high-resolution, and object-centric features. We propose the integration of an additional high-resolution visual encoder to capture fine-grained details, which are then fused with base visual features through a Conv-Gate fusion network. To further refine the model’s object recognition abilities, we incorporate object-level features derived from bounding boxes identified by offline detectors. Being trained solely on publicly available multimodal data through instruction tuning, MG-LLaVA demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide variety of language encoders, ranging from 3.8B to 34B, to evaluate the model’s performance comprehensively. Extensive evaluations across multiple benchmarks demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy.
Published: 2025-12-12T18:37:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyu Zhao; Xiangtai Li; Haodong Duan; Haian Huang; Yining Li; Kai Chen; Hua Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643469"&gt;10.1109/tcsvt.2025.3643469&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal large language models (MLLMs) have made significant strides in various visual understanding tasks. However, the majority of these models are constrained to process low-resolution images, which limits their effectiveness in perception tasks that necessitate detailed visual information. In our study, we present MG-LLaVA, an innovative MLLM that enhances the model’s visual processing capabilities by incorporating a multi-granularity vision flow, which includes low-resolution, high-resolution, and object-centric features. We propose the integration of an additional high-resolution visual encoder to capture fine-grained details, which are then fused with base visual features through a Conv-Gate fusion network. To further refine the model’s object recognition abilities, we incorporate object-level features derived from bounding boxes identified by offline detectors. Being trained solely on publicly available multimodal data through instruction tuning, MG-LLaVA demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide variety of language encoders, ranging from 3.8B to 34B, to evaluate the model’s performance comprehensively. Extensive evaluations across multiple benchmarks demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy.&lt;/p&gt;</content:encoded></item><item><title>Attention-driven feature enhancement network for object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132028</link><guid>10.1016/j.neucom.2025.132028</guid><pubDate>Fri, 12 Dec 2025 07:57:51 +0000</pubDate><dc:creator>Yang Li</dc:creator><dc:creator>Yongsheng Dong</dc:creator><dc:creator>Siming Jia</dc:creator><dc:creator>Zhifan Li</dc:creator><dc:creator>Lintao Zheng</dc:creator><dc:creator>Yaxin Li</dc:creator><dc:creator>Ruijuan Zheng</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132028</prism:doi><description>In recent years, object detection technology based on deep learning makes great progress. However, the existing deep learning-based object detection methods can not achieve satisfactory detection performance for small-size objects because they have defects in the processing of detailed information in the process of extracting features layer by layer. To alleviate this issue, in this paper we propose an Attention-driven Feature Enhancement Network (AFENet) for object detection. Particularly, we first propose a Multi-branch Feature preservation Enhancement Module (MFEM), which employs a multi-branch architecture and path design, allowing each layer within the module to learn feature extraction from the original features rich in detailed information. Furthermore, we propose a Joint Residual Attention Mechanism (JRAM). It focuses on the corresponding important weight information through an attention mechanism and utilizes residual connections are employed to retain the initial features and support the characteristics of deep learning, helping the model to perform better in deep learning and to capture the details of small targets more effectively. Experimental results on the PASCAL VOC2007+2012, Microsoft COCO2017, and VisDrone2019 datasets reveal that our proposed AFENet is effective and can achieve competitive detection performance when compared to several representative methods. The code is available at https://github.com/yang-Detection/AFENet .
Published: 2025-12-12T07:57:51+00:00
Venue: Neurocomputing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Li; Yongsheng Dong; Siming Jia; Zhifan Li; Lintao Zheng; Yaxin Li; Ruijuan Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132028"&gt;10.1016/j.neucom.2025.132028&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, object detection technology based on deep learning makes great progress. However, the existing deep learning-based object detection methods can not achieve satisfactory detection performance for small-size objects because they have defects in the processing of detailed information in the process of extracting features layer by layer. To alleviate this issue, in this paper we propose an Attention-driven Feature Enhancement Network (AFENet) for object detection. Particularly, we first propose a Multi-branch Feature preservation Enhancement Module (MFEM), which employs a multi-branch architecture and path design, allowing each layer within the module to learn feature extraction from the original features rich in detailed information. Furthermore, we propose a Joint Residual Attention Mechanism (JRAM). It focuses on the corresponding important weight information through an attention mechanism and utilizes residual connections are employed to retain the initial features and support the characteristics of deep learning, helping the model to perform better in deep learning and to capture the details of small targets more effectively. Experimental results on the PASCAL VOC2007+2012, Microsoft COCO2017, and VisDrone2019 datasets reveal that our proposed AFENet is effective and can achieve competitive detection performance when compared to several representative methods. The code is available at https://github.com/yang-Detection/AFENet .&lt;/p&gt;</content:encoded></item><item><title>DuaDiff: Dual-Conditional Diffusion Model for Guided Thermal Image Super-Resolution</title><link>https://doi.org/10.1109/tnnls.2025.3640168</link><guid>10.1109/tnnls.2025.3640168</guid><pubDate>Thu, 11 Dec 2025 18:46:28 +0000</pubDate><dc:creator>Linrui Shi</dc:creator><dc:creator>Gaochang Wu</dc:creator><dc:creator>Yingqian Wang</dc:creator><dc:creator>Yebin Liu</dc:creator><dc:creator>Tianyou Chai</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3640168</prism:doi><description>Thermal imaging offers valuable properties, but suffers from inherently low spatial resolution, which can be enhanced using a high-resolution (HR) visible image as guidance. However, the substantial modality differences between thermal and visible images, coupled with significant resolution gaps, pose challenges to existing guided super-resolution (SR) approaches. In this article, we present dual-conditional diffusion (DuaDiff), an innovative diffusion model featuring a dual-conditioning mechanism to enhance guided thermal image SR. Unlike typical conditional diffusion models, DuaDiff integrates a learnable Laplacian pyramid to extract high-frequency details from the visible image, serving as one of the conditioning inputs. By capturing multiscale high-frequency components, DuaDiff effectively focuses on intricate textures and edges in the HR visible images, significantly enhancing thermal image fidelity. Furthermore, we project both thermal and visible images into a semantic latent space, constructing another conditioning input. Leveraging these complementary conditions, DuaDiff employs a multimodal latent feature cross-attention module to facilitate effective interaction between noise, thermal, and visible latent representations. Extensive experiments on the FLIR-ADAS and CATS datasets for 4 × 4 imes and 8 × 8 imes guided SR demonstrate that combining learnable Laplacian conditioning with semantic latent conditioning enables DuaDiff to surpass state-of-the-art methods in both visual quality and metric evaluation, particularly in scenarios with a large resolution gap. Besides, the applications to downstream tasks further confirm the capability of DuaDiff to recover high-fidelity semantic information. The code will be released.
Published: 2025-12-11T18:46:28+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Linrui Shi; Gaochang Wu; Yingqian Wang; Yebin Liu; Tianyou Chai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3640168"&gt;10.1109/tnnls.2025.3640168&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Thermal imaging offers valuable properties, but suffers from inherently low spatial resolution, which can be enhanced using a high-resolution (HR) visible image as guidance. However, the substantial modality differences between thermal and visible images, coupled with significant resolution gaps, pose challenges to existing guided super-resolution (SR) approaches. In this article, we present dual-conditional diffusion (DuaDiff), an innovative diffusion model featuring a dual-conditioning mechanism to enhance guided thermal image SR. Unlike typical conditional diffusion models, DuaDiff integrates a learnable Laplacian pyramid to extract high-frequency details from the visible image, serving as one of the conditioning inputs. By capturing multiscale high-frequency components, DuaDiff effectively focuses on intricate textures and edges in the HR visible images, significantly enhancing thermal image fidelity. Furthermore, we project both thermal and visible images into a semantic latent space, constructing another conditioning input. Leveraging these complementary conditions, DuaDiff employs a multimodal latent feature cross-attention module to facilitate effective interaction between noise, thermal, and visible latent representations. Extensive experiments on the FLIR-ADAS and CATS datasets for 4 × 4 imes and 8 × 8 imes guided SR demonstrate that combining learnable Laplacian conditioning with semantic latent conditioning enables DuaDiff to surpass state-of-the-art methods in both visual quality and metric evaluation, particularly in scenarios with a large resolution gap. Besides, the applications to downstream tasks further confirm the capability of DuaDiff to recover high-fidelity semantic information. The code will be released.&lt;/p&gt;</content:encoded></item><item><title>A Dual-Driven Hybrid Tracking Architecture for Radar Targets Based on Innovation</title><link>https://doi.org/10.1016/j.inffus.2025.104056</link><guid>10.1016/j.inffus.2025.104056</guid><pubDate>Fri, 12 Dec 2025 17:34:27 +0000</pubDate><dc:creator>Yanwen Bai</dc:creator><dc:creator>Jibin Zheng</dc:creator><dc:creator>Hanxing Shao</dc:creator><dc:creator>Hongwei Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104056</prism:doi><description>Targets such as hypersonic missiles and stealth aircraft are characterized by complex motion patterns, strong maneuverability, and anomalous radar measurement statistics. Although model-driven radar target tracking methods offer physical interpretability, they suffer from their dependence on explicit prior assumptions. Data-driven methods can theoretically approximate arbitrarily complex motions through nonlinear mappings, but suffer from poor interpretability, vulnerability to noise during feature extraction, and loss of low-frequency maneuvering features due to sample imbalance. Therefore, this paper proposes a Dual-Driven Hybrid Tracking Architecture Based on Innovation (DDHTA), which fuses the advantages of both model-driven and data-driven approaches. First, a model-driven approach is adopted for basic state estimation, and a Dual Condition Judgment Adjustment (DCJA) method is proposed to adaptively adjust the measurement error variance, thereby providing a high-quality baseline estimate for the data-driven layer and reducing the interference of anomalous noise on feature extraction. Further, in the data-driven layer, a Dual-Scale Temporal Network (DSTNet) is designed. By learning the mapping from the innovation to the estimation errors, it combines the strengths of causal dilated convolution and multi-head self-attention to provide dynamic compensation, which corrects the estimation errors of the model-driven method. Numerical simulation results demonstrate that the proposed method enhances the algorithm’s ability to handle target maneuvers in complex environments, achieving higher tracking accuracy and robustness.
Published: 2025-12-12T17:34:27+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanwen Bai; Jibin Zheng; Hanxing Shao; Hongwei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104056"&gt;10.1016/j.inffus.2025.104056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Targets such as hypersonic missiles and stealth aircraft are characterized by complex motion patterns, strong maneuverability, and anomalous radar measurement statistics. Although model-driven radar target tracking methods offer physical interpretability, they suffer from their dependence on explicit prior assumptions. Data-driven methods can theoretically approximate arbitrarily complex motions through nonlinear mappings, but suffer from poor interpretability, vulnerability to noise during feature extraction, and loss of low-frequency maneuvering features due to sample imbalance. Therefore, this paper proposes a Dual-Driven Hybrid Tracking Architecture Based on Innovation (DDHTA), which fuses the advantages of both model-driven and data-driven approaches. First, a model-driven approach is adopted for basic state estimation, and a Dual Condition Judgment Adjustment (DCJA) method is proposed to adaptively adjust the measurement error variance, thereby providing a high-quality baseline estimate for the data-driven layer and reducing the interference of anomalous noise on feature extraction. Further, in the data-driven layer, a Dual-Scale Temporal Network (DSTNet) is designed. By learning the mapping from the innovation to the estimation errors, it combines the strengths of causal dilated convolution and multi-head self-attention to provide dynamic compensation, which corrects the estimation errors of the model-driven method. Numerical simulation results demonstrate that the proposed method enhances the algorithm’s ability to handle target maneuvers in complex environments, achieving higher tracking accuracy and robustness.&lt;/p&gt;</content:encoded></item><item><title>A global-local interaction and conditional consistency constrained diffusion model for SAR-guided optical image cloud removal</title><link>https://doi.org/10.1016/j.jag.2025.105013</link><guid>10.1016/j.jag.2025.105013</guid><pubDate>Thu, 11 Dec 2025 17:57:40 +0000</pubDate><dc:creator>Liwen Cao</dc:creator><dc:creator>Jun Pan</dc:creator><dc:creator>Jiangong Xu</dc:creator><dc:creator>Tao Chen</dc:creator><dc:creator>Qiangqiang Yuan</dc:creator><dc:creator>Jizhang Sang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105013</prism:doi><description>Cloud cover constitutes a formidable obstacle in the field of optical remote sensing image processing, substantially impeding the extraction and utilization of surface information. Synthetic Aperture Radar (SAR) imagery, serving as a complementary informational resource, is capable of furnishing crucial auxiliary data for optical images. In recent years, diffusion-based cloud removal methodologies have made significant progress. Nevertheless, their inherent generative diversity and randomness pose challenges in meeting the realism requirements for cloud removal in optical remote sensing imagery. To address this, this paper presents a SAR-guided optical imagery cloud removal method based on global–local interaction and conditional consistency-constrained diffusion models (GLCdiffcr). Specifically, the method integrates a multi-scale residual self-attention network in the denoising module. This network captures both global and local details of SAR imagery and the captured details provide precise guidance for cloud removal. Additionally, within the reverse diffusion framework, the method directly predicts cloud-free optical images and iterates over multiple steps, reducing errors caused by generative randomness and improving consistency. Meanwhile, in order to enhance the realism of the generated images, the method employs a novel multi-condition consistency-constrained loss function, which combines pixel-level errors with structural similarity measures. Through this loss function, the gap between the generated images and real-world land cover types is further minimized. Experimental results demonstrate that the proposed method outperforms current state-of-the-art methods in both quantitative metrics and visual quality, particularly in complex regions, with higher accuracy and reliability.
Published: 2025-12-11T17:57:40+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liwen Cao; Jun Pan; Jiangong Xu; Tao Chen; Qiangqiang Yuan; Jizhang Sang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105013"&gt;10.1016/j.jag.2025.105013&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Cloud cover constitutes a formidable obstacle in the field of optical remote sensing image processing, substantially impeding the extraction and utilization of surface information. Synthetic Aperture Radar (SAR) imagery, serving as a complementary informational resource, is capable of furnishing crucial auxiliary data for optical images. In recent years, diffusion-based cloud removal methodologies have made significant progress. Nevertheless, their inherent generative diversity and randomness pose challenges in meeting the realism requirements for cloud removal in optical remote sensing imagery. To address this, this paper presents a SAR-guided optical imagery cloud removal method based on global–local interaction and conditional consistency-constrained diffusion models (GLCdiffcr). Specifically, the method integrates a multi-scale residual self-attention network in the denoising module. This network captures both global and local details of SAR imagery and the captured details provide precise guidance for cloud removal. Additionally, within the reverse diffusion framework, the method directly predicts cloud-free optical images and iterates over multiple steps, reducing errors caused by generative randomness and improving consistency. Meanwhile, in order to enhance the realism of the generated images, the method employs a novel multi-condition consistency-constrained loss function, which combines pixel-level errors with structural similarity measures. Through this loss function, the gap between the generated images and real-world land cover types is further minimized. Experimental results demonstrate that the proposed method outperforms current state-of-the-art methods in both quantitative metrics and visual quality, particularly in complex regions, with higher accuracy and reliability.&lt;/p&gt;</content:encoded></item><item><title>Began+: Leveraging bi-temporal SAR-optical data fusion to reconstruct clear-sky satellite imagery under large cloud cover</title><link>https://doi.org/10.1016/j.rse.2025.115171</link><guid>10.1016/j.rse.2025.115171</guid><pubDate>Thu, 11 Dec 2025 17:30:56 +0000</pubDate><dc:creator>Yu Xia</dc:creator><dc:creator>Wei He</dc:creator><dc:creator>Liangpei Zhang</dc:creator><dc:creator>Hongyan Zhang</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115171</prism:doi><description>In recent years, optical remote sensing imagery has played an increasingly vital role in Earth observation, but cloud contamination exists as an inevitable degradation. Combining synthetic aperture radar (SAR) and optical data with machine learning offers a promising solution for reconstructing clear-sky satellite imagery. Nevertheless, several challenges persist, including insufficient attention to large cloud cover, difficulties in restoring temporal changes, and limited practicality of deep models. To address these issues, this paper introduces a novel deep learning-based cloud removal framework, termed Began+, which integrates bi-temporal SAR-optical data to deal with cloudy images with high cover ratios. The Began+ framework comprises two primary components: a deep network and a flexible post-processing step, combining the strengths of data-driven models for restoring change information and traditional gap-filling algorithms for mitigating radiance discrepancies. First, a bi-output enhanced generative adversarial network, abbreviated as Began, is designed for image synthesis, featuring an enhanced channel-wise fusion block (ECFB) and a multi-scale depth-wise convolution residual block (MDRB). By applying the dual-tasking optimization and co-learning strategy, the Began model identifies potential change areas from bi-temporal SAR and pre-temporal optical inputs, guiding the synthesis of target optical images. Second, a range of cloud masking and gap-filling techniques can be optionally employed to effectively reduce radiometric discrepancies between the synthesized images and the cloudy data, ultimately yielding high-quality, clear-sky imagery. To meet the big data requirements of deep learning, we constructed two globally distributed cloud removal datasets, named BiS1L8-CR and BiS1S2-CR. Supported by these datasets, extensive experiments demonstrated that the Began+ framework effectively captures bi-temporal change features, reconstructing precise surface information in both Landsat-8 and Sentinel-2 satellite images under large cloud cover. Compared to the latest solutions and algorithms, our proposed Began+ framework exhibits significant advantages from both qualitative and quantitative perspectives in both simulated and real experiments. Furthermore, without strict constraints on input timing, the Began+ framework enables accurate reconstruction of large-scale dual-sensor imagery under high-ratio cloud cover, effectively restoring changing surfaces and improving the quality of unsupervised vegetation extraction.
Published: 2025-12-11T17:30:56+00:00
Venue: Remote Sensing of Environment
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Xia; Wei He; Liangpei Zhang; Hongyan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115171"&gt;10.1016/j.rse.2025.115171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, optical remote sensing imagery has played an increasingly vital role in Earth observation, but cloud contamination exists as an inevitable degradation. Combining synthetic aperture radar (SAR) and optical data with machine learning offers a promising solution for reconstructing clear-sky satellite imagery. Nevertheless, several challenges persist, including insufficient attention to large cloud cover, difficulties in restoring temporal changes, and limited practicality of deep models. To address these issues, this paper introduces a novel deep learning-based cloud removal framework, termed Began+, which integrates bi-temporal SAR-optical data to deal with cloudy images with high cover ratios. The Began+ framework comprises two primary components: a deep network and a flexible post-processing step, combining the strengths of data-driven models for restoring change information and traditional gap-filling algorithms for mitigating radiance discrepancies. First, a bi-output enhanced generative adversarial network, abbreviated as Began, is designed for image synthesis, featuring an enhanced channel-wise fusion block (ECFB) and a multi-scale depth-wise convolution residual block (MDRB). By applying the dual-tasking optimization and co-learning strategy, the Began model identifies potential change areas from bi-temporal SAR and pre-temporal optical inputs, guiding the synthesis of target optical images. Second, a range of cloud masking and gap-filling techniques can be optionally employed to effectively reduce radiometric discrepancies between the synthesized images and the cloudy data, ultimately yielding high-quality, clear-sky imagery. To meet the big data requirements of deep learning, we constructed two globally distributed cloud removal datasets, named BiS1L8-CR and BiS1S2-CR. Supported by these datasets, extensive experiments demonstrated that the Began+ framework effectively captures bi-temporal change features, reconstructing precise surface information in both Landsat-8 and Sentinel-2 satellite images under large cloud cover. Compared to the latest solutions and algorithms, our proposed Began+ framework exhibits significant advantages from both qualitative and quantitative perspectives in both simulated and real experiments. Furthermore, without strict constraints on input timing, the Began+ framework enables accurate reconstruction of large-scale dual-sensor imagery under high-ratio cloud cover, effectively restoring changing surfaces and improving the quality of unsupervised vegetation extraction.&lt;/p&gt;</content:encoded></item><item><title>Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving</title><link>https://arxiv.org/abs/2512.09296v1</link><guid>http://arxiv.org/abs/2512.09296v1</guid><pubDate>Wed, 10 Dec 2025 03:46:57 +0000</pubDate><dc:creator>Songhan Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.
Published: 2025-12-10T03:46:57+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songhan Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model&amp;#x27;s contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.&lt;/p&gt;</content:encoded></item><item><title>A Unified Experience Replay Framework for Spiking Deep Reinforcement Learning</title><link>https://doi.org/10.1109/tpami.2025.3642900</link><guid>10.1109/tpami.2025.3642900</guid><pubDate>Thu, 11 Dec 2025 18:45:23 +0000</pubDate><dc:creator>Meng Xu</dc:creator><dc:creator>Xinhong Chen</dc:creator><dc:creator>Bingyi Liu</dc:creator><dc:creator>Yi-Rong Lin</dc:creator><dc:creator>Yung-Hui Li</dc:creator><dc:creator>Jianping Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3642900</prism:doi><description>Deep Reinforcement Learning (DRL) methods have shown remarkable success in many applications, yet their high energy consumption limits their practicability. Recent studies incorporated energy-efficient Spiking Neural Networks (SNNs) to build Spiking DRL methods and lower energy consumption by setting a shorter simulation duration for SNNs to compute fewer gradients. However, these existing Spiking DRL methods fail to sample sufficient high-quality samples within a fixed-size replay buffer and perform poorly when the simulation duration is small, introducing the challenging tradeoff between energy consumption and model performance. Motivated by such observations, we develop a generic resilient experience replay method that can be seamlessly integrated into existing spiking DRL methods to effectively address the above tradeoff. Specifically, we allow the replay buffer to dynamically expand as the number of training samples increases, thereby accommodating more potentially valuable candidate samples for policy training. Meanwhile, we introduce an adaptive approach to manage the buffer size by determining when to shrink the replay buffer and removing redundant samples automatically. This strategy prevents the buffer from expanding unnecessarily, thereby mitigating the potential negative impact on model performance. Extensive experimental results demonstrate that our approach significantly enhances the performance of five state-of-the-art (SOTA) spiking DRL methods across various simulation durations in sixteen tasks, in terms of return, without compromising their energy efficiency.
Published: 2025-12-11T18:45:23+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Xu; Xinhong Chen; Bingyi Liu; Yi-Rong Lin; Yung-Hui Li; Jianping Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3642900"&gt;10.1109/tpami.2025.3642900&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Deep Reinforcement Learning (DRL) methods have shown remarkable success in many applications, yet their high energy consumption limits their practicability. Recent studies incorporated energy-efficient Spiking Neural Networks (SNNs) to build Spiking DRL methods and lower energy consumption by setting a shorter simulation duration for SNNs to compute fewer gradients. However, these existing Spiking DRL methods fail to sample sufficient high-quality samples within a fixed-size replay buffer and perform poorly when the simulation duration is small, introducing the challenging tradeoff between energy consumption and model performance. Motivated by such observations, we develop a generic resilient experience replay method that can be seamlessly integrated into existing spiking DRL methods to effectively address the above tradeoff. Specifically, we allow the replay buffer to dynamically expand as the number of training samples increases, thereby accommodating more potentially valuable candidate samples for policy training. Meanwhile, we introduce an adaptive approach to manage the buffer size by determining when to shrink the replay buffer and removing redundant samples automatically. This strategy prevents the buffer from expanding unnecessarily, thereby mitigating the potential negative impact on model performance. Extensive experimental results demonstrate that our approach significantly enhances the performance of five state-of-the-art (SOTA) spiking DRL methods across various simulation durations in sixteen tasks, in terms of return, without compromising their energy efficiency.&lt;/p&gt;</content:encoded></item><item><title>Self-Rectification Historical Consistency Learning for Coupled Noisy Visible-Infrared Person Re-identification</title><link>https://doi.org/10.1109/tcsvt.2025.3642770</link><guid>10.1109/tcsvt.2025.3642770</guid><pubDate>Thu, 11 Dec 2025 18:47:52 +0000</pubDate><dc:creator>Jiacheng Zhao</dc:creator><dc:creator>Yongxi Li</dc:creator><dc:creator>Changsheng Xu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642770</prism:doi><description>Visible-infrared person re-identification (VI-ReID) retrieves cross-modal identity matches between visible and infrared images, offering significant value for round-the-clock surveillance. Despite recent advances, challenges remain: the task relies heavily on high-quality annotations, and factors such as occlusion, viewpoint variations, and the inherent difficulty of labeling infrared images inevitably introduce noisy annotations (NA) into the dataset during large-scale dataset construction. Moreover, coupled noisy labels in two modalities lead to noisy correspondence (NC), further complicating the learning process. Although prior research has achieved relatively stable results in addressing the NA and NC problem for VI-ReID through noise detection and robust loss functions, they still exhibit certain limitations: 1) Underutilization of training data. Existing methods often discard noisy samples to mitigate their negative impact, overlooking their potential value. 2) Lack of historical relevance. Unstable learning dynamics under noisy labels lead to inconsistent outputs, yet current approaches ignore the valuable historical information embedded in these fluctuations. Focusing on these challenges in VI-ReID, we propose Self-Rectification Historical Consistency Learning (SRHCL) for VI-ReID, which consists of noise detection, self-refined label rectification, and historical consistency learning modules. Firstly, the noise detection module calculates confidence weights for each sample by modeling the model’s loss response, thereby mitigating the adverse impact of noisy samples in subsequent training phases. Secondly, we propose a self-refined label rectification module to rectify noisy labels by reliable historical predictions, progressively collating the training data at fixed intervals. Finally, we introduce cross-modal contrastive learning and early learning regularization based on momentum-updated memories to facilitate historical consistency learning. Extensive exp...
Published: 2025-12-11T18:47:52+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiacheng Zhao; Yongxi Li; Changsheng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642770"&gt;10.1109/tcsvt.2025.3642770&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Visible-infrared person re-identification (VI-ReID) retrieves cross-modal identity matches between visible and infrared images, offering significant value for round-the-clock surveillance. Despite recent advances, challenges remain: the task relies heavily on high-quality annotations, and factors such as occlusion, viewpoint variations, and the inherent difficulty of labeling infrared images inevitably introduce noisy annotations (NA) into the dataset during large-scale dataset construction. Moreover, coupled noisy labels in two modalities lead to noisy correspondence (NC), further complicating the learning process. Although prior research has achieved relatively stable results in addressing the NA and NC problem for VI-ReID through noise detection and robust loss functions, they still exhibit certain limitations: 1) Underutilization of training data. Existing methods often discard noisy samples to mitigate their negative impact, overlooking their potential value. 2) Lack of historical relevance. Unstable learning dynamics under noisy labels lead to inconsistent outputs, yet current approaches ignore the valuable historical information embedded in these fluctuations. Focusing on these challenges in VI-ReID, we propose Self-Rectification Historical Consistency Learning (SRHCL) for VI-ReID, which consists of noise detection, self-refined label rectification, and historical consistency learning modules. Firstly, the noise detection module calculates confidence weights for each sample by modeling the model’s loss response, thereby mitigating the adverse impact of noisy samples in subsequent training phases. Secondly, we propose a self-refined label rectification module to rectify noisy labels by reliable historical predictions, progressively collating the training data at fixed intervals. Finally, we introduce cross-modal contrastive learning and early learning regularization based on momentum-updated memories to facilitate historical consistency learning. Extensive exp...&lt;/p&gt;</content:encoded></item><item><title>Hands-on Evaluation of Visual Transformers for Object Recognition and Detection</title><link>https://arxiv.org/abs/2512.09579v1</link><guid>http://arxiv.org/abs/2512.09579v1</guid><pubDate>Wed, 10 Dec 2025 12:15:48 +0000</pubDate><dc:creator>Dimitrios N. Vlachogiannis</dc:creator><dc:creator>Dimitrios A. Koutsomitropoulos</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.
Published: 2025-12-10T12:15:48+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dimitrios N. Vlachogiannis; Dimitrios A. Koutsomitropoulos&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.&lt;/p&gt;</content:encoded></item><item><title>Recent Advances in Discrete Speech Tokens: A Review</title><link>https://doi.org/10.1109/tpami.2025.3643619</link><guid>10.1109/tpami.2025.3643619</guid><pubDate>Fri, 12 Dec 2025 18:35:38 +0000</pubDate><dc:creator>Yiwei Guo</dc:creator><dc:creator>Zhihan Li</dc:creator><dc:creator>Hankun Wang</dc:creator><dc:creator>Bohan Li</dc:creator><dc:creator>Chongtian Shao</dc:creator><dc:creator>Hanglei Zhang</dc:creator><dc:creator>Chenpeng Du</dc:creator><dc:creator>Xie Chen</dc:creator><dc:creator>Shujie Liu</dc:creator><dc:creator>Kai Yu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643619</prism:doi><description>The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.
Published: 2025-12-12T18:35:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiwei Guo; Zhihan Li; Hankun Wang; Bohan Li; Chongtian Shao; Hanglei Zhang; Chenpeng Du; Xie Chen; Shujie Liu; Kai Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643619"&gt;10.1109/tpami.2025.3643619&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.&lt;/p&gt;</content:encoded></item><item><title>LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery</title><link>https://arxiv.org/abs/2512.09700v1</link><guid>http://arxiv.org/abs/2512.09700v1</guid><pubDate>Wed, 10 Dec 2025 14:48:58 +0000</pubDate><dc:creator>Seon-Hoon Kim</dc:creator><dc:creator>Hyeji Sim</dc:creator><dc:creator>Youeyun Jung</dc:creator><dc:creator>Ok-Chul Jung</dc:creator><dc:creator>Yerin Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.
Published: 2025-12-10T14:48:58+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seon-Hoon Kim; Hyeji Sim; Youeyun Jung; Ok-Chul Jung; Yerin Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.&lt;/p&gt;</content:encoded></item><item><title>A prototype-based semi-supervised learning method for few-shot SAR target recognition</title><link>https://doi.org/10.1109/jstars.2025.3643525</link><guid>10.1109/jstars.2025.3643525</guid><pubDate>Fri, 12 Dec 2025 18:36:08 +0000</pubDate><dc:creator>Ruikang Hu</dc:creator><dc:creator>Ye Li</dc:creator><dc:creator>Haiyan Zhu</dc:creator><dc:creator>Xu Lan</dc:creator><dc:creator>Li Liu</dc:creator><dc:creator>Jingyuan Xia</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3643525</prism:doi><description>Deep learning based methods have achieved extraordinary success in SAR automatic target recognition. However, deep learning conventionally necessitates a substantial number of labeled samples to achieve effective training, and labeled samples of new classes in real-world scenarios are scarce, which limits the performance of existing methods in the few-shot task. In response to this issue, this paper proposes a prototype based semi-supervised learning method for few-shot SAR target recognition, named WST-DRFSL. The method consists of two stages: the base learning stage and the dynamic refinement stage. In the first stage, a robust encoder is trained on both labeled and unlabeled samples of base classes via Consistency Regularization (CR). Then, in the second stage, pseudo-labels and CR are iteratively applied to new classes' few labeled samples and abundant unlabeled samples to achieve superior new-class recognition performance. Furthermore, the Wavelet Scattering Transform (WST) is employed in both stages to fully exploit the scattering characteristics of SAR images. Extensive simulations on MSTAR, FUSAR, OpenSARShip, and SAMPLE datasets have demonstrated that the proposed method surpasses the state-of the-art recognition accuracy on the few-shot learning tasks. The code is available at https://github.com/Cthanta/WST-DRFSL.
Published: 2025-12-12T18:36:08+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruikang Hu; Ye Li; Haiyan Zhu; Xu Lan; Li Liu; Jingyuan Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3643525"&gt;10.1109/jstars.2025.3643525&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning based methods have achieved extraordinary success in SAR automatic target recognition. However, deep learning conventionally necessitates a substantial number of labeled samples to achieve effective training, and labeled samples of new classes in real-world scenarios are scarce, which limits the performance of existing methods in the few-shot task. In response to this issue, this paper proposes a prototype based semi-supervised learning method for few-shot SAR target recognition, named WST-DRFSL. The method consists of two stages: the base learning stage and the dynamic refinement stage. In the first stage, a robust encoder is trained on both labeled and unlabeled samples of base classes via Consistency Regularization (CR). Then, in the second stage, pseudo-labels and CR are iteratively applied to new classes&amp;#x27; few labeled samples and abundant unlabeled samples to achieve superior new-class recognition performance. Furthermore, the Wavelet Scattering Transform (WST) is employed in both stages to fully exploit the scattering characteristics of SAR images. Extensive simulations on MSTAR, FUSAR, OpenSARShip, and SAMPLE datasets have demonstrated that the proposed method surpasses the state-of the-art recognition accuracy on the few-shot learning tasks. The code is available at https://github.com/Cthanta/WST-DRFSL.&lt;/p&gt;</content:encoded></item><item><title>UZSDD: Universal Zero-shot Deepfake Detection via Domain-Invariant Meta-Learning</title><link>https://doi.org/10.1109/tcsvt.2025.3642749</link><guid>10.1109/tcsvt.2025.3642749</guid><pubDate>Thu, 11 Dec 2025 18:47:52 +0000</pubDate><dc:creator>Qin Wang</dc:creator><dc:creator>Xiaofeng Wang</dc:creator><dc:creator>Ningning Bai</dc:creator><dc:creator>Zinian Liu</dc:creator><dc:creator>Jianghua Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3642749</prism:doi><description>Existing zero-shot deepfake detection methods are often constrained to specific scenarios and struggle in diverse, complex scenarios. To address this limitation, we propose a universal zero-shot deepfake detection method. This method models the common forgery traces across different domains as domain-invariant features and introduces a novel domain-invariant meta-learning strategy. This strategy embeds the mechanism of domain-invariant learning into a meta-learning framework, enabling the model not only to extract specific domain-invariant features from certain domains, but also to leverage the meta-learning mechanism of fast adaptation to new domains. As a result, the model is capable of effectively capturing the intrinsic domain-invariant characteristics of deepfake images, thereby achieving universal zero-shot deepfake detection. Extensive comparative experiments demonstrate that the proposed method achieves the highest average detection AUC (86.96%) across 28 unseen datasets, representing an improvement of 8.02% over the second-best method (78.94%). Moreover, it is the only method that is effective in all four zero-shot scenarios, which strongly validates its superior zero-shot detection performance and universality. Code is released at https://github.com/QinQin741/DIML.
Published: 2025-12-11T18:47:52+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qin Wang; Xiaofeng Wang; Ningning Bai; Zinian Liu; Jianghua Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3642749"&gt;10.1109/tcsvt.2025.3642749&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Existing zero-shot deepfake detection methods are often constrained to specific scenarios and struggle in diverse, complex scenarios. To address this limitation, we propose a universal zero-shot deepfake detection method. This method models the common forgery traces across different domains as domain-invariant features and introduces a novel domain-invariant meta-learning strategy. This strategy embeds the mechanism of domain-invariant learning into a meta-learning framework, enabling the model not only to extract specific domain-invariant features from certain domains, but also to leverage the meta-learning mechanism of fast adaptation to new domains. As a result, the model is capable of effectively capturing the intrinsic domain-invariant characteristics of deepfake images, thereby achieving universal zero-shot deepfake detection. Extensive comparative experiments demonstrate that the proposed method achieves the highest average detection AUC (86.96%) across 28 unseen datasets, representing an improvement of 8.02% over the second-best method (78.94%). Moreover, it is the only method that is effective in all four zero-shot scenarios, which strongly validates its superior zero-shot detection performance and universality. Code is released at https://github.com/QinQin741/DIML.&lt;/p&gt;</content:encoded></item><item><title>Redundancy Mitigation: Towards Accurate and Efficient Image-Text Retrieval</title><link>https://doi.org/10.1109/tcsvt.2025.3643601</link><guid>10.1109/tcsvt.2025.3643601</guid><pubDate>Fri, 12 Dec 2025 18:37:50 +0000</pubDate><dc:creator>Kun Wang</dc:creator><dc:creator>Yupeng Hu</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Lirong Jie</dc:creator><dc:creator>Liqiang Nie</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643601</prism:doi><description>Image-text retrieval (ITR) is a pivotal task in cross-modal research. However, existing methods often suffer from a fundamental yet overlooked challenge: redundancy. This issue manifests as both semantic redundancy within unimodal representations and relationship redundancy in cross-modal alignments. This not only inflates computational costs but also degrades retrieval accuracy by masking salient features and reinforcing spurious correlations. In this work, we are the first to explicitly analyze and address the ITR problem from a redundancy perspective by proposing the iMage-text rEtrieval rEdundancy miTigation (MEET) framework. MEET employs a cascaded, two-stage process to systematically mitigate both forms of redundancy. First, for Semantic Redundancy Mitigation, it repurposes deep hashing and quantization as synergistic tools, producing compact yet highly discriminative representations. Second, for Relationship Redundancy Mitigation, it progressively refines the cross-modal alignment space by filtering misleading negative samples and adaptively reweighting informative pairs. The structural integration of these modules under a unified optimization objective provides a clear and interpretable pathway to retrieval. Extensive experiments on multiple benchmarks demonstrate that MEET consistently surpasses state-of-the-art methods, validating its effectiveness and generalizability.
Published: 2025-12-12T18:37:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kun Wang; Yupeng Hu; Hao Liu; Lirong Jie; Liqiang Nie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643601"&gt;10.1109/tcsvt.2025.3643601&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Image-text retrieval (ITR) is a pivotal task in cross-modal research. However, existing methods often suffer from a fundamental yet overlooked challenge: redundancy. This issue manifests as both semantic redundancy within unimodal representations and relationship redundancy in cross-modal alignments. This not only inflates computational costs but also degrades retrieval accuracy by masking salient features and reinforcing spurious correlations. In this work, we are the first to explicitly analyze and address the ITR problem from a redundancy perspective by proposing the iMage-text rEtrieval rEdundancy miTigation (MEET) framework. MEET employs a cascaded, two-stage process to systematically mitigate both forms of redundancy. First, for Semantic Redundancy Mitigation, it repurposes deep hashing and quantization as synergistic tools, producing compact yet highly discriminative representations. Second, for Relationship Redundancy Mitigation, it progressively refines the cross-modal alignment space by filtering misleading negative samples and adaptively reweighting informative pairs. The structural integration of these modules under a unified optimization objective provides a clear and interpretable pathway to retrieval. Extensive experiments on multiple benchmarks demonstrate that MEET consistently surpasses state-of-the-art methods, validating its effectiveness and generalizability.&lt;/p&gt;</content:encoded></item></channel></rss>