<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 05 Feb 2026 03:37:14 +0000</lastBuildDate><item><title>Efficient Object Detection in Remote Sensing Images Based on Feature Weaving and Redundancy Suppression</title><link>https://doi.org/10.1109/tgrs.2026.3660762</link><guid>10.1109/tgrs.2026.3660762</guid><pubDate>Tue, 03 Feb 2026 20:54:52 +0000</pubDate><dc:creator>Yunze Bai</dc:creator><dc:creator>Changming Song</dc:creator><dc:creator>Yun Wang</dc:creator><dc:creator>Peiyan Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3660762</prism:doi><description>Remote sensing object detection faces significant challenges in handling scale variations, complex backgrounds and dense target distributions. Existing models often struggle with high computational costs and massive number of parameters, hindering deployment on resource-limited devices. To address these issues, we propose an efficient object detection network based on feature weaving and redundancy suppression called EWS-YOLO, featuring two components: (1) The Weave-Gated Parallel (WGP) module employs grouped convolution with interlaced cascading and gating mechanisms to achieve implicit multi-scale feature extraction and adaptive fusion, significantly enhancing multi-scale target detection capability. (2) The Redundancy-Suppressed Feature Pyramid Network (RFS-FPN) integrates an Progressive Shuffle Upsampling mechanism coupled with Dynamic Feature Fusion, enabling intelligent fusion of low-level spatial details and high-level semantic features while effectively suppressing redundant information through adaptive channel-wise feature recalibration. Experimental results on VisDrone, DIOR and UAVDT datasets demonstrate EWS-YOLO's superior performance, achieving 42.4%, 95.2%, 98.7% mAP@.50 respectively while reducing parameters by 41% compared to YOLOv8s, making it particularly suitable for resource-constrained applications.
Published: 2026-02-03T20:54:52+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunze Bai; Changming Song; Yun Wang; Peiyan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3660762"&gt;10.1109/tgrs.2026.3660762&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing object detection faces significant challenges in handling scale variations, complex backgrounds and dense target distributions. Existing models often struggle with high computational costs and massive number of parameters, hindering deployment on resource-limited devices. To address these issues, we propose an efficient object detection network based on feature weaving and redundancy suppression called EWS-YOLO, featuring two components: (1) The Weave-Gated Parallel (WGP) module employs grouped convolution with interlaced cascading and gating mechanisms to achieve implicit multi-scale feature extraction and adaptive fusion, significantly enhancing multi-scale target detection capability. (2) The Redundancy-Suppressed Feature Pyramid Network (RFS-FPN) integrates an Progressive Shuffle Upsampling mechanism coupled with Dynamic Feature Fusion, enabling intelligent fusion of low-level spatial details and high-level semantic features while effectively suppressing redundant information through adaptive channel-wise feature recalibration. Experimental results on VisDrone, DIOR and UAVDT datasets demonstrate EWS-YOLO&amp;#x27;s superior performance, achieving 42.4%, 95.2%, 98.7% mAP@.50 respectively while reducing parameters by 41% compared to YOLOv8s, making it particularly suitable for resource-constrained applications.&lt;/p&gt;</content:encoded></item><item><title>Self-supervised global − local collaborative network for real SAR despeckling</title><link>https://doi.org/10.1016/j.jag.2026.105135</link><guid>10.1016/j.jag.2026.105135</guid><pubDate>Tue, 03 Feb 2026 01:02:06 +0000</pubDate><dc:creator>Yang Yang</dc:creator><dc:creator>Jiangong Xu</dc:creator><dc:creator>Yuchuan Bai</dc:creator><dc:creator>Liangyu Chen</dc:creator><dc:creator>Junli Li</dc:creator><dc:creator>Jun Pan</dc:creator><dc:creator>Mi Wang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105135</prism:doi><description>Synthetic Aperture Radar (SAR) is crucial for Earth observation because it can acquire high-resolution images in all weather conditions. However, the presence of speckles—an inherent multiplicative noise caused by the coherent imaging process—severely degrades image quality and impairs the performance of subsequent interpretation tasks. To effectively capture both global contextual cues and fine-grained structural details in SAR image despeckling, we design a dual-branch Global-Local Collaborative Network (GLCNet) based on blind-spot convolution. GLCNet is trained in a self-supervised manner, requiring only original images for learning, making it well-suited for SAR data without ground truth. In the global branch, the SAR image is first decomposed into multiple frequency sub-bands through a Wavelet-Shuffle Downsampling (WSD), which decorrelates speckle components across scales and frequencies. A multi-scale blind-spot convolution is then applied to each sub-band in parallel, enabling the extraction of global textures without introducing speckle bias. In contrast, the local branch focuses on structure-aware restoration by jointly modeling frequency and spatial priors. By leveraging neighboring-pixel dependencies, this branch enhances local detail recovery and edge sharpness. Finally, an adaptive Detail-Guided Module (DGM) dynamically integrates complementary features from both branches, ensuring a harmonious balance between texture smoothness and structural fidelity. The proposed method is validated using various SAR sensors, including Sentinel-1, GF-3, TerraSAR-X, and Capella-X, demonstrating its superiority over traditional and deep learning approaches. Additionally, the application analysis confirms that the method enhances both the visual quality and analytical reliability of SAR images, making it a valuable preprocessing step for real-world scenarios. For reproducibility, our code and data are available at https://github.com/yangyang12318/LGCN.
Published: 2026-02-03T01:02:06+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Yang; Jiangong Xu; Yuchuan Bai; Liangyu Chen; Junli Li; Jun Pan; Mi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105135"&gt;10.1016/j.jag.2026.105135&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is crucial for Earth observation because it can acquire high-resolution images in all weather conditions. However, the presence of speckles—an inherent multiplicative noise caused by the coherent imaging process—severely degrades image quality and impairs the performance of subsequent interpretation tasks. To effectively capture both global contextual cues and fine-grained structural details in SAR image despeckling, we design a dual-branch Global-Local Collaborative Network (GLCNet) based on blind-spot convolution. GLCNet is trained in a self-supervised manner, requiring only original images for learning, making it well-suited for SAR data without ground truth. In the global branch, the SAR image is first decomposed into multiple frequency sub-bands through a Wavelet-Shuffle Downsampling (WSD), which decorrelates speckle components across scales and frequencies. A multi-scale blind-spot convolution is then applied to each sub-band in parallel, enabling the extraction of global textures without introducing speckle bias. In contrast, the local branch focuses on structure-aware restoration by jointly modeling frequency and spatial priors. By leveraging neighboring-pixel dependencies, this branch enhances local detail recovery and edge sharpness. Finally, an adaptive Detail-Guided Module (DGM) dynamically integrates complementary features from both branches, ensuring a harmonious balance between texture smoothness and structural fidelity. The proposed method is validated using various SAR sensors, including Sentinel-1, GF-3, TerraSAR-X, and Capella-X, demonstrating its superiority over traditional and deep learning approaches. Additionally, the application analysis confirms that the method enhances both the visual quality and analytical reliability of SAR images, making it a valuable preprocessing step for real-world scenarios. For reproducibility, our code and data are available at https://github.com/yangyang12318/LGCN.&lt;/p&gt;</content:encoded></item><item><title>Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales</title><link>https://doi.org/10.1109/tip.2026.3659033</link><guid>10.1109/tip.2026.3659033</guid><pubDate>Tue, 03 Feb 2026 20:57:02 +0000</pubDate><dc:creator>Kun Huang</dc:creator><dc:creator>Fang-Lue Zhang</dc:creator><dc:creator>Neil Dodgson</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3659033</prism:doi><description>360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection’s 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.
Published: 2026-02-03T20:57:02+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kun Huang; Fang-Lue Zhang; Neil Dodgson&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3659033"&gt;10.1109/tip.2026.3659033&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection’s 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.&lt;/p&gt;</content:encoded></item><item><title>Full-Scope Vectorization of Geographical Elements from Large-Size Remote Sensing Imagery</title><link>https://doi.org/10.1109/tpami.2026.3660934</link><guid>10.1109/tpami.2026.3660934</guid><pubDate>Tue, 03 Feb 2026 20:54:47 +0000</pubDate><dc:creator>Yansheng Li</dc:creator><dc:creator>Wanchun Li</dc:creator><dc:creator>Bo Dang</dc:creator><dc:creator>Yu Wang</dc:creator><dc:creator>Wei Chen</dc:creator><dc:creator>Lei Wang</dc:creator><dc:creator>Bingnan Yang</dc:creator><dc:creator>Yongjun Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3660934</prism:doi><description>Large-size very-high-resolution (VHR) remote sensing imagery has emerged as a critical data source for high-precision vector mapping of multi-scale geographical elements such as building, water, road and etc. When dealing with the large-size image, due to the limited memory of GPU, the deep learning-based vector mapping methods often employ the sliding block strategy. This inevitably leads to the degenerated performance because of the stitching difficulty of the sliding blocks' vector mapping results. Therefore, it is necessary to conduct full-scope vector mapping via mining the consistent cue in large-size remote sensing imagery. To this end, this paper presents a novel global context-aware local point optimization method. To leverage the global context, this paper proposes a novel pyramid fusion network (PFNet) to conduct semantic segmentation of the large-size image in an end-to-end manner. Under the constraint of the global semantic segmentation result, a new inflection-point perception network (IPNet) is proposed to generate a set of stable points to depict the boundary of each element. Extensive experiments on building, water and road datasets, where each image has over 100 million pixels, show that our method obviously outperforms the existing methods. The project page is at https://li-99.github.io/project/Vectorization.html.
Published: 2026-02-03T20:54:47+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yansheng Li; Wanchun Li; Bo Dang; Yu Wang; Wei Chen; Lei Wang; Bingnan Yang; Yongjun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3660934"&gt;10.1109/tpami.2026.3660934&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Large-size very-high-resolution (VHR) remote sensing imagery has emerged as a critical data source for high-precision vector mapping of multi-scale geographical elements such as building, water, road and etc. When dealing with the large-size image, due to the limited memory of GPU, the deep learning-based vector mapping methods often employ the sliding block strategy. This inevitably leads to the degenerated performance because of the stitching difficulty of the sliding blocks&amp;#x27; vector mapping results. Therefore, it is necessary to conduct full-scope vector mapping via mining the consistent cue in large-size remote sensing imagery. To this end, this paper presents a novel global context-aware local point optimization method. To leverage the global context, this paper proposes a novel pyramid fusion network (PFNet) to conduct semantic segmentation of the large-size image in an end-to-end manner. Under the constraint of the global semantic segmentation result, a new inflection-point perception network (IPNet) is proposed to generate a set of stable points to depict the boundary of each element. Extensive experiments on building, water and road datasets, where each image has over 100 million pixels, show that our method obviously outperforms the existing methods. The project page is at https://li-99.github.io/project/Vectorization.html.&lt;/p&gt;</content:encoded></item><item><title>GADet: Geometry-Aware Oriented Object Detection for Remote Sensing</title><link>https://doi.org/10.1016/j.knosys.2026.115475</link><guid>10.1016/j.knosys.2026.115475</guid><pubDate>Wed, 04 Feb 2026 07:59:10 +0000</pubDate><dc:creator>Haodong Li</dc:creator><dc:creator>Yan Gong</dc:creator><dc:creator>Xinyu Zhang</dc:creator><dc:creator>Ziying Song</dc:creator><dc:creator>Lei Yang</dc:creator><dc:creator>Haicheng Qu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115475</prism:doi><description>Oriented object detection in remote sensing images is a key technology for accurately perceiving the geometric properties of objects on the Earth’s surface, playing a significant role in smart cities, national defense and security, and disaster emergency response. However, existing anchor-free methods have obvious limitations in geometric feature adaptation and orientation-aware modeling, and their large number of parameters makes real-time deployment difficult. To address these issues, we propose the geometry-aware detector GADet, a single-stage anchor-free detector comprising three key components: a geometrically structured adaptive convolution (GSA-Conv) module for enhanced feature extraction, a rotation-sensitive attention (RSA) module for robust orientation awareness, and a channel-isomorphic adaptive (CIA) pruning method for model compression. Comprehensive experiments demonstrate that GADet achieves mAP scores of 76.90%, 70.20%, and 97.47% on the DOTA-v1.0, DIOR-R, and UCAS-AOD datasets, respectively, while running at 56.5 FPS, achieving the optimal balance between accuracy and efficiency compared to recent state-of-the-art methods.
Published: 2026-02-04T07:59:10+00:00
Venue: Knowledge-Based Systems
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haodong Li; Yan Gong; Xinyu Zhang; Ziying Song; Lei Yang; Haicheng Qu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115475"&gt;10.1016/j.knosys.2026.115475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Oriented object detection in remote sensing images is a key technology for accurately perceiving the geometric properties of objects on the Earth’s surface, playing a significant role in smart cities, national defense and security, and disaster emergency response. However, existing anchor-free methods have obvious limitations in geometric feature adaptation and orientation-aware modeling, and their large number of parameters makes real-time deployment difficult. To address these issues, we propose the geometry-aware detector GADet, a single-stage anchor-free detector comprising three key components: a geometrically structured adaptive convolution (GSA-Conv) module for enhanced feature extraction, a rotation-sensitive attention (RSA) module for robust orientation awareness, and a channel-isomorphic adaptive (CIA) pruning method for model compression. Comprehensive experiments demonstrate that GADet achieves mAP scores of 76.90%, 70.20%, and 97.47% on the DOTA-v1.0, DIOR-R, and UCAS-AOD datasets, respectively, while running at 56.5 FPS, achieving the optimal balance between accuracy and efficiency compared to recent state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Allies Teach Better than Enemies: Inverse Adversaries for Robust Knowledge Distillation</title><link>https://doi.org/10.1109/tpami.2026.3660863</link><guid>10.1109/tpami.2026.3660863</guid><pubDate>Tue, 03 Feb 2026 20:54:47 +0000</pubDate><dc:creator>Junhao Dong</dc:creator><dc:creator>Raoof Zare Moayedi</dc:creator><dc:creator>Yew-Soon Ong</dc:creator><dc:creator>Seyed-Mohsen Moosavi-Dezfooli</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3660863</prism:doi><description>Adversarially robust knowledge distillation aims to compress a large-scale robust teacher model into a lightweight student counterpart while preserving adversarial robustness and natural performance. Previous methods primarily focused on aligning knowledge (e.g., predictions) between teacher and student models to transfer robustness. However, potentially incorrect predictions from the teacher can misguide the student, negatively impacting robustness transfer. To circumvent this, we propose a novel adversarially robust knowledge distillation scheme that promotes alignment towards more benign predictions rather than incorrect ones by refining inputs into so-called “inverse adversarial examples” via simply reversing the sign of adversarial perturbation. Through a comprehensive investigation of the properties of inverse adversaries, we provide new theoretical insights showing how mimicking the behavior of the teacher model on inverse adversaries facilitates reliable robustness transfer built upon the implicit connection between robustness and the input gradient information. We thus design a gradient matching mechanism between teacher and student models utilizing inverse adversaries to facilitate robust knowledge alignment. Furthermore, inspired by our analysis of the correlation between robustness and adversarial transferability, we propose a weight-space disruption strategy that jointly interacts with both teacher and student models to find a shared direction for better robustness transfer. Empirical evaluations across various datasets demonstrate that our method achieves state-of-the-art robustness and natural performance. Notably, on ImageNet, our approach outperforms prior methods by approximately 3.8% in both clean and robust accuracy. Moreover, we show that incorporating auxiliary generated data into distillation further boosts robustness. Our method can also be generalized to multimodal architectures.
Published: 2026-02-03T20:54:47+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junhao Dong; Raoof Zare Moayedi; Yew-Soon Ong; Seyed-Mohsen Moosavi-Dezfooli&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3660863"&gt;10.1109/tpami.2026.3660863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Adversarially robust knowledge distillation aims to compress a large-scale robust teacher model into a lightweight student counterpart while preserving adversarial robustness and natural performance. Previous methods primarily focused on aligning knowledge (e.g., predictions) between teacher and student models to transfer robustness. However, potentially incorrect predictions from the teacher can misguide the student, negatively impacting robustness transfer. To circumvent this, we propose a novel adversarially robust knowledge distillation scheme that promotes alignment towards more benign predictions rather than incorrect ones by refining inputs into so-called “inverse adversarial examples” via simply reversing the sign of adversarial perturbation. Through a comprehensive investigation of the properties of inverse adversaries, we provide new theoretical insights showing how mimicking the behavior of the teacher model on inverse adversaries facilitates reliable robustness transfer built upon the implicit connection between robustness and the input gradient information. We thus design a gradient matching mechanism between teacher and student models utilizing inverse adversaries to facilitate robust knowledge alignment. Furthermore, inspired by our analysis of the correlation between robustness and adversarial transferability, we propose a weight-space disruption strategy that jointly interacts with both teacher and student models to find a shared direction for better robustness transfer. Empirical evaluations across various datasets demonstrate that our method achieves state-of-the-art robustness and natural performance. Notably, on ImageNet, our approach outperforms prior methods by approximately 3.8% in both clean and robust accuracy. Moreover, we show that incorporating auxiliary generated data into distillation further boosts robustness. Our method can also be generalized to multimodal architectures.&lt;/p&gt;</content:encoded></item><item><title>YOLD: You Only Look Denseness for Tiny Object Detection in Aerial Images</title><link>https://doi.org/10.1109/tgrs.2026.3660635</link><guid>10.1109/tgrs.2026.3660635</guid><pubDate>Tue, 03 Feb 2026 20:54:52 +0000</pubDate><dc:creator>Guangshuai Gao</dc:creator><dc:creator>Yunqi Shang</dc:creator><dc:creator>Junyu Gao</dc:creator><dc:creator>Haojie Guo</dc:creator><dc:creator>Yan Dong</dc:creator><dc:creator>Chunlei Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3660635</prism:doi><description>Aerial image object detection has gained increasing attention recently. Compared to natural scenes, this task is more challenging due to two main factors: 1) a large number of densely packed tiny objects with blurred appearances that hinder detection accuracy, and 2) non-uniform data distributions that limit detection efficiency. To address these challenges, we present YOLD (You Only Look Denseness), a simple yet efficient detection framework. Specifically, for dense small objects, we propose an end-to-end anchor-free detector guided solely by point supervision, improved from the classical CenterNet, the key difference of which lies in using localization-based distance transform maps instead of heatmaps. To tackle non-uniformly data distribution, a straightforward and effective Dense Region Extraction (DRE) module is designed to identify clustered regions containing challenging instances such as dense tiny objects, thereby improving detection efficiency. Additionally, we design the regression loss by incorporating the Generalized Euclidean Distance (GED) with an adaptive weighted mechanism based on objects’ areas, which improves the detection accuracy of tiny object detection. Finally, extensive experiments are carried out on three publicly available large-scale aerial image object detection datasets, namely VisDrone, UAVDT, and DTOD. The results confirm both the effectiveness of the proposed approach and its superiority over state-of-the-art methods.
Published: 2026-02-03T20:54:52+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangshuai Gao; Yunqi Shang; Junyu Gao; Haojie Guo; Yan Dong; Chunlei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3660635"&gt;10.1109/tgrs.2026.3660635&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Aerial image object detection has gained increasing attention recently. Compared to natural scenes, this task is more challenging due to two main factors: 1) a large number of densely packed tiny objects with blurred appearances that hinder detection accuracy, and 2) non-uniform data distributions that limit detection efficiency. To address these challenges, we present YOLD (You Only Look Denseness), a simple yet efficient detection framework. Specifically, for dense small objects, we propose an end-to-end anchor-free detector guided solely by point supervision, improved from the classical CenterNet, the key difference of which lies in using localization-based distance transform maps instead of heatmaps. To tackle non-uniformly data distribution, a straightforward and effective Dense Region Extraction (DRE) module is designed to identify clustered regions containing challenging instances such as dense tiny objects, thereby improving detection efficiency. Additionally, we design the regression loss by incorporating the Generalized Euclidean Distance (GED) with an adaptive weighted mechanism based on objects’ areas, which improves the detection accuracy of tiny object detection. Finally, extensive experiments are carried out on three publicly available large-scale aerial image object detection datasets, namely VisDrone, UAVDT, and DTOD. The results confirm both the effectiveness of the proposed approach and its superiority over state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>RVFormer: Keypoint-Based Fusion of 4D Radar and Vision for 3D Object Detection in Autonomous Driving</title><link>https://doi.org/10.1016/j.eswa.2026.131497</link><guid>10.1016/j.eswa.2026.131497</guid><pubDate>Tue, 03 Feb 2026 15:57:55 +0000</pubDate><dc:creator>Xin Bi</dc:creator><dc:creator>Caien Weng</dc:creator><dc:creator>Panpan Tong</dc:creator><dc:creator>Arno Eichberger</dc:creator><dc:creator>Lu Xiong</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131497</prism:doi><description>Multi-modal fusion is crucial in autonomous driving perception, enhancing reliability, completeness, and accuracy, which extends the performance limits of perception systems. Specifically, large-scale perception through 4D radar and vision fusion has become a key research focus aimed at improving driving safety, enhancing complex scene understanding, and supporting fine-grained local planning and control. However, existing 3D object detection methods typically rely on fixed-voxel representations to maintain detection accuracy. As the perception range increases, these methods incur considerable computational overhead. While transformer-based query methods show strong potential in capturing dependencies over large receptive fields in image-domain tasks, their application in radar-vision fusion is limited due to radar point cloud sparsity and cross-modal alignment challenges. To address these limitations, we propose RVFormer, a dual-branch feature-level fusion network that uses a sparse keypoint-based query strategy to integrate features from both modalities, thereby mitigating the impact of large-scale scenes on inference speed. Additionally, we introduce clustered voxel query initialization (CVQI) to accelerate convergence and enhance object localization. By incorporating the radar voxel painter (RVP), radar-image cross-attention (RICA), and gated adaptive fusion (GAF) modules, our framework enables deep and adaptive fusion of radar and visual features, effectively mitigating issues caused by point cloud sparsity and modality inconsistency. Compared to existing radar-vision fusion models, RVFormer demonstrates competitive performance, with an inference speed of approximately 15.2 frames per second. It delivers accuracy comparable to CNN-based approaches, while outperforming baseline methods by at least 4.72% in 3D mean average precision and 5.82% in bird’s eye view mean average precision.
Published: 2026-02-03T15:57:55+00:00
Venue: Expert Systems with Applications
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Bi; Caien Weng; Panpan Tong; Arno Eichberger; Lu Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131497"&gt;10.1016/j.eswa.2026.131497&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal fusion is crucial in autonomous driving perception, enhancing reliability, completeness, and accuracy, which extends the performance limits of perception systems. Specifically, large-scale perception through 4D radar and vision fusion has become a key research focus aimed at improving driving safety, enhancing complex scene understanding, and supporting fine-grained local planning and control. However, existing 3D object detection methods typically rely on fixed-voxel representations to maintain detection accuracy. As the perception range increases, these methods incur considerable computational overhead. While transformer-based query methods show strong potential in capturing dependencies over large receptive fields in image-domain tasks, their application in radar-vision fusion is limited due to radar point cloud sparsity and cross-modal alignment challenges. To address these limitations, we propose RVFormer, a dual-branch feature-level fusion network that uses a sparse keypoint-based query strategy to integrate features from both modalities, thereby mitigating the impact of large-scale scenes on inference speed. Additionally, we introduce clustered voxel query initialization (CVQI) to accelerate convergence and enhance object localization. By incorporating the radar voxel painter (RVP), radar-image cross-attention (RICA), and gated adaptive fusion (GAF) modules, our framework enables deep and adaptive fusion of radar and visual features, effectively mitigating issues caused by point cloud sparsity and modality inconsistency. Compared to existing radar-vision fusion models, RVFormer demonstrates competitive performance, with an inference speed of approximately 15.2 frames per second. It delivers accuracy comparable to CNN-based approaches, while outperforming baseline methods by at least 4.72% in 3D mean average precision and 5.82% in bird’s eye view mean average precision.&lt;/p&gt;</content:encoded></item><item><title>Multi-Scale Pattern-Aware Task-Gating Network for Aerial Small Object Detection</title><link>https://doi.org/10.1016/j.neunet.2026.108680</link><guid>10.1016/j.neunet.2026.108680</guid><pubDate>Tue, 03 Feb 2026 07:45:19 +0000</pubDate><dc:creator>Ben Liang</dc:creator><dc:creator>Yuan Liu</dc:creator><dc:creator>Chao Sui</dc:creator><dc:creator>Yihong Wang</dc:creator><dc:creator>Lin Xiao</dc:creator><dc:creator>Xiubao Sui</dc:creator><dc:creator>Qian Chen</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108680</prism:doi><description>With the advancement of high-precision remote sensing equipment and precision measurement technology, object detection based on remote sensing images (RSIs) has been widely used in military and civilian fields. Different from traditional general-purpose environments, remote sensing presents unique challenges that significantly complicate the detection process. Specifically: (1) RSIs cover extensive monitoring areas, resulting in complex and textured backgrounds; and (2) objects often exhibit cluttered distributions, small sizes, and considerable scale variations across categories. To effectively address these challenges, we propose a Multi-Scale Pattern-Aware Task-Gating Network (MPTNet) for remote sensing object detection. First, we design a Multi-Scale Pattern-Aware Network (MPNet) backbone that employs a small and large kernel convolutional complementary strategy to capture both large-scale and small-scale spatial patterns, yielding more comprehensive semantic features. Next, we introduce a Multi-Head Cross-Space Encoder (MCE) that improves semantic fusion and spatial representation across hierarchical levels. By combining a multi-head mechanism with directional one-dimensional strip convolutions, MCE enhances spatial sensitivity at the pixel level, thus improving object localization in densely textured scenes. To harmonize cross-task synergy, we propose a Dynamic Task-Gating (DTG) head that adaptively recalibrates spatial feature representations between classification and localization branches. Extensive experimental validations on three publicly available datasets, including VisDrone, DIOR, and COCO-mini, demonstrate that our method achieves excellent performance, obtaining AP 50 scores of 43.3%, 80.6%, and 49.5%, respectively.
Published: 2026-02-03T07:45:19+00:00
Venue: Neural Networks
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ben Liang; Yuan Liu; Chao Sui; Yihong Wang; Lin Xiao; Xiubao Sui; Qian Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108680"&gt;10.1016/j.neunet.2026.108680&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;With the advancement of high-precision remote sensing equipment and precision measurement technology, object detection based on remote sensing images (RSIs) has been widely used in military and civilian fields. Different from traditional general-purpose environments, remote sensing presents unique challenges that significantly complicate the detection process. Specifically: (1) RSIs cover extensive monitoring areas, resulting in complex and textured backgrounds; and (2) objects often exhibit cluttered distributions, small sizes, and considerable scale variations across categories. To effectively address these challenges, we propose a Multi-Scale Pattern-Aware Task-Gating Network (MPTNet) for remote sensing object detection. First, we design a Multi-Scale Pattern-Aware Network (MPNet) backbone that employs a small and large kernel convolutional complementary strategy to capture both large-scale and small-scale spatial patterns, yielding more comprehensive semantic features. Next, we introduce a Multi-Head Cross-Space Encoder (MCE) that improves semantic fusion and spatial representation across hierarchical levels. By combining a multi-head mechanism with directional one-dimensional strip convolutions, MCE enhances spatial sensitivity at the pixel level, thus improving object localization in densely textured scenes. To harmonize cross-task synergy, we propose a Dynamic Task-Gating (DTG) head that adaptively recalibrates spatial feature representations between classification and localization branches. Extensive experimental validations on three publicly available datasets, including VisDrone, DIOR, and COCO-mini, demonstrate that our method achieves excellent performance, obtaining AP 50 scores of 43.3%, 80.6%, and 49.5%, respectively.&lt;/p&gt;</content:encoded></item><item><title>Dynamic High-frequency Convolution for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2602.02969v1</link><guid>http://arxiv.org/abs/2602.02969v1</guid><pubDate>Tue, 03 Feb 2026 01:07:55 +0000</pubDate><dc:creator>Ruojing Li</dc:creator><dc:creator>Chao Xiao</dc:creator><dc:creator>Qian Yin</dc:creator><dc:creator>Wei An</dc:creator><dc:creator>Nuo Chen</dc:creator><dc:creator>Xinyi Ying</dc:creator><dc:creator>Miao Li</dc:creator><dc:creator>Yingqian Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TCSVT.2026.3661285</prism:doi><description>Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.
Published: 2026-02-03T01:07:55+00:00
Venue: arXiv
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruojing Li; Chao Xiao; Qian Yin; Wei An; Nuo Chen; Xinyi Ying; Miao Li; Yingqian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TCSVT.2026.3661285"&gt;10.1109/TCSVT.2026.3661285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.&lt;/p&gt;</content:encoded></item><item><title>PARTNER: Level Up the Polar Representation for 3D Object Detection</title><link>https://doi.org/10.1007/s11263-026-02735-0</link><guid>10.1007/s11263-026-02735-0</guid><pubDate>Tue, 03 Feb 2026 07:41:43 +0000</pubDate><dc:creator>Ming Nie</dc:creator><dc:creator>Chunwei Wang</dc:creator><dc:creator>Hang Xu</dc:creator><dc:creator>Li Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-026-02735-0</prism:doi><description>Polar-based representation has shown promising properties in perceptual tasks. Unlike Cartesian-based approaches, which divide space into a uniform grid regardless of the uneven distribution of the foreground along the radial direction, representing space as polar coordinates aligns more closely with the physical properties of sensors, whether LiDAR or surround cameras. Moreover, polar-based methods are recognized as a superior alternative due to (1) their advantages in robust performance across different resolutions and (2) their efficacy in streaming-based approaches. However, state-of-the-art polar-based detection methods inevitably suffer from the feature distortion problem because of the non-uniform division of polar representation, resulting in a non-negligible performance gap compared to Cartesian-based approaches. To tackle this issue, we present PARTNER, a novel 3D object detector in the polar coordinate. PARTNER alleviates the dilemma of feature distortion with global representation re-alignment and facilitates the regression by introducing instance-level geometric information into the detection head. Building upon this, we further propose a novel polar-coordinate-based PV-to-BEV view transformation module, enabling a unified framework for multi-modal detection in polar space. Extensive experiments demonstrate the superiority of our method in streaming-based detection and across varying resolutions, outperforming prior polar-based approaches on Waymo, nuScenes and ONCE. Beyond empirical results, we also explore whether more effective partitioning strategies and regression targets can be designed specifically for the polar coordinate system. We provide in-depth insights into the nature of feature distortion in polar space and present visualizations that demonstrate the corrective effects of our proposed modules, further validating the design rationale and effectiveness of our approach.
Published: 2026-02-03T07:41:43+00:00
Venue: International Journal of Computer Vision
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Nie; Chunwei Wang; Hang Xu; Li Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-026-02735-0"&gt;10.1007/s11263-026-02735-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Polar-based representation has shown promising properties in perceptual tasks. Unlike Cartesian-based approaches, which divide space into a uniform grid regardless of the uneven distribution of the foreground along the radial direction, representing space as polar coordinates aligns more closely with the physical properties of sensors, whether LiDAR or surround cameras. Moreover, polar-based methods are recognized as a superior alternative due to (1) their advantages in robust performance across different resolutions and (2) their efficacy in streaming-based approaches. However, state-of-the-art polar-based detection methods inevitably suffer from the feature distortion problem because of the non-uniform division of polar representation, resulting in a non-negligible performance gap compared to Cartesian-based approaches. To tackle this issue, we present PARTNER, a novel 3D object detector in the polar coordinate. PARTNER alleviates the dilemma of feature distortion with global representation re-alignment and facilitates the regression by introducing instance-level geometric information into the detection head. Building upon this, we further propose a novel polar-coordinate-based PV-to-BEV view transformation module, enabling a unified framework for multi-modal detection in polar space. Extensive experiments demonstrate the superiority of our method in streaming-based detection and across varying resolutions, outperforming prior polar-based approaches on Waymo, nuScenes and ONCE. Beyond empirical results, we also explore whether more effective partitioning strategies and regression targets can be designed specifically for the polar coordinate system. We provide in-depth insights into the nature of feature distortion in polar space and present visualizations that demonstrate the corrective effects of our proposed modules, further validating the design rationale and effectiveness of our approach.&lt;/p&gt;</content:encoded></item><item><title>Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion</title><link>https://doi.org/10.1109/tmm.2026.3660142</link><guid>10.1109/tmm.2026.3660142</guid><pubDate>Tue, 03 Feb 2026 20:55:41 +0000</pubDate><dc:creator>Mengyu Wang</dc:creator><dc:creator>Zhenyu Liu</dc:creator><dc:creator>Kun Li</dc:creator><dc:creator>Yu Wang</dc:creator><dc:creator>Yuwei Wang</dc:creator><dc:creator>Yanyan Wei</dc:creator><dc:creator>Fei Wang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660142</prism:doi><description>Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-frequency detail destruction, and task-specific limitations. To address these challenges, we propose AdaSFFuse, a novel framework for task-generalized MMIF through adaptive cross-domain co-fusion learning. AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high- and low-frequency components of multimodal images from different scenes, enabling fine-grained extraction and alignment of distinct frequency characteristics for each modality. The Spatial-Frequency Mamba Blocks facilitate cross-domain fusion in both spatial and frequency domains, enhancing this process. These blocks dynamically adjust through learnable mappings to ensure robust fusion across diverse modalities. By combining these components, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments on four MMIF tasks-Infrared-Visible Image Fusion (IVF), Multi-Focus Image Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF)-demonstrate AdaSFFuse's superior fusion performance, ensuring both low computational cost and a compact network, offering a strong balance between performance and efficiency. The code will be publicly available at https://github.com/Zhen-yu-Liu/AdaSFFuse.
Published: 2026-02-03T20:55:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengyu Wang; Zhenyu Liu; Kun Li; Yu Wang; Yuwei Wang; Yanyan Wei; Fei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660142"&gt;10.1109/tmm.2026.3660142&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-frequency detail destruction, and task-specific limitations. To address these challenges, we propose AdaSFFuse, a novel framework for task-generalized MMIF through adaptive cross-domain co-fusion learning. AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high- and low-frequency components of multimodal images from different scenes, enabling fine-grained extraction and alignment of distinct frequency characteristics for each modality. The Spatial-Frequency Mamba Blocks facilitate cross-domain fusion in both spatial and frequency domains, enhancing this process. These blocks dynamically adjust through learnable mappings to ensure robust fusion across diverse modalities. By combining these components, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments on four MMIF tasks-Infrared-Visible Image Fusion (IVF), Multi-Focus Image Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF)-demonstrate AdaSFFuse&amp;#x27;s superior fusion performance, ensuring both low computational cost and a compact network, offering a strong balance between performance and efficiency. The code will be publicly available at https://github.com/Zhen-yu-Liu/AdaSFFuse.&lt;/p&gt;</content:encoded></item><item><title>Learning Modality Knowledge with Proxy for RGB-Infrared Object Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113227</link><guid>10.1016/j.patcog.2026.113227</guid><pubDate>Tue, 03 Feb 2026 07:42:22 +0000</pubDate><dc:creator>You Ma</dc:creator><dc:creator>Lin Chai</dc:creator><dc:creator>Shihan Mao</dc:creator><dc:creator>Yucheng Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113227</prism:doi><description>RGB-infrared object detection aims to improve detection performance in complex environments by integrating complementary information from RGB and infrared images. While transformer-based methods have demonstrated significant advancements in this field by directly modeling dense relationships between modality tokens to enable cross-modality long-range interactions, they neglect the inherent discrepancies in feature distributions across modalities. Such discrepancies attenuate the reliability of the established relationships, thereby restricting the effective exploitation of complementary information between modalities. To alleviate this problem, we propose a framework for learning modality knowledge with proxy. The core innovation lies in the design of a proxy-guided cross-modality feature fusion module, which realizes dual-modality interactions by using lightweight proxy tokens as intermediate representations. Specifically, self-attention is firstly utilized to facilitate the proxy tokens to learn the global information of each modality; then, the relationship between dual-modality proxy tokens is constructed to capture modality complementary information while also mitigating the interference of modality discrepancies; and finally, the knowledge in the updated proxy tokens is fed back to each modality through cross-attention for enhancing the features of each modality. Additionally, a mixture of knowledge decoupled experts module is designed to effectively fuse enhanced features of the two modalities. This module leverages multiple gating networks to assign modality-specific and modality-shared knowledge to separate expert groups for learning, thus highlighting the advantageous features of the different modalities. Extensive experiments on four RGB-infrared datasets demonstrate that our method outperforms existing state-of-the-art methods.
Published: 2026-02-03T07:42:22+00:00
Venue: Pattern Recognition
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; You Ma; Lin Chai; Shihan Mao; Yucheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113227"&gt;10.1016/j.patcog.2026.113227&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;RGB-infrared object detection aims to improve detection performance in complex environments by integrating complementary information from RGB and infrared images. While transformer-based methods have demonstrated significant advancements in this field by directly modeling dense relationships between modality tokens to enable cross-modality long-range interactions, they neglect the inherent discrepancies in feature distributions across modalities. Such discrepancies attenuate the reliability of the established relationships, thereby restricting the effective exploitation of complementary information between modalities. To alleviate this problem, we propose a framework for learning modality knowledge with proxy. The core innovation lies in the design of a proxy-guided cross-modality feature fusion module, which realizes dual-modality interactions by using lightweight proxy tokens as intermediate representations. Specifically, self-attention is firstly utilized to facilitate the proxy tokens to learn the global information of each modality; then, the relationship between dual-modality proxy tokens is constructed to capture modality complementary information while also mitigating the interference of modality discrepancies; and finally, the knowledge in the updated proxy tokens is fed back to each modality through cross-attention for enhancing the features of each modality. Additionally, a mixture of knowledge decoupled experts module is designed to effectively fuse enhanced features of the two modalities. This module leverages multiple gating networks to assign modality-specific and modality-shared knowledge to separate expert groups for learning, thus highlighting the advantageous features of the different modalities. Extensive experiments on four RGB-infrared datasets demonstrate that our method outperforms existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Anti-DETR: End-to-End Anti-Drone Visual Detection Network Based on Wavelet Convolution</title><link>https://doi.org/10.1016/j.neucom.2026.132935</link><guid>10.1016/j.neucom.2026.132935</guid><pubDate>Tue, 03 Feb 2026 16:39:34 +0000</pubDate><dc:creator>Jiarui Zhang</dc:creator><dc:creator>Zhihua Chen</dc:creator><dc:creator>Chun Zheng</dc:creator><dc:creator>Wenjun Yi</dc:creator><dc:creator>Guoxu Yan</dc:creator><dc:creator>Yi Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132935</prism:doi><description>With the advancement of unmanned aerial vehicle technology, visual detection for anti-drone tasks in air-to-air scenarios has become increasingly critical. However, detecting fast-moving small UAVs in complex backgrounds remains challenging due to interference from background noise and blurred target edges, resulting in poor detection accuracy. To address these issues, we propose Anti-DETR, an end-to-end detection network leveraging wavelet convolution specifically for small-target UAV detection. Anti-DETR consists of three key components: first, the Global Multi-channel Wavelet Residual Network, which expands the receptive field through wavelet convolution and efficiently localizes targets with a global multi-channel attention mechanism; second, the Multi-scale Refined Feature Pyramid Network, employing an Adaptive Global Calibration Attention Unit to integrate fine-grained shallow features and deep semantic features, enhancing multi-scale feature representation; finally, the Histogram Self-Attention mechanism, which classifies pixel-level features to improve feature perception in complex backgrounds. Evaluations on the Det-Fly, DUT-Anti-UAV, and HazyDet datasets demonstrate that Anti-DETR surpasses several state-of-the-art methods and classical detectors, confirming its effectiveness and generalizability for accurate anti-UAV detection tasks under challenging environmental conditions. The code is available at https://github.com/Image-Zhang/anti-detr .
Published: 2026-02-03T16:39:34+00:00
Venue: Neurocomputing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiarui Zhang; Zhihua Chen; Chun Zheng; Wenjun Yi; Guoxu Yan; Yi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132935"&gt;10.1016/j.neucom.2026.132935&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;With the advancement of unmanned aerial vehicle technology, visual detection for anti-drone tasks in air-to-air scenarios has become increasingly critical. However, detecting fast-moving small UAVs in complex backgrounds remains challenging due to interference from background noise and blurred target edges, resulting in poor detection accuracy. To address these issues, we propose Anti-DETR, an end-to-end detection network leveraging wavelet convolution specifically for small-target UAV detection. Anti-DETR consists of three key components: first, the Global Multi-channel Wavelet Residual Network, which expands the receptive field through wavelet convolution and efficiently localizes targets with a global multi-channel attention mechanism; second, the Multi-scale Refined Feature Pyramid Network, employing an Adaptive Global Calibration Attention Unit to integrate fine-grained shallow features and deep semantic features, enhancing multi-scale feature representation; finally, the Histogram Self-Attention mechanism, which classifies pixel-level features to improve feature perception in complex backgrounds. Evaluations on the Det-Fly, DUT-Anti-UAV, and HazyDet datasets demonstrate that Anti-DETR surpasses several state-of-the-art methods and classical detectors, confirming its effectiveness and generalizability for accurate anti-UAV detection tasks under challenging environmental conditions. The code is available at https://github.com/Image-Zhang/anti-detr .&lt;/p&gt;</content:encoded></item><item><title>SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection</title><link>https://arxiv.org/abs/2602.01843v1</link><guid>http://arxiv.org/abs/2602.01843v1</guid><pubDate>Mon, 02 Feb 2026 09:15:29 +0000</pubDate><dc:creator>Qian Xu</dc:creator><dc:creator>Xi Li</dc:creator><dc:creator>Fei Gao</dc:creator><dc:creator>Jie Guo</dc:creator><dc:creator>Haojuan Yuan</dc:creator><dc:creator>Shuaipeng Fan</dc:creator><dc:creator>Mingjin Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.
Published: 2026-02-02T09:15:29+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Xu; Xi Li; Fei Gao; Jie Guo; Haojuan Yuan; Shuaipeng Fan; Mingjin Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.&lt;/p&gt;</content:encoded></item><item><title>Relaxed Knowledge Distillation</title><link>https://doi.org/10.1007/s11263-025-02705-y</link><guid>10.1007/s11263-025-02705-y</guid><pubDate>Wed, 04 Feb 2026 06:42:54 +0000</pubDate><dc:creator>Zheng Qu</dc:creator><dc:creator>Xiwen Yao</dc:creator><dc:creator>Xuguang Yang</dc:creator><dc:creator>Jie Tang</dc:creator><dc:creator>Lang Li</dc:creator><dc:creator>Gong Cheng</dc:creator><dc:creator>Junwei Han</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02705-y</prism:doi><description>Knowledge distillation, aiming to improve a compact student model using supervision from another cumbersome teacher model, has been a quite prevalent technique for model compression on various computer vision tasks. Existing methods mainly adopt a one-to-one knowledge transfer, where the student model will be forced to achieve a specific result provided by the teacher model. However, the performance of this training paradigm will deteriorate as the model capacity gap expands, since high-level teacher knowledge is too abstract and difficult to understand for the student models with low capacity. Based on this, we propose a novel feature-based Knowledge distillation framework dubbed ReKD, which can provide the student model with multiple choices in feature distillation, thereby relaxing the alignment process in knowledge transfer. Specifically, we transform the teacher features into latent variables through variational inference, with the posterior following Gaussian distribution. It renders the feature knowledge into a region instead of a specific point in the distillation space, which enables the student features to select suitable distillation targets from learned distribution adaptively. Furthermore, to ensure the high quality of latent variables, we utilize the student features as prior to reversely regularize the posterior inspired by mutual learning. Experimental results on three typical visual recognition datasets i.e., CIFAR-100, ImageNet-1K, and MS-COCO, have significantly demonstrated the superiority of our proposed method.
Published: 2026-02-04T06:42:54+00:00
Venue: International Journal of Computer Vision
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheng Qu; Xiwen Yao; Xuguang Yang; Jie Tang; Lang Li; Gong Cheng; Junwei Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02705-y"&gt;10.1007/s11263-025-02705-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation, aiming to improve a compact student model using supervision from another cumbersome teacher model, has been a quite prevalent technique for model compression on various computer vision tasks. Existing methods mainly adopt a one-to-one knowledge transfer, where the student model will be forced to achieve a specific result provided by the teacher model. However, the performance of this training paradigm will deteriorate as the model capacity gap expands, since high-level teacher knowledge is too abstract and difficult to understand for the student models with low capacity. Based on this, we propose a novel feature-based Knowledge distillation framework dubbed ReKD, which can provide the student model with multiple choices in feature distillation, thereby relaxing the alignment process in knowledge transfer. Specifically, we transform the teacher features into latent variables through variational inference, with the posterior following Gaussian distribution. It renders the feature knowledge into a region instead of a specific point in the distillation space, which enables the student features to select suitable distillation targets from learned distribution adaptively. Furthermore, to ensure the high quality of latent variables, we utilize the student features as prior to reversely regularize the posterior inspired by mutual learning. Experimental results on three typical visual recognition datasets i.e., CIFAR-100, ImageNet-1K, and MS-COCO, have significantly demonstrated the superiority of our proposed method.&lt;/p&gt;</content:encoded></item><item><title>SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?</title><link>https://arxiv.org/abs/2602.02765v1</link><guid>http://arxiv.org/abs/2602.02765v1</guid><pubDate>Mon, 02 Feb 2026 20:17:34 +0000</pubDate><dc:creator>Haruhiko Murata</dc:creator><dc:creator>Kazuhiro Hotta</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.
Published: 2026-02-02T20:17:34+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haruhiko Murata; Kazuhiro Hotta&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.&lt;/p&gt;</content:encoded></item><item><title>FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion</title><link>https://arxiv.org/abs/2602.03137v1</link><guid>http://arxiv.org/abs/2602.03137v1</guid><pubDate>Tue, 03 Feb 2026 05:45:22 +0000</pubDate><dc:creator>Chen-Bin Feng</dc:creator><dc:creator>Youyang Sha</dc:creator><dc:creator>Longfei Liu</dc:creator><dc:creator>Yongjun Yu</dc:creator><dc:creator>Chi Man Vong</dc:creator><dc:creator>Xuanlong Yu</dc:creator><dc:creator>Xi Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.
Published: 2026-02-03T05:45:22+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen-Bin Feng; Youyang Sha; Longfei Liu; Yongjun Yu; Chi Man Vong; Xuanlong Yu; Xi Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.&lt;/p&gt;</content:encoded></item><item><title>ATARS: Adaptive Task-Aware Feature Learning for Few-Shot Fine-Grained Classification</title><link>https://doi.org/10.1016/j.knosys.2026.115485</link><guid>10.1016/j.knosys.2026.115485</guid><pubDate>Wed, 04 Feb 2026 07:59:18 +0000</pubDate><dc:creator>Xiaomei Long</dc:creator><dc:creator>Xinyue Wang</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Zongbo He</dc:creator><dc:creator>Qian He</dc:creator><dc:creator>Xiangdong Chen</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115485</prism:doi><description>Few-shot fine-grained classification is challenging due to subtle inter-class differences and limited annotations. Existing methods often fail to fully exploit task-level information, limiting adaptation to scarce samples. We present ATARS, a task-aware framework that organizes alignment, feature reconstruction, and task-conditioned channel selection into a coordinated pipeline. These components progressively refine task-adaptive feature representations, enhancing intra-class consistency and discriminative capacity. Extensive experiments on five fine-grained benchmarks demonstrate the effectiveness of this design: ATARS achieves 5-way 5-shot accuracies of 97.38% on Cars, 94.40% on CUB, and 89.78% on Dogs, consistently outperforming previous reconstruction-based and task-aware approaches. The results highlight the benefits of coordinated component design under task-aware guidance in few-shot scenarios. The source code is available at: https://github.com/lxm-hjk/ATARS-FSL .
Published: 2026-02-04T07:59:18+00:00
Venue: Knowledge-Based Systems
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaomei Long; Xinyue Wang; Cheng Yang; Zongbo He; Qian He; Xiangdong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115485"&gt;10.1016/j.knosys.2026.115485&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot fine-grained classification is challenging due to subtle inter-class differences and limited annotations. Existing methods often fail to fully exploit task-level information, limiting adaptation to scarce samples. We present ATARS, a task-aware framework that organizes alignment, feature reconstruction, and task-conditioned channel selection into a coordinated pipeline. These components progressively refine task-adaptive feature representations, enhancing intra-class consistency and discriminative capacity. Extensive experiments on five fine-grained benchmarks demonstrate the effectiveness of this design: ATARS achieves 5-way 5-shot accuracies of 97.38% on Cars, 94.40% on CUB, and 89.78% on Dogs, consistently outperforming previous reconstruction-based and task-aware approaches. The results highlight the benefits of coordinated component design under task-aware guidance in few-shot scenarios. The source code is available at: https://github.com/lxm-hjk/ATARS-FSL .&lt;/p&gt;</content:encoded></item><item><title>SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation</title><link>https://arxiv.org/abs/2602.04712v1</link><guid>http://arxiv.org/abs/2602.04712v1</guid><pubDate>Wed, 04 Feb 2026 16:23:16 +0000</pubDate><dc:creator>David F. Ramirez</dc:creator><dc:creator>Tim Overman</dc:creator><dc:creator>Kristen Jaskie</dc:creator><dc:creator>Joe Marvin</dc:creator><dc:creator>Andreas Spanias</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.
Published: 2026-02-04T16:23:16+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David F. Ramirez; Tim Overman; Kristen Jaskie; Joe Marvin; Andreas Spanias&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.&lt;/p&gt;</content:encoded></item><item><title>A Triple-Branch Architecture with Multi-Scale Attention for Spatiotemporal Remote Sensing Fusion</title><link>https://doi.org/10.1109/tgrs.2026.3660753</link><guid>10.1109/tgrs.2026.3660753</guid><pubDate>Tue, 03 Feb 2026 20:54:52 +0000</pubDate><dc:creator>Yawen Bai</dc:creator><dc:creator>Weisheng Li</dc:creator><dc:creator>Yidong Peng</dc:creator><dc:creator>Yusha Liu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3660753</prism:doi><description>Existing remote sensing spatiotemporal fusion methods often suffer from excessive smoothing at object boundaries, insufficient modeling of global contextual information, and underutilization of temporal change features. To improve these issues, a triple-branch convolutional neural network is proposed, consisting of a dual-stream spatial network and a temporal difference network.Within the spatial branches, a spatiotemporal adaptive modulation (STAM) module is integrated, in which spatial attention is combined with squeeze-and-excitation (SE) channel attention to enhance feature discrimination. In the temporal branch, lightweight depthwise separable convolutions are employed to efficiently refine temporal difference features. Furthermore, a composite loss function is designed to simultaneously optimize pixel-level accuracy, structural fidelity, and edge detail restoration. Experiments on the CIA, LGC, and AHB datasets demonstrate that the proposed method achieves marked improvements over current models in reconstructing high-frequency edges and low-frequency structures, maintaining global semantic consistency, and suppressing dynamic change noise, with an average increase of 1.46 dB in PSNR and an average decrease of 0.15 in ERGAS.
Published: 2026-02-03T20:54:52+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yawen Bai; Weisheng Li; Yidong Peng; Yusha Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3660753"&gt;10.1109/tgrs.2026.3660753&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Existing remote sensing spatiotemporal fusion methods often suffer from excessive smoothing at object boundaries, insufficient modeling of global contextual information, and underutilization of temporal change features. To improve these issues, a triple-branch convolutional neural network is proposed, consisting of a dual-stream spatial network and a temporal difference network.Within the spatial branches, a spatiotemporal adaptive modulation (STAM) module is integrated, in which spatial attention is combined with squeeze-and-excitation (SE) channel attention to enhance feature discrimination. In the temporal branch, lightweight depthwise separable convolutions are employed to efficiently refine temporal difference features. Furthermore, a composite loss function is designed to simultaneously optimize pixel-level accuracy, structural fidelity, and edge detail restoration. Experiments on the CIA, LGC, and AHB datasets demonstrate that the proposed method achieves marked improvements over current models in reconstructing high-frequency edges and low-frequency structures, maintaining global semantic consistency, and suppressing dynamic change noise, with an average increase of 1.46 dB in PSNR and an average decrease of 0.15 in ERGAS.&lt;/p&gt;</content:encoded></item><item><title>Graph-guided Cross-image Correlation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual Representation</title><link>https://doi.org/10.1016/j.inffus.2026.104204</link><guid>10.1016/j.inffus.2026.104204</guid><pubDate>Wed, 04 Feb 2026 00:27:18 +0000</pubDate><dc:creator>Hongxing You</dc:creator><dc:creator>Yangtao Wang</dc:creator><dc:creator>Xiaocui Li</dc:creator><dc:creator>Yanzhao Xie</dc:creator><dc:creator>Da Chen</dc:creator><dc:creator>Xinyu Zhang</dc:creator><dc:creator>Wensheng Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104204</prism:doi><description>Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .
Published: 2026-02-04T00:27:18+00:00
Venue: Information Fusion
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongxing You; Yangtao Wang; Xiaocui Li; Yanzhao Xie; Da Chen; Xinyu Zhang; Wensheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104204"&gt;10.1016/j.inffus.2026.104204&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .&lt;/p&gt;</content:encoded></item><item><title>How Well Do Vision Models Understand Tasks With Multiple Labels?</title><link>https://doi.org/10.1016/j.eswa.2026.131479</link><guid>10.1016/j.eswa.2026.131479</guid><pubDate>Tue, 03 Feb 2026 00:32:48 +0000</pubDate><dc:creator>Yunus Can Bilge</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131479</prism:doi><description>The increasing availability of pre-trained vision backbones has significantly advanced multi-label image classification, yet the comparative transferability and generalization behavior of these models across diverse target domains remain underexplored. In this study, we present a comprehensive empirical analysis of 80 pre-trained backbones, evaluated in a consistent setting across five benchmark datasets: MS-COCO, NUS-WIDE, CelebA, PA-100K, and MS-COCO-2012. While the architectures and benchmarks used in our study are established, our work provides the first large-scale, standardized analysis of backbone transferability in multi-label settings, offering practical insights and reproducible tools that are currently lacking in the literature and remain highly relevant for real-world deployment and benchmarking. Using a standardized multi-label image classification framework and seven evaluation metrics, we systematically assess the performance, robustness, and efficiency of each model. We investigate the influence of object scale, dataset diversity and size, classifier depth, and relationship between evaluation metrics, and evaluate the alignment of them. We further observe that accuracy and recall metrics are strongly aligned, while instance-level precision behaves more independently, suggesting the need for a measure for backbone selection. To support it, we introduce TAME and TAME eff , composite scoring strategies that account for predictive performance and model efficiency. Our findings provide actionable insights and a composite metric and efficiency analysis to guide backbone selection in multi-label settings in real-world and resource-constrained multi-label applications. All model outputs, evaluation scripts, and diagnostics will be publicly available to support reproducibility and further research.
Published: 2026-02-03T00:32:48+00:00
Venue: Expert Systems with Applications
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunus Can Bilge&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131479"&gt;10.1016/j.eswa.2026.131479&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;The increasing availability of pre-trained vision backbones has significantly advanced multi-label image classification, yet the comparative transferability and generalization behavior of these models across diverse target domains remain underexplored. In this study, we present a comprehensive empirical analysis of 80 pre-trained backbones, evaluated in a consistent setting across five benchmark datasets: MS-COCO, NUS-WIDE, CelebA, PA-100K, and MS-COCO-2012. While the architectures and benchmarks used in our study are established, our work provides the first large-scale, standardized analysis of backbone transferability in multi-label settings, offering practical insights and reproducible tools that are currently lacking in the literature and remain highly relevant for real-world deployment and benchmarking. Using a standardized multi-label image classification framework and seven evaluation metrics, we systematically assess the performance, robustness, and efficiency of each model. We investigate the influence of object scale, dataset diversity and size, classifier depth, and relationship between evaluation metrics, and evaluate the alignment of them. We further observe that accuracy and recall metrics are strongly aligned, while instance-level precision behaves more independently, suggesting the need for a measure for backbone selection. To support it, we introduce TAME and TAME eff , composite scoring strategies that account for predictive performance and model efficiency. Our findings provide actionable insights and a composite metric and efficiency analysis to guide backbone selection in multi-label settings in real-world and resource-constrained multi-label applications. All model outputs, evaluation scripts, and diagnostics will be publicly available to support reproducibility and further research.&lt;/p&gt;</content:encoded></item><item><title>Boundary-Aware and Multi-Angle Modeling-Based Object Tracking in Polarimetric Images</title><link>https://doi.org/10.1016/j.knosys.2026.115442</link><guid>10.1016/j.knosys.2026.115442</guid><pubDate>Tue, 03 Feb 2026 15:58:07 +0000</pubDate><dc:creator>Qiaohui Wang</dc:creator><dc:creator>Fan Shi</dc:creator><dc:creator>Mianzhao Wang</dc:creator><dc:creator>Xinbo Geng</dc:creator><dc:creator>Meng Zhao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115442</prism:doi><description>Object tracking is a fundamental task in computer vision with applications ranging from surveillance to autonomous driving. Although RGB-based tracking methods have seen significant advancements by leveraging color and texture features, they often struggle under challenging conditions such as low light, occlusions, and fast motion. Polarimetric imaging, which encodes surface properties, material characteristics, and geometric structures, offers unique advantages as a complementary modality. However, its potential remains underexplored due to the lack of large-scale datasets and specialized algorithms designed for polarization-specific features. To address this gap, we introduce POL, the first large-scale benchmark dataset for polarimetric vision that enables comprehensive evaluations under diverse conditions. Building on this dataset, we propose PMTT, a cross-modal transformer framework that integrates polarimetric and RGB data. The Detailed Feature Prompter (DFP) module extracts boundary and multi-angle features from polarimetric images, while the Spatial-Channel Attention (SCA) mechanism enhances feature recognition in complex environments. Extensive experiments confirm that PMTT superior performance and robustness, highlighting the transformative potential of polarimetric imaging for dynamic object tracking.
Published: 2026-02-03T15:58:07+00:00
Venue: Knowledge-Based Systems
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiaohui Wang; Fan Shi; Mianzhao Wang; Xinbo Geng; Meng Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115442"&gt;10.1016/j.knosys.2026.115442&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Object tracking is a fundamental task in computer vision with applications ranging from surveillance to autonomous driving. Although RGB-based tracking methods have seen significant advancements by leveraging color and texture features, they often struggle under challenging conditions such as low light, occlusions, and fast motion. Polarimetric imaging, which encodes surface properties, material characteristics, and geometric structures, offers unique advantages as a complementary modality. However, its potential remains underexplored due to the lack of large-scale datasets and specialized algorithms designed for polarization-specific features. To address this gap, we introduce POL, the first large-scale benchmark dataset for polarimetric vision that enables comprehensive evaluations under diverse conditions. Building on this dataset, we propose PMTT, a cross-modal transformer framework that integrates polarimetric and RGB data. The Detailed Feature Prompter (DFP) module extracts boundary and multi-angle features from polarimetric images, while the Spatial-Channel Attention (SCA) mechanism enhances feature recognition in complex environments. Extensive experiments confirm that PMTT superior performance and robustness, highlighting the transformative potential of polarimetric imaging for dynamic object tracking.&lt;/p&gt;</content:encoded></item><item><title>Quantization-Aware Regularizers for Deep Neural Networks Compression</title><link>https://arxiv.org/abs/2602.03614v1</link><guid>http://arxiv.org/abs/2602.03614v1</guid><pubDate>Tue, 03 Feb 2026 15:07:43 +0000</pubDate><dc:creator>Dario Malchiodi</dc:creator><dc:creator>Mattia Ferraretto</dc:creator><dc:creator>Marco Frasca</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.
Published: 2026-02-03T15:07:43+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dario Malchiodi; Mattia Ferraretto; Marco Frasca&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.&lt;/p&gt;</content:encoded></item><item><title>SuperCL: Superpixel Guided Contrastive Learning for Medical Image Segmentation Pre-training</title><link>https://doi.org/10.1109/tip.2026.3657233</link><guid>10.1109/tip.2026.3657233</guid><pubDate>Tue, 03 Feb 2026 20:57:02 +0000</pubDate><dc:creator>Shuang Zeng</dc:creator><dc:creator>Lei Zhu</dc:creator><dc:creator>Xinliang Zhang</dc:creator><dc:creator>Hangzhou He</dc:creator><dc:creator>Yanye Lu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657233</prism:doi><description>Medical image segmentation is a critical yet challenging task, primarily due to the difficulty of obtaining extensive datasets of high-quality, expert-annotated images. Contrastive learning presents a potential but still problematic solution to this issue. Because most existing methods focus on extracting instance-level or pixel-to-pixel representation, which ignores the characteristics between intra-image similar pixel groups. Moreover, when considering contrastive pairs generation, most SOTA methods mainly rely on manually setting thresholds, which requires a large number of gradient experiments and lacks efficiency and generalization. To address these issues, we propose a novel contrastive learning approach named SuperCL for medical image segmentation pre-training. Specifically, our SuperCL exploits the structural prior and pixel correlation of images by introducing two novel contrastive pairs generation strategies: Intra-image Local Contrastive Pairs (ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation. Considering superpixel cluster aligns well with the concept of contrastive pairs generation, we utilize the superpixel map to generate pseudo masks for both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also propose two modules named Average SuperPixel Feature Map Generation (ASP) and Connected Components Label Generation (CCL) to better exploit the prior structural information for IGCP. Finally, experiments on 8 medical image datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL achieves a superior performance with more precise predictions from visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best results on MMWHS, CHAOS, Spleen with 10% annotations. Our code is released at https://github.com/stevezs315/SuperCL.
Published: 2026-02-03T20:57:02+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuang Zeng; Lei Zhu; Xinliang Zhang; Hangzhou He; Yanye Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657233"&gt;10.1109/tip.2026.3657233&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Medical image segmentation is a critical yet challenging task, primarily due to the difficulty of obtaining extensive datasets of high-quality, expert-annotated images. Contrastive learning presents a potential but still problematic solution to this issue. Because most existing methods focus on extracting instance-level or pixel-to-pixel representation, which ignores the characteristics between intra-image similar pixel groups. Moreover, when considering contrastive pairs generation, most SOTA methods mainly rely on manually setting thresholds, which requires a large number of gradient experiments and lacks efficiency and generalization. To address these issues, we propose a novel contrastive learning approach named SuperCL for medical image segmentation pre-training. Specifically, our SuperCL exploits the structural prior and pixel correlation of images by introducing two novel contrastive pairs generation strategies: Intra-image Local Contrastive Pairs (ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation. Considering superpixel cluster aligns well with the concept of contrastive pairs generation, we utilize the superpixel map to generate pseudo masks for both ILCP and IGCP to guide supervised contrastive learning. Moreover, we also propose two modules named Average SuperPixel Feature Map Generation (ASP) and Connected Components Label Generation (CCL) to better exploit the prior structural information for IGCP. Finally, experiments on 8 medical image datasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL achieves a superior performance with more precise predictions from visualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best results on MMWHS, CHAOS, Spleen with 10% annotations. Our code is released at https://github.com/stevezs315/SuperCL.&lt;/p&gt;</content:encoded></item><item><title>Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images</title><link>https://arxiv.org/abs/2602.01954v1</link><guid>http://arxiv.org/abs/2602.01954v1</guid><pubDate>Mon, 02 Feb 2026 11:03:01 +0000</pubDate><dc:creator>Shuai Yang</dc:creator><dc:creator>Ziyue Huang</dc:creator><dc:creator>Jiaxin Chen</dc:creator><dc:creator>Qingjie Liu</dc:creator><dc:creator>Yunhong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.
Published: 2026-02-02T11:03:01+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Yang; Ziyue Huang; Jiaxin Chen; Qingjie Liu; Yunhong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.&lt;/p&gt;</content:encoded></item><item><title>DEEP: Decoupled Semantic Prompt Learning, Guiding and Embedding for Multi-Spectral Object Re-Identification</title><link>https://doi.org/10.1109/tmm.2026.3660160</link><guid>10.1109/tmm.2026.3660160</guid><pubDate>Tue, 03 Feb 2026 20:55:41 +0000</pubDate><dc:creator>Shihao Li</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Aihua Zheng</dc:creator><dc:creator>Jin Tang</dc:creator><dc:creator>Bin Luo</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660160</prism:doi><description>Multi-spectral object re-identification (ReID) captures diverse object semantics to robustly recognize identity in complex environments. However, without explicit semantic guidance (e.g., attributes, masks, and keypoints), existing modal fusion-based methods struggle to comprehensively capture person or vehicle semantics across spectra. Thanks to the large-scale vision-language pre-training, CLIP effectively aligns visual concepts across different image modalities to a unified semantic prompt. In this paper, we propose DEEP, a DEcoupled sEmantic Prompt Learning, Guiding and Embedding framework for Multi-Spectral Object ReID. Specifically, to address the challenges posed by low-quality modality noise and spectral style discrepancies, we first propose a Decoupled Semantic Prompt (DSP) strategy, which explicitly decouples the semantic alignment into spectral-style learning with spectral-shared prompts and object content learning with instance-specific inversion token. Second, to lead the model focusing on semantically faithful regions, we propose a Semantic-Guided Spectral Fusion (SGSF) module that builds a semantic interaction bridge between spectra to explore complementary semantics across modalities. Finally, to further empower the spectral representation, we propose a Spectral Semantic Embedding (SSE) module constrained by semantic-aware structural consistency to refine the fine-grained identity semantics in each spectrum. Extensive experiments on five public benchmarks, RGBNT201, Market-MM, MSVR310, WMVEID863, and RGBNT100, demonstrate the proposed method outperforms the state-of-the-art methods. The source code is released at this link: https://github.com/lsh-ahu/DEEP-ReID.
Published: 2026-02-03T20:55:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shihao Li; Chenglong Li; Aihua Zheng; Jin Tang; Bin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660160"&gt;10.1109/tmm.2026.3660160&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-spectral object re-identification (ReID) captures diverse object semantics to robustly recognize identity in complex environments. However, without explicit semantic guidance (e.g., attributes, masks, and keypoints), existing modal fusion-based methods struggle to comprehensively capture person or vehicle semantics across spectra. Thanks to the large-scale vision-language pre-training, CLIP effectively aligns visual concepts across different image modalities to a unified semantic prompt. In this paper, we propose DEEP, a DEcoupled sEmantic Prompt Learning, Guiding and Embedding framework for Multi-Spectral Object ReID. Specifically, to address the challenges posed by low-quality modality noise and spectral style discrepancies, we first propose a Decoupled Semantic Prompt (DSP) strategy, which explicitly decouples the semantic alignment into spectral-style learning with spectral-shared prompts and object content learning with instance-specific inversion token. Second, to lead the model focusing on semantically faithful regions, we propose a Semantic-Guided Spectral Fusion (SGSF) module that builds a semantic interaction bridge between spectra to explore complementary semantics across modalities. Finally, to further empower the spectral representation, we propose a Spectral Semantic Embedding (SSE) module constrained by semantic-aware structural consistency to refine the fine-grained identity semantics in each spectrum. Extensive experiments on five public benchmarks, RGBNT201, Market-MM, MSVR310, WMVEID863, and RGBNT100, demonstrate the proposed method outperforms the state-of-the-art methods. The source code is released at this link: https://github.com/lsh-ahu/DEEP-ReID.&lt;/p&gt;</content:encoded></item><item><title>Towards Real-world Holistic Privacy-Preserving Person Re-identification</title><link>https://doi.org/10.1109/tpami.2026.3660922</link><guid>10.1109/tpami.2026.3660922</guid><pubDate>Tue, 03 Feb 2026 20:54:47 +0000</pubDate><dc:creator>Qianxiang Meng</dc:creator><dc:creator>He Li</dc:creator><dc:creator>Min Cao</dc:creator><dc:creator>Mang Ye</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3660922</prism:doi><description>Real-world person re-identification (Re-ID) systems are susceptible to malicious attacks, leading to the leakage of pedestrian images and the Re-ID model, posing severe threats to the privacy of both system owners and pedestrians. Existing privacy-preserving person re-identification (PPPR) methods fail to simultaneously resist data leakage, model leakage, and data &amp; model leakage while compromising the normal functionality of Re-ID systems. In this paper, we begin with an in-depth analysis of prior methodologies and identify the gap between existing works and the ideal PPPR paradigm. Inspired by the concept of 'Let the invisible perturbation become the system trigger', we propose SHIELD, a pioneering and comprehensive two-stage privacy-preserving framework. To resist data leakage, we propose a self-supervised method for Protected Dataset Generation in the first stage, which obviates the dependence on identity labels and ensures image quality. To resist model leakage without compromising the normal retrieval accuracy, we propose Original Feature Deconstruction and Protected Feature Alignment to train the system model with paired protected and original images. Extensive experiments substantiate that SHIELD significantly outperforms existing PPPR methods, offering robust and holistic protection for Re-ID systems while maintaining decent retrieval accuracy for authorized users. The code will be released soon.
Published: 2026-02-03T20:54:47+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qianxiang Meng; He Li; Min Cao; Mang Ye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3660922"&gt;10.1109/tpami.2026.3660922&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Real-world person re-identification (Re-ID) systems are susceptible to malicious attacks, leading to the leakage of pedestrian images and the Re-ID model, posing severe threats to the privacy of both system owners and pedestrians. Existing privacy-preserving person re-identification (PPPR) methods fail to simultaneously resist data leakage, model leakage, and data &amp;amp; model leakage while compromising the normal functionality of Re-ID systems. In this paper, we begin with an in-depth analysis of prior methodologies and identify the gap between existing works and the ideal PPPR paradigm. Inspired by the concept of &amp;#x27;Let the invisible perturbation become the system trigger&amp;#x27;, we propose SHIELD, a pioneering and comprehensive two-stage privacy-preserving framework. To resist data leakage, we propose a self-supervised method for Protected Dataset Generation in the first stage, which obviates the dependence on identity labels and ensures image quality. To resist model leakage without compromising the normal retrieval accuracy, we propose Original Feature Deconstruction and Protected Feature Alignment to train the system model with paired protected and original images. Extensive experiments substantiate that SHIELD significantly outperforms existing PPPR methods, offering robust and holistic protection for Re-ID systems while maintaining decent retrieval accuracy for authorized users. The code will be released soon.&lt;/p&gt;</content:encoded></item><item><title>OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth</title><link>https://arxiv.org/abs/2602.01268v1</link><guid>http://arxiv.org/abs/2602.01268v1</guid><pubDate>Sun, 01 Feb 2026 14:57:33 +0000</pubDate><dc:creator>Jaehyeon Cho</dc:creator><dc:creator>Jhonghyun An</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.
Published: 2026-02-01T14:57:33+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jaehyeon Cho; Jhonghyun An&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.&lt;/p&gt;</content:encoded></item></channel></rss>