<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 30 Nov 2025 02:50:11 +0000</lastBuildDate><item><title>GLUE3D: General Language Understanding Evaluation for 3D Point Clouds</title><link>https://doi.org/10.1016/j.inffus.2025.104007</link><guid>10.1016/j.inffus.2025.104007</guid><pubDate>Sat, 29 Nov 2025 04:50:54 +0000</pubDate><dc:creator>Giorgio Mariani</dc:creator><dc:creator>Alessandro Raganato</dc:creator><dc:creator>Simone Melzi</dc:creator><dc:creator>Gabriella Pasi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104007</prism:doi><description>Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.
Published: 2025-11-29T04:50:54+00:00
Venue: Information Fusion
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Giorgio Mariani; Alessandro Raganato; Simone Melzi; Gabriella Pasi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104007"&gt;10.1016/j.inffus.2025.104007&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models have achieved impressive results on text and image benchmarks, yet their capacity to ground language in 3D geometry is still largely unexplored. Existing 3D evaluations are either confined to specialised domains, such as indoor scans, or hampered by poor texture fidelity, and none allow a fair, modality-aligned comparison with the 2D counterparts. Without a rigorous benchmark, it remains unclear whether current 3D-aware models genuinely grasp shape, colour, pose, and quantity, or merely echo memorised textual priors.&lt;/p&gt;</content:encoded></item><item><title>The Duality of Generative AI and Reinforcement Learning in Robotics: A Review</title><link>https://doi.org/10.1016/j.inffus.2025.104003</link><guid>10.1016/j.inffus.2025.104003</guid><pubDate>Sat, 29 Nov 2025 16:05:06 +0000</pubDate><dc:creator>Angelo Moroncelli</dc:creator><dc:creator>Vishal Soni</dc:creator><dc:creator>Marco Forgione</dc:creator><dc:creator>Dario Piga</dc:creator><dc:creator>Blerina Spahiu</dc:creator><dc:creator>Loris Roveda</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104003</prism:doi><description>Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.
Published: 2025-11-29T16:05:06+00:00
Venue: Information Fusion
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Angelo Moroncelli; Vishal Soni; Marco Forgione; Dario Piga; Blerina Spahiu; Loris Roveda&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104003"&gt;10.1016/j.inffus.2025.104003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.&lt;/p&gt;</content:encoded></item><item><title>M3FNet: Multi-modal multi-temporal multi-scale data fusion network for tree species composition mapping</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.026</link><guid>10.1016/j.isprsjprs.2025.11.026</guid><pubDate>Sat, 29 Nov 2025 05:22:57 +0000</pubDate><dc:creator>Yuwei Cao</dc:creator><dc:creator>Nicholas C. Coops</dc:creator><dc:creator>Brent A. Murray</dc:creator><dc:creator>Ian Sinclair</dc:creator><dc:creator>Robere-McGugan Geordie</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.026</prism:doi><description>Accurate estimation and mapping of t ree s pecies c omposition (TSC) is crucial for sustainable forest management. Recent advances in Light Detection and Ranging (lidar) technology and the availability of moderate spatial resolution, surface reflectance time series passive optical imagery offer scalable and efficient approaches for automated TSC estimation. In this research we develop a novel deep learning framework, M3F-Net (Multi-modal, Multi-temporal, and Multi-scale Fusion Network), that integrates multi-temporal Sentinel-2 (S2) imagery and single photon lidar (SPL) data to estimate TSC for nine common species across the 630,000-hectare Romeo Malette Forest in Ontario, Canada. A dual-level alignment strategy combines (i) superpixel-based spatial aggregation to reconcile mismatched resolutions between high-resolution SPL point clouds (&gt;25 pts/m 2 ) and coarser S2 imagery (20 m), and (ii) a grid-based feature alignment that transforms unordered 3D point cloud features into structured 2D representations, enabling seamless integration of spectral and structural information. Within this aligned space, a multi-level Mamba-Fusion module jointly models multi-scale spatial patterns and seasonal dynamics through selective state-space modelling, efficiently capturing long-range dependencies while filtering redundant information. The framework achieves an R 2 score of 0.676, outperforming existing point cloud-based methods by 6% in TSC estimation. For leading species classification, our results are 6% better in terms of weighted F1, using either the TSC-based method or the standalone leading species classification method. Addition of seasonal S2 imagery added a 10% R 2 gain compared to the SPL-only mode. These results underscore the potential of fusing multi-modal and multi-temporal data with deep learning for scalable, high-accurate TSC estimation, offering a robust tool for large-scale management applications.
Published: 2025-11-29T05:22:57+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuwei Cao; Nicholas C. Coops; Brent A. Murray; Ian Sinclair; Robere-McGugan Geordie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.026"&gt;10.1016/j.isprsjprs.2025.11.026&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate estimation and mapping of t ree s pecies c omposition (TSC) is crucial for sustainable forest management. Recent advances in Light Detection and Ranging (lidar) technology and the availability of moderate spatial resolution, surface reflectance time series passive optical imagery offer scalable and efficient approaches for automated TSC estimation. In this research we develop a novel deep learning framework, M3F-Net (Multi-modal, Multi-temporal, and Multi-scale Fusion Network), that integrates multi-temporal Sentinel-2 (S2) imagery and single photon lidar (SPL) data to estimate TSC for nine common species across the 630,000-hectare Romeo Malette Forest in Ontario, Canada. A dual-level alignment strategy combines (i) superpixel-based spatial aggregation to reconcile mismatched resolutions between high-resolution SPL point clouds (&amp;gt;25 pts/m 2 ) and coarser S2 imagery (20 m), and (ii) a grid-based feature alignment that transforms unordered 3D point cloud features into structured 2D representations, enabling seamless integration of spectral and structural information. Within this aligned space, a multi-level Mamba-Fusion module jointly models multi-scale spatial patterns and seasonal dynamics through selective state-space modelling, efficiently capturing long-range dependencies while filtering redundant information. The framework achieves an R 2 score of 0.676, outperforming existing point cloud-based methods by 6% in TSC estimation. For leading species classification, our results are 6% better in terms of weighted F1, using either the TSC-based method or the standalone leading species classification method. Addition of seasonal S2 imagery added a 10% R 2 gain compared to the SPL-only mode. These results underscore the potential of fusing multi-modal and multi-temporal data with deep learning for scalable, high-accurate TSC estimation, offering a robust tool for large-scale management applications.&lt;/p&gt;</content:encoded></item><item><title>ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images</title><link>https://arxiv.org/abs/2511.21606v1</link><guid>http://arxiv.org/abs/2511.21606v1</guid><pubDate>Wed, 26 Nov 2025 17:26:00 +0000</pubDate><dc:creator>M. Naseer Subhani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.
Published: 2025-11-26T17:26:00+00:00
Venue: arXiv
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; M. Naseer Subhani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM&amp;#x27;s segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>Label Noise Learning Based SAR Target Classification Method</title><link>https://doi.org/10.1016/j.neunet.2025.108373</link><guid>10.1016/j.neunet.2025.108373</guid><pubDate>Sat, 29 Nov 2025 23:41:11 +0000</pubDate><dc:creator>Hongqiang Wang</dc:creator><dc:creator>Yuqing Lan</dc:creator><dc:creator>Fuzhan Yue</dc:creator><dc:creator>Zhenghuan Xia</dc:creator><dc:creator>Tao Zhang</dc:creator><dc:creator>Yue Pang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108373</prism:doi><description>The recognition of Synthetic Aperture Radar (SAR) Target is a critical task in SAR image interpretation. With their exceptional capacity to model complex data structures, Convolutional Neural Networks(CNNs) are now the standard architecture for addressing SAR image classification problems. However, these methods typically require large-scale labeled datasets for training. SAR images are inherently susceptible to both feature and label noise due to the technical sophistication of the imaging process and the high likelihood of human error during annotation. This often leads to a significant degradation in the performance of CNN-based classifiers. To mitigate feature noise, we propose a dynamic L p -norm regularization-based scattering feature extraction method that leverages neural networks to automatically estimate and adapt the regularization parameters at each layer. To address label noise, we further develop a robust representation learning framework for SAR target classification, which enhances model robustness by minimizing the distances between samples and their corresponding class prototypes. Extensive experiments conducted on three widely-used SAR datasets — MSTAR, SAR-ACD, and FUSAR — show that the proposed method consistently achieves robust classification accuracy across label noise levels from 0% to 60%, significantly mitigating the adverse effects of annotation inaccuracies.
Published: 2025-11-29T23:41:11+00:00
Venue: Neural Networks
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongqiang Wang; Yuqing Lan; Fuzhan Yue; Zhenghuan Xia; Tao Zhang; Yue Pang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108373"&gt;10.1016/j.neunet.2025.108373&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;The recognition of Synthetic Aperture Radar (SAR) Target is a critical task in SAR image interpretation. With their exceptional capacity to model complex data structures, Convolutional Neural Networks(CNNs) are now the standard architecture for addressing SAR image classification problems. However, these methods typically require large-scale labeled datasets for training. SAR images are inherently susceptible to both feature and label noise due to the technical sophistication of the imaging process and the high likelihood of human error during annotation. This often leads to a significant degradation in the performance of CNN-based classifiers. To mitigate feature noise, we propose a dynamic L p -norm regularization-based scattering feature extraction method that leverages neural networks to automatically estimate and adapt the regularization parameters at each layer. To address label noise, we further develop a robust representation learning framework for SAR target classification, which enhances model robustness by minimizing the distances between samples and their corresponding class prototypes. Extensive experiments conducted on three widely-used SAR datasets — MSTAR, SAR-ACD, and FUSAR — show that the proposed method consistently achieves robust classification accuracy across label noise levels from 0% to 60%, significantly mitigating the adverse effects of annotation inaccuracies.&lt;/p&gt;</content:encoded></item><item><title>HTTM: Head-wise Temporal Token Merging for Faster VGGT</title><link>https://arxiv.org/abs/2511.21317v1</link><guid>http://arxiv.org/abs/2511.21317v1</guid><pubDate>Wed, 26 Nov 2025 12:04:03 +0000</pubDate><dc:creator>Weitian Wang</dc:creator><dc:creator>Lukas Meiner</dc:creator><dc:creator>Rai Shubham</dc:creator><dc:creator>Cecilia De La Parra</dc:creator><dc:creator>Akash Kumar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.
Published: 2025-11-26T12:04:03+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weitian Wang; Lukas Meiner; Rai Shubham; Cecilia De La Parra; Akash Kumar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers&amp;#x27; output, which hinders the model&amp;#x27;s representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.&lt;/p&gt;</content:encoded></item><item><title>DivineTree: All-in-One 3D Tree Modeling with Diverse and Fused Visual Guidance</title><link>https://doi.org/10.1016/j.inffus.2025.104013</link><guid>10.1016/j.inffus.2025.104013</guid><pubDate>Sat, 29 Nov 2025 08:10:00 +0000</pubDate><dc:creator>Jiabo Xu</dc:creator><dc:creator>Bo Su</dc:creator><dc:creator>Jingbo Wei</dc:creator><dc:creator>Xiangyun Hu</dc:creator><dc:creator>Hengming Dai</dc:creator><dc:creator>Tao Ke</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104013</prism:doi><description>3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.
Published: 2025-11-29T08:10:00+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiabo Xu; Bo Su; Jingbo Wei; Xiangyun Hu; Hengming Dai; Tao Ke&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104013"&gt;10.1016/j.inffus.2025.104013&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;3D tree modeling is crucial in fields ranging from gaming and film production to environmental science. However, current learning-based methods are typically restricted to a single input modality (e.g., only images or only point clouds), and requiring difficult-to-acquire paired training data for each new input type. To overcome these limitations, we propose DivineTree, a novel method that generates 3D trees from diverse visual guidance-including point clouds (LiDAR or image-matched), images (photos, sketches, paintings), and crown polygons-using a single, unified model, in a zero-shot manner without requiring paired data or retraining. DivineTree consists of two core components: 1) An unconditional diffusion model trained on synthetic data to learn the distribution of 3D tree structures, represented as sequences of 4D line segments. 2) A Point Guidance sampling technique that incorporates diverse visual inputs as spatial constraints during the generative process, guiding the diffusion to produce a 3D tree that matches the input. Extensive experiments demonstrate that our method rapidly generates realistic and geometrically accurate 3D trees. On the challenging 10-forest benchmark for crown-to-tree generation, DivineTree achieves state-of-the-art performance in both geometric accuracy and visual realism. Furthermore, our method enables the fusion of multiple inputs, such as combining both side-view and top-view conditions, to generate 3D tree models that simultaneously satisfy multiple constraints.&lt;/p&gt;</content:encoded></item><item><title>Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models</title><link>https://arxiv.org/abs/2511.21320v1</link><guid>http://arxiv.org/abs/2511.21320v1</guid><pubDate>Wed, 26 Nov 2025 12:05:44 +0000</pubDate><dc:creator>Heiko Oppel</dc:creator><dc:creator>Andreas Spilz</dc:creator><dc:creator>Michael Munz</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.
Published: 2025-11-26T12:05:44+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Heiko Oppel; Andreas Spilz; Michael Munz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.&lt;/p&gt;</content:encoded></item><item><title>Escaping the Verifier: Learning to Reason via Demonstrations</title><link>https://arxiv.org/abs/2511.21667v1</link><guid>http://arxiv.org/abs/2511.21667v1</guid><pubDate>Wed, 26 Nov 2025 18:42:52 +0000</pubDate><dc:creator>Locke Cai</dc:creator><dc:creator>Ivan Provilkov</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
Published: 2025-11-26T18:42:52+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Locke Cai; Ivan Provilkov&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.&lt;/p&gt;</content:encoded></item><item><title>Seeing without Pixels: Perception from Camera Trajectories</title><link>https://arxiv.org/abs/2511.21681v1</link><guid>http://arxiv.org/abs/2511.21681v1</guid><pubDate>Wed, 26 Nov 2025 18:57:01 +0000</pubDate><dc:creator>Zihui Xue</dc:creator><dc:creator>Kristen Grauman</dc:creator><dc:creator>Dima Damen</dc:creator><dc:creator>Andrew Zisserman</dc:creator><dc:creator>Tengda Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.
Published: 2025-11-26T18:57:01+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihui Xue; Kristen Grauman; Dima Damen; Andrew Zisserman; Tengda Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Can one perceive a video&amp;#x27;s content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, &amp;quot;how you move&amp;quot; can indeed reveal &amp;quot;what you are doing&amp;quot; (egocentric) or &amp;quot;observing&amp;quot; (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.&lt;/p&gt;</content:encoded></item><item><title>OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection</title><link>https://arxiv.org/abs/2511.21064v1</link><guid>http://arxiv.org/abs/2511.21064v1</guid><pubDate>Wed, 26 Nov 2025 05:08:26 +0000</pubDate><dc:creator>Chujie Wang</dc:creator><dc:creator>Jianyu Lu</dc:creator><dc:creator>Zhiyuan Luo</dc:creator><dc:creator>Xi Chen</dc:creator><dc:creator>Chu He</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.
Published: 2025-11-26T05:08:26+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chujie Wang; Jianyu Lu; Zhiyuan Luo; Xi Chen; Chu He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD&amp;#x27;s lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent&amp;#x27;s state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.&lt;/p&gt;</content:encoded></item><item><title>MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts</title><link>https://arxiv.org/abs/2511.21089v1</link><guid>http://arxiv.org/abs/2511.21089v1</guid><pubDate>Wed, 26 Nov 2025 06:14:26 +0000</pubDate><dc:creator>Ivan Novikov</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1
Published: 2025-11-26T06:14:26+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ivan Novikov&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1&lt;/p&gt;</content:encoded></item><item><title>Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs</title><link>https://arxiv.org/abs/2511.21050v1</link><guid>http://arxiv.org/abs/2511.21050v1</guid><pubDate>Wed, 26 Nov 2025 04:36:34 +0000</pubDate><dc:creator>Dongkyu Derek Cho</dc:creator><dc:creator>Huan Song</dc:creator><dc:creator>Arijit Ghosh Chowdhury</dc:creator><dc:creator>Haotian An</dc:creator><dc:creator>Yawei Wang</dc:creator><dc:creator>Rohit Thekkanal</dc:creator><dc:creator>Negin Sokhandan</dc:creator><dc:creator>Sharlina Keshava</dc:creator><dc:creator>Hannah Marlowe</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.
Published: 2025-11-26T04:36:34+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dongkyu Derek Cho; Huan Song; Arijit Ghosh Chowdhury; Haotian An; Yawei Wang; Rohit Thekkanal; Negin Sokhandan; Sharlina Keshava; Hannah Marlowe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.&lt;/p&gt;</content:encoded></item><item><title>Scaling Foundation Models for Radar Scene Understanding</title><link>https://arxiv.org/abs/2511.21105v1</link><guid>http://arxiv.org/abs/2511.21105v1</guid><pubDate>Wed, 26 Nov 2025 06:41:00 +0000</pubDate><dc:creator>Pushkal Mishra</dc:creator><dc:creator>Kshitiz Bansal</dc:creator><dc:creator>Dinesh Bharadia</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.
Published: 2025-11-26T06:41:00+00:00
Venue: arXiv
Score: 0.789 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pushkal Mishra; Kshitiz Bansal; Dinesh Bharadia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (consider)&lt;/p&gt;
&lt;p&gt;Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.&lt;/p&gt;</content:encoded></item><item><title>The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment</title><link>https://arxiv.org/abs/2511.21331v1</link><guid>http://arxiv.org/abs/2511.21331v1</guid><pubDate>Wed, 26 Nov 2025 12:25:55 +0000</pubDate><dc:creator>Stefanos Koutoupis</dc:creator><dc:creator>Michaela Areti Zervou</dc:creator><dc:creator>Konstantinos Kontras</dc:creator><dc:creator>Maarten De Vos</dc:creator><dc:creator>Panagiotis Tsakalides</dc:creator><dc:creator>Grigorios Tsagatakis</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.
Published: 2025-11-26T12:25:55+00:00
Venue: arXiv
Score: 0.788 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Stefanos Koutoupis; Michaela Areti Zervou; Konstantinos Kontras; Maarten De Vos; Panagiotis Tsakalides; Grigorios Tsagatakis&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (consider)&lt;/p&gt;
&lt;p&gt;Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.&lt;/p&gt;</content:encoded></item><item><title>Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO</title><link>https://arxiv.org/abs/2511.21638v1</link><guid>http://arxiv.org/abs/2511.21638v1</guid><pubDate>Wed, 26 Nov 2025 18:12:16 +0000</pubDate><dc:creator>Daniel R. Jiang</dc:creator><dc:creator>Jalaj Bhandari</dc:creator><dc:creator>Yukai Yang</dc:creator><dc:creator>Rémi Munos</dc:creator><dc:creator>Tyler Lu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.
Published: 2025-11-26T18:12:16+00:00
Venue: arXiv
Score: 0.788 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daniel R. Jiang; Jalaj Bhandari; Yukai Yang; Rémi Munos; Tyler Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (consider)&lt;/p&gt;
&lt;p&gt;Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.&lt;/p&gt;</content:encoded></item><item><title>ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21005v1</link><guid>http://arxiv.org/abs/2511.21005v1</guid><pubDate>Wed, 26 Nov 2025 03:10:15 +0000</pubDate><dc:creator>Jinpeng Wang</dc:creator><dc:creator>Chao Li</dc:creator><dc:creator>Ting Ye</dc:creator><dc:creator>Mengyuan Zhang</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Jian Luan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.
Published: 2025-11-26T03:10:15+00:00
Venue: arXiv
Score: 0.788 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinpeng Wang; Chao Li; Ting Ye; Mengyuan Zhang; Wei Liu; Jian Luan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (consider)&lt;/p&gt;
&lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.&lt;/p&gt;</content:encoded></item><item><title>A duet of perception and reasoning: CLIP and LLM brainstorming for scene text recognition</title><link>https://doi.org/10.1016/j.neucom.2025.132236</link><guid>10.1016/j.neucom.2025.132236</guid><pubDate>Sat, 29 Nov 2025 16:00:38 +0000</pubDate><dc:creator>Zeguang Jia</dc:creator><dc:creator>Jianming Wang</dc:creator><dc:creator>Kehui Song</dc:creator><dc:creator>Zhilan Wang</dc:creator><dc:creator>Xiaohan Ma</dc:creator><dc:creator>Rize Jin</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132236</prism:doi><description>Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.
Published: 2025-11-29T16:00:38+00:00
Venue: Neurocomputing
Score: 0.786 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeguang Jia; Jianming Wang; Kehui Song; Zhilan Wang; Xiaohan Ma; Rize Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132236"&gt;10.1016/j.neucom.2025.132236&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (consider)&lt;/p&gt;
&lt;p&gt;Deciphering ambiguous or contextually complex text remains a major challenge in the field of Scene Text Recognition (STR). Most existing STR recognizers rely on specialized, small-scale decoders that lack access to higher-level world knowledge and are prone to propagating local prediction errors, making it difficult to perform the higher-order reasoning required in complex contexts. These limitations are especially pronounced when using unimodal visual backbones that are incapable of capturing semantic information. In this study, we propose a novel STR paradigm called the Visual-Linguistic Enhancement Network (VLENet), which aims to jointly enhance visual perception and linguistic reasoning. Specifically, VLENet employs a cross-modal pre-trained model (CLIP) to extract visual representations that are semantically aligned with textual content. Based on the recognizer’s initial visual and linguistic predictions, a large language model (LLM) is prompted to “brainstorm” a diverse set of plausible text candidates. Finally, a carefully designed visual-linguistic matching module computes similarity scores between the original image and each candidate to select the most accurate transcription.We demonstrate the effectiveness of VLENet across a wide range of Chinese and English benchmarks, achieving new state-of-the-art (SOTA) results. Furthermore, our analysis shows that VLENet performs particularly well on challenging datasets such as COCO and Uber, highlighting its strong ability to reason about and correct text in complex real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens</title><link>https://arxiv.org/abs/2511.21106v1</link><guid>http://arxiv.org/abs/2511.21106v1</guid><pubDate>Wed, 26 Nov 2025 06:45:59 +0000</pubDate><dc:creator>Ze Feng</dc:creator><dc:creator>Sen Yang</dc:creator><dc:creator>Boqiang Duan</dc:creator><dc:creator>Wankou Yang</dc:creator><dc:creator>Jingdong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.
Published: 2025-11-26T06:45:59+00:00
Venue: arXiv
Score: 0.785 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ze Feng; Sen Yang; Boqiang Duan; Wankou Yang; Jingdong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (consider)&lt;/p&gt;
&lt;p&gt;Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.&lt;/p&gt;</content:encoded></item><item><title>Frequency-Aware Token Reduction for Efficient Vision Transformer</title><link>https://arxiv.org/abs/2511.21477v1</link><guid>http://arxiv.org/abs/2511.21477v1</guid><pubDate>Wed, 26 Nov 2025 15:10:04 +0000</pubDate><dc:creator>Dong-Jae Lee</dc:creator><dc:creator>Jiwan Hur</dc:creator><dc:creator>Jaehyun Choi</dc:creator><dc:creator>Jaemyung Yu</dc:creator><dc:creator>Junmo Kim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.
Published: 2025-11-26T15:10:04+00:00
Venue: arXiv
Score: 0.785 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dong-Jae Lee; Jiwan Hur; Jaehyun Choi; Jaemyung Yu; Junmo Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (consider)&lt;/p&gt;
&lt;p&gt;Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.&lt;/p&gt;</content:encoded></item><item><title>PCNet3D++: A pillar-based cascaded 3D object detection model with an enhanced 2D backbone</title><link>https://doi.org/10.1016/j.imavis.2025.105854</link><guid>10.1016/j.imavis.2025.105854</guid><pubDate>Sat, 29 Nov 2025 23:39:29 +0000</pubDate><dc:creator>Thurimerla Prasanth</dc:creator><dc:creator>Ram Prasad Padhy</dc:creator><dc:creator>B. Sivaselvan</dc:creator><prism:publicationName>Image and Vision Computing</prism:publicationName><prism:doi>10.1016/j.imavis.2025.105854</prism:doi><description>Autonomous Vehicles (AVs) depend on sophisticated perception systems to serve as the vital component of intelligent transportation to ensure secure and smooth navigation. Perception is an essential component of AVs and enables real-time analysis and understanding of the environment for effective decision-making. 3D object detection (3D-OD) is crucial among perception tasks as it accurately determines the 3D geometry and spatial positioning of surrounding objects. The commonly used modalities for 3D-OD are camera, LiDAR, and sensor fusion. In this work, we propose a LiDAR-based 3D-OD approach using point cloud data. The proposed model achieves superior performance while maintaining computational efficiency. This approach utilizes Pillar-based LiDAR processing and uses only 2D convolutions. The model pipeline becomes simple and more efficient by employing only 2D convolutions. We propose a Cascaded Convolutional Backbone (CCB) integrated with 1 × 1 convolutions to improve detection accuracy. We combined the fast Pillar-based encoding with our lightweight backbone. The proposed model reduces complexity to make it well-suited for real-time navigation of an AV. We evaluated our model on the official KITTI test server. The model results are decent in 3D and Bird’s Eye View (BEV) detection benchmarks for the car and cyclist classes. The results of our proposed model are featured on the official KITTI leaderboard.
Published: 2025-11-29T23:39:29+00:00
Venue: Image and Vision Computing
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Thurimerla Prasanth; Ram Prasad Padhy; B. Sivaselvan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Image and Vision Computing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.imavis.2025.105854"&gt;10.1016/j.imavis.2025.105854&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Autonomous Vehicles (AVs) depend on sophisticated perception systems to serve as the vital component of intelligent transportation to ensure secure and smooth navigation. Perception is an essential component of AVs and enables real-time analysis and understanding of the environment for effective decision-making. 3D object detection (3D-OD) is crucial among perception tasks as it accurately determines the 3D geometry and spatial positioning of surrounding objects. The commonly used modalities for 3D-OD are camera, LiDAR, and sensor fusion. In this work, we propose a LiDAR-based 3D-OD approach using point cloud data. The proposed model achieves superior performance while maintaining computational efficiency. This approach utilizes Pillar-based LiDAR processing and uses only 2D convolutions. The model pipeline becomes simple and more efficient by employing only 2D convolutions. We propose a Cascaded Convolutional Backbone (CCB) integrated with 1 × 1 convolutions to improve detection accuracy. We combined the fast Pillar-based encoding with our lightweight backbone. The proposed model reduces complexity to make it well-suited for real-time navigation of an AV. We evaluated our model on the official KITTI test server. The model results are decent in 3D and Bird’s Eye View (BEV) detection benchmarks for the car and cyclist classes. The results of our proposed model are featured on the official KITTI leaderboard.&lt;/p&gt;</content:encoded></item><item><title>CAT: A High-performance Cross-Attributes and Cross-Tasks for one-stage 3D object detection</title><link>https://doi.org/10.1016/j.knosys.2025.114995</link><guid>10.1016/j.knosys.2025.114995</guid><pubDate>Sat, 29 Nov 2025 23:41:51 +0000</pubDate><dc:creator>Yu Qin</dc:creator><dc:creator>Yiqiang Wu</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Chenghai Mao</dc:creator><dc:creator>Jia Liu</dc:creator><dc:creator>Jiacheng Sun</dc:creator><dc:creator>Yan Peng</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.114995</prism:doi><description>Real-time 3D object detection is a critical component of autonomous driving systems, yet existing one-stage detectors still face performance bottlenecks. We experimentally reveal that two kinds of incongruities suppress detection performance: (1) Attribute inconsistency refers to poor cooperation among regression attributes, which causes poor localization quality. (2) Task incongruity refers to the lack of correlation between regression and classification tasks, resulting in inefficient category prediction. To address these issues, this paper proposes a Cross-Attribute and Cross-Task (CAT) detector based on collaboration. This is the first framework to explicitly promote collaboration between regression attributes and regression and classification tasks. Specifically, to mitigate the incongruity among attributes, Regression Attribute Collaboration (RAC) is proposed to conduct joint prediction. RAC merges prediction branches to enhance the correlation between coordinates and geometric attributes in regression tasks. As for task incongruity, Cross-Task Collaboration (CTC) is designed based on a weighted incentive strategy. In particular, CTC uses geometric distribution features to incentivize classification scores, to establish an association between the regression and classification tasks. Comprehensive experiments demonstrate that CAT effectively mitigates cross-attribute and cross-task incongruity. The CAT method achieves state-of-the-art performance on the ONCE dataset and the Waymo dataset.
Published: 2025-11-29T23:41:51+00:00
Venue: Knowledge-Based Systems
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Qin; Yiqiang Wu; Chang Liu; Chenghai Mao; Jia Liu; Jiacheng Sun; Yan Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.114995"&gt;10.1016/j.knosys.2025.114995&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Real-time 3D object detection is a critical component of autonomous driving systems, yet existing one-stage detectors still face performance bottlenecks. We experimentally reveal that two kinds of incongruities suppress detection performance: (1) Attribute inconsistency refers to poor cooperation among regression attributes, which causes poor localization quality. (2) Task incongruity refers to the lack of correlation between regression and classification tasks, resulting in inefficient category prediction. To address these issues, this paper proposes a Cross-Attribute and Cross-Task (CAT) detector based on collaboration. This is the first framework to explicitly promote collaboration between regression attributes and regression and classification tasks. Specifically, to mitigate the incongruity among attributes, Regression Attribute Collaboration (RAC) is proposed to conduct joint prediction. RAC merges prediction branches to enhance the correlation between coordinates and geometric attributes in regression tasks. As for task incongruity, Cross-Task Collaboration (CTC) is designed based on a weighted incentive strategy. In particular, CTC uses geometric distribution features to incentivize classification scores, to establish an association between the regression and classification tasks. Comprehensive experiments demonstrate that CAT effectively mitigates cross-attribute and cross-task incongruity. The CAT method achieves state-of-the-art performance on the ONCE dataset and the Waymo dataset.&lt;/p&gt;</content:encoded></item><item><title>Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning</title><link>https://arxiv.org/abs/2511.21002v1</link><guid>http://arxiv.org/abs/2511.21002v1</guid><pubDate>Wed, 26 Nov 2025 03:03:52 +0000</pubDate><dc:creator>Xiaoxing You</dc:creator><dc:creator>Qiang Huang</dc:creator><dc:creator>Lingyu Li</dc:creator><dc:creator>Chi Zhang</dc:creator><dc:creator>Xiaopeng Liu</dc:creator><dc:creator>Min Zhang</dc:creator><dc:creator>Jun Yu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.
Published: 2025-11-26T03:03:52+00:00
Venue: arXiv
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxing You; Qiang Huang; Lingyu Li; Chi Zhang; Xiaopeng Liu; Min Zhang; Jun Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.&lt;/p&gt;</content:encoded></item><item><title>G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</title><link>https://arxiv.org/abs/2511.21688v1</link><guid>http://arxiv.org/abs/2511.21688v1</guid><pubDate>Wed, 26 Nov 2025 18:59:39 +0000</pubDate><dc:creator>Wenbo Hu</dc:creator><dc:creator>Jingli Lin</dc:creator><dc:creator>Yilin Long</dc:creator><dc:creator>Yunlong Ran</dc:creator><dc:creator>Lihan Jiang</dc:creator><dc:creator>Yifan Wang</dc:creator><dc:creator>Chenming Zhu</dc:creator><dc:creator>Runsen Xu</dc:creator><dc:creator>Tai Wang</dc:creator><dc:creator>Jiangmiao Pang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.
Published: 2025-11-26T18:59:39+00:00
Venue: arXiv
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenbo Hu; Jingli Lin; Yilin Long; Yunlong Ran; Lihan Jiang; Yifan Wang; Chenming Zhu; Runsen Xu; Tai Wang; Jiangmiao Pang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.&lt;/p&gt;</content:encoded></item><item><title>From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition</title><link>https://arxiv.org/abs/2511.20996v1</link><guid>http://arxiv.org/abs/2511.20996v1</guid><pubDate>Wed, 26 Nov 2025 02:50:07 +0000</pubDate><dc:creator>Jingxi Chen</dc:creator><dc:creator>Yixiao Zhang</dc:creator><dc:creator>Xiaoye Qian</dc:creator><dc:creator>Zongxia Li</dc:creator><dc:creator>Cornelia Fermuller</dc:creator><dc:creator>Caren Chen</dc:creator><dc:creator>Yiannis Aloimonos</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.
Published: 2025-11-26T02:50:07+00:00
Venue: arXiv
Score: 0.778 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingxi Chen; Yixiao Zhang; Xiaoye Qian; Zongxia Li; Cornelia Fermuller; Caren Chen; Yiannis Aloimonos&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (consider)&lt;/p&gt;
&lt;p&gt;Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.&lt;/p&gt;</content:encoded></item><item><title>Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21011v1</link><guid>http://arxiv.org/abs/2511.21011v1</guid><pubDate>Wed, 26 Nov 2025 03:20:08 +0000</pubDate><dc:creator>Sid Bharthulwar</dc:creator><dc:creator>Stone Tao</dc:creator><dc:creator>Hao Su</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.
Published: 2025-11-26T03:20:08+00:00
Venue: arXiv
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sid Bharthulwar; Stone Tao; Hao Su&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Massively parallel GPU simulation environments have accelerated reinforcement learning (RL) research by enabling fast data collection for on-policy RL algorithms like Proximal Policy Optimization (PPO). To maximize throughput, it is common to use short rollouts per policy update, increasing the update-to-data (UTD) ra- tio. However, we find that, in this setting, standard synchronous resets introduce harmful nonstationarity, skewing the learning signal and destabilizing training. We introduce staggered resets, a simple yet effective technique where environments are initialized and reset at varied points within the task horizon. This yields training batches with greater temporal diversity, reducing the nonstationarity induced by synchronized rollouts. We characterize dimensions along which RL environments can benefit significantly from staggered resets through illustrative toy environ- ments. We then apply this technique to challenging high-dimensional robotics environments, achieving significantly higher sample efficiency, faster wall-clock convergence, and stronger final performance. Finally, this technique scales better with more parallel environments compared to naive synchronized rollouts.&lt;/p&gt;</content:encoded></item><item><title>Co-Training Vision Language Models for Remote Sensing Multi-task Learning</title><link>https://arxiv.org/abs/2511.21272v1</link><guid>http://arxiv.org/abs/2511.21272v1</guid><pubDate>Wed, 26 Nov 2025 10:55:07 +0000</pubDate><dc:creator>Qingyun Li</dc:creator><dc:creator>Shuran Ma</dc:creator><dc:creator>Junwei Luo</dc:creator><dc:creator>Yi Yu</dc:creator><dc:creator>Yue Zhou</dc:creator><dc:creator>Fengxiang Wang</dc:creator><dc:creator>Xudong Lu</dc:creator><dc:creator>Xiaoxing Wang</dc:creator><dc:creator>Xin He</dc:creator><dc:creator>Yushi Chen</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Junchi Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.
Published: 2025-11-26T10:55:07+00:00
Venue: arXiv
Score: 0.776 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingyun Li; Shuran Ma; Junwei Luo; Yi Yu; Yue Zhou; Fengxiang Wang; Xudong Lu; Xiaoxing Wang; Xin He; Yushi Chen; Xue Yang; Junchi Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (consider)&lt;/p&gt;
&lt;p&gt;With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&amp;#x27;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.&lt;/p&gt;</content:encoded></item><item><title>CAMN-FSOD: Class-aware memory network for few-shot infrared object detection</title><link>https://doi.org/10.1016/j.patrec.2025.11.033</link><guid>10.1016/j.patrec.2025.11.033</guid><pubDate>Sat, 29 Nov 2025 15:56:30 +0000</pubDate><dc:creator>Jing Hu</dc:creator><dc:creator>Hengkang Ye</dc:creator><dc:creator>Weiwei Zhong</dc:creator><dc:creator>Zican Shi</dc:creator><dc:creator>Yifan Chen</dc:creator><dc:creator>Jie Ren</dc:creator><dc:creator>Xiaohui Zhu</dc:creator><dc:creator>Li Fan</dc:creator><prism:publicationName>Pattern Recognition Letters</prism:publicationName><prism:doi>10.1016/j.patrec.2025.11.033</prism:doi><description>Cross-Domain Few-Shot Object Detection (CD-FSOD) from visible to infrared domains faces a critical challenge: object classification proves significantly more error-prone than localization under fine-tuning adaptation. This stems from substantial representational discrepancies in internal object features between domains, which hinder effective transfer. To enhance the saliency of infrared internal object features and mitigate classification errors in few-shot visible-to-infrared transfer, we propose the Class-Aware Memory Network for Few-Shot Object Detection (CAMN-FSOD). CAMN explicitly memories high-quality internal object features during fine-tuning and leverages memory to augment features,boosting recognition accuracy during inference. Furthermore, we introduce our two-stage Decoupled-Coupled Fine-tuning approach (DCFA) to combat CAMN overfitting in few-shot training and maximize its effectiveness. We establish a visible-infrared FSOD benchmark dataset for evaluation. Extensive experiments demonstrate that CAMN-FSOD significantly enhances the few-shot learning capability of the base model without increasing trainable parameters. In the 1-shot setting, our method achieves 42.0 mAP 50 , which is 14.4 points higher than the baseline, and an overall mAP of 25.2, showing an improvement of 2.3 points, outperforming existing methods.
Published: 2025-11-29T15:56:30+00:00
Venue: Pattern Recognition Letters
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Hu; Hengkang Ye; Weiwei Zhong; Zican Shi; Yifan Chen; Jie Ren; Xiaohui Zhu; Li Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patrec.2025.11.033"&gt;10.1016/j.patrec.2025.11.033&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;Cross-Domain Few-Shot Object Detection (CD-FSOD) from visible to infrared domains faces a critical challenge: object classification proves significantly more error-prone than localization under fine-tuning adaptation. This stems from substantial representational discrepancies in internal object features between domains, which hinder effective transfer. To enhance the saliency of infrared internal object features and mitigate classification errors in few-shot visible-to-infrared transfer, we propose the Class-Aware Memory Network for Few-Shot Object Detection (CAMN-FSOD). CAMN explicitly memories high-quality internal object features during fine-tuning and leverages memory to augment features,boosting recognition accuracy during inference. Furthermore, we introduce our two-stage Decoupled-Coupled Fine-tuning approach (DCFA) to combat CAMN overfitting in few-shot training and maximize its effectiveness. We establish a visible-infrared FSOD benchmark dataset for evaluation. Extensive experiments demonstrate that CAMN-FSOD significantly enhances the few-shot learning capability of the base model without increasing trainable parameters. In the 1-shot setting, our method achieves 42.0 mAP 50 , which is 14.4 points higher than the baseline, and an overall mAP of 25.2, showing an improvement of 2.3 points, outperforming existing methods.&lt;/p&gt;</content:encoded></item><item><title>Spatial Coherence Loss: All Objects Matter in Salient and Camouflaged Object Detection</title><link>https://doi.org/10.1016/j.patcog.2025.112798</link><guid>10.1016/j.patcog.2025.112798</guid><pubDate>Sat, 29 Nov 2025 07:57:45 +0000</pubDate><dc:creator>Ziyun Yang</dc:creator><dc:creator>Kevin Choy</dc:creator><dc:creator>Sina Farsiu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112798</prism:doi><description>Generic object detection is a category-independent task that relies on accurate modeling of objectness. We show that for accurate semantic analysis, the network needs to learn all object-level predictions that appear at any stage of learning, including the pre-defined ground truth (GT) objects and the ambiguous decoy objects that the network misidentifies as foreground. Yet, most relevant models focused mainly on improving the learning of the GT objects. A few methods that consider decoy objects utilize loss functions that only focus on the single-response, i.e., the loss response of a single ambiguous pixel, and thus do not benefit from the wealth of information that an object-level ambiguity learning design can provide. Inspired by the human visual system, which first discerns the boundaries of ambiguous regions before delving into the semantic meaning, we propose a novel loss function, Spatial Coherence Loss (SCLoss), that incorporates the mutual response between adjacent pixels into the widely-used single-response loss functions. We demonstrate that the proposed SCLoss can gradually learn the ambiguous regions by detecting and emphasizing their boundaries in a self-adaptive manner. Through comprehensive experiments, we demonstrate that replacing popular loss functions with SCLoss can improve the performance of current state-of-the-art (SOTA) salient or camouflaged object detection (SOD or COD) models. We also demonstrate that combining SCLoss with other loss functions can further improve performance and result in SOTA outcomes for different applications. The codes will be released to https://github.com/TBD upon acceptance.
Published: 2025-11-29T07:57:45+00:00
Venue: Pattern Recognition
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyun Yang; Kevin Choy; Sina Farsiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112798"&gt;10.1016/j.patcog.2025.112798&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;Generic object detection is a category-independent task that relies on accurate modeling of objectness. We show that for accurate semantic analysis, the network needs to learn all object-level predictions that appear at any stage of learning, including the pre-defined ground truth (GT) objects and the ambiguous decoy objects that the network misidentifies as foreground. Yet, most relevant models focused mainly on improving the learning of the GT objects. A few methods that consider decoy objects utilize loss functions that only focus on the single-response, i.e., the loss response of a single ambiguous pixel, and thus do not benefit from the wealth of information that an object-level ambiguity learning design can provide. Inspired by the human visual system, which first discerns the boundaries of ambiguous regions before delving into the semantic meaning, we propose a novel loss function, Spatial Coherence Loss (SCLoss), that incorporates the mutual response between adjacent pixels into the widely-used single-response loss functions. We demonstrate that the proposed SCLoss can gradually learn the ambiguous regions by detecting and emphasizing their boundaries in a self-adaptive manner. Through comprehensive experiments, we demonstrate that replacing popular loss functions with SCLoss can improve the performance of current state-of-the-art (SOTA) salient or camouflaged object detection (SOD or COD) models. We also demonstrate that combining SCLoss with other loss functions can further improve performance and result in SOTA outcomes for different applications. The codes will be released to https://github.com/TBD upon acceptance.&lt;/p&gt;</content:encoded></item><item><title>Canvas-to-Image: Compositional Image Generation with Multimodal Controls</title><link>https://arxiv.org/abs/2511.21691v1</link><guid>http://arxiv.org/abs/2511.21691v1</guid><pubDate>Wed, 26 Nov 2025 18:59:56 +0000</pubDate><dc:creator>Yusuf Dalva</dc:creator><dc:creator>Guocheng Gordon Qian</dc:creator><dc:creator>Maya Goldenberg</dc:creator><dc:creator>Tsai-Shien Chen</dc:creator><dc:creator>Kfir Aberman</dc:creator><dc:creator>Sergey Tulyakov</dc:creator><dc:creator>Pinar Yanardag</dc:creator><dc:creator>Kuan-Chieh Jackson Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.
Published: 2025-11-26T18:59:56+00:00
Venue: arXiv
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yusuf Dalva; Guocheng Gordon Qian; Maya Goldenberg; Tsai-Shien Chen; Kfir Aberman; Sergey Tulyakov; Pinar Yanardag; Kuan-Chieh Jackson Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.&lt;/p&gt;</content:encoded></item></channel></rss>