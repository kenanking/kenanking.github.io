<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 04 Jan 2026 02:51:16 +0000</lastBuildDate><item><title>Regional Defeats Global: An Efficient Regional Feature Fusion via Convolutional Architecture for Multispectral Object Detection</title><link>https://doi.org/10.1016/j.inffus.2025.104110</link><guid>10.1016/j.inffus.2025.104110</guid><pubDate>Fri, 02 Jan 2026 07:42:39 +0000</pubDate><dc:creator>Zhenhao Wang</dc:creator><dc:creator>Tian Tian</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104110</prism:doi><description>Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .
Published: 2026-01-02T07:42:39+00:00
Venue: Information Fusion
Score: 0.838 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenhao Wang; Tian Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104110"&gt;10.1016/j.inffus.2025.104110&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.838 (must_read)&lt;/p&gt;
&lt;p&gt;Multispectral object detection continues to face significant challenges in achieving a balanced optimization between accuracy and efficiency. Most existing approaches rely heavily on global modeling, which, although capable of integrating multi-band information, incurs substantial computational overhead and fails to fully exploit the spatial correlations across spectral bands. To address this issue, this paper introduces a convolutional architecture-based region feature computation mechanism that leverages the inherent advantage of convolutional operations in preserving spatial structure, enabling spatial cues to be fully retained during feature representation learning and explicitly incorporated into multispectral feature interaction. Meanwhile, by reconstructing global attention computation into localized regional modeling, the proposed method markedly reduces computational cost while maintaining effective feature fusion, thereby facilitating a lightweight architectural design. Experimental results demonstrate that the proposed module achieves the lowest computational overhead while improving mAP@50 by 1.97% and 1.66% on the DroneVehicle and VEDAI remote-sensing datasets, respectively, compared with state-of-the-art methods. Moreover, it exhibits strong applicability on the pedestrian detection datasets FLIR and LLVIP. The code is available https://github.com/wzh326/LMFFM_CARFCOM.git .&lt;/p&gt;</content:encoded></item><item><title>CCAI-YOLO: A High-Precision Synthetic Aperture Radar Ship Detection Model Based on YOLOv8n Algorithm</title><link>https://doi.org/10.3390/rs18010145</link><guid>10.3390/rs18010145</guid><pubDate>Fri, 02 Jan 2026 09:18:56 +0000</pubDate><dc:creator>Hui Liu</dc:creator><dc:creator>Haoyu Dong</dc:creator><dc:creator>Hongyin Shi</dc:creator><dc:creator>Fang Li</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010145</prism:doi><description>To tackle core challenges in detecting ship targets within synthetic aperture radar (SAR) images—including coherent speckle noise interference, complex background clutter, and multi-scale target distribution—this paper proposes a high-accuracy detection model, CCAI-YOLO. This model is based on the YOLOv8n framework, achieving systematic enhancements through the collaborative optimisation of key components: within the backbone network, the original C2f structure is replaced with the dynamic convolution module C2f-ODConv, improving the model’s extraction capabilities under noisy interference; the C2f-ACmix module is integrated into the neck network, introducing a self-attention mechanism to strengthen global context information modelling, thereby better distinguishing targets from structured backgrounds; the ASFF detection head optimises multi-scale feature fusion, enhancing detection consistency across different-sized targets. Concurrently, the Inner-SIoU loss function further improves bounding box regression accuracy and accelerates convergence. Experimental results demonstrate that on the public datasets SSDD and SAR-Ship-Dataset, CCAI-YOLO achieves consistent improvements over the baseline model YOLOv8n across key metrics including F1 score, mAP50, and mAP50-95. Its overall performance surpasses current mainstream SAR ship detection methods, providing an effective solution for robust and efficient ship detection in complex scenarios.
Published: 2026-01-02T09:18:56+00:00
Venue: Remote Sensing
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hui Liu; Haoyu Dong; Hongyin Shi; Fang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010145"&gt;10.3390/rs18010145&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;To tackle core challenges in detecting ship targets within synthetic aperture radar (SAR) images—including coherent speckle noise interference, complex background clutter, and multi-scale target distribution—this paper proposes a high-accuracy detection model, CCAI-YOLO. This model is based on the YOLOv8n framework, achieving systematic enhancements through the collaborative optimisation of key components: within the backbone network, the original C2f structure is replaced with the dynamic convolution module C2f-ODConv, improving the model’s extraction capabilities under noisy interference; the C2f-ACmix module is integrated into the neck network, introducing a self-attention mechanism to strengthen global context information modelling, thereby better distinguishing targets from structured backgrounds; the ASFF detection head optimises multi-scale feature fusion, enhancing detection consistency across different-sized targets. Concurrently, the Inner-SIoU loss function further improves bounding box regression accuracy and accelerates convergence. Experimental results demonstrate that on the public datasets SSDD and SAR-Ship-Dataset, CCAI-YOLO achieves consistent improvements over the baseline model YOLOv8n across key metrics including F1 score, mAP50, and mAP50-95. Its overall performance surpasses current mainstream SAR ship detection methods, providing an effective solution for robust and efficient ship detection in complex scenarios.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Harmful Meme Detection via Self-adaption Mixture-of-Experts</title><link>https://doi.org/10.1016/j.inffus.2026.104122</link><guid>10.1016/j.inffus.2026.104122</guid><pubDate>Sat, 03 Jan 2026 16:23:50 +0000</pubDate><dc:creator>Zou Li</dc:creator><dc:creator>Jinzhi Liao</dc:creator><dc:creator>Jiting Li</dc:creator><dc:creator>Ji Wang</dc:creator><dc:creator>Xiang Zhao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104122</prism:doi><description>The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.
Published: 2026-01-03T16:23:50+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zou Li; Jinzhi Liao; Jiting Li; Ji Wang; Xiang Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104122"&gt;10.1016/j.inffus.2026.104122&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.&lt;/p&gt;</content:encoded></item><item><title>Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection</title><link>https://arxiv.org/abs/2512.24922v1</link><guid>http://arxiv.org/abs/2512.24922v1</guid><pubDate>Wed, 31 Dec 2025 15:26:09 +0000</pubDate><dc:creator>Bartłomiej Olber</dc:creator><dc:creator>Jakub Winter</dc:creator><dc:creator>Paweł Wawrzyński</dc:creator><dc:creator>Andrii Gamalii</dc:creator><dc:creator>Daniel Górniak</dc:creator><dc:creator>Marcin Łojek</dc:creator><dc:creator>Robert Nowak</dc:creator><dc:creator>Krystian Radlak</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.
Published: 2025-12-31T15:26:09+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bartłomiej Olber; Jakub Winter; Paweł Wawrzyński; Andrii Gamalii; Daniel Górniak; Marcin Łojek; Robert Nowak; Krystian Radlak&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.&lt;/p&gt;</content:encoded></item><item><title>GrowSP++: Growing Superpoints and Primitives for Unsupervised 3D Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3650165</link><guid>10.1109/tpami.2025.3650165</guid><pubDate>Fri, 02 Jan 2026 18:16:22 +0000</pubDate><dc:creator>Zihui Zhang</dc:creator><dc:creator>Weisheng Dai</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Bo Li</dc:creator><dc:creator>Bo Yang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650165</prism:doi><description>We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.
Published: 2026-01-02T18:16:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihui Zhang; Weisheng Dai; Bing Wang; Bo Li; Bo Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650165"&gt;10.1109/tpami.2025.3650165&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we proposes GrowSP++, an unsupervised method to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels. Our method is composed of three major components: 1) a feature extractor incorporating 2D-3D feature distillation, 2) a superpoint constructor featuring progressively growing superpoints, and 3) a semantic primitive constructor with an additional growing strategy. The key to our method is the superpoint constructor together with the progressive growing strategy on both super points and semantic primitives, driving the feature extractor to progressively learn similar features for 3D points belonging to the same semantic class. We extensively evaluate our method on five challenging indoor and outdoor datasets, demonstrating state of-the-art performance over all unsupervised baselines. We hope our work could inspire more advanced methods for unsupervised 3D semantic learning.&lt;/p&gt;</content:encoded></item><item><title>LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving</title><link>https://doi.org/10.1016/j.patcog.2026.113046</link><guid>10.1016/j.patcog.2026.113046</guid><pubDate>Sat, 03 Jan 2026 23:24:25 +0000</pubDate><dc:creator>Carlo Sgaravatti</dc:creator><dc:creator>Riccardo Pieroni</dc:creator><dc:creator>Matteo Corno</dc:creator><dc:creator>Sergio M. Savaresi</dc:creator><dc:creator>Luca Magri</dc:creator><dc:creator>Giacomo Boracchi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113046</prism:doi><description>Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .
Published: 2026-01-03T23:24:25+00:00
Venue: Pattern Recognition
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Carlo Sgaravatti; Riccardo Pieroni; Matteo Corno; Sergio M. Savaresi; Luca Magri; Giacomo Boracchi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113046"&gt;10.1016/j.patcog.2026.113046&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .&lt;/p&gt;</content:encoded></item><item><title>Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds</title><link>https://doi.org/10.1109/tip.2025.3648203</link><guid>10.1109/tip.2025.3648203</guid><pubDate>Fri, 02 Jan 2026 18:17:51 +0000</pubDate><dc:creator>Hao Jing</dc:creator><dc:creator>Anhong Wang</dc:creator><dc:creator>Yifan Zhang</dc:creator><dc:creator>Donghan Bu</dc:creator><dc:creator>Junhui Hou</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648203</prism:doi><description>Regarding intelligent transportation systems, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among connected agents, such as vehicles and infrastructures, under restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as the student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our cross-source distillation training strategy (CDTS) equips the student detector with robustness to low-quality compressed data while preserving the accuracy benefits of raw data through transferred distillation knowledge. Experimental results on the KITTI and DAIR-V2X-V datasets demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. We will release the code publicly at https://github.com/HaoJing-SX/RPKD.
Published: 2026-01-02T18:17:51+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Jing; Anhong Wang; Yifan Zhang; Donghan Bu; Junhui Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648203"&gt;10.1109/tip.2025.3648203&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Regarding intelligent transportation systems, low-bitrate transmission via lossy point cloud compression is vital for facilitating real-time collaborative perception among connected agents, such as vehicles and infrastructures, under restricted bandwidth. In existing compression transmission systems, the sender lossily compresses point coordinates and reflectance to generate a transmission code stream, which faces transmission burdens from reflectance encoding and limited detection robustness due to information loss. To address these issues, this paper proposes a 3D object detection framework with reflectance prediction-based knowledge distillation (RPKD). We compress point coordinates while discarding reflectance during low-bitrate transmission, and feed the decoded non-reflectance compressed point clouds into a student detector. The discarded reflectance is then reconstructed by a geometry-based reflectance prediction (RP) module within the student detector for precise detection. A teacher detector with the same structure as the student detector is designed for performing reflectance knowledge distillation (RKD) and detection knowledge distillation (DKD) from raw to compressed point clouds. Our cross-source distillation training strategy (CDTS) equips the student detector with robustness to low-quality compressed data while preserving the accuracy benefits of raw data through transferred distillation knowledge. Experimental results on the KITTI and DAIR-V2X-V datasets demonstrate that our method can boost detection accuracy for compressed point clouds across multiple code rates. We will release the code publicly at https://github.com/HaoJing-SX/RPKD.&lt;/p&gt;</content:encoded></item><item><title>HVTC-GAN: A High-level Vision Task Cooperative GAN for SAR-to-Optical translation via Semantic Segmentation</title><link>https://doi.org/10.1109/jstars.2025.3650182</link><guid>10.1109/jstars.2025.3650182</guid><pubDate>Fri, 02 Jan 2026 18:16:44 +0000</pubDate><dc:creator>Hang Liu</dc:creator><dc:creator>Jiarui Lin</dc:creator><dc:creator>Cang Gu</dc:creator><dc:creator>Yujie Zhang</dc:creator><dc:creator>Huihui Li</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650182</prism:doi><description>Synthetic Aperture Radar (SAR), an active remote sensing technology that can be deployed on various aerial plat forms, generates images by emitting microwaves and analyzing the intensity of backscattered signals. The penetration capability of microwaves enables observation of Earth at all weather, day and night, broadening the application of SAR. However, SAR image interpretation remains challenging for non-experts. In contrast, optical images provide intuitive visual features but are weather-sensitive. To synergize these modalities, SAR-to-Optical Translation (S2OT) has gained research attention, but most studies prioritize visual quality or similarity metrics over practical applicability to downstream tasks. This study proposes a high level vision task-coordinated S2OT framework to address this gap. Semantic segmentation, emulating land cover classification, is integrated as the downstream task. Semantic segmentation loss guides the network to generate optical images that enhance task-relevant features. To preserve structural information in SAR images, we introduce SSIM loss and incorporate SAR derived semantic segmentation maps as auxiliary inputs. An identity loss further aligns the distributions of generated and real optical images, mitigating domain discrepancies. Extensive experiments confirm that S2OT improves downstream land cover classification. The inclusion of task-specific losses elevates translation quality: our HVTC-GAN surpasses the baseline methods in SSIM and PSNR metrics. Ablation studies validate the effectiveness of co-training S2OT with high-level vision tasks, demonstrating that task-oriented constraints enhance both translation fidelity and downstream utility. Code will be available at https://github.com/NWPU-LHH/HVTC-GAN
Published: 2026-01-02T18:16:44+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hang Liu; Jiarui Lin; Cang Gu; Yujie Zhang; Huihui Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650182"&gt;10.1109/jstars.2025.3650182&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR), an active remote sensing technology that can be deployed on various aerial plat forms, generates images by emitting microwaves and analyzing the intensity of backscattered signals. The penetration capability of microwaves enables observation of Earth at all weather, day and night, broadening the application of SAR. However, SAR image interpretation remains challenging for non-experts. In contrast, optical images provide intuitive visual features but are weather-sensitive. To synergize these modalities, SAR-to-Optical Translation (S2OT) has gained research attention, but most studies prioritize visual quality or similarity metrics over practical applicability to downstream tasks. This study proposes a high level vision task-coordinated S2OT framework to address this gap. Semantic segmentation, emulating land cover classification, is integrated as the downstream task. Semantic segmentation loss guides the network to generate optical images that enhance task-relevant features. To preserve structural information in SAR images, we introduce SSIM loss and incorporate SAR derived semantic segmentation maps as auxiliary inputs. An identity loss further aligns the distributions of generated and real optical images, mitigating domain discrepancies. Extensive experiments confirm that S2OT improves downstream land cover classification. The inclusion of task-specific losses elevates translation quality: our HVTC-GAN surpasses the baseline methods in SSIM and PSNR metrics. Ablation studies validate the effectiveness of co-training S2OT with high-level vision tasks, demonstrating that task-oriented constraints enhance both translation fidelity and downstream utility. Code will be available at https://github.com/NWPU-LHH/HVTC-GAN&lt;/p&gt;</content:encoded></item><item><title>An Empirical Analysis of Deep Learning Methods for Small Object Detection from Satellite Imagery</title><link>https://doi.org/10.1016/j.eswa.2025.131061</link><guid>10.1016/j.eswa.2025.131061</guid><pubDate>Fri, 02 Jan 2026 23:48:17 +0000</pubDate><dc:creator>Xiaohui Yuan</dc:creator><dc:creator>Aniv Chakravarty</dc:creator><dc:creator>Elinor M. Lichtenberg</dc:creator><dc:creator>Lichuan Gu</dc:creator><dc:creator>Zhenchun Wei</dc:creator><dc:creator>Tian Chen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131061</prism:doi><description>Despite a substantial body of literature on object detection, there is a notable lack of empirical studies on detecting small objects. Additionally, the definition of a small object remains unclear. This paper presents a thorough evaluation of six state-of-the-art deep learning methods for small object detection from satellite imagery. Three public high-resolution datasets are used to understand various influential aspects and the generalization ability. Among the six methods, YOLOv11 achieves a balanced performance for localization and adaptability, while Faster R-CNN maintains consistent detection coverage. Anchor box-based methods require extensive fine-tuning, whereas transformer-based methods demand greater computational resources to achieve competitive results. In addition, anchor-based methods, including SSD, Faster R-CNN, and Cascade R-CNN, are sensitive to the anchor box size, and, for small object detection, a small to moderate size is preferred. Both deformable and RT-DETR methods are susceptible to overfitting. RT-DETR exhibits superior detection in partial occlusion scenarios, particularly through vegetation and shadows, whereas deformable DETR struggles to identify individual small objects in dense clusters. Comparing computational efficiency with a batch size of one reveals that RT-DETR and YOLOv11 are more training-intensive, with optimizations focused on inference. Methods such as Faster R-CNN have a larger memory footprint but lower computational costs and time requirements.
Published: 2026-01-02T23:48:17+00:00
Venue: Expert Systems with Applications
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaohui Yuan; Aniv Chakravarty; Elinor M. Lichtenberg; Lichuan Gu; Zhenchun Wei; Tian Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131061"&gt;10.1016/j.eswa.2025.131061&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Despite a substantial body of literature on object detection, there is a notable lack of empirical studies on detecting small objects. Additionally, the definition of a small object remains unclear. This paper presents a thorough evaluation of six state-of-the-art deep learning methods for small object detection from satellite imagery. Three public high-resolution datasets are used to understand various influential aspects and the generalization ability. Among the six methods, YOLOv11 achieves a balanced performance for localization and adaptability, while Faster R-CNN maintains consistent detection coverage. Anchor box-based methods require extensive fine-tuning, whereas transformer-based methods demand greater computational resources to achieve competitive results. In addition, anchor-based methods, including SSD, Faster R-CNN, and Cascade R-CNN, are sensitive to the anchor box size, and, for small object detection, a small to moderate size is preferred. Both deformable and RT-DETR methods are susceptible to overfitting. RT-DETR exhibits superior detection in partial occlusion scenarios, particularly through vegetation and shadows, whereas deformable DETR struggles to identify individual small objects in dense clusters. Comparing computational efficiency with a batch size of one reveals that RT-DETR and YOLOv11 are more training-intensive, with optimizations focused on inference. Methods such as Faster R-CNN have a larger memory footprint but lower computational costs and time requirements.&lt;/p&gt;</content:encoded></item><item><title>DAK-Pose: Dual-Augmentor Knowledge Fusion for Generalizable Video-Based 3D Human Pose Estimation</title><link>https://doi.org/10.1016/j.inffus.2025.104100</link><guid>10.1016/j.inffus.2025.104100</guid><pubDate>Sat, 03 Jan 2026 16:24:02 +0000</pubDate><dc:creator>Yachuan Wang</dc:creator><dc:creator>Bin Zhang</dc:creator><dc:creator>Hao Yuan</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104100</prism:doi><description>Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.
Published: 2026-01-03T16:24:02+00:00
Venue: Information Fusion
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yachuan Wang; Bin Zhang; Hao Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104100"&gt;10.1016/j.inffus.2025.104100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.&lt;/p&gt;</content:encoded></item><item><title>IGCDet: Independence Guided Co-Training for Sparsely Annotated Object Detection</title><link>https://doi.org/10.1016/j.knosys.2025.115217</link><guid>10.1016/j.knosys.2025.115217</guid><pubDate>Fri, 02 Jan 2026 07:39:05 +0000</pubDate><dc:creator>Jian-Xun Mi</dc:creator><dc:creator>Jiahui Feng</dc:creator><dc:creator>Haiyang Wang</dc:creator><dc:creator>Yanjun Wu</dc:creator><dc:creator>Ranzhi Zhao</dc:creator><dc:creator>Chang Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115217</prism:doi><description>Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.
Published: 2026-01-02T07:39:05+00:00
Venue: Knowledge-Based Systems
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian-Xun Mi; Jiahui Feng; Haiyang Wang; Yanjun Wu; Ranzhi Zhao; Chang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115217"&gt;10.1016/j.knosys.2025.115217&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection models can achieve excellent detection performance with fully annotated instances. However, requiring complete annotations for every dataset is impractical due to high labor and time costs, as well as the inevitable occurrence of missing annotations. As a result, the absence of annotations can potentially provide misleading supervision and harm the training process. Recent methodologies have achieved remarkable effectiveness through the application of Co-Mining. However, the independence of each branch in Co-Mining cannot be guaranteed, overlooking valuable information during multi-perspective training. To address this issue, we introduce an Independence Guided Co-Training Model (IGCDet) that leverages Image Independence Decomposition to ensure the independence of each co-training branch. This model aims to capture diverse perspectives from images as extensively as possible, identifying missing annotations and incorporating them as positive supervision in the training process. Additionally, we propose the use of Joint-Confidence, derived from the combination of classification and regression, as pseudo-label scores, effectively mitigating issues associated with pseudo-label bias. Extensive experiments have verified the effectiveness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>MEFPNet: A multi-scale enhanced feature pyramid network for similarity-confounded substation surface-defect detection</title><link>https://doi.org/10.1016/j.neucom.2025.132598</link><guid>10.1016/j.neucom.2025.132598</guid><pubDate>Fri, 02 Jan 2026 16:45:38 +0000</pubDate><dc:creator>Yunfei Zhou</dc:creator><dc:creator>Quanbo Ge</dc:creator><dc:creator>Mingchuan Zhang</dc:creator><dc:creator>Xinliang He</dc:creator><dc:creator>Kuan Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132598</prism:doi><description>Substation surface-defect detection remains a challenging task due to the coexistence of domain and illumination shifts, large scale variation, and visually similar backgrounds. To address these issues, this paper proposes a Multi-attention Enhanced Feature Pyramid Network (MEFPNet), a detection framework tailored for similarity-confounded substation equipment inspection. First, a Context-Aware Feature Aggregation Module (CAFAM) is designed to enhance the perception of large-scale structures while preserving fine-grained local cues in complex backgrounds. Second, a Learnable Weighted Feature Pyramid Network (LWFPN) is introduced to adaptively select and reweight hierarchical features, thereby improving cross-scale interaction. Third, the original AIFI module is replaced with a lightweight Simplified Spatial Pyramid Pooling (SimSPPF) block to capture rich spatial context at low computational cost. Experiments are conducted on two datasets—the Substation Dataset and the YOLO Annotated 15-class Ground Truth Dataset for Substation Equipment. Results show that MEFPNet achieves superior accuracy and robustness compared with baseline detectors. Specifically, on the Substation dataset, MEFPNet improves mAP@50 by 6.64% and mAP@50:95 by 3.65%, demonstrating its effectiveness and applicability in real-world substation defect detection scenarios.
Published: 2026-01-02T16:45:38+00:00
Venue: Neurocomputing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunfei Zhou; Quanbo Ge; Mingchuan Zhang; Xinliang He; Kuan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132598"&gt;10.1016/j.neucom.2025.132598&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Substation surface-defect detection remains a challenging task due to the coexistence of domain and illumination shifts, large scale variation, and visually similar backgrounds. To address these issues, this paper proposes a Multi-attention Enhanced Feature Pyramid Network (MEFPNet), a detection framework tailored for similarity-confounded substation equipment inspection. First, a Context-Aware Feature Aggregation Module (CAFAM) is designed to enhance the perception of large-scale structures while preserving fine-grained local cues in complex backgrounds. Second, a Learnable Weighted Feature Pyramid Network (LWFPN) is introduced to adaptively select and reweight hierarchical features, thereby improving cross-scale interaction. Third, the original AIFI module is replaced with a lightweight Simplified Spatial Pyramid Pooling (SimSPPF) block to capture rich spatial context at low computational cost. Experiments are conducted on two datasets—the Substation Dataset and the YOLO Annotated 15-class Ground Truth Dataset for Substation Equipment. Results show that MEFPNet achieves superior accuracy and robustness compared with baseline detectors. Specifically, on the Substation dataset, MEFPNet improves mAP@50 by 6.64% and mAP@50:95 by 3.65%, demonstrating its effectiveness and applicability in real-world substation defect detection scenarios.&lt;/p&gt;</content:encoded></item><item><title>Progressive Temporal Compensation and Semantic Enhancement for Exo-to-Ego Video Generation</title><link>https://doi.org/10.1016/j.inffus.2025.104117</link><guid>10.1016/j.inffus.2025.104117</guid><pubDate>Sat, 03 Jan 2026 16:24:00 +0000</pubDate><dc:creator>Xingyue Wang</dc:creator><dc:creator>Weipeng Hu</dc:creator><dc:creator>Jiun Tian Hoe</dc:creator><dc:creator>Jianhui Li</dc:creator><dc:creator>Ping Hu</dc:creator><dc:creator>Yap-Peng Tan</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104117</prism:doi><description>Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.
Published: 2026-01-03T16:24:00+00:00
Venue: Information Fusion
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyue Wang; Weipeng Hu; Jiun Tian Hoe; Jianhui Li; Ping Hu; Yap-Peng Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104117"&gt;10.1016/j.inffus.2025.104117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.&lt;/p&gt;</content:encoded></item><item><title>RankSAM: Lightweight adapters and prompt generation in zero-shot semantic segmentation</title><link>https://doi.org/10.1016/j.neucom.2025.132594</link><guid>10.1016/j.neucom.2025.132594</guid><pubDate>Fri, 02 Jan 2026 16:07:19 +0000</pubDate><dc:creator>Yue Zhuo</dc:creator><dc:creator>Zhaocheng Xu</dc:creator><dc:creator>Di Zhou</dc:creator><dc:creator>Pengpeng Xu</dc:creator><dc:creator>Yan Tian</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132594</prism:doi><description>Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .
Published: 2026-01-02T16:07:19+00:00
Venue: Neurocomputing
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Zhuo; Zhaocheng Xu; Di Zhou; Pengpeng Xu; Yan Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132594"&gt;10.1016/j.neucom.2025.132594&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Zero-shot segmentation plays a crucial role in neurocomputing, such as embodied intelligence systems and autonomous driving technologies. However, current approaches struggle to preserve the intrinsic generalization ability of SAM as input quality declines. In addition, prompt generation still faces an embarrassment in the balance between effectiveness and efficiency. Motivated by low-rank adaptation (LoRA), we design RankSAM, which integrates slim, adaptable modules into the middle layers of the frozen SAM framework. These modules dynamically fine-tune the operational rank of their weight matrices in response to input data, leveraging a trainable gating mechanism to selectively activate specific (rank-1) matrix components as needed. In addition, a learnable prompt predictor is designed to learn and generate prompt confidence maps and point prompts, and any remaining prompts that would produce the same mask are filtered out to enhance efficiency in prompt generation. The experimental results on multiple datasets indicate that our approach improves the mean intersection over union (mIoU) by a margin of 2.5%–2.8% compared to the prevailing approaches. Project page: https://messeyamumu.github.io/RankSAM .&lt;/p&gt;</content:encoded></item><item><title>OFL-SAM2: Prompt SAM2 with Online Few-shot Learner for Efficient Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.24861v1</link><guid>http://arxiv.org/abs/2512.24861v1</guid><pubDate>Wed, 31 Dec 2025 13:41:16 +0000</pubDate><dc:creator>Meng Lan</dc:creator><dc:creator>Lefei Zhang</dc:creator><dc:creator>Xiaomeng Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model's generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.
Published: 2025-12-31T13:41:16+00:00
Venue: arXiv
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Lan; Lefei Zhang; Xiaomeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model 2 (SAM2) has demonstrated remarkable promptable visual segmentation capabilities in video data, showing potential for extension to medical image segmentation (MIS) tasks involving 3D volumes and temporally correlated 2D image sequences. However, adapting SAM2 to MIS presents several challenges, including the need for extensive annotated medical data for fine-tuning and high-quality manual prompts, which are both labor-intensive and require intervention from medical experts. To address these challenges, we introduce OFL-SAM2, a prompt-free SAM2 framework for label-efficient MIS. Our core idea is to leverage limited annotated samples to train a lightweight mapping network that captures medical knowledge and transforms generic image features into target features, thereby providing additional discriminative target representations for each frame and eliminating the need for manual prompts. Crucially, the mapping network supports online parameter update during inference, enhancing the model&amp;#x27;s generalization across test sequences. Technically, we introduce two key components: (1) an online few-shot learner that trains the mapping network to generate target features using limited data, and (2) an adaptive fusion module that dynamically integrates the target features with the memory-attention features generated by frozen SAM2, leading to accurate and robust target representation. Extensive experiments on three diverse MIS datasets demonstrate that OFL-SAM2 achieves state-of-the-art performance with limited training data.&lt;/p&gt;</content:encoded></item><item><title>DAPU: Distribution-aware patch upsampling for point cloud based 3D object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132579</link><guid>10.1016/j.neucom.2025.132579</guid><pubDate>Fri, 02 Jan 2026 16:45:38 +0000</pubDate><dc:creator>Yinghao Hu</dc:creator><dc:creator>Yan Wu</dc:creator><dc:creator>Yujian Mo</dc:creator><dc:creator>Jijun Wang</dc:creator><dc:creator>Yuwei Zhang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132579</prism:doi><description>The sparsity and quality of point clouds significantly constrain the development of LiDAR-based 3D object detectors. Previous approaches supplemented point clouds through depth completion or upsampling. However, the former suffers from the inconsistency caused by differences in multimodal data, resulting in uneven point cloud quality. Meanwhile, previous upsampling methods convert point clouds into range images which results in a loss of point accuracy. In this paper, we present DAPU, a novel real-time point cloud upsampling method designed to address these challenges. This method consists of three key components: (1) GPR (Ground Points Recognizer), which analyzes the height difference distribution between coplanar and non-coplanar points within patches to identify ground points. GPR establishes a sparse-to-dense index matrix for fast large-scale point cloud queries. (2) DAPKNN (Distribution-Aware Patch KNN), which dynamically adjusts the sampling radius threshold based on distribution to reduce computation and ensure enough neighbors sampling for distant points. (3) Neighbors Upsampling, which linearly upsamples between each pair of neighbors to preserve all point features. KITTI experiments show gains of up to +1.2% AP 3 D " role="presentation"&gt; 3 D 3 D and +1.4% AP B E V " role="presentation"&gt; B E V B E V . Additional evaluations on mini-nuScenes and Waymo further demonstrate consistent improvements across Vehicle, Pedestrian, and Cyclist detection, confirming DAPU’s robustness under diverse LiDAR settings and real-time suitability.
Published: 2026-01-02T16:45:38+00:00
Venue: Neurocomputing
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinghao Hu; Yan Wu; Yujian Mo; Jijun Wang; Yuwei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132579"&gt;10.1016/j.neucom.2025.132579&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;The sparsity and quality of point clouds significantly constrain the development of LiDAR-based 3D object detectors. Previous approaches supplemented point clouds through depth completion or upsampling. However, the former suffers from the inconsistency caused by differences in multimodal data, resulting in uneven point cloud quality. Meanwhile, previous upsampling methods convert point clouds into range images which results in a loss of point accuracy. In this paper, we present DAPU, a novel real-time point cloud upsampling method designed to address these challenges. This method consists of three key components: (1) GPR (Ground Points Recognizer), which analyzes the height difference distribution between coplanar and non-coplanar points within patches to identify ground points. GPR establishes a sparse-to-dense index matrix for fast large-scale point cloud queries. (2) DAPKNN (Distribution-Aware Patch KNN), which dynamically adjusts the sampling radius threshold based on distribution to reduce computation and ensure enough neighbors sampling for distant points. (3) Neighbors Upsampling, which linearly upsamples between each pair of neighbors to preserve all point features. KITTI experiments show gains of up to +1.2% AP 3 D &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; 3 D 3 D and +1.4% AP B E V &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; B E V B E V . Additional evaluations on mini-nuScenes and Waymo further demonstrate consistent improvements across Vehicle, Pedestrian, and Cyclist detection, confirming DAPU’s robustness under diverse LiDAR settings and real-time suitability.&lt;/p&gt;</content:encoded></item><item><title>EPSO-Net: A Multi-Objective Evolutionary Neural Architecture Search with PSO-Guided Mutation Fusion for Explainable Brain Tumor Segmentation</title><link>https://doi.org/10.1016/j.inffus.2025.104119</link><guid>10.1016/j.inffus.2025.104119</guid><pubDate>Sat, 03 Jan 2026 07:35:20 +0000</pubDate><dc:creator>Farhana Yasmin</dc:creator><dc:creator>Yu Xue</dc:creator><dc:creator>Mahade Hasan</dc:creator><dc:creator>Ghulam Muhammad</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104119</prism:doi><description>Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .
Published: 2026-01-03T07:35:20+00:00
Venue: Information Fusion
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Farhana Yasmin; Yu Xue; Mahade Hasan; Ghulam Muhammad&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104119"&gt;10.1016/j.inffus.2025.104119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .&lt;/p&gt;</content:encoded></item><item><title>A Ship Incremental Recognition Framework via Unknown Extraction and Joint Optimization Learning</title><link>https://doi.org/10.3390/rs18010149</link><guid>10.3390/rs18010149</guid><pubDate>Fri, 02 Jan 2026 09:18:56 +0000</pubDate><dc:creator>Yugao Li</dc:creator><dc:creator>Guangzhen Bao</dc:creator><dc:creator>Jianming Hu</dc:creator><dc:creator>Xiyang Zhi</dc:creator><dc:creator>Tianyi Hu</dc:creator><dc:creator>Junjie Wang</dc:creator><dc:creator>Wenbo Wu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010149</prism:doi><description>With the rapid growth of the marine economy and the increasing demand for maritime security, ship target detection has become critically important in both military and civilian applications. However, in complex remote sensing scenarios, challenges such as visual similarity among ships, subtle inter-class differences, and the continual emergence of new categories make traditional closed-world detection methods inadequate. To address these issues, this paper proposes an open-world detection framework for remote sensing ships. The framework integrates two key modules: (1) a Fine-Grained Feature and Extreme Value-based Unknown Recognition (FEUR) module, which leverages tail distribution modeling and adaptive thresholding to achieve precise detection and effective differentiation of unknown ship targets; and (2) a Joint Optimization-based Incremental Learning (JOIL) module, which employs hierarchical elastic weight constraints to differentially update the backbone and detection head, thereby alleviating catastrophic forgetting while incorporating new categories with only a few labeled samples. Extensive experiments on the FGSRCS dataset demonstrate that the proposed method not only maintains high accuracy on known categories but also significantly outperforms mainstream open-world detection approaches in unknown recognition and incremental learning. This work provides both theoretical value and practical potential for continuous ship detection and recognition in complex open environments.
Published: 2026-01-02T09:18:56+00:00
Venue: Remote Sensing
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yugao Li; Guangzhen Bao; Jianming Hu; Xiyang Zhi; Tianyi Hu; Junjie Wang; Wenbo Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010149"&gt;10.3390/rs18010149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;With the rapid growth of the marine economy and the increasing demand for maritime security, ship target detection has become critically important in both military and civilian applications. However, in complex remote sensing scenarios, challenges such as visual similarity among ships, subtle inter-class differences, and the continual emergence of new categories make traditional closed-world detection methods inadequate. To address these issues, this paper proposes an open-world detection framework for remote sensing ships. The framework integrates two key modules: (1) a Fine-Grained Feature and Extreme Value-based Unknown Recognition (FEUR) module, which leverages tail distribution modeling and adaptive thresholding to achieve precise detection and effective differentiation of unknown ship targets; and (2) a Joint Optimization-based Incremental Learning (JOIL) module, which employs hierarchical elastic weight constraints to differentially update the backbone and detection head, thereby alleviating catastrophic forgetting while incorporating new categories with only a few labeled samples. Extensive experiments on the FGSRCS dataset demonstrate that the proposed method not only maintains high accuracy on known categories but also significantly outperforms mainstream open-world detection approaches in unknown recognition and incremental learning. This work provides both theoretical value and practical potential for continuous ship detection and recognition in complex open environments.&lt;/p&gt;</content:encoded></item><item><title>Efficiently Estimating Data Efficiency for Language Model Fine-tuning</title><link>https://arxiv.org/abs/2512.24991v1</link><guid>http://arxiv.org/abs/2512.24991v1</guid><pubDate>Wed, 31 Dec 2025 17:37:29 +0000</pubDate><dc:creator>Gyung Hyun Je</dc:creator><dc:creator>Colin Raffel</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task's data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task's data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.
Published: 2025-12-31T17:37:29+00:00
Venue: arXiv
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gyung Hyun Je; Colin Raffel&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task&amp;#x27;s data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demonstrate across a curated set of 30 specialized tasks that performant LLMs may struggle zero-shot but can attain stronger performance after fine-tuning. This motivates the need for methods to predict a task&amp;#x27;s data efficiency without requiring incremental annotation. After introducing a concrete metric that quantifies a task&amp;#x27;s data efficiency, we propose using the gradient cosine similarity of low-confidence examples to predict data efficiency based on a small number of labeled samples. We validate our approach on a diverse set of tasks with varying data efficiencies, attaining 8.6% error in overall data efficiency prediction and typically eliminating hundreds of unnecessary annotations on each task. Our experiment results and implementation code are available on GitHub.&lt;/p&gt;</content:encoded></item><item><title>Recursive Language Models</title><link>https://arxiv.org/abs/2512.24601v1</link><guid>http://arxiv.org/abs/2512.24601v1</guid><pubDate>Wed, 31 Dec 2025 03:43:41 +0000</pubDate><dc:creator>Alex L. Zhang</dc:creator><dc:creator>Tim Kraska</dc:creator><dc:creator>Omar Khattab</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.
Published: 2025-12-31T03:43:41+00:00
Venue: arXiv
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alex L. Zhang; Tim Kraska; Omar Khattab&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.&lt;/p&gt;</content:encoded></item><item><title>EchoNet: A Hierarchical Collaborative Network for Point Cloud-based 3D Action Recognition</title><link>https://doi.org/10.1016/j.knosys.2025.115257</link><guid>10.1016/j.knosys.2025.115257</guid><pubDate>Fri, 02 Jan 2026 16:07:28 +0000</pubDate><dc:creator>Guojia Huang</dc:creator><dc:creator>Zhenjie Hou</dc:creator><dc:creator>Xing Li</dc:creator><dc:creator>Jiuzhen Liang</dc:creator><dc:creator>Xinwen Zhou</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115257</prism:doi><description>Dynamic point clouds provide inherent geometric fidelity for 3D action recognition, yet their unstructured nature makes it challenging to capture complex spatiotemporal patterns. Existing approaches either rely on local neighborhood aggregation, employ explicit spatiotemporal decoupling, or adopt parallel global modeling. However, they often suffer from limited spatiotemporal awareness, fragmented short-term motion continuity, and a lack of hierarchical progression. To address these issues, we propose the hierarchical collaboration hypothesis: effective representations of dynamic point clouds should follow a progressive abstraction from points to regions to the global level, while maintaining semantic consistency across layers. Building on this hypothesis, we introduce EchoNet, a hierarchical collaborative network composed of three complementary modules: the Point Feature Constructor (PFC) for capturing fine-grained geometric details, the Layered Abstraction Synthesizer (LAS) for hierarchical structural abstraction, and the Temporal Context Refiner (TCR) for enhancing cross-frame temporal dependencies. Furthermore, we design a Multi-Scale Regional Channel Attention (MSRCA) module, which adaptively emphasizes critical action regions by integrating positional encoding with multi-regional context. Experiments on NTU RGB+D 60/120, UTD-MHAD, and MSR Action3D demonstrate that EchoNet achieves state-of-the-art or highly competitive performance, exemplified by a top-tier accuracy of 97.07% on MSR Action3D for complex action recognition. The model also proves effective in large-scale scenarios, attaining 84.3% on the challenging NTU-120 Cross-Subject benchmark. While performance on the large-scale NTU-120 dataset shows the potential for further improvement, our analysis underscores the promise of hierarchical models for building scalable and efficient dynamic point cloud representations.
Published: 2026-01-02T16:07:28+00:00
Venue: Knowledge-Based Systems
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guojia Huang; Zhenjie Hou; Xing Li; Jiuzhen Liang; Xinwen Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115257"&gt;10.1016/j.knosys.2025.115257&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Dynamic point clouds provide inherent geometric fidelity for 3D action recognition, yet their unstructured nature makes it challenging to capture complex spatiotemporal patterns. Existing approaches either rely on local neighborhood aggregation, employ explicit spatiotemporal decoupling, or adopt parallel global modeling. However, they often suffer from limited spatiotemporal awareness, fragmented short-term motion continuity, and a lack of hierarchical progression. To address these issues, we propose the hierarchical collaboration hypothesis: effective representations of dynamic point clouds should follow a progressive abstraction from points to regions to the global level, while maintaining semantic consistency across layers. Building on this hypothesis, we introduce EchoNet, a hierarchical collaborative network composed of three complementary modules: the Point Feature Constructor (PFC) for capturing fine-grained geometric details, the Layered Abstraction Synthesizer (LAS) for hierarchical structural abstraction, and the Temporal Context Refiner (TCR) for enhancing cross-frame temporal dependencies. Furthermore, we design a Multi-Scale Regional Channel Attention (MSRCA) module, which adaptively emphasizes critical action regions by integrating positional encoding with multi-regional context. Experiments on NTU RGB+D 60/120, UTD-MHAD, and MSR Action3D demonstrate that EchoNet achieves state-of-the-art or highly competitive performance, exemplified by a top-tier accuracy of 97.07% on MSR Action3D for complex action recognition. The model also proves effective in large-scale scenarios, attaining 84.3% on the challenging NTU-120 Cross-Subject benchmark. While performance on the large-scale NTU-120 dataset shows the potential for further improvement, our analysis underscores the promise of hierarchical models for building scalable and efficient dynamic point cloud representations.&lt;/p&gt;</content:encoded></item><item><title>A two-stage self-supervised learning framework for breast cancer detection with multi-scale vision transformers</title><link>https://doi.org/10.1016/j.ins.2025.123061</link><guid>10.1016/j.ins.2025.123061</guid><pubDate>Sat, 03 Jan 2026 23:23:29 +0000</pubDate><dc:creator>Shahriar Mohammadi</dc:creator><dc:creator>Mohammad Ahmadi Livani</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2025.123061</prism:doi><description>Breast cancer detection through mammography remains a cornerstone of early diagnosis, yet the limited availability of large, expertly annotated datasets poses a significant challenge for developing robust AI models. To address this data scarcity, we propose a novel Two-Stage Self-Supervised Learning (TSSL) framework named TSSL-MSViT, which utilizes a Multi-Scale Vision Transformer (MSViT) to learn data-efficient mammographic representations. In Stage 1, the MSViT backbone is pretrained using a dual-objective strategy that integrates Multi-Scale Masked Reconstruction (MS-MR) and Cross-Scale Contrastive Learning (CS-C). Unlike prior single-task SSL pipelines, MS-MR captures fine- and coarse-grained structures, while CS-C explicitly aligns multi-resolution and multi-view (CC/MLO) semantics, yielding representations that are simultaneously hierarchical and view-consistent. This synergistic design provides a principled foundation—beyond empirical gains—for learning stable and transferable mammographic features from unlabeled data. In Stage 2, the pretrained MSViT backbone is fine-tuned with limited labeled data for breast-level classification. Comprehensive experiments on the CBIS-DDSM and INbreast datasets demonstrate that TSSL-MSViT consistently outperforms both Convolutional Neural Network (CNN) and Vision Transformer baselines. The model achieves state-of-the-art AUCs of 0.967 (CBIS-DDSM) and 0.972 (INbreast), significantly surpassing the Swin Transformer and other leading architectures. These results highlight the effectiveness of combining multi-scale feature modeling with self-supervised representation learning for data-efficient, generalizable, and accurate mammographic analysis. The proposed framework establishes a strong foundation for future AI-driven diagnostic systems, reducing dependence on extensive expert annotations while enhancing clinical reliability.
Published: 2026-01-03T23:23:29+00:00
Venue: Information Sciences
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shahriar Mohammadi; Mohammad Ahmadi Livani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2025.123061"&gt;10.1016/j.ins.2025.123061&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Breast cancer detection through mammography remains a cornerstone of early diagnosis, yet the limited availability of large, expertly annotated datasets poses a significant challenge for developing robust AI models. To address this data scarcity, we propose a novel Two-Stage Self-Supervised Learning (TSSL) framework named TSSL-MSViT, which utilizes a Multi-Scale Vision Transformer (MSViT) to learn data-efficient mammographic representations. In Stage 1, the MSViT backbone is pretrained using a dual-objective strategy that integrates Multi-Scale Masked Reconstruction (MS-MR) and Cross-Scale Contrastive Learning (CS-C). Unlike prior single-task SSL pipelines, MS-MR captures fine- and coarse-grained structures, while CS-C explicitly aligns multi-resolution and multi-view (CC/MLO) semantics, yielding representations that are simultaneously hierarchical and view-consistent. This synergistic design provides a principled foundation—beyond empirical gains—for learning stable and transferable mammographic features from unlabeled data. In Stage 2, the pretrained MSViT backbone is fine-tuned with limited labeled data for breast-level classification. Comprehensive experiments on the CBIS-DDSM and INbreast datasets demonstrate that TSSL-MSViT consistently outperforms both Convolutional Neural Network (CNN) and Vision Transformer baselines. The model achieves state-of-the-art AUCs of 0.967 (CBIS-DDSM) and 0.972 (INbreast), significantly surpassing the Swin Transformer and other leading architectures. These results highlight the effectiveness of combining multi-scale feature modeling with self-supervised representation learning for data-efficient, generalizable, and accurate mammographic analysis. The proposed framework establishes a strong foundation for future AI-driven diagnostic systems, reducing dependence on extensive expert annotations while enhancing clinical reliability.&lt;/p&gt;</content:encoded></item><item><title>Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions</title><link>https://arxiv.org/abs/2512.24971v1</link><guid>http://arxiv.org/abs/2512.24971v1</guid><pubDate>Wed, 31 Dec 2025 17:00:01 +0000</pubDate><dc:creator>Itallo Patrick Castro Alves Da Silva</dc:creator><dc:creator>Emanuel Adler Medeiros Pereira</dc:creator><dc:creator>Erick de Andrade Barboza</dc:creator><dc:creator>Baldoino Fonseca dos Santos Neto</dc:creator><dc:creator>Marcio de Medeiros Ribeiro</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Compressed deep learning models are crucial for deploying computer vision systems on resource-constrained devices. However, model compression may affect robustness, especially under natural corruption. Therefore, it is important to consider robustness evaluation while validating computer vision systems. This paper presents a comprehensive evaluation of compression techniques - quantization, pruning, and weight clustering applied individually and in combination to convolutional neural networks (ResNet-50, VGG-19, and MobileNetV2). Using the CIFAR-10-C and CIFAR 100-C datasets, we analyze the trade-offs between robustness, accuracy, and compression ratio. Our results show that certain compression strategies not only preserve but can also improve robustness, particularly on networks with more complex architectures. Utilizing multiobjective assessment, we determine the best configurations, showing that customized technique combinations produce beneficial multi-objective results. This study provides insights into selecting compression methods for robust and efficient deployment of models in corrupted real-world environments.
Published: 2025-12-31T17:00:01+00:00
Venue: arXiv
Score: 0.766 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Itallo Patrick Castro Alves Da Silva; Emanuel Adler Medeiros Pereira; Erick de Andrade Barboza; Baldoino Fonseca dos Santos Neto; Marcio de Medeiros Ribeiro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (consider)&lt;/p&gt;
&lt;p&gt;Compressed deep learning models are crucial for deploying computer vision systems on resource-constrained devices. However, model compression may affect robustness, especially under natural corruption. Therefore, it is important to consider robustness evaluation while validating computer vision systems. This paper presents a comprehensive evaluation of compression techniques - quantization, pruning, and weight clustering applied individually and in combination to convolutional neural networks (ResNet-50, VGG-19, and MobileNetV2). Using the CIFAR-10-C and CIFAR 100-C datasets, we analyze the trade-offs between robustness, accuracy, and compression ratio. Our results show that certain compression strategies not only preserve but can also improve robustness, particularly on networks with more complex architectures. Utilizing multiobjective assessment, we determine the best configurations, showing that customized technique combinations produce beneficial multi-objective results. This study provides insights into selecting compression methods for robust and efficient deployment of models in corrupted real-world environments.&lt;/p&gt;</content:encoded></item><item><title>Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</title><link>https://arxiv.org/abs/2512.24617v1</link><guid>http://arxiv.org/abs/2512.24617v1</guid><pubDate>Wed, 31 Dec 2025 04:19:33 +0000</pubDate><dc:creator>Xingwei Qu</dc:creator><dc:creator>Shaowen Wang</dc:creator><dc:creator>Zihao Huang</dc:creator><dc:creator>Kai Hua</dc:creator><dc:creator>Fan Yin</dc:creator><dc:creator>Rui-Jie Zhu</dc:creator><dc:creator>Jundong Zhou</dc:creator><dc:creator>Qiyang Min</dc:creator><dc:creator>Zihao Wang</dc:creator><dc:creator>Yizhi Li</dc:creator><dc:creator>Tianyu Zhang</dc:creator><dc:creator>He Xing</dc:creator><dc:creator>Zheng Zhang</dc:creator><dc:creator>Yuxuan Song</dc:creator><dc:creator>Tianyu Zheng</dc:creator><dc:creator>Zhiyuan Zeng</dc:creator><dc:creator>Chenghua Lin</dc:creator><dc:creator>Ge Zhang</dc:creator><dc:creator>Wenhao Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.
Published: 2025-12-31T04:19:33+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingwei Qu; Shaowen Wang; Zihao Huang; Kai Hua; Fan Yin; Rui-Jie Zhu; Jundong Zhou; Qiyang Min; Zihao Wang; Yizhi Li; Tianyu Zhang; He Xing; Zheng Zhang; Yuxuan Song; Tianyu Zheng; Zhiyuan Zeng; Chenghua Lin; Ge Zhang; Wenhao Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $μ$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.&lt;/p&gt;</content:encoded></item><item><title>Collaborative Low-Rank Adaptation for Pre-Trained Vision Transformers</title><link>https://arxiv.org/abs/2512.24603v1</link><guid>http://arxiv.org/abs/2512.24603v1</guid><pubDate>Wed, 31 Dec 2025 03:46:49 +0000</pubDate><dc:creator>Zheng Liu</dc:creator><dc:creator>Jinchao Zhu</dc:creator><dc:creator>Gao Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Low-rank adaptation (LoRA) has achieved remarkable success in fine-tuning pre-trained vision transformers for various downstream tasks. Existing studies mainly focus on exploring more parameter-efficient strategies or more effective representation learning schemes. However, these methods either sacrifice fine-tuning performance or introduce excessive trainable parameters, failing to strike a balance between learning performance and parameter efficiency. To address this problem, we propose a novel tuning method named collaborative low-rank adaptation (CLoRA) in this paper. CLoRA consists of base-space sharing and sample-agnostic diversity enhancement (SADE) components. To maintain parameter efficiency while expanding the learning capacity of low-rank modules (LRMs), base-space sharing allows all LRMs to share a set of down/up-projection spaces. In CLoRA, the low-rank matrices obtained from the shared spaces collaboratively construct each LRM. Since the representations extracted by these matrices may contain redundant information, SADE is employed to regularize the similarities among them to encourage diverse representations in the training process. We conduct extensive experiments on widely used image and point cloud datasets to evaluate the performance of CLoRA. Experimental results demonstrate that CLoRA strikes a better balance between learning performance and parameter efficiency, while requiring the fewest GFLOPs for point cloud analysis, compared with the state-of-the-art methods.
Published: 2025-12-31T03:46:49+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheng Liu; Jinchao Zhu; Gao Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Low-rank adaptation (LoRA) has achieved remarkable success in fine-tuning pre-trained vision transformers for various downstream tasks. Existing studies mainly focus on exploring more parameter-efficient strategies or more effective representation learning schemes. However, these methods either sacrifice fine-tuning performance or introduce excessive trainable parameters, failing to strike a balance between learning performance and parameter efficiency. To address this problem, we propose a novel tuning method named collaborative low-rank adaptation (CLoRA) in this paper. CLoRA consists of base-space sharing and sample-agnostic diversity enhancement (SADE) components. To maintain parameter efficiency while expanding the learning capacity of low-rank modules (LRMs), base-space sharing allows all LRMs to share a set of down/up-projection spaces. In CLoRA, the low-rank matrices obtained from the shared spaces collaboratively construct each LRM. Since the representations extracted by these matrices may contain redundant information, SADE is employed to regularize the similarities among them to encourage diverse representations in the training process. We conduct extensive experiments on widely used image and point cloud datasets to evaluate the performance of CLoRA. Experimental results demonstrate that CLoRA strikes a better balance between learning performance and parameter efficiency, while requiring the fewest GFLOPs for point cloud analysis, compared with the state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Research Challenges and Future Directions in Transformer-Based Neural Machine Translation</title><link>https://doi.org/10.1016/j.eswa.2025.131062</link><guid>10.1016/j.eswa.2025.131062</guid><pubDate>Fri, 02 Jan 2026 23:48:36 +0000</pubDate><dc:creator>Anasua Banerjee</dc:creator><dc:creator>Binod Kumar Singh</dc:creator><dc:creator>Vinay Kumar</dc:creator><dc:creator>Debajyoty Banik</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131062</prism:doi><description>Despite significant progress, neural machine translation continues to face challenges, such as data scarcity and limitations in evaluation methods. Because NMT systems are data-driven, we need a large amount of parallel corpora to achieve robust performance. This paper discusses the NMT techniques and explores how integrating large language models (LLMs) can enhance translation quality. We discuss strategies to overcome data scarcity, including pre-training and transfer learning. Our comparative analysis of LLM-integrated architectures shows that BERT-fused models consistently outperform other fusion-based approaches for specific language pairs, and we propose methods to reduce their computational overhead, such as LoRA and other parameter-efficient fine-tuning techniques. Furthermore, we present a practical roadmap for adapting COMET and xCOMET metrics to underrepresented languages with limited annotated data. Overall, this study highlights open challenges and emerging trends in NMT, offering actionable insights to guide future research and help address gaps in machine translation.
Published: 2026-01-02T23:48:36+00:00
Venue: Expert Systems with Applications
Score: 0.762 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anasua Banerjee; Binod Kumar Singh; Vinay Kumar; Debajyoty Banik&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131062"&gt;10.1016/j.eswa.2025.131062&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (consider)&lt;/p&gt;
&lt;p&gt;Despite significant progress, neural machine translation continues to face challenges, such as data scarcity and limitations in evaluation methods. Because NMT systems are data-driven, we need a large amount of parallel corpora to achieve robust performance. This paper discusses the NMT techniques and explores how integrating large language models (LLMs) can enhance translation quality. We discuss strategies to overcome data scarcity, including pre-training and transfer learning. Our comparative analysis of LLM-integrated architectures shows that BERT-fused models consistently outperform other fusion-based approaches for specific language pairs, and we propose methods to reduce their computational overhead, such as LoRA and other parameter-efficient fine-tuning techniques. Furthermore, we present a practical roadmap for adapting COMET and xCOMET metrics to underrepresented languages with limited annotated data. Overall, this study highlights open challenges and emerging trends in NMT, offering actionable insights to guide future research and help address gaps in machine translation.&lt;/p&gt;</content:encoded></item><item><title>A Multi-Granularity Scene-Aware Graph Convolution Method for Weakly Supervised Person Search</title><link>https://doi.org/10.1007/s11263-025-02665-3</link><guid>10.1007/s11263-025-02665-3</guid><pubDate>Sat, 03 Jan 2026 09:25:59 +0000</pubDate><dc:creator>De Cheng</dc:creator><dc:creator>Haichun Tai</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Xiangqian Zhao</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02665-3</prism:doi><description>One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .
Published: 2026-01-03T09:25:59+00:00
Venue: International Journal of Computer Vision
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; De Cheng; Haichun Tai; Nannan Wang; Xiangqian Zhao; Jie Li; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02665-3"&gt;10.1007/s11263-025-02665-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .&lt;/p&gt;</content:encoded></item><item><title>FireRescue: A UAV-Based Dataset and Enhanced YOLO Model for Object Detection in Fire Rescue Scenes</title><link>https://arxiv.org/abs/2512.24622v1</link><guid>http://arxiv.org/abs/2512.24622v1</guid><pubDate>Wed, 31 Dec 2025 04:37:51 +0000</pubDate><dc:creator>Qingyu Xu</dc:creator><dc:creator>Runtong Zhang</dc:creator><dc:creator>Zihuan Qiu</dc:creator><dc:creator>Fanman Meng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Object detection in fire rescue scenarios is importance for command and decision-making in firefighting operations. However, existing research still suffers from two main limitations. First, current work predominantly focuses on environments such as mountainous or forest areas, while paying insufficient attention to urban rescue scenes, which are more frequent and structurally complex. Second, existing detection systems include a limited number of classes, such as flames and smoke, and lack a comprehensive system covering key targets crucial for command decisions, such as fire trucks and firefighters. To address the above issues, this paper first constructs a new dataset named "FireRescue" for rescue command, which covers multiple rescue scenarios, including urban, mountainous, forest, and water areas, and contains eight key categories such as fire trucks and firefighters, with a total of 15,980 images and 32,000 bounding boxes. Secondly, to tackle the problems of inter-class confusion and missed detection of small targets caused by chaotic scenes, diverse targets, and long-distance shooting, this paper proposes an improved model named FRS-YOLO. On the one hand, the model introduces a plug-and-play multidi-mensional collaborative enhancement attention module, which enhances the discriminative representation of easily confused categories (e.g., fire trucks vs. ordinary trucks) through cross-dimensional feature interaction. On the other hand, it integrates a dynamic feature sampler to strengthen high-response foreground features, thereby mitigating the effects of smoke occlusion and background interference. Experimental results demonstrate that object detection in fire rescue scenarios is highly challenging, and the proposed method effectively improves the detection performance of YOLO series models in this context.
Published: 2025-12-31T04:37:51+00:00
Venue: arXiv
Score: 0.760 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingyu Xu; Runtong Zhang; Zihuan Qiu; Fanman Meng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.760 (consider)&lt;/p&gt;
&lt;p&gt;Object detection in fire rescue scenarios is importance for command and decision-making in firefighting operations. However, existing research still suffers from two main limitations. First, current work predominantly focuses on environments such as mountainous or forest areas, while paying insufficient attention to urban rescue scenes, which are more frequent and structurally complex. Second, existing detection systems include a limited number of classes, such as flames and smoke, and lack a comprehensive system covering key targets crucial for command decisions, such as fire trucks and firefighters. To address the above issues, this paper first constructs a new dataset named &amp;quot;FireRescue&amp;quot; for rescue command, which covers multiple rescue scenarios, including urban, mountainous, forest, and water areas, and contains eight key categories such as fire trucks and firefighters, with a total of 15,980 images and 32,000 bounding boxes. Secondly, to tackle the problems of inter-class confusion and missed detection of small targets caused by chaotic scenes, diverse targets, and long-distance shooting, this paper proposes an improved model named FRS-YOLO. On the one hand, the model introduces a plug-and-play multidi-mensional collaborative enhancement attention module, which enhances the discriminative representation of easily confused categories (e.g., fire trucks vs. ordinary trucks) through cross-dimensional feature interaction. On the other hand, it integrates a dynamic feature sampler to strengthen high-response foreground features, thereby mitigating the effects of smoke occlusion and background interference. Experimental results demonstrate that object detection in fire rescue scenarios is highly challenging, and the proposed method effectively improves the detection performance of YOLO series models in this context.&lt;/p&gt;</content:encoded></item><item><title>Extreme Weakly Supervised Binary Semantic Image Segmentation via One-Pixel Supervision</title><link>https://doi.org/10.1016/j.patcog.2026.113048</link><guid>10.1016/j.patcog.2026.113048</guid><pubDate>Sat, 03 Jan 2026 07:24:55 +0000</pubDate><dc:creator>Matthaios Tzimas</dc:creator><dc:creator>Vasileios Mygdalis</dc:creator><dc:creator>Christos Papaioannidis</dc:creator><dc:creator>Ioannis Pitas</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113048</prism:doi><description>Despite recent advancements, Unsupervised Semantic Segmentation (USS) methods still exhibit a significant performance deficit compared to supervised approaches, particularly in binary semantic segmentation. This limitation arises because, without supervision, USS methods struggle to distinguish foreground from background image regions, particularly when the foreground contains small or uncommon objects. This issue is addressed by our proposed Extremely Weakly Supervised Binary Semantic Segmentation (EWS) framework. EWS expects minimal supervision, consisting only of a small set of one-pixel annotations explicitly belonging to the foreground class across the entire image dataset. Our approach leverages these one-pixel annotations and employs two contrastive losses to map visual transformer features into well-separated foreground and background feature clusters. Additionally, we propose a novel loss function to eliminate the need for hyperparameter tuning of the contrastive loss threshold, by dynamically computing it based on the similarity between the input image features. Even if we employ a single one-pixel annotation, EWS achieves competitive results in binary segmentation tasks while maintaining low computational costs, making it an efficient solution for critical segmentation applications. GitHub Repo: https://github.com/matJTzimas/EWS
Published: 2026-01-03T07:24:55+00:00
Venue: Pattern Recognition
Score: 0.760 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Matthaios Tzimas; Vasileios Mygdalis; Christos Papaioannidis; Ioannis Pitas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113048"&gt;10.1016/j.patcog.2026.113048&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.760 (consider)&lt;/p&gt;
&lt;p&gt;Despite recent advancements, Unsupervised Semantic Segmentation (USS) methods still exhibit a significant performance deficit compared to supervised approaches, particularly in binary semantic segmentation. This limitation arises because, without supervision, USS methods struggle to distinguish foreground from background image regions, particularly when the foreground contains small or uncommon objects. This issue is addressed by our proposed Extremely Weakly Supervised Binary Semantic Segmentation (EWS) framework. EWS expects minimal supervision, consisting only of a small set of one-pixel annotations explicitly belonging to the foreground class across the entire image dataset. Our approach leverages these one-pixel annotations and employs two contrastive losses to map visual transformer features into well-separated foreground and background feature clusters. Additionally, we propose a novel loss function to eliminate the need for hyperparameter tuning of the contrastive loss threshold, by dynamically computing it based on the similarity between the input image features. Even if we employ a single one-pixel annotation, EWS achieves competitive results in binary segmentation tasks while maintaining low computational costs, making it an efficient solution for critical segmentation applications. GitHub Repo: https://github.com/matJTzimas/EWS&lt;/p&gt;</content:encoded></item><item><title>CAMT: A novel symmetric cross-modal adaptive modulation framework for RGB-T tracking</title><link>https://doi.org/10.1016/j.neucom.2025.132602</link><guid>10.1016/j.neucom.2025.132602</guid><pubDate>Fri, 02 Jan 2026 16:07:15 +0000</pubDate><dc:creator>Yisong Xiao</dc:creator><dc:creator>Guixi Liu</dc:creator><dc:creator>Hanlin Huang</dc:creator><dc:creator>Ruke Xiong</dc:creator><dc:creator>Yinghao Li</dc:creator><dc:creator>Qian Lu</dc:creator><dc:creator>Zhiyu Wu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132602</prism:doi><description>Benefiting from the robustness of multi-modal tracking in complex scenarios, an increasing number of studies have incorporated thermal infrared (TIR) into object tracking. Existing asymmetric methods mainly rely on prompt-guided fine-tuning to leverage modality-specific knowledge. Yet structural limitations make them over-rely on a specific dominant modality, limiting other modalities’ information utilization. Other asymmetric structures mainly adopt the feature fusion method trained with scarce multi-modal data, which leads to overfitting risks. To address these problems, we proposed a novel symmetric multi-modal tracking framework named CAMTrack that uses a feature extraction network with shared weights to extract the features from the two modalities respectively and fuse the feature information at the same time, reducing a large amount of redundant learning and the number of parameters. We also proposed a plug-and-play lightweight cross-modal adaptive modulator (CAM), which does not specify a dominant modality; instead, the information of the two modalities circulates bidirectionally, enabling both to utilize each other’s advantageous features and thus achieving more robust tracking. In addition, we designed a cross-modal token elimination strategy to further accelerate inference speed and improve accuracy. Extensive experiments on three large RGB-T benchmarks show our method outperforms other state-of-the-art trackers and runs at 150.9fps.
Published: 2026-01-02T16:07:15+00:00
Venue: Neurocomputing
Score: 0.759 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yisong Xiao; Guixi Liu; Hanlin Huang; Ruke Xiong; Yinghao Li; Qian Lu; Zhiyu Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132602"&gt;10.1016/j.neucom.2025.132602&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (consider)&lt;/p&gt;
&lt;p&gt;Benefiting from the robustness of multi-modal tracking in complex scenarios, an increasing number of studies have incorporated thermal infrared (TIR) into object tracking. Existing asymmetric methods mainly rely on prompt-guided fine-tuning to leverage modality-specific knowledge. Yet structural limitations make them over-rely on a specific dominant modality, limiting other modalities’ information utilization. Other asymmetric structures mainly adopt the feature fusion method trained with scarce multi-modal data, which leads to overfitting risks. To address these problems, we proposed a novel symmetric multi-modal tracking framework named CAMTrack that uses a feature extraction network with shared weights to extract the features from the two modalities respectively and fuse the feature information at the same time, reducing a large amount of redundant learning and the number of parameters. We also proposed a plug-and-play lightweight cross-modal adaptive modulator (CAM), which does not specify a dominant modality; instead, the information of the two modalities circulates bidirectionally, enabling both to utilize each other’s advantageous features and thus achieving more robust tracking. In addition, we designed a cross-modal token elimination strategy to further accelerate inference speed and improve accuracy. Extensive experiments on three large RGB-T benchmarks show our method outperforms other state-of-the-art trackers and runs at 150.9fps.&lt;/p&gt;</content:encoded></item></channel></rss>