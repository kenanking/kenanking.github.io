<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 30 Dec 2025 02:46:36 +0000</lastBuildDate><item><title>CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3649001</link><guid>10.1109/tpami.2025.3649001</guid><pubDate>Mon, 29 Dec 2025 18:38:19 +0000</pubDate><dc:creator>Ziyang Gong</dc:creator><dc:creator>Zhixiang Wei</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Xiaoxing Hu</dc:creator><dc:creator>Xianzheng Ma</dc:creator><dc:creator>Hongruixuan Chen</dc:creator><dc:creator>Yuru Jia</dc:creator><dc:creator>Yupeng Deng</dc:creator><dc:creator>Zhenming Ji</dc:creator><dc:creator>Xiangwei Zhu</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Naoto Yokoya</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Bo Du</dc:creator><dc:creator>Junchi Yan</dc:creator><dc:creator>Liangpei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649001</prism:doi><description>Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.
Published: 2025-12-29T18:38:19+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.838 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyang Gong; Zhixiang Wei; Di Wang; Xiaoxing Hu; Xianzheng Ma; Hongruixuan Chen; Yuru Jia; Yupeng Deng; Zhenming Ji; Xiangwei Zhu; Xue Yang; Naoto Yokoya; Jing Zhang; Bo Du; Junchi Yan; Liangpei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649001"&gt;10.1109/tpami.2025.3649001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.838 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Versatile cardiovascular signal generation with a unified diffusion transformer</title><link>https://doi.org/10.1038/s42256-025-01147-y</link><guid>10.1038/s42256-025-01147-y</guid><pubDate>Mon, 29 Dec 2025 10:01:34 +0000</pubDate><dc:creator>Zehua Chen</dc:creator><dc:creator>Yuyang Miao</dc:creator><dc:creator>Liyuan Wang</dc:creator><dc:creator>Luyun Fan</dc:creator><dc:creator>Danilo P. Mandic</dc:creator><dc:creator>Jun Zhu</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01147-y</prism:doi><description>Cardiovascular signals such as photoplethysmography, electrocardiography and blood pressure are inherently correlated and complementary, together reflecting the health of the cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multimodal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, as well as ensuring interpretability for human experts. These advantages establish UniCardio as a practical and robust framework for advancing artificial-intelligence-assisted healthcare. UniCardio is a unified framework for versatile multimodal cardiovascular signal generation, enabling robust signal restoration and cross-modal translation to detect abnormal conditions and estimate vital signs in real-time health monitoring.
Published: 2025-12-29T10:01:34+00:00
Venue: Nature Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zehua Chen; Yuyang Miao; Liyuan Wang; Luyun Fan; Danilo P. Mandic; Jun Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01147-y"&gt;10.1038/s42256-025-01147-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Cardiovascular signals such as photoplethysmography, electrocardiography and blood pressure are inherently correlated and complementary, together reflecting the health of the cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multimodal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, as well as ensuring interpretability for human experts. These advantages establish UniCardio as a practical and robust framework for advancing artificial-intelligence-assisted healthcare. UniCardio is a unified framework for versatile multimodal cardiovascular signal generation, enabling robust signal restoration and cross-modal translation to detect abnormal conditions and estimate vital signs in real-time health monitoring.&lt;/p&gt;</content:encoded></item><item><title>Exploring Syn-to-Real Domain Adaptation for Military Target Detection</title><link>https://arxiv.org/abs/2512.23208v1</link><guid>http://arxiv.org/abs/2512.23208v1</guid><pubDate>Mon, 29 Dec 2025 05:05:41 +0000</pubDate><dc:creator>Jongoh Jeong</dc:creator><dc:creator>Youngjin Oh</dc:creator><dc:creator>Gyeongrae Nam</dc:creator><dc:creator>Jeongeun Lee</dc:creator><dc:creator>Kuk-Jin Yoon</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.
Published: 2025-12-29T05:05:41+00:00
Venue: arXiv
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jongoh Jeong; Youngjin Oh; Gyeongrae Nam; Jeongeun Lee; Kuk-Jin Yoon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.&lt;/p&gt;</content:encoded></item><item><title>Cross-domain Distillation for Unsupervised Domain Adaptation with Large Vision-language Models</title><link>https://doi.org/10.1016/j.patcog.2025.112985</link><guid>10.1016/j.patcog.2025.112985</guid><pubDate>Sun, 28 Dec 2025 15:02:53 +0000</pubDate><dc:creator>Xingwei Deng</dc:creator><dc:creator>Yangtao Wang</dc:creator><dc:creator>Yanzhao Xie</dc:creator><dc:creator>Xin Tan</dc:creator><dc:creator>Maobin Tang</dc:creator><dc:creator>Meie Fang</dc:creator><dc:creator>Wensheng Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112985</prism:doi><description>Large vision-language models (VLMs), incorporating the prompt learning mechanism, have achieved promising results in cross-domain tasks. However, leveraging VLMs to transfer the knowledge from the source domain to the target domain remains a challenging task for unsupervised domain adaptation (UDA). To this end, we propose C ross-domain D istillation for U DA with LVMs (termed as CDU). Firstly, CDU trains a source model by embedding the knowledge of the source domain (including both each sample and its corresponding class category) into VLMs in a lightweight manner. Secondly, CDU makes full use of the image and text semantics from the source model to guide the target model learning, thereby achieving domain alignment to yield semantically consistent representations across domains. We conduct extensive experiments on 3 popular UDA datasets including Office-31, Office-Home, and DomainNet. Experimental results verify our method consistently surpasses the state-of-the-art (SOTA) UDA methods by a large margin with higher performance and lower model complexity on various UDA benchmarks. Take Office-Home as an example, the average accuracy of CDU exceeds existing methods by at least 3%, yet the number of learnable parameters only accounts for 17.9% and the inference time only takes up 4.3% compared to the strongest candidates. The code of this paper is available at GitHub: https://github.com/1d1x1w/CDU .
Published: 2025-12-28T15:02:53+00:00
Venue: Pattern Recognition
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingwei Deng; Yangtao Wang; Yanzhao Xie; Xin Tan; Maobin Tang; Meie Fang; Wensheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112985"&gt;10.1016/j.patcog.2025.112985&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision-language models (VLMs), incorporating the prompt learning mechanism, have achieved promising results in cross-domain tasks. However, leveraging VLMs to transfer the knowledge from the source domain to the target domain remains a challenging task for unsupervised domain adaptation (UDA). To this end, we propose C ross-domain D istillation for U DA with LVMs (termed as CDU). Firstly, CDU trains a source model by embedding the knowledge of the source domain (including both each sample and its corresponding class category) into VLMs in a lightweight manner. Secondly, CDU makes full use of the image and text semantics from the source model to guide the target model learning, thereby achieving domain alignment to yield semantically consistent representations across domains. We conduct extensive experiments on 3 popular UDA datasets including Office-31, Office-Home, and DomainNet. Experimental results verify our method consistently surpasses the state-of-the-art (SOTA) UDA methods by a large margin with higher performance and lower model complexity on various UDA benchmarks. Take Office-Home as an example, the average accuracy of CDU exceeds existing methods by at least 3%, yet the number of learnable parameters only accounts for 17.9% and the inference time only takes up 4.3% compared to the strongest candidates. The code of this paper is available at GitHub: https://github.com/1d1x1w/CDU .&lt;/p&gt;</content:encoded></item><item><title>Multi-granularity Feature Fusion Network integrating ASC for SAR Target Recognition</title><link>https://doi.org/10.1109/lgrs.2025.3649343</link><guid>10.1109/lgrs.2025.3649343</guid><pubDate>Mon, 29 Dec 2025 18:40:34 +0000</pubDate><dc:creator>Haohao Ren</dc:creator><dc:creator>Shuyi Liang</dc:creator><dc:creator>Lei Miao</dc:creator><dc:creator>Qiuyan Huang</dc:creator><dc:creator>Youxin Lv</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3649343</prism:doi><description>Deep learning has achieved remarkable progress in synthetic aperture radar automatic target recognition (SAR ATR). However, data-driven methods suffer from spurious correlations that conflate target attributes with confounding factors limiting generalization and robustness in complex conditions. The attribute scattering center (ASC) model, which can map SAR data to physically interpretable features, offers insights for addressing the aforementioned problem. Therefore, this article proposes a multi-granularity feature fusion network (MgFFNet) leveraging ASC to realize robust and interpretable target recognition in complex scenarios. First, we extract multi-granularity scattering features based on ASC model including scattering points feature and component feature. On this basis, we develop a multi-granularity deep feature embedding module that leverages scattering features to hierarchically guide vision features toward causally interpretable representations. Finally, we propose a multi-granularity feature fusion module to achieve different features aggregation, so as to facilitate target identity decision making. Evaluation experiments on two publicly released datasets demonstrate that the proposed approach surpasses numerous advanced SAR ATR methods.
Published: 2025-12-29T18:40:34+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haohao Ren; Shuyi Liang; Lei Miao; Qiuyan Huang; Youxin Lv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3649343"&gt;10.1109/lgrs.2025.3649343&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning has achieved remarkable progress in synthetic aperture radar automatic target recognition (SAR ATR). However, data-driven methods suffer from spurious correlations that conflate target attributes with confounding factors limiting generalization and robustness in complex conditions. The attribute scattering center (ASC) model, which can map SAR data to physically interpretable features, offers insights for addressing the aforementioned problem. Therefore, this article proposes a multi-granularity feature fusion network (MgFFNet) leveraging ASC to realize robust and interpretable target recognition in complex scenarios. First, we extract multi-granularity scattering features based on ASC model including scattering points feature and component feature. On this basis, we develop a multi-granularity deep feature embedding module that leverages scattering features to hierarchically guide vision features toward causally interpretable representations. Finally, we propose a multi-granularity feature fusion module to achieve different features aggregation, so as to facilitate target identity decision making. Evaluation experiments on two publicly released datasets demonstrate that the proposed approach surpasses numerous advanced SAR ATR methods.&lt;/p&gt;</content:encoded></item><item><title>UDG-Prom: A Unified Dense-Guided Semantic Prompting for Cross-Domain Few-Shot Image Segmentation</title><link>https://doi.org/10.1016/j.knosys.2025.115207</link><guid>10.1016/j.knosys.2025.115207</guid><pubDate>Mon, 29 Dec 2025 17:00:32 +0000</pubDate><dc:creator>Jiaqi Yang</dc:creator><dc:creator>Xiangjian He</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Yaning Zhang</dc:creator><dc:creator>Jingxi Hu</dc:creator><dc:creator>Linlin Shen</dc:creator><dc:creator>Guoping Qiu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115207</prism:doi><description>Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.
Published: 2025-12-29T17:00:32+00:00
Venue: Knowledge-Based Systems
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Yang; Xiangjian He; Xin Chen; Yaning Zhang; Jingxi Hu; Linlin Shen; Guoping Qiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115207"&gt;10.1016/j.knosys.2025.115207&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.&lt;/p&gt;</content:encoded></item><item><title>YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection</title><link>https://arxiv.org/abs/2512.23273v1</link><guid>http://arxiv.org/abs/2512.23273v1</guid><pubDate>Mon, 29 Dec 2025 07:54:49 +0000</pubDate><dc:creator>Xu Lin</dc:creator><dc:creator>Jinlong Peng</dc:creator><dc:creator>Zhenye Gan</dc:creator><dc:creator>Jiawen Zhu</dc:creator><dc:creator>Jun Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.
Published: 2025-12-29T07:54:49+00:00
Venue: arXiv
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xu Lin; Jinlong Peng; Zhenye Gan; Jiawen Zhu; Jun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.&lt;/p&gt;</content:encoded></item><item><title>Continuous Review and Timely Correction: Enhancing the Resistance to Noisy Labels Via Self-Not-True and Class-Wise Distillation</title><link>https://doi.org/10.1109/tpami.2025.3649111</link><guid>10.1109/tpami.2025.3649111</guid><pubDate>Mon, 29 Dec 2025 18:38:19 +0000</pubDate><dc:creator>Long Lan</dc:creator><dc:creator>Jingyi Wang</dc:creator><dc:creator>Xinghao Wu</dc:creator><dc:creator>Bo Han</dc:creator><dc:creator>Xinwang Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649111</prism:doi><description>Deep neural networks possess remarkable learning capabilities and expressive power, but this makes them vulnerable to overfitting, especially when they encounter mislabeled data. A notable phenomenon called the memorization effect occurs when networks first learn the correctly labeled data and later memorize the mislabeled instances. While early stopping can mitigate overfitting, it doesn't entirely prevent networks from adapting to incorrect labels during the initial training phases, which can result in losing valuable insights from accurate data. Moreover, early stopping cannot rectify the mistakes caused by mislabeled inputs, underscoring the need for improved strategies. In this paper, we introduce an innovative mechanism for continuous review and timely correction of learned knowledge. Our approach allows the network to repeatedly revisit and reinforce correct information while promptly addressing any inaccuracies stemming from mislabeled data. We present a novel method called self-not-true-distillation (SNTD). This technique employs self-distillation, where the network from previous training iterations acts as a teacher, guiding the current network to review and solidify its understanding of accurate labels. Crucially, SNTD masks the true class label in the logits during this process, concentrating on the non-true classes to correct any erroneous knowledge that may have been acquired. We also recognize that different data classes follow distinct learning trajectories. A single teacher network might struggle to effectively guide the learning of all classes at once, which necessitates selecting different teacher networks for each specific class. Additionally, the influence of the teacher network's guidance varies throughout the training process. To address these challenges, we propose SNTD+, which integrates a class-wise distillation strategy along with a dynamic weight adjustment mechanism. Together, these enhancements significantly bolster SNTD's robustness in...
Published: 2025-12-29T18:38:19+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Long Lan; Jingyi Wang; Xinghao Wu; Bo Han; Xinwang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649111"&gt;10.1109/tpami.2025.3649111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Deep neural networks possess remarkable learning capabilities and expressive power, but this makes them vulnerable to overfitting, especially when they encounter mislabeled data. A notable phenomenon called the memorization effect occurs when networks first learn the correctly labeled data and later memorize the mislabeled instances. While early stopping can mitigate overfitting, it doesn&amp;#x27;t entirely prevent networks from adapting to incorrect labels during the initial training phases, which can result in losing valuable insights from accurate data. Moreover, early stopping cannot rectify the mistakes caused by mislabeled inputs, underscoring the need for improved strategies. In this paper, we introduce an innovative mechanism for continuous review and timely correction of learned knowledge. Our approach allows the network to repeatedly revisit and reinforce correct information while promptly addressing any inaccuracies stemming from mislabeled data. We present a novel method called self-not-true-distillation (SNTD). This technique employs self-distillation, where the network from previous training iterations acts as a teacher, guiding the current network to review and solidify its understanding of accurate labels. Crucially, SNTD masks the true class label in the logits during this process, concentrating on the non-true classes to correct any erroneous knowledge that may have been acquired. We also recognize that different data classes follow distinct learning trajectories. A single teacher network might struggle to effectively guide the learning of all classes at once, which necessitates selecting different teacher networks for each specific class. Additionally, the influence of the teacher network&amp;#x27;s guidance varies throughout the training process. To address these challenges, we propose SNTD+, which integrates a class-wise distillation strategy along with a dynamic weight adjustment mechanism. Together, these enhancements significantly bolster SNTD&amp;#x27;s robustness in...&lt;/p&gt;</content:encoded></item><item><title>CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision</title><link>https://arxiv.org/abs/2512.22969v1</link><guid>http://arxiv.org/abs/2512.22969v1</guid><pubDate>Sun, 28 Dec 2025 15:21:20 +0000</pubDate><dc:creator>Behnam Raoufi</dc:creator><dc:creator>Hossein Sharify</dc:creator><dc:creator>Mohamad Mahdee Ramezanee</dc:creator><dc:creator>Khosrow Hajsadeghi</dc:creator><dc:creator>Saeed Bagheri Shouraki</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.
Published: 2025-12-28T15:21:20+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Behnam Raoufi; Hossein Sharify; Mohamad Mahdee Ramezanee; Khosrow Hajsadeghi; Saeed Bagheri Shouraki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.&lt;/p&gt;</content:encoded></item><item><title>Foundation Model-based Auxiliary Framework for Object Detection in Aerial Remote Sensing Images</title><link>https://doi.org/10.1109/taes.2025.3649185</link><guid>10.1109/taes.2025.3649185</guid><pubDate>Mon, 29 Dec 2025 18:40:09 +0000</pubDate><dc:creator>Wanjie Lu</dc:creator><dc:creator>Chaoyang Niu</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Chaozhen Lan</dc:creator><dc:creator>Shiju Wang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3649185</prism:doi><description>When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.
Published: 2025-12-29T18:40:09+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wanjie Lu; Chaoyang Niu; Wei Liu; Chaozhen Lan; Shiju Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3649185"&gt;10.1109/taes.2025.3649185&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.&lt;/p&gt;</content:encoded></item><item><title>CLIP-driven feature disambiguation and cross-modal synergy for few-shot semantic segmentation</title><link>https://doi.org/10.1016/j.eswa.2025.130892</link><guid>10.1016/j.eswa.2025.130892</guid><pubDate>Mon, 29 Dec 2025 13:09:57 +0000</pubDate><dc:creator>Shangjing Chen</dc:creator><dc:creator>Feng Xu</dc:creator><dc:creator>Xin Lyu</dc:creator><dc:creator>Dafa Wang</dc:creator><dc:creator>Xin Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130892</prism:doi><description>Few-shot semantic segmentation aims to segment novel objects with limited annotations but faces challenges from ambiguous feature representations caused by intra-class diversity and inter-class similarity. While existing methods integrate CLIP for cross-modal guidance, these approaches tend to overlook two critical limitations. First, because the CLIP visual encoder encodes not only local appearance but also global context from the entire image, foreground and background regions may exhibit similar responses in the visual representations. Second, static text prompts are unable to dynamically model the actual-scenario interactions between visual content and text, leading to suboptimal guidance for segmentation tasks. To address these problems, we propose the CLIP-Driven Feature Disambiguation and Cross-Modal Synergy Network (FDCMNet). For the ambiguity from coarse-grained semantics, we design a Contrastive Feature Disentanglement module (CFD), which explicitly discriminates foreground and background by contrasting pixel-wise correlations between query features and support embeddings from CLIP. To improve cross-modal guidance of text prompts, we develop a Context-Aware Cross-Modal Fusion module (CACM), which dynamically aligns global image-level and local pixel-level visual features with text embeddings. By integrating scene semantics and structural details from visual features, it overcomes the limitations of fixed prompts, enabling adaptive alignment between visual and textual modalities. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the PASCAL-5 i and COCO-20 i datasets. Our code will be available at https://github.com/hhu-csj/FDCMNet .
Published: 2025-12-29T13:09:57+00:00
Venue: Expert Systems with Applications
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shangjing Chen; Feng Xu; Xin Lyu; Dafa Wang; Xin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130892"&gt;10.1016/j.eswa.2025.130892&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot semantic segmentation aims to segment novel objects with limited annotations but faces challenges from ambiguous feature representations caused by intra-class diversity and inter-class similarity. While existing methods integrate CLIP for cross-modal guidance, these approaches tend to overlook two critical limitations. First, because the CLIP visual encoder encodes not only local appearance but also global context from the entire image, foreground and background regions may exhibit similar responses in the visual representations. Second, static text prompts are unable to dynamically model the actual-scenario interactions between visual content and text, leading to suboptimal guidance for segmentation tasks. To address these problems, we propose the CLIP-Driven Feature Disambiguation and Cross-Modal Synergy Network (FDCMNet). For the ambiguity from coarse-grained semantics, we design a Contrastive Feature Disentanglement module (CFD), which explicitly discriminates foreground and background by contrasting pixel-wise correlations between query features and support embeddings from CLIP. To improve cross-modal guidance of text prompts, we develop a Context-Aware Cross-Modal Fusion module (CACM), which dynamically aligns global image-level and local pixel-level visual features with text embeddings. By integrating scene semantics and structural details from visual features, it overcomes the limitations of fixed prompts, enabling adaptive alignment between visual and textual modalities. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the PASCAL-5 i and COCO-20 i datasets. Our code will be available at https://github.com/hhu-csj/FDCMNet .&lt;/p&gt;</content:encoded></item><item><title>ReDiF: Reinforced Distillation for Few Step Diffusion</title><link>https://arxiv.org/abs/2512.22802v1</link><guid>http://arxiv.org/abs/2512.22802v1</guid><pubDate>Sun, 28 Dec 2025 06:27:24 +0000</pubDate><dc:creator>Amirhossein Tighkhorshid</dc:creator><dc:creator>Zahra Dehghanian</dc:creator><dc:creator>Gholamali Aminian</dc:creator><dc:creator>Chengchun Shi</dc:creator><dc:creator>Hamid R. Rabiee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.
Published: 2025-12-28T06:27:24+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Amirhossein Tighkhorshid; Zahra Dehghanian; Gholamali Aminian; Chengchun Shi; Hamid R. Rabiee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher&amp;#x27;s outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.&lt;/p&gt;</content:encoded></item><item><title>Bi-C
                    &lt;sup&gt;2&lt;/sup&gt;
                    R: Bidirectional Continual Compatible Representation for Re-Indexing Free Lifelong Person Re-Identification</title><link>https://doi.org/10.1109/tpami.2025.3649078</link><guid>10.1109/tpami.2025.3649078</guid><pubDate>Mon, 29 Dec 2025 18:38:19 +0000</pubDate><dc:creator>Zhenyu Cui</dc:creator><dc:creator>Jiahuan Zhou</dc:creator><dc:creator>Yuxin Peng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649078</prism:doi><description>Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as “re-indexing”. However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. Specifically, a bidirectional compatible transfer network is first designed to bridge the relationship between new and old knowledge and continuously update the old gallery features to the new feature space after the updating. Secondly, a bidirectional compatible distillation module and a bidirectional anti-forgetting distillation model are designed to balance the compatibility between the new and old knowledge in dual feature spaces. Finally, a feature-level exponential moving average ...
Published: 2025-12-29T18:38:19+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyu Cui; Jiahuan Zhou; Yuxin Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649078"&gt;10.1109/tpami.2025.3649078&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as “re-indexing”. However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. Specifically, a bidirectional compatible transfer network is first designed to bridge the relationship between new and old knowledge and continuously update the old gallery features to the new feature space after the updating. Secondly, a bidirectional compatible distillation module and a bidirectional anti-forgetting distillation model are designed to balance the compatibility between the new and old knowledge in dual feature spaces. Finally, a feature-level exponential moving average ...&lt;/p&gt;</content:encoded></item><item><title>Additional encoder is not what you need: Unleashing the potential of CLIP for weakly-supervised semantic segmentation</title><link>https://doi.org/10.1016/j.eswa.2025.130932</link><guid>10.1016/j.eswa.2025.130932</guid><pubDate>Mon, 29 Dec 2025 10:29:55 +0000</pubDate><dc:creator>You Lv</dc:creator><dc:creator>Guoliang Kang</dc:creator><dc:creator>Wei Wei</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130932</prism:doi><description>In this paper, we focus on weakly supervised semantic segmentation (WSSS) task, which aims to generate accurate pixel-level predictions with only image-level labels available. A typical solution is to generate dense masks from sparse attention maps with the guidance of image-level labels. Recent works utilize Contrastive Language-Image Pre-training (CLIP) in WSSS task. As CLIP aligns visual features with textural features, most previous works directly adopt CLIP to generate class-aware attention maps. However, they usually train a separate segmentation network to obtain satisfactory performance with attention maps as pseudo masks. In this work, we find that a frozen CLIP as segmentation encoder may perform quite well in WSSS task, and additional encoder training is not necessary. Specifically, we first generate pixel-level pseudo masks via Grad-CAM from the frozen CLIP model. To obtain fine-grained attention, we compute CAMs by jointly leveraging gradients from both the last and intermediate transformer layers. Subsequently, we append a lightweight convolutional decoder to produce initial segmentation predictions. To mitigate the class-preference and space-preference biases inherent in CLIP, the Bias Rectification Module is employed to adaptively rectify these biases. We further impose a contrastive loss between masked image features and textual features to better align the visual and textual representations within CLIP. In addition, a supervised loss between the generated pseudo masks and the rectified predictions is employed to refine the dense outputs. Extensive experiments on PASCAL VOC2012 and MS COCO2014 datasets demonstrate that our method performs favorably against previous state-of-the-art WSSS approaches.
Published: 2025-12-29T10:29:55+00:00
Venue: Expert Systems with Applications
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; You Lv; Guoliang Kang; Wei Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130932"&gt;10.1016/j.eswa.2025.130932&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we focus on weakly supervised semantic segmentation (WSSS) task, which aims to generate accurate pixel-level predictions with only image-level labels available. A typical solution is to generate dense masks from sparse attention maps with the guidance of image-level labels. Recent works utilize Contrastive Language-Image Pre-training (CLIP) in WSSS task. As CLIP aligns visual features with textural features, most previous works directly adopt CLIP to generate class-aware attention maps. However, they usually train a separate segmentation network to obtain satisfactory performance with attention maps as pseudo masks. In this work, we find that a frozen CLIP as segmentation encoder may perform quite well in WSSS task, and additional encoder training is not necessary. Specifically, we first generate pixel-level pseudo masks via Grad-CAM from the frozen CLIP model. To obtain fine-grained attention, we compute CAMs by jointly leveraging gradients from both the last and intermediate transformer layers. Subsequently, we append a lightweight convolutional decoder to produce initial segmentation predictions. To mitigate the class-preference and space-preference biases inherent in CLIP, the Bias Rectification Module is employed to adaptively rectify these biases. We further impose a contrastive loss between masked image features and textual features to better align the visual and textual representations within CLIP. In addition, a supervised loss between the generated pseudo masks and the rectified predictions is employed to refine the dense outputs. Extensive experiments on PASCAL VOC2012 and MS COCO2014 datasets demonstrate that our method performs favorably against previous state-of-the-art WSSS approaches.&lt;/p&gt;</content:encoded></item><item><title>CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation</title><link>https://arxiv.org/abs/2512.22681v1</link><guid>http://arxiv.org/abs/2512.22681v1</guid><pubDate>Sat, 27 Dec 2025 19:08:18 +0000</pubDate><dc:creator>ZhenQi Chen</dc:creator><dc:creator>TsaiChing Ni</dc:creator><dc:creator>YuanFu Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt's intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.
Published: 2025-12-27T19:08:18+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; ZhenQi Chen; TsaiChing Ni; YuanFu Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt&amp;#x27;s intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.&lt;/p&gt;</content:encoded></item><item><title>LLMBoost: Make Large Language Models Stronger with Boosting</title><link>https://arxiv.org/abs/2512.22309v1</link><guid>http://arxiv.org/abs/2512.22309v1</guid><pubDate>Fri, 26 Dec 2025 07:16:41 +0000</pubDate><dc:creator>Zehao Chen</dc:creator><dc:creator>Tianxiang Ai</dc:creator><dc:creator>Yifei Li</dc:creator><dc:creator>Gongxun Li</dc:creator><dc:creator>Yuyang Wei</dc:creator><dc:creator>Wang Zhou</dc:creator><dc:creator>Guanghui Li</dc:creator><dc:creator>Bin Yu</dc:creator><dc:creator>Zhijun Chen</dc:creator><dc:creator>Hailong Sun</dc:creator><dc:creator>Fuzhen Zhuang</dc:creator><dc:creator>Jianxin Li</dc:creator><dc:creator>Deqing Wang</dc:creator><dc:creator>Yikun Ban</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.
Published: 2025-12-26T07:16:41+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zehao Chen; Tianxiang Ai; Yifei Li; Gongxun Li; Yuyang Wei; Wang Zhou; Guanghui Li; Bin Yu; Zhijun Chen; Hailong Sun; Fuzhen Zhuang; Jianxin Li; Deqing Wang; Yikun Ban&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.&lt;/p&gt;</content:encoded></item><item><title>Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects</title><link>https://arxiv.org/abs/2512.22949v1</link><guid>http://arxiv.org/abs/2512.22949v1</guid><pubDate>Sun, 28 Dec 2025 14:27:55 +0000</pubDate><dc:creator>Zhicheng Zhao</dc:creator><dc:creator>Xuanang Fan</dc:creator><dc:creator>Lingma Sun</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.
Published: 2025-12-28T14:27:55+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhicheng Zhao; Xuanang Fan; Lingma Sun; Chenglong Li; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.&lt;/p&gt;</content:encoded></item><item><title>Visual Autoregressive Modelling for Monocular Depth Estimation</title><link>https://arxiv.org/abs/2512.22653v1</link><guid>http://arxiv.org/abs/2512.22653v1</guid><pubDate>Sat, 27 Dec 2025 17:08:03 +0000</pubDate><dc:creator>Amir El-Ghoussani</dc:creator><dc:creator>André Kaup</dc:creator><dc:creator>Nassir Navab</dc:creator><dc:creator>Gustavo Carneiro</dc:creator><dc:creator>Vasileios Belagiannis</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at "https://github.com/AmirMaEl/VAR-Depth".
Published: 2025-12-27T17:08:03+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Amir El-Ghoussani; André Kaup; Nassir Navab; Gustavo Carneiro; Vasileios Belagiannis&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at &amp;quot;https://github.com/AmirMaEl/VAR-Depth&amp;quot;.&lt;/p&gt;</content:encoded></item><item><title>FDPFNet: A Frequency-Domain Progressive Fusion Network for Optical-SAR Multi-Label Remote Sensing Scene Classification</title><link>https://doi.org/10.1109/jstars.2025.3649036</link><guid>10.1109/jstars.2025.3649036</guid><pubDate>Mon, 29 Dec 2025 18:38:55 +0000</pubDate><dc:creator>Yiming Zhao</dc:creator><dc:creator>Kunlun Qi</dc:creator><dc:creator>Yaxian Qing</dc:creator><dc:creator>Kelong Tu</dc:creator><dc:creator>Jiajun Tao</dc:creator><dc:creator>Hongge Li</dc:creator><dc:creator>Chao Yang</dc:creator><dc:creator>Hongyan Zhang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649036</prism:doi><description>The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.
Published: 2025-12-29T18:38:55+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Zhao; Kunlun Qi; Yaxian Qing; Kelong Tu; Jiajun Tao; Hongge Li; Chao Yang; Hongyan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649036"&gt;10.1109/jstars.2025.3649036&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.&lt;/p&gt;</content:encoded></item><item><title>Self-Evaluation Unlocks Any-Step Text-to-Image Generation</title><link>https://arxiv.org/abs/2512.22374v1</link><guid>http://arxiv.org/abs/2512.22374v1</guid><pubDate>Fri, 26 Dec 2025 20:42:11 +0000</pubDate><dc:creator>Xin Yu</dc:creator><dc:creator>Xiaojuan Qi</dc:creator><dc:creator>Zhengqi Li</dc:creator><dc:creator>Kai Zhang</dc:creator><dc:creator>Richard Zhang</dc:creator><dc:creator>Zhe Lin</dc:creator><dc:creator>Eli Shechtman</dc:creator><dc:creator>Tianyu Wang</dc:creator><dc:creator>Yotam Nitzan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.
Published: 2025-12-26T20:42:11+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Yu; Xiaojuan Qi; Zhengqi Li; Kai Zhang; Richard Zhang; Zhe Lin; Eli Shechtman; Tianyu Wang; Yotam Nitzan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.&lt;/p&gt;</content:encoded></item><item><title>All You Need Is Two Domains: Unified RGB-Wavelet Transformer for Visual Representation Learning</title><link>https://doi.org/10.1016/j.knosys.2025.115239</link><guid>10.1016/j.knosys.2025.115239</guid><pubDate>Sun, 28 Dec 2025 07:00:46 +0000</pubDate><dc:creator>Yu Fu</dc:creator><dc:creator>Weichao Yi</dc:creator><dc:creator>Liquan Dong</dc:creator><dc:creator>Ming Liu</dc:creator><dc:creator>Lingqin Kong</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115239</prism:doi><description>Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .
Published: 2025-12-28T07:00:46+00:00
Venue: Knowledge-Based Systems
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Fu; Weichao Yi; Liquan Dong; Ming Liu; Lingqin Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115239"&gt;10.1016/j.knosys.2025.115239&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .&lt;/p&gt;</content:encoded></item><item><title>YOLOSAM: A YOLO-Guided SAM for Accurate Building Segmentation in Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2025.3649081</link><guid>10.1109/jstars.2025.3649081</guid><pubDate>Mon, 29 Dec 2025 18:38:55 +0000</pubDate><dc:creator>Musarat Hussain</dc:creator><dc:creator>Ji Huang</dc:creator><dc:creator>Xiankui Liu</dc:creator><dc:creator>Yulin Duan</dc:creator><dc:creator>Hongyan Wu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649081</prism:doi><description>Accurate building segmentation in remote sensing images is crucial for applications like disaster assessment, 3D urban modeling, and monitoring urban transformations. However, this task presents significant challenges due to the vast geographical coverage, dense building clusters, and the complexity of building contours, roof geometries, and surrounding environments. While the Segment Anything Model (SAM) offers a promising solution for extracting building masks in remote sensing images, its reliance on interactive input cues, difficulty in capturing fine edge details, and inability to integrate global semantic context with local fine-grained visual features often result in poor boundary detection and fragmented masks, limiting its effectiveness in fully automated, end-to-end building segmentation. To address these limitations, we propose YOLOSAM, a YOLO-guided adaptation of SAM designed for precise and automated building segmentation. Our framework introduces three lightweight yet effective innovations: (i) an Automatic Prompt Generator, based on YOLOv8, that automatically produces bounding box prompts to eliminate manual input; (ii) a High-Quality Token (HQ-Token) that improves edge fidelity and mask coherence by refining SAM's decoder representations; and (iii) a Global-Local Feature Fusion module, which enhances segmentation quality by fusing semantic context from deeper layers with fine edge details from earlier stages of SAM's frozen architecture. Importantly, our method preserves SAM's pre-trained generalization ability by freezing the original encoder and decoder while training only the lightweight modules. Experimental results demonstrate a significant improvement in segmentation accuracy, with mIoU increasing to 76.7% on the WHU building segmentation dataset, 69.1% on the Vaihingen building dataset, and 73.2% on the Inria Aerial Image Labeling dataset, compared to SAM's “segment everything” mode. Our model also significantly outperforms both classical deep...
Published: 2025-12-29T18:38:55+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Musarat Hussain; Ji Huang; Xiankui Liu; Yulin Duan; Hongyan Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649081"&gt;10.1109/jstars.2025.3649081&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate building segmentation in remote sensing images is crucial for applications like disaster assessment, 3D urban modeling, and monitoring urban transformations. However, this task presents significant challenges due to the vast geographical coverage, dense building clusters, and the complexity of building contours, roof geometries, and surrounding environments. While the Segment Anything Model (SAM) offers a promising solution for extracting building masks in remote sensing images, its reliance on interactive input cues, difficulty in capturing fine edge details, and inability to integrate global semantic context with local fine-grained visual features often result in poor boundary detection and fragmented masks, limiting its effectiveness in fully automated, end-to-end building segmentation. To address these limitations, we propose YOLOSAM, a YOLO-guided adaptation of SAM designed for precise and automated building segmentation. Our framework introduces three lightweight yet effective innovations: (i) an Automatic Prompt Generator, based on YOLOv8, that automatically produces bounding box prompts to eliminate manual input; (ii) a High-Quality Token (HQ-Token) that improves edge fidelity and mask coherence by refining SAM&amp;#x27;s decoder representations; and (iii) a Global-Local Feature Fusion module, which enhances segmentation quality by fusing semantic context from deeper layers with fine edge details from earlier stages of SAM&amp;#x27;s frozen architecture. Importantly, our method preserves SAM&amp;#x27;s pre-trained generalization ability by freezing the original encoder and decoder while training only the lightweight modules. Experimental results demonstrate a significant improvement in segmentation accuracy, with mIoU increasing to 76.7% on the WHU building segmentation dataset, 69.1% on the Vaihingen building dataset, and 73.2% on the Inria Aerial Image Labeling dataset, compared to SAM&amp;#x27;s “segment everything” mode. Our model also significantly outperforms both classical deep...&lt;/p&gt;</content:encoded></item><item><title>DAE-YOLO: Remote Sensing Small Object Detection Method Integrating YOLO and State Space Models</title><link>https://doi.org/10.3390/rs18010109</link><guid>10.3390/rs18010109</guid><pubDate>Sun, 28 Dec 2025 23:54:36 +0000</pubDate><dc:creator>Bing Li</dc:creator><dc:creator>Yongtao Kang</dc:creator><dc:creator>Yao Ding</dc:creator><dc:creator>Shaopeng Li</dc:creator><dc:creator>Zhili Zhang</dc:creator><dc:creator>Decao Ma</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010109</prism:doi><description>Small object detection in remote sensing images provides significant value for urban monitoring, aerospace reconnaissance, and other fields. However, detection accuracy still faces multiple challenges including limited target information, weak feature representation, and complex backgrounds. This research aims to improve the performance of the YOLO11 model for small object detection in remote sensing imagery by addressing key issues in long-distance spatial dependency modeling, multi-scale feature adaptive fusion, and computational efficiency. We constructed a specialized Remote Sensing Airport-Plane Detection (RS-APD) dataset and used the public VisDrone2019 dataset for generalization verification. Based on the YOLO11 architecture, we proposed the DAE-YOLO model with three innovative modules: Dynamic Spatial Sequence Module (DSSM) for enhanced long-distance spatial dependency capture; Adaptive Multi-scale Feature Enhancement (AMFE) for multi-scale feature adaptive receptive field adjustment; and Efficient Dual-level Attention Mechanism (EDAM) to reduce computational complexity while maintaining feature expression capability. Experimental results demonstrate that compared to the baseline YOLO11, our proposed model improved mAP50 and mAP50:95 on the RS-APD dataset by 2.1% and 2.5%, respectively, with APs increasing by 2.8%. This research provides an efficient and reliable small object detection solution for remote sensing applications.
Published: 2025-12-28T23:54:36+00:00
Venue: Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bing Li; Yongtao Kang; Yao Ding; Shaopeng Li; Zhili Zhang; Decao Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010109"&gt;10.3390/rs18010109&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Small object detection in remote sensing images provides significant value for urban monitoring, aerospace reconnaissance, and other fields. However, detection accuracy still faces multiple challenges including limited target information, weak feature representation, and complex backgrounds. This research aims to improve the performance of the YOLO11 model for small object detection in remote sensing imagery by addressing key issues in long-distance spatial dependency modeling, multi-scale feature adaptive fusion, and computational efficiency. We constructed a specialized Remote Sensing Airport-Plane Detection (RS-APD) dataset and used the public VisDrone2019 dataset for generalization verification. Based on the YOLO11 architecture, we proposed the DAE-YOLO model with three innovative modules: Dynamic Spatial Sequence Module (DSSM) for enhanced long-distance spatial dependency capture; Adaptive Multi-scale Feature Enhancement (AMFE) for multi-scale feature adaptive receptive field adjustment; and Efficient Dual-level Attention Mechanism (EDAM) to reduce computational complexity while maintaining feature expression capability. Experimental results demonstrate that compared to the baseline YOLO11, our proposed model improved mAP50 and mAP50:95 on the RS-APD dataset by 2.1% and 2.5%, respectively, with APs increasing by 2.8%. This research provides an efficient and reliable small object detection solution for remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers</title><link>https://arxiv.org/abs/2512.22760v1</link><guid>http://arxiv.org/abs/2512.22760v1</guid><pubDate>Sun, 28 Dec 2025 03:25:45 +0000</pubDate><dc:creator>Yunge Li</dc:creator><dc:creator>Lanyu Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.
Published: 2025-12-28T03:25:45+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunge Li; Lanyu Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.&lt;/p&gt;</content:encoded></item><item><title>See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</title><link>https://arxiv.org/abs/2512.22120v1</link><guid>http://arxiv.org/abs/2512.22120v1</guid><pubDate>Fri, 26 Dec 2025 18:59:47 +0000</pubDate><dc:creator>Shuoshuo Zhang</dc:creator><dc:creator>Yizhen Zhang</dc:creator><dc:creator>Jingjing Fu</dc:creator><dc:creator>Lei Song</dc:creator><dc:creator>Jiang Bian</dc:creator><dc:creator>Yujiu Yang</dc:creator><dc:creator>Rui Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.
Published: 2025-12-26T18:59:47+00:00
Venue: arXiv
Score: 0.785 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuoshuo Zhang; Yizhen Zhang; Jingjing Fu; Lei Song; Jiang Bian; Yujiu Yang; Rui Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (consider)&lt;/p&gt;
&lt;p&gt;Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.&lt;/p&gt;</content:encoded></item><item><title>Task-Guided Prompting for Unified Remote Sensing Image Restoration</title><link>https://doi.org/10.1109/tgrs.2025.3649021</link><guid>10.1109/tgrs.2025.3649021</guid><pubDate>Mon, 29 Dec 2025 18:38:33 +0000</pubDate><dc:creator>Wenli Huang</dc:creator><dc:creator>Yang Wu</dc:creator><dc:creator>Xiaomeng Xin</dc:creator><dc:creator>Zhihong Liu</dc:creator><dc:creator>Jinjun Wang</dc:creator><dc:creator>Ye Deng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649021</prism:doi><description>Remote sensing image restoration (RSIR) is essential for recovering high-fidelity imagery from degraded observations, enabling accurate downstream analysis. However, most existing methods focus on single degradation types within homogeneous data, restricting their practicality in real-world scenarios where multiple degradations often across diverse spectral bands or sensor modalities, creating a significant operational bottleneck. To address this fundamental gap, we propose TGPNet, a unified framework capable of handling denoising, cloud removal, shadow removal, deblurring, and SAR despeckling within a single, unified architecture. The core of our framework is a novel Task-Guided Prompting (TGP) strategy. TGP leverages learnable, task-specific embeddings to generate degradation-aware cues, which then hierarchically modulate features throughout the decoder. This task-adaptive mechanism allows the network to precisely tailor its restoration process for distinct degradation patterns while maintaining a single set of shared weights. To validate our framework, we construct a unified RSIR benchmark covering RGB, multispectral, SAR, and thermal infrared modalities for five aforementioned restoration tasks. Experimental results demonstrate that TGPNet achieves state-of-the-art performance on both unified multi-task scenarios and unseen composite degradations, surpassing even specialized models in individual domains such as cloud removal. By successfully unifying heterogeneous degradation removal within a single adaptive framework, this work presents a significant advancement for multi-task RSIR, offering a practical and scalable solution for operational pipelines. The code and benchmark will be released at https://github.com/huangwenwenlili/TGPNet.
Published: 2025-12-29T18:38:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.784 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenli Huang; Yang Wu; Xiaomeng Xin; Zhihong Liu; Jinjun Wang; Ye Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649021"&gt;10.1109/tgrs.2025.3649021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing image restoration (RSIR) is essential for recovering high-fidelity imagery from degraded observations, enabling accurate downstream analysis. However, most existing methods focus on single degradation types within homogeneous data, restricting their practicality in real-world scenarios where multiple degradations often across diverse spectral bands or sensor modalities, creating a significant operational bottleneck. To address this fundamental gap, we propose TGPNet, a unified framework capable of handling denoising, cloud removal, shadow removal, deblurring, and SAR despeckling within a single, unified architecture. The core of our framework is a novel Task-Guided Prompting (TGP) strategy. TGP leverages learnable, task-specific embeddings to generate degradation-aware cues, which then hierarchically modulate features throughout the decoder. This task-adaptive mechanism allows the network to precisely tailor its restoration process for distinct degradation patterns while maintaining a single set of shared weights. To validate our framework, we construct a unified RSIR benchmark covering RGB, multispectral, SAR, and thermal infrared modalities for five aforementioned restoration tasks. Experimental results demonstrate that TGPNet achieves state-of-the-art performance on both unified multi-task scenarios and unseen composite degradations, surpassing even specialized models in individual domains such as cloud removal. By successfully unifying heterogeneous degradation removal within a single adaptive framework, this work presents a significant advancement for multi-task RSIR, offering a practical and scalable solution for operational pipelines. The code and benchmark will be released at https://github.com/huangwenwenlili/TGPNet.&lt;/p&gt;</content:encoded></item><item><title>Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection</title><link>https://arxiv.org/abs/2512.22972v1</link><guid>http://arxiv.org/abs/2512.22972v1</guid><pubDate>Sun, 28 Dec 2025 15:32:17 +0000</pubDate><dc:creator>Runwei Guan</dc:creator><dc:creator>Jianan Liu</dc:creator><dc:creator>Shaofeng Liang</dc:creator><dc:creator>Fangqiang Ding</dc:creator><dc:creator>Shanliang Yao</dc:creator><dc:creator>Xiaokai Bai</dc:creator><dc:creator>Daizong Liu</dc:creator><dc:creator>Tao Huang</dc:creator><dc:creator>Guoqiang Mao</dc:creator><dc:creator>Hui Xiong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.
Published: 2025-12-28T15:32:17+00:00
Venue: arXiv
Score: 0.784 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runwei Guan; Jianan Liu; Shaofeng Liang; Fangqiang Ding; Shanliang Yao; Xiaokai Bai; Daizong Liu; Tao Huang; Guoqiang Mao; Hui Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (consider)&lt;/p&gt;
&lt;p&gt;4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.&lt;/p&gt;</content:encoded></item><item><title>SAR and Visible Image Fusion via Retinex-Guided SAR Reconstruction</title><link>https://doi.org/10.3390/rs18010111</link><guid>10.3390/rs18010111</guid><pubDate>Sun, 28 Dec 2025 23:54:36 +0000</pubDate><dc:creator>Yuman Yuan</dc:creator><dc:creator>Tianyu Deng</dc:creator><dc:creator>Yi Le</dc:creator><dc:creator>Hongyang Bai</dc:creator><dc:creator>Shuai Guo</dc:creator><dc:creator>Shangjing Sun</dc:creator><dc:creator>Yuanbo Chen</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010111</prism:doi><description>The fusion of synthetic aperture radar (SAR) and visible images offers complementary spatial and spectral information, enabling more reliable and comprehensive scene interpretation. However, SAR speckle noise and the intrinsic modality gap pose significant challenges for existing methods in extracting consistent and complementary features. To address these issues, we propose VGSRF-Net, a Retinex-guided SAR reconstruction-driven fusion network that leverages visible-image priors to refine SAR features. This approach effectively reduces modality discrepancies before fusion, enabling improved multi-modal representation. The cross-modality reconstruction module (CMRM) reconstructs SAR features guided by visible priors, effectively reducing modality discrepancies before fusion and enabling improved multi-modal representation. The multi-modal feature joint representation module (MFJRM) enhances cross-modal complementarity by integrating global contextual interactions and local dynamic convolution, thereby achieving further feature alignment. Finally, the feature enhancement module (FEM) refines multi-scale spatial features and selectively enhances high-frequency details in the frequency domain, improving structural clarity and texture fidelity. Extensive experiments on diverse real-world remote sensing datasets demonstrate that VGSRF-Net surpasses state-of-the-art methods in denoising, structural preservation, and generalization under varying noise and illumination conditions.
Published: 2025-12-28T23:54:36+00:00
Venue: Remote Sensing
Score: 0.784 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuman Yuan; Tianyu Deng; Yi Le; Hongyang Bai; Shuai Guo; Shangjing Sun; Yuanbo Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010111"&gt;10.3390/rs18010111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (consider)&lt;/p&gt;
&lt;p&gt;The fusion of synthetic aperture radar (SAR) and visible images offers complementary spatial and spectral information, enabling more reliable and comprehensive scene interpretation. However, SAR speckle noise and the intrinsic modality gap pose significant challenges for existing methods in extracting consistent and complementary features. To address these issues, we propose VGSRF-Net, a Retinex-guided SAR reconstruction-driven fusion network that leverages visible-image priors to refine SAR features. This approach effectively reduces modality discrepancies before fusion, enabling improved multi-modal representation. The cross-modality reconstruction module (CMRM) reconstructs SAR features guided by visible priors, effectively reducing modality discrepancies before fusion and enabling improved multi-modal representation. The multi-modal feature joint representation module (MFJRM) enhances cross-modal complementarity by integrating global contextual interactions and local dynamic convolution, thereby achieving further feature alignment. Finally, the feature enhancement module (FEM) refines multi-scale spatial features and selectively enhances high-frequency details in the frequency domain, improving structural clarity and texture fidelity. Extensive experiments on diverse real-world remote sensing datasets demonstrate that VGSRF-Net surpasses state-of-the-art methods in denoising, structural preservation, and generalization under varying noise and illumination conditions.&lt;/p&gt;</content:encoded></item><item><title>Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework</title><link>https://arxiv.org/abs/2512.22447v1</link><guid>http://arxiv.org/abs/2512.22447v1</guid><pubDate>Sat, 27 Dec 2025 03:16:48 +0000</pubDate><dc:creator>Zhicheng Zhao</dc:creator><dc:creator>Yuancheng Xu</dc:creator><dc:creator>Andong Lu</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.
Published: 2025-12-27T03:16:48+00:00
Venue: arXiv
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhicheng Zhao; Yuancheng Xu; Andong Lu; Chenglong Li; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.&lt;/p&gt;</content:encoded></item><item><title>Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks</title><link>https://arxiv.org/abs/2512.23210v1</link><guid>http://arxiv.org/abs/2512.23210v1</guid><pubDate>Mon, 29 Dec 2025 05:19:01 +0000</pubDate><dc:creator>Changgyoon Oh</dc:creator><dc:creator>Jongoh Jeong</dc:creator><dc:creator>Jegyeong Cho</dc:creator><dc:creator>Kuk-Jin Yoon</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.
Published: 2025-12-29T05:19:01+00:00
Venue: arXiv
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changgyoon Oh; Jongoh Jeong; Jegyeong Cho; Kuk-Jin Yoon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.&lt;/p&gt;</content:encoded></item></channel></rss>