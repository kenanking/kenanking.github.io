<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 21 Jan 2026 02:59:31 +0000</lastBuildDate><item><title>A General Image Fusion Approach Exploiting Gradient Transfer Learning and Fusion Rule Unfolding</title><link>https://doi.org/10.1109/tpami.2026.3655694</link><guid>10.1109/tpami.2026.3655694</guid><pubDate>Mon, 19 Jan 2026 20:54:53 +0000</pubDate><dc:creator>Wu Wang</dc:creator><dc:creator>Liang-Jian Deng</dc:creator><dc:creator>Qi Cao</dc:creator><dc:creator>Gemine Vivone</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3655694</prism:doi><description>The goal of a deep learning-based general image fusion method is to solve multiple image fusion tasks with a single model, thereby facilitating the deployment of models in practical applications. However, existing methods fail to provide an efficient and comprehensive solution from both model training and network design perspectives. Regarding model training, current approaches cannot effectively leverage complementary information across different tasks. In terms of network design, they rely on experience-based network designs. To address these issues, we propose a comprehensive framework for general image fusion using the newly proposed gradient transfer learning and fusion rule unfolding. To leverage complementary information across different tasks during training, we propose a sequential gradient-transfer framework based on the idea that different image fusion tasks often exhibit complementary structural details and that image gradients effectively capture these details. To move beyond heuristic-based network design, we evolved a fundamental image fusion rule and integrated it into a deep equilibrium model, resulting in a more efficient and versatile image fusion network capable of uniformly handling various fusion tasks. Considering three different image fusion tasks, i.e., multi-focus image fusion, multi-exposure image fusion, and infrared and visible image fusion, our method not only produces images with richer structural information but also achieves highly competitive objective metrics. Furthermore, the results of generalization experiments on previously unseen image fusion tasks, i.e., medical image fusion, demonstrate that our method significantly outperforms competing approaches. The code will be available upon possible acceptance.
Published: 2026-01-19T20:54:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.836 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wu Wang; Liang-Jian Deng; Qi Cao; Gemine Vivone&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3655694"&gt;10.1109/tpami.2026.3655694&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.836 (must_read)&lt;/p&gt;
&lt;p&gt;The goal of a deep learning-based general image fusion method is to solve multiple image fusion tasks with a single model, thereby facilitating the deployment of models in practical applications. However, existing methods fail to provide an efficient and comprehensive solution from both model training and network design perspectives. Regarding model training, current approaches cannot effectively leverage complementary information across different tasks. In terms of network design, they rely on experience-based network designs. To address these issues, we propose a comprehensive framework for general image fusion using the newly proposed gradient transfer learning and fusion rule unfolding. To leverage complementary information across different tasks during training, we propose a sequential gradient-transfer framework based on the idea that different image fusion tasks often exhibit complementary structural details and that image gradients effectively capture these details. To move beyond heuristic-based network design, we evolved a fundamental image fusion rule and integrated it into a deep equilibrium model, resulting in a more efficient and versatile image fusion network capable of uniformly handling various fusion tasks. Considering three different image fusion tasks, i.e., multi-focus image fusion, multi-exposure image fusion, and infrared and visible image fusion, our method not only produces images with richer structural information but also achieves highly competitive objective metrics. Furthermore, the results of generalization experiments on previously unseen image fusion tasks, i.e., medical image fusion, demonstrate that our method significantly outperforms competing approaches. The code will be available upon possible acceptance.&lt;/p&gt;</content:encoded></item><item><title>语义引导对比学习的SAR与光学图像转换</title><link>https://doi.org/10.11834/jig.250526</link><guid>10.11834/jig.250526</guid><pubDate>Tue, 20 Jan 2026 06:30:29 +0000</pubDate><dc:creator>Du Wenliang</dc:creator><dc:creator>Guo Bo</dc:creator><dc:creator>Zhao Jiaqi</dc:creator><dc:creator>Yao Rui</dc:creator><dc:creator>Zhou Yong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250526</prism:doi><description>目的合成孔径雷达（synthetic aperture radar， SAR）与光学图像转换能够融合两种模态数据的优势，提供全天时、全天候与高分辨率的观测能力。然而，当前基于循环一致性生成对抗网络的方法主要侧重于图像结构的宏观重建，未能充分利用跨模态间的深层语义信息来指导图像生成，限制了生成图像的语义保真度和在下游任务中的性能。同时，现有基于对比学习的转换方法在处理遥感图像时，因同类地物特征高度自相关导致正负样本难以区分，造成对比机制失效。针对上述问题，提出了一种语义引导对比学习的SAR与光学图像转换方法。方法提出了基于语义分割的特征提取模块，利用预训练的SAR与光学语义分割模型提取像素级语义信息；提出了语义引导的对比学习模块，利用先验的语义分割信息，在对比学习空间中显式构建基于类别一致性的正负样本筛选机制，有效解决了遥感图像特征同质化导致的传统对比学习失效问题；设计了融合循环生成结构与对比学习的联合优化框架，通过引入循环语义分割损失与生成对抗损失，约束生成图像在结构、纹理和语义层面的一致性。结果实验在WHU-OPT-SAR和DDHRNet两个公开数据集上进行。实验结果表明，与当前最优方法相比，在SAR到光学及光学到SAR的图像转换任务中，生成质量指标分别最高提升了11.9%和3.8%；在下游任务中，语义分割准确率分别提升了16.29%和10.19%，特征匹配的正确内点比例最高提升了1%。消融实验研究表明，语义引导对比学习模块与循环语义分割损失对提升模型性能均起到关键作用。结论本文提出的语义引导对比学习的SAR与光学图像转换方法，能够有效解决传统对比学习在遥感图像转换中的失效问题，显著提升了生成图像的语义保真度与跨模态特征对齐能力，在下游语义分割和图像匹配任务中取得了最优的综合性能，为无监督SAR与光学图像转换提供了新的解决思路。本文代码开源在链接：https：//www.scidb.cn/s/VVVBnu。
Published: 2026-01-20T06:30:29+00:00
Venue: Journal of Image and Graphics
Score: 0.833 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Du Wenliang; Guo Bo; Zhao Jiaqi; Yao Rui; Zhou Yong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250526"&gt;10.11834/jig.250526&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.833 (must_read)&lt;/p&gt;
&lt;p&gt;目的合成孔径雷达（synthetic aperture radar， SAR）与光学图像转换能够融合两种模态数据的优势，提供全天时、全天候与高分辨率的观测能力。然而，当前基于循环一致性生成对抗网络的方法主要侧重于图像结构的宏观重建，未能充分利用跨模态间的深层语义信息来指导图像生成，限制了生成图像的语义保真度和在下游任务中的性能。同时，现有基于对比学习的转换方法在处理遥感图像时，因同类地物特征高度自相关导致正负样本难以区分，造成对比机制失效。针对上述问题，提出了一种语义引导对比学习的SAR与光学图像转换方法。方法提出了基于语义分割的特征提取模块，利用预训练的SAR与光学语义分割模型提取像素级语义信息；提出了语义引导的对比学习模块，利用先验的语义分割信息，在对比学习空间中显式构建基于类别一致性的正负样本筛选机制，有效解决了遥感图像特征同质化导致的传统对比学习失效问题；设计了融合循环生成结构与对比学习的联合优化框架，通过引入循环语义分割损失与生成对抗损失，约束生成图像在结构、纹理和语义层面的一致性。结果实验在WHU-OPT-SAR和DDHRNet两个公开数据集上进行。实验结果表明，与当前最优方法相比，在SAR到光学及光学到SAR的图像转换任务中，生成质量指标分别最高提升了11.9%和3.8%；在下游任务中，语义分割准确率分别提升了16.29%和10.19%，特征匹配的正确内点比例最高提升了1%。消融实验研究表明，语义引导对比学习模块与循环语义分割损失对提升模型性能均起到关键作用。结论本文提出的语义引导对比学习的SAR与光学图像转换方法，能够有效解决传统对比学习在遥感图像转换中的失效问题，显著提升了生成图像的语义保真度与跨模态特征对齐能力，在下游语义分割和图像匹配任务中取得了最优的综合性能，为无监督SAR与光学图像转换提供了新的解决思路。本文代码开源在链接：https：//www.scidb.cn/s/VVVBnu。&lt;/p&gt;</content:encoded></item><item><title>Multiscale Feature Fusion Spatial-channel Attention Network for Infrared Small Target Segmentation</title><link>https://doi.org/10.1109/tmm.2026.3655470</link><guid>10.1109/tmm.2026.3655470</guid><pubDate>Mon, 19 Jan 2026 20:55:34 +0000</pubDate><dc:creator>Xuedong Guo</dc:creator><dc:creator>Lei Deng</dc:creator><dc:creator>Maoyong Li</dc:creator><dc:creator>Zhixiang Chen</dc:creator><dc:creator>Heng Yu</dc:creator><dc:creator>Hanrui Chen</dc:creator><dc:creator>Mingli Dong</dc:creator><dc:creator>Lianqing Zhu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3655470</prism:doi><description>Infrared small target segmentation technology plays an important role in fields such as missile warning, maritime rescue, and military reconnaissance. However, CNN methods based on convolution tend to lose information regarding infrared small targets, resulting in poor segmentation performance. On the other hand, methods based on transformers, lacking convolution-induced biases, also struggle to achieve good results. To address this issue, this article proposes a model called Multiscale Feature Fusion Spatial-channel Attention Network (MFFSANet) for the segmentation of infrared small targets. The MFFSANet model consists of three blocks: the Multi-scale Convolution Fusion Attention (MCFA) block, the Hierarchical Guided Channel Attention (HGCA) block, and the Atrous Residual U-Block (ARU). The MCFA block leverages multi-scale atrous convolutions and self-attention mechanisms to obtain both local and global information about the image, learning the difference between target features and background noise features, thus enabling the model to suppress background noise in infrared images. The HGCA block leverages coarser information to guide the learning of finer features, assigning weights to decisive channels, and reducing redundant information. This reduces background noise in infrared images, making small targets stand out more clearly against the background. The ARU facilitates interaction between feature maps of different layers and scales, enabling the model to recognize the characteristics of small infrared targets in a more detailed and comprehensive manner. Extensive experiments conducted on four publicly available datasets, namely SIRST, IRSTD-1k, NUDT-SIRST, and SIRST-Aug, demonstrate the effectiveness and superiority of the proposed MFFSANet method compared to several SOTA infrared small target segmentation methods. The source code is available at https://github.com/change68/MFFSANet.
Published: 2026-01-19T20:55:34+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.832 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuedong Guo; Lei Deng; Maoyong Li; Zhixiang Chen; Heng Yu; Hanrui Chen; Mingli Dong; Lianqing Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3655470"&gt;10.1109/tmm.2026.3655470&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.832 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target segmentation technology plays an important role in fields such as missile warning, maritime rescue, and military reconnaissance. However, CNN methods based on convolution tend to lose information regarding infrared small targets, resulting in poor segmentation performance. On the other hand, methods based on transformers, lacking convolution-induced biases, also struggle to achieve good results. To address this issue, this article proposes a model called Multiscale Feature Fusion Spatial-channel Attention Network (MFFSANet) for the segmentation of infrared small targets. The MFFSANet model consists of three blocks: the Multi-scale Convolution Fusion Attention (MCFA) block, the Hierarchical Guided Channel Attention (HGCA) block, and the Atrous Residual U-Block (ARU). The MCFA block leverages multi-scale atrous convolutions and self-attention mechanisms to obtain both local and global information about the image, learning the difference between target features and background noise features, thus enabling the model to suppress background noise in infrared images. The HGCA block leverages coarser information to guide the learning of finer features, assigning weights to decisive channels, and reducing redundant information. This reduces background noise in infrared images, making small targets stand out more clearly against the background. The ARU facilitates interaction between feature maps of different layers and scales, enabling the model to recognize the characteristics of small infrared targets in a more detailed and comprehensive manner. Extensive experiments conducted on four publicly available datasets, namely SIRST, IRSTD-1k, NUDT-SIRST, and SIRST-Aug, demonstrate the effectiveness and superiority of the proposed MFFSANet method compared to several SOTA infrared small target segmentation methods. The source code is available at https://github.com/change68/MFFSANet.&lt;/p&gt;</content:encoded></item><item><title>Cross-scale Channel Attention and Feature Fusion-aware Aggregation for Sonar Images Object Detection</title><link>https://doi.org/10.1016/j.knosys.2026.115371</link><guid>10.1016/j.knosys.2026.115371</guid><pubDate>Tue, 20 Jan 2026 07:43:28 +0000</pubDate><dc:creator>Pengfei Shi</dc:creator><dc:creator>Hanren Wang</dc:creator><dc:creator>Qianqian Zhang</dc:creator><dc:creator>Yuanxue Xin</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115371</prism:doi><description>Feature extraction and feature fusion are crucial for sonar image target detection. In terms of feature extraction, due to device limitations and the complexity of the underwater environment, sonar images often exhibit high levels of noise, which results in high similarity between targets and background, thus affecting feature extraction. In terms of feature fusion, transformer-based models rely on self-attention mechanisms, but this leads to a lack of local prior information. The interference from noise and the similarity between targets and background disrupt the computation of global relationships, confusing noisy features with useful ones, leading to insufficient geometric information and ultimately affecting detection accuracy. To address these issues, we propose an advanced detection framework that combines effective feature extraction and multi-scale feature fusion. We introduce a cross-scale channel attention module that dynamically adjusts channel weights by integrating the advantages of the squeeze-and-excitation (SE) module and the efficient multi-scale attention (EMA) module, capturing multi-scale dependencies, suppressing background noise, and enhancing global feature representation. Moreover, to further improve the effectiveness of feature fusion and better leverage geometric information, we design a CNN-based feature fusion perception aggregation network. This network promotes interaction between low-level geometric details and high-level semantic information through skip connections, enhancing feature representation and improving detection accuracy. Experimental results show that our method outperforms some advanced detection models in terms of detection performance.
Published: 2026-01-20T07:43:28+00:00
Venue: Knowledge-Based Systems
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Shi; Hanren Wang; Qianqian Zhang; Yuanxue Xin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115371"&gt;10.1016/j.knosys.2026.115371&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Feature extraction and feature fusion are crucial for sonar image target detection. In terms of feature extraction, due to device limitations and the complexity of the underwater environment, sonar images often exhibit high levels of noise, which results in high similarity between targets and background, thus affecting feature extraction. In terms of feature fusion, transformer-based models rely on self-attention mechanisms, but this leads to a lack of local prior information. The interference from noise and the similarity between targets and background disrupt the computation of global relationships, confusing noisy features with useful ones, leading to insufficient geometric information and ultimately affecting detection accuracy. To address these issues, we propose an advanced detection framework that combines effective feature extraction and multi-scale feature fusion. We introduce a cross-scale channel attention module that dynamically adjusts channel weights by integrating the advantages of the squeeze-and-excitation (SE) module and the efficient multi-scale attention (EMA) module, capturing multi-scale dependencies, suppressing background noise, and enhancing global feature representation. Moreover, to further improve the effectiveness of feature fusion and better leverage geometric information, we design a CNN-based feature fusion perception aggregation network. This network promotes interaction between low-level geometric details and high-level semantic information through skip connections, enhancing feature representation and improving detection accuracy. Experimental results show that our method outperforms some advanced detection models in terms of detection performance.&lt;/p&gt;</content:encoded></item><item><title>Toward Accurate Image Generation via Dynamic Generative Image Transformer</title><link>https://doi.org/10.1109/tpami.2026.3653620</link><guid>10.1109/tpami.2026.3653620</guid><pubDate>Mon, 19 Jan 2026 20:54:53 +0000</pubDate><dc:creator>Zhendong Mao</dc:creator><dc:creator>Mengqi Huang</dc:creator><dc:creator>Yijing Lin</dc:creator><dc:creator>Quan Wang</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Yongdong Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653620</prism:doi><description>Existing generative image transformers follow a two-stage generation paradigm, where the first stage learns a codebook to encode images into discrete codes via vector quantization, and the second stage completes the image generation based on the learned codebook. However, existing methods ignore the naturally varying information densities across different image regions and indiscriminately encode fixed-size regions into fixed-length codes, resulting in insufficient encoding in important regions and redundant encoding in unimportant ones, which degrades both the image generation quality and speed. To address this challenge, we propose a novel information-density-based variable-length image coding and generation framework. In the first stage, our Dynamic Quantization VAE++ (DQVAE++) performs information-adaptive encoding by assigning variable-length codes to image regions according to their information densities, yielding more accurate and robust code representations. In the second stage, the Dynamic Generative Image Transformer (DGiT) enables information-adaptive image generation in both autoregressive and non-autoregressive manners. Specifically, for autoregressive (AR) generation, DGiT-AR generates images autoregressively from coarse-grained regions (smooth areas with fewer codes) to fine-grained regions (detailed areas with more codes). This is accomplished through a novel stacked-transformer architecture that alternately models the position and content of image codes, and a novel heterogeneous embedding scheme to distinguish codes of different granularities. Similarly, for non-autoregressive (NAR) generation, DGiT-NAR introduces a novel information-prioritized mask scheduling mechanism, prioritizing the generation of key structural regions with higher information density. This enables more coherent modeling of global structures initially, followed by a more effective synthesis of local details subsequently. Comprehensive experiments on unconditional and condition...
Published: 2026-01-19T20:54:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhendong Mao; Mengqi Huang; Yijing Lin; Quan Wang; Lei Zhang; Yongdong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653620"&gt;10.1109/tpami.2026.3653620&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Existing generative image transformers follow a two-stage generation paradigm, where the first stage learns a codebook to encode images into discrete codes via vector quantization, and the second stage completes the image generation based on the learned codebook. However, existing methods ignore the naturally varying information densities across different image regions and indiscriminately encode fixed-size regions into fixed-length codes, resulting in insufficient encoding in important regions and redundant encoding in unimportant ones, which degrades both the image generation quality and speed. To address this challenge, we propose a novel information-density-based variable-length image coding and generation framework. In the first stage, our Dynamic Quantization VAE++ (DQVAE++) performs information-adaptive encoding by assigning variable-length codes to image regions according to their information densities, yielding more accurate and robust code representations. In the second stage, the Dynamic Generative Image Transformer (DGiT) enables information-adaptive image generation in both autoregressive and non-autoregressive manners. Specifically, for autoregressive (AR) generation, DGiT-AR generates images autoregressively from coarse-grained regions (smooth areas with fewer codes) to fine-grained regions (detailed areas with more codes). This is accomplished through a novel stacked-transformer architecture that alternately models the position and content of image codes, and a novel heterogeneous embedding scheme to distinguish codes of different granularities. Similarly, for non-autoregressive (NAR) generation, DGiT-NAR introduces a novel information-prioritized mask scheduling mechanism, prioritizing the generation of key structural regions with higher information density. This enables more coherent modeling of global structures initially, followed by a more effective synthesis of local details subsequently. Comprehensive experiments on unconditional and condition...&lt;/p&gt;</content:encoded></item><item><title>Modulation and Perturbation in Frequency Domain for SAR Ship Detection</title><link>https://doi.org/10.3390/rs18020338</link><guid>10.3390/rs18020338</guid><pubDate>Tue, 20 Jan 2026 11:27:58 +0000</pubDate><dc:creator>Mengqin Fu</dc:creator><dc:creator>Wencong Zhang</dc:creator><dc:creator>Xiaochen Quan</dc:creator><dc:creator>Dahu Shi</dc:creator><dc:creator>Luowei Tan</dc:creator><dc:creator>Jia Zhang</dc:creator><dc:creator>Yinghui Xing</dc:creator><dc:creator>Shizhou Zhang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020338</prism:doi><description>Synthetic Aperture Radar (SAR) has unique advantages in ship monitoring at sea due to its all-weather imaging capability. However, its unique imaging mechanism presents two major challenges. First, speckle noise in the frequency domain reduces the contrast between the target and the background. Second, side-lobe scattering blurs the ship outline, especially in nearshore complex scenes, and strong scattering characteristics make it difficult to separate the target from the background. The above two challenges significantly limit the performance of tailored CNN-based detection models in optical images when applied directly to SAR images. To address these challenges, this paper proposes a modulation and perturbation mechanism in the frequency domain based on a lightweight CNN detector. Specifically, the wavelet transform is firstly used to extract high-frequency features in different directions, and feature expression is dynamically adjusted according to the global statistical information to realize the selective enhancement of the ship edge and detail information. In terms of frequency-domain perturbation, a perturbation mechanism guided by frequency-domain weight is introduced to effectively suppress background interference while maintaining key target characteristics, which improves the robustness of the model in complex scenes. Extensive experiments on four widely adopted benchmark datasets, namely LS-SSDD-v1.0, SSDD, SAR-Ship-Dataset, and AIR-SARShip-2.0, demonstrate that our FMP-Net significantly outperforms 18 existing state-of-the-art methods, especially in complex nearshore scenes and sea surface interference scenes.
Published: 2026-01-20T11:27:58+00:00
Venue: Remote Sensing
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengqin Fu; Wencong Zhang; Xiaochen Quan; Dahu Shi; Luowei Tan; Jia Zhang; Yinghui Xing; Shizhou Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020338"&gt;10.3390/rs18020338&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) has unique advantages in ship monitoring at sea due to its all-weather imaging capability. However, its unique imaging mechanism presents two major challenges. First, speckle noise in the frequency domain reduces the contrast between the target and the background. Second, side-lobe scattering blurs the ship outline, especially in nearshore complex scenes, and strong scattering characteristics make it difficult to separate the target from the background. The above two challenges significantly limit the performance of tailored CNN-based detection models in optical images when applied directly to SAR images. To address these challenges, this paper proposes a modulation and perturbation mechanism in the frequency domain based on a lightweight CNN detector. Specifically, the wavelet transform is firstly used to extract high-frequency features in different directions, and feature expression is dynamically adjusted according to the global statistical information to realize the selective enhancement of the ship edge and detail information. In terms of frequency-domain perturbation, a perturbation mechanism guided by frequency-domain weight is introduced to effectively suppress background interference while maintaining key target characteristics, which improves the robustness of the model in complex scenes. Extensive experiments on four widely adopted benchmark datasets, namely LS-SSDD-v1.0, SSDD, SAR-Ship-Dataset, and AIR-SARShip-2.0, demonstrate that our FMP-Net significantly outperforms 18 existing state-of-the-art methods, especially in complex nearshore scenes and sea surface interference scenes.&lt;/p&gt;</content:encoded></item><item><title>Spatial-X fusion for multi-source satellite imageries</title><link>https://doi.org/10.1016/j.rse.2025.115214</link><guid>10.1016/j.rse.2025.115214</guid><pubDate>Mon, 19 Jan 2026 11:34:50 +0000</pubDate><dc:creator>Jiang He</dc:creator><dc:creator>Liupeng Lin</dc:creator><dc:creator>Zhuo Zheng</dc:creator><dc:creator>Qiangqiang Yuan</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Liangpei Zhang</dc:creator><dc:creator>Xiao xiang Zhu</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115214</prism:doi><description>Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.
Published: 2026-01-19T11:34:50+00:00
Venue: Remote Sensing of Environment
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiang He; Liupeng Lin; Zhuo Zheng; Qiangqiang Yuan; Jie Li; Liangpei Zhang; Xiao xiang Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115214"&gt;10.1016/j.rse.2025.115214&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-source remote sensing data can highlight different types of information based on user needs, resulting in large volumes of data and significant challenges. Hardware and environmental constraints create mutual dependencies between information types, particularly between spatial data and other types, limiting the development of high-precision applications. Traditional methods are task-specific, leading to many algorithms without a unified solution, which greatly increases the computational and deployment costs of image fusion. In this paper, we summarize four remote sensing fusion tasks, including pan-sharpening, hyperspectral-multispectral fusion, spatio-temporal fusion, and polarimetric SAR fusion. By defining the spectral, temporal, and polarimetric information, as X, we propose the concept of generalized spatial-channel fusion, referred to as Spatial-X fusion. Then, we design an end-to-end network SpaXFus, a generalized spatial-channel fusion framework through a model-driven unfolding approach that exploits spatial-X intrinsic interactions to capture internal dependencies and self-interactions. Comprehensive experimental results demonstrate the superiority of SpaXFus, e.g., SpaXFus can achieve four remote sensing image fusion tasks with superior performance (across all fusion tasks, spectral distortion decreases by 25.48 %, while spatial details improve by 7.5 %) and shows huge improvements across multiple types of downstream applications, including vegetation index generation, fine-grained image classification, change detection, and SAR vegetation extraction.&lt;/p&gt;</content:encoded></item><item><title>Scalable Semi-supervised Learning with Discriminative Label Propagation and Correction</title><link>https://doi.org/10.1109/tpami.2026.3655456</link><guid>10.1109/tpami.2026.3655456</guid><pubDate>Mon, 19 Jan 2026 20:54:53 +0000</pubDate><dc:creator>Bingbing Jiang</dc:creator><dc:creator>Jie Wen</dc:creator><dc:creator>Zidong Wang</dc:creator><dc:creator>Weiguo Sheng</dc:creator><dc:creator>Zhiwen Yu</dc:creator><dc:creator>Huanhuan Chen</dc:creator><dc:creator>Weiping Ding</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3655456</prism:doi><description>Semi-supervised learning can leverage both labeled and unlabeled samples simultaneously to improve performance. However, existing methods often present the following issues: (1) The emphasis of learning is put on either the similarity structures or the regression losses of data, neglecting the interaction between them. (2) The similarity structures among boundary samples might be unreliable, which misleads label propagation and impairs the performance of models on out-of-sample data. (3) They often involve the inverses of high-order matrices, making them inefficient in computation. To overcome these issues, we propose a scalable semi-supervised learning framework with Discriminative Label Propagation and Correction (DLPC), which collaboratively exploits the regression losses and similarity structures of data. Particularly, each sample is projected onto the independent class labels associated with nonnegative adjustment vectors rather than the propagated labels, such that the distances between samples from different classes are naturally enlarged, making regression losses more effective for boundary samples. Benefiting from this, the regression losses can guide the propagation of labels in boundary areas. Thus, the label information is first propagated through dynamically optimized graph structures and then corrected by the regression losses, effectively improving the quality of labels and facilitating feature projection learning. Furthermore, an accelerated solution has been developed to reduce the computational costs of DLPC on sample scales, thereby making it scalable to relatively large-scale problems. Moreover, the proposed DLPC can not only be applied to single-view scenarios but also extended to multi-view tasks. Additionally, an optimization strategy with fast convergence has been presented for DLPC, and extensive experiments demonstrate the effectiveness and superiority of DLPC over state-of-the-art competitors.
Published: 2026-01-19T20:54:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bingbing Jiang; Jie Wen; Zidong Wang; Weiguo Sheng; Zhiwen Yu; Huanhuan Chen; Weiping Ding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3655456"&gt;10.1109/tpami.2026.3655456&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Semi-supervised learning can leverage both labeled and unlabeled samples simultaneously to improve performance. However, existing methods often present the following issues: (1) The emphasis of learning is put on either the similarity structures or the regression losses of data, neglecting the interaction between them. (2) The similarity structures among boundary samples might be unreliable, which misleads label propagation and impairs the performance of models on out-of-sample data. (3) They often involve the inverses of high-order matrices, making them inefficient in computation. To overcome these issues, we propose a scalable semi-supervised learning framework with Discriminative Label Propagation and Correction (DLPC), which collaboratively exploits the regression losses and similarity structures of data. Particularly, each sample is projected onto the independent class labels associated with nonnegative adjustment vectors rather than the propagated labels, such that the distances between samples from different classes are naturally enlarged, making regression losses more effective for boundary samples. Benefiting from this, the regression losses can guide the propagation of labels in boundary areas. Thus, the label information is first propagated through dynamically optimized graph structures and then corrected by the regression losses, effectively improving the quality of labels and facilitating feature projection learning. Furthermore, an accelerated solution has been developed to reduce the computational costs of DLPC on sample scales, thereby making it scalable to relatively large-scale problems. Moreover, the proposed DLPC can not only be applied to single-view scenarios but also extended to multi-view tasks. Additionally, an optimization strategy with fast convergence has been presented for DLPC, and extensive experiments demonstrate the effectiveness and superiority of DLPC over state-of-the-art competitors.&lt;/p&gt;</content:encoded></item><item><title>Defying Distractions in Multimodal Tasks: A Novel Benchmark for Large Vision-Language Models</title><link>https://doi.org/10.1109/tpami.2026.3655641</link><guid>10.1109/tpami.2026.3655641</guid><pubDate>Mon, 19 Jan 2026 20:54:53 +0000</pubDate><dc:creator>Jinhui Yang</dc:creator><dc:creator>Ming Jiang</dc:creator><dc:creator>Qi Zhao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3655641</prism:doi><description>Large Vision-Language Models (LVLMs) with “multimodal distractibility,” where plausible but irrelevant visual or textual inputs cause significant drops in reasoning consistency and lead to unreliable outputs. This paper introduces a comprehensive framework to systematically diagnose, evaluate, and mitigate this critical challenge. We present three core components: the large-scale IR-VQA benchmark to surface these vulnerabilities across four paradigms; novel diagnostic metrics, Positive Consistency (PC) and Negative Consistency (NC), which move beyond standard accuracy to rigorously measure a model's reasoning stability; and the Relevance-Gated Multimodal Routing (RGMR) mechanism, a novel, lightweight module that proactively and dynamically filters distractions at inference time. Our experiments reveal that state-of-the-art models exhibit significant drops in consistency on IR-VQA. We demonstrate that finetuning on IR-VQA and deploying RGMR substantially improve model robustness where standard prompting fails. Our comprehensive analysis of model behaviors under different types of distractions and the underlying reasoning failures provides a clear path forward for developing more reliable multimodal systems.
Published: 2026-01-19T20:54:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinhui Yang; Ming Jiang; Qi Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3655641"&gt;10.1109/tpami.2026.3655641&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Large Vision-Language Models (LVLMs) with “multimodal distractibility,” where plausible but irrelevant visual or textual inputs cause significant drops in reasoning consistency and lead to unreliable outputs. This paper introduces a comprehensive framework to systematically diagnose, evaluate, and mitigate this critical challenge. We present three core components: the large-scale IR-VQA benchmark to surface these vulnerabilities across four paradigms; novel diagnostic metrics, Positive Consistency (PC) and Negative Consistency (NC), which move beyond standard accuracy to rigorously measure a model&amp;#x27;s reasoning stability; and the Relevance-Gated Multimodal Routing (RGMR) mechanism, a novel, lightweight module that proactively and dynamically filters distractions at inference time. Our experiments reveal that state-of-the-art models exhibit significant drops in consistency on IR-VQA. We demonstrate that finetuning on IR-VQA and deploying RGMR substantially improve model robustness where standard prompting fails. Our comprehensive analysis of model behaviors under different types of distractions and the underlying reasoning failures provides a clear path forward for developing more reliable multimodal systems.&lt;/p&gt;</content:encoded></item><item><title>AMS-Former: Adaptive multi-scale transformer for multi-modal image matching</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.021</link><guid>10.1016/j.isprsjprs.2026.01.021</guid><pubDate>Mon, 19 Jan 2026 08:34:48 +0000</pubDate><dc:creator>Jiahao Rao</dc:creator><dc:creator>Rui Liu</dc:creator><dc:creator>Jianjun Guan</dc:creator><dc:creator>Xin Tian</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.021</prism:doi><description>Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .
Published: 2026-01-19T08:34:48+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Rao; Rui Liu; Jianjun Guan; Xin Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.021"&gt;10.1016/j.isprsjprs.2026.01.021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal image (MMI) matching plays a crucial role in the fusion of multi-source image information. However, due to the significant geometric and modality differences in MMI, existing methods often fail to achieve satisfactory matching performance. To address these challenges, we propose an end-to-end MMI matching approach, named adaptive multi-scale transformer (AMS-Former). First, AMS-Former constructs a multi-scale image matching framework that integrates contextual information across different scales, effectively identifying potential corresponding points and thereby improving matching accuracy. To handle the challenges caused by modality differences, we design a cross-modal feature extraction module with an adaptive modulation strategy. This module effectively couples features from different modalities, enhancing feature representation and improving model robustness under complex modality differences. To further enhance matching performance, we design a suitable loss function for the proposed AMS-Former to guide the optimization of network parameters. Finally, we use a cross-scale mutual supervision strategy to remove incorrect corresponding points and enhance the reliability of the matching results. Extensive experiments on five MMI datasets demonstrate that AMS-Former outperforms state-of-the-art methods, including RIFT, ASS, COFSM, POS-GIFT, Matchformer, SEMLA, TopicFM, and Lightglue. Our code is available at: https://github.com/Henryrjh/AMS_Former .&lt;/p&gt;</content:encoded></item><item><title>LightKD-SAR: Lightweight Architecture with Knowledge Distillation for High-Performance SAR Object Detection</title><link>https://doi.org/10.1109/jstars.2026.3655160</link><guid>10.1109/jstars.2026.3655160</guid><pubDate>Mon, 19 Jan 2026 20:55:15 +0000</pubDate><dc:creator>Zhuang Zhou</dc:creator><dc:creator>Shengyang Li</dc:creator><dc:creator>Yixuan Lv</dc:creator><dc:creator>Shicheng Guo</dc:creator><dc:creator>Han Wang</dc:creator><dc:creator>Jian Yang</dc:creator><dc:creator>Jianing You</dc:creator><dc:creator>Kailun Zhang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3655160</prism:doi><description>Synthetic Aperture Radar (SAR) object detection plays a crucial role in remote sensing applications. However, conventional methods often require high computational and memory costs, limiting their deployment in resource constrained environments. The challenges of SAR imagery such as sparse object distribution, speckle noise, and multi-scale variations make it difficult for existing lightweight detectors to achieve both high accuracy and efficiency. To address this issue, we propose LightKD-SAR, a lightweight SAR object detection framework that combines an efficient network architecture with enhanced instance selection based knowledge distillation. Specifically, we design a lightweight detection network using customized inverted residual modules, and further reduce computational complexity through optimized feature extraction and fusion strategies while maintaining robust detection performance. Additionally, we introduce an improved instance selection mechanism combined with multi-dimensional knowledge transfer, focusing on samples with large prediction discrepancies to enhance learning of ambiguous objects and complex backgrounds in SAR images. Extensive experiments on the large-scale SARDet-100k dataset demonstrate that LightKD-SAR achieves a mAP of 50.92% with only 15.7 GFLOPs and 11.43M parameters. Compared with state-of-the-art methods, the proposed framework demonstrates superior trade-off between detection accuracy and computational efficiency, making it well-suited for practical deployment in real-world SAR-based remote sensing systems.
Published: 2026-01-19T20:55:15+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhuang Zhou; Shengyang Li; Yixuan Lv; Shicheng Guo; Han Wang; Jian Yang; Jianing You; Kailun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3655160"&gt;10.1109/jstars.2026.3655160&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) object detection plays a crucial role in remote sensing applications. However, conventional methods often require high computational and memory costs, limiting their deployment in resource constrained environments. The challenges of SAR imagery such as sparse object distribution, speckle noise, and multi-scale variations make it difficult for existing lightweight detectors to achieve both high accuracy and efficiency. To address this issue, we propose LightKD-SAR, a lightweight SAR object detection framework that combines an efficient network architecture with enhanced instance selection based knowledge distillation. Specifically, we design a lightweight detection network using customized inverted residual modules, and further reduce computational complexity through optimized feature extraction and fusion strategies while maintaining robust detection performance. Additionally, we introduce an improved instance selection mechanism combined with multi-dimensional knowledge transfer, focusing on samples with large prediction discrepancies to enhance learning of ambiguous objects and complex backgrounds in SAR images. Extensive experiments on the large-scale SARDet-100k dataset demonstrate that LightKD-SAR achieves a mAP of 50.92% with only 15.7 GFLOPs and 11.43M parameters. Compared with state-of-the-art methods, the proposed framework demonstrates superior trade-off between detection accuracy and computational efficiency, making it well-suited for practical deployment in real-world SAR-based remote sensing systems.&lt;/p&gt;</content:encoded></item><item><title>SMWG-DETR: DETR Enhanced by Fourier Spectral Modulation and Wavelet-Guided Fusion for Tiny Object Detection</title><link>https://doi.org/10.1109/tgrs.2026.3655425</link><guid>10.1109/tgrs.2026.3655425</guid><pubDate>Mon, 19 Jan 2026 20:54:55 +0000</pubDate><dc:creator>Mingshu Chen</dc:creator><dc:creator>Wei Zhao</dc:creator><dc:creator>Nannan Li</dc:creator><dc:creator>Dongjin Li</dc:creator><dc:creator>Rufei Zhang</dc:creator><dc:creator>Jingyu Xu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3655425</prism:doi><description>Tiny object detection is a crucial task in the intelligent interpretation of remote sensing imagery, with significant applications in transportation, public security, and emergency management. However, the performance of existing detectors in remote sensing scenarios is still constrained by the extremely small object sizes and the presence of complex background clutter. In this paper, we propose DETR enhanced by Fourier Spectral Modulation and Wavelet-Guided Fusion (SMWG-DETR), which addresses the issues of spectral distribution bias during feature extraction as well as feature misalignment and detailed feature loss during feature fusion. First, Fourier spectral modulation is employed to suppress redundant frequency components in single-scale feature maps while preserving critical ones, thereby reducing spurious responses caused by cluttered backgrounds. Second, in the feature fusion stage, we apply Discrete Wavelet Transform (DWT) to lower-level feature maps, where the resulting low-frequency and high-frequency sub-bands are used to guide higher-level feature map upsampling and detailed feature refinement, thus leveraging the complementary information across multi-scale features. Finally, a Dynamic Denoising Query Selection (DDQS) strategy is introduced to discard potentially misleading queries in the contrastive denoising process, providing more accurate supervision during training. In experiments conducted on the AI-TOD and AI-TODv2 datasets, SMWG-DETR achieves average precision scores of 32.1% and 30.5%, respectively, achieving state-of-the-art performance. The complete code will be made publicly available at https://github.com/abdbdb/SMWG-DETR.
Published: 2026-01-19T20:54:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingshu Chen; Wei Zhao; Nannan Li; Dongjin Li; Rufei Zhang; Jingyu Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3655425"&gt;10.1109/tgrs.2026.3655425&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Tiny object detection is a crucial task in the intelligent interpretation of remote sensing imagery, with significant applications in transportation, public security, and emergency management. However, the performance of existing detectors in remote sensing scenarios is still constrained by the extremely small object sizes and the presence of complex background clutter. In this paper, we propose DETR enhanced by Fourier Spectral Modulation and Wavelet-Guided Fusion (SMWG-DETR), which addresses the issues of spectral distribution bias during feature extraction as well as feature misalignment and detailed feature loss during feature fusion. First, Fourier spectral modulation is employed to suppress redundant frequency components in single-scale feature maps while preserving critical ones, thereby reducing spurious responses caused by cluttered backgrounds. Second, in the feature fusion stage, we apply Discrete Wavelet Transform (DWT) to lower-level feature maps, where the resulting low-frequency and high-frequency sub-bands are used to guide higher-level feature map upsampling and detailed feature refinement, thus leveraging the complementary information across multi-scale features. Finally, a Dynamic Denoising Query Selection (DDQS) strategy is introduced to discard potentially misleading queries in the contrastive denoising process, providing more accurate supervision during training. In experiments conducted on the AI-TOD and AI-TODv2 datasets, SMWG-DETR achieves average precision scores of 32.1% and 30.5%, respectively, achieving state-of-the-art performance. The complete code will be made publicly available at https://github.com/abdbdb/SMWG-DETR.&lt;/p&gt;</content:encoded></item><item><title>HL-SAM-Seg: Complementary High- and Low-Resolution Features Based on SAM for Remote Sensing Image Semantic Segmentation</title><link>https://doi.org/10.1109/tgrs.2026.3655448</link><guid>10.1109/tgrs.2026.3655448</guid><pubDate>Mon, 19 Jan 2026 20:54:55 +0000</pubDate><dc:creator>Siting Xiong</dc:creator><dc:creator>Linfeng Wu</dc:creator><dc:creator>Bochen Zhang</dc:creator><dc:creator>Dejin Zhang</dc:creator><dc:creator>Yu Tao</dc:creator><dc:creator>Yuzhi Tang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3655448</prism:doi><description>The groundbreaking segment anything model (SAM), built on a vision transformer (ViT) design with millions of parameters and trained on the large SA-1B dataset, acts as a vision foundation model that can be used for various segmentation tasks. However, this model cannot be directly applied to the semantic segmentation of remote sensing images, as it tends to over-segment objects rather than preserving their semantics. Furthermore, it does not align well with object boundaries, which are usually required for the high-accuracy segmentation of high-resolution remote sensing images. To address this issue, we adjust the image encoder and mask decoder of SAM and propose an HL-SAM-Seg network. The image encoder is extensively adjusted to comprise an adapter, a high-resolution (high-res) path, and a low-resolution (low-res) path. The latter two paths extract and update the high-res and low-res features, which are merged in the mask decoder to perform multi-class segmentation. Specifically, we design a highlow- resolution cross-attention (HL) module inserted into the transformer blocks of the low-res path to align and update the low-res and high-res features. Experimental results on the ISPRS Vaihingen, ISPRS Potsdam, and FloodNet datasets show that the proposed HL-SAM-Seg outperformed conventional state-of-theart semantic segmentation algorithms overall, with underperformance in some small sample categories. Moreover, it has fewer trainable parameters, suggesting the potential for leveraging SAM for remote sensing image segmentation.
Published: 2026-01-19T20:54:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siting Xiong; Linfeng Wu; Bochen Zhang; Dejin Zhang; Yu Tao; Yuzhi Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3655448"&gt;10.1109/tgrs.2026.3655448&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;The groundbreaking segment anything model (SAM), built on a vision transformer (ViT) design with millions of parameters and trained on the large SA-1B dataset, acts as a vision foundation model that can be used for various segmentation tasks. However, this model cannot be directly applied to the semantic segmentation of remote sensing images, as it tends to over-segment objects rather than preserving their semantics. Furthermore, it does not align well with object boundaries, which are usually required for the high-accuracy segmentation of high-resolution remote sensing images. To address this issue, we adjust the image encoder and mask decoder of SAM and propose an HL-SAM-Seg network. The image encoder is extensively adjusted to comprise an adapter, a high-resolution (high-res) path, and a low-resolution (low-res) path. The latter two paths extract and update the high-res and low-res features, which are merged in the mask decoder to perform multi-class segmentation. Specifically, we design a highlow- resolution cross-attention (HL) module inserted into the transformer blocks of the low-res path to align and update the low-res and high-res features. Experimental results on the ISPRS Vaihingen, ISPRS Potsdam, and FloodNet datasets show that the proposed HL-SAM-Seg outperformed conventional state-of-theart semantic segmentation algorithms overall, with underperformance in some small sample categories. Moreover, it has fewer trainable parameters, suggesting the potential for leveraging SAM for remote sensing image segmentation.&lt;/p&gt;</content:encoded></item><item><title>ESCVehicle: A Drone-based Visible-Infrared Vehicle Benchmark with Extensive Scene Coverage</title><link>https://doi.org/10.1109/tgrs.2026.3655959</link><guid>10.1109/tgrs.2026.3655959</guid><pubDate>Mon, 19 Jan 2026 20:54:55 +0000</pubDate><dc:creator>Jiamin Song</dc:creator><dc:creator>Nan Zhang</dc:creator><dc:creator>Zhenhao Wang</dc:creator><dc:creator>Tian Tian</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3655959</prism:doi><description>UAV-based vehicle detection aims to efficiently identify and distinguish vehicle targets from aerial remote sensing imagery. It has been widely applied in areas such as traffic management and emergency response. However, existing datasets are limited by scene diversity and insufficient environmental complexity, making it challenging to effectively train and comprehensively evaluate multimodal algorithms under real-world complex scenarios. To address this limitation, we introduce ESCVehicle, a novel visible-infrared vehicle detection dataset captured by drones with extensive scene coverage and annotations. The dataset comprises 10,727 pairs of aligned visible and infrared images, over 360,000 finely annotated rotated bounding boxes, and spans seven vehicle categories, nine representative scene types, and typical day-night periods. Data collection was conducted under both natural and adverse weather conditions, offering rich and comprehensive data support for vehicle detection and classification tasks. In addition, to tackle the challenges of robust detection in complex environments, we propose a novel Cross-Modal Complex Scene Vehicle Detection framework (C2-VeD). The framework incorporates an Adaptive Feature Enhancement Convolution (AFEConv), which focuses on distinguishing target features from complex background patterns. By employing feature selection and channel reorganization mechanisms, AFEConv enhances the representation of target-relevant features while suppressing background interference. Furthermore, a Cross-Modal Context-Aware Fusion (CM-CAF) module is introduced to model the spatial dependencies of local features through cross-modal fusion and spatial context awareness, thereby reinforcing the complementarity between modalities and improving detection accuracy and robustness. Experimental results on the ESCVehicle dataset demonstrate that the proposed framework achieves superior performance under various complex scene conditions. The dataset and code wil...
Published: 2026-01-19T20:54:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiamin Song; Nan Zhang; Zhenhao Wang; Tian Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3655959"&gt;10.1109/tgrs.2026.3655959&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;UAV-based vehicle detection aims to efficiently identify and distinguish vehicle targets from aerial remote sensing imagery. It has been widely applied in areas such as traffic management and emergency response. However, existing datasets are limited by scene diversity and insufficient environmental complexity, making it challenging to effectively train and comprehensively evaluate multimodal algorithms under real-world complex scenarios. To address this limitation, we introduce ESCVehicle, a novel visible-infrared vehicle detection dataset captured by drones with extensive scene coverage and annotations. The dataset comprises 10,727 pairs of aligned visible and infrared images, over 360,000 finely annotated rotated bounding boxes, and spans seven vehicle categories, nine representative scene types, and typical day-night periods. Data collection was conducted under both natural and adverse weather conditions, offering rich and comprehensive data support for vehicle detection and classification tasks. In addition, to tackle the challenges of robust detection in complex environments, we propose a novel Cross-Modal Complex Scene Vehicle Detection framework (C2-VeD). The framework incorporates an Adaptive Feature Enhancement Convolution (AFEConv), which focuses on distinguishing target features from complex background patterns. By employing feature selection and channel reorganization mechanisms, AFEConv enhances the representation of target-relevant features while suppressing background interference. Furthermore, a Cross-Modal Context-Aware Fusion (CM-CAF) module is introduced to model the spatial dependencies of local features through cross-modal fusion and spatial context awareness, thereby reinforcing the complementarity between modalities and improving detection accuracy and robustness. Experimental results on the ESCVehicle dataset demonstrate that the proposed framework achieves superior performance under various complex scene conditions. The dataset and code wil...&lt;/p&gt;</content:encoded></item><item><title>Unsupervised SAR Image Super-Resolution with Despeckling via Region-Specific Diffusion Models</title><link>https://doi.org/10.1109/jstars.2026.3655710</link><guid>10.1109/jstars.2026.3655710</guid><pubDate>Mon, 19 Jan 2026 20:55:15 +0000</pubDate><dc:creator>Yi Kuang</dc:creator><dc:creator>Fei Ma</dc:creator><dc:creator>Yingbing Liu</dc:creator><dc:creator>Fangfang Li</dc:creator><dc:creator>Fan Zhang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3655710</prism:doi><description>Super-resolution (SR) and despeckling for Synthetic Aperture Radar (SAR) images are critical tasks. However, these tasks are challenging due to speckle noise and low resolution. Speckle noise, caused by the coherent imaging mechanism, severely degrades image quality, while low resolution limits the preservation of structural details and textures. Existing methods often fail to effectively balance noise suppression and texture reconstruction, especially when addressing both tasks simultaneously. To tackle these challenges, this paper proposes an unsupervised framework that combines Training for Region-Specific Diffusion Model (RSDM) and Latent Space Integration for Reconstruction (LSIR). RSDM uses Low-Rank Adaptation (LoRA) to train diffusion models tailored for homogeneous and inhomogeneous regions, enabling it to capture statistical uniformity and low-frequency features in homogeneous regions, while focusing on structural complexity and high-frequency details in inhomogeneous regions. LSIR integrates these region-specific models through Bézier interpolation for latent noise and linear interpolation for functional integration, allowing simultaneous high-quality despeckling and super-resolution. Experiments conducted on multiple SAR datasets confirm the effectiveness of the proposed framework. The results demonstrate significant improvements over state-of-the-art methods in structural detail preservation, noise reduction, and overall visual quality.
Published: 2026-01-19T20:55:15+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Kuang; Fei Ma; Yingbing Liu; Fangfang Li; Fan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3655710"&gt;10.1109/jstars.2026.3655710&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Super-resolution (SR) and despeckling for Synthetic Aperture Radar (SAR) images are critical tasks. However, these tasks are challenging due to speckle noise and low resolution. Speckle noise, caused by the coherent imaging mechanism, severely degrades image quality, while low resolution limits the preservation of structural details and textures. Existing methods often fail to effectively balance noise suppression and texture reconstruction, especially when addressing both tasks simultaneously. To tackle these challenges, this paper proposes an unsupervised framework that combines Training for Region-Specific Diffusion Model (RSDM) and Latent Space Integration for Reconstruction (LSIR). RSDM uses Low-Rank Adaptation (LoRA) to train diffusion models tailored for homogeneous and inhomogeneous regions, enabling it to capture statistical uniformity and low-frequency features in homogeneous regions, while focusing on structural complexity and high-frequency details in inhomogeneous regions. LSIR integrates these region-specific models through Bézier interpolation for latent noise and linear interpolation for functional integration, allowing simultaneous high-quality despeckling and super-resolution. Experiments conducted on multiple SAR datasets confirm the effectiveness of the proposed framework. The results demonstrate significant improvements over state-of-the-art methods in structural detail preservation, noise reduction, and overall visual quality.&lt;/p&gt;</content:encoded></item><item><title>DRPose: A Diffusion-based Pose Refinement Framework for 3D Human Pose Estimation</title><link>https://doi.org/10.1109/tcsvt.2026.3655768</link><guid>10.1109/tcsvt.2026.3655768</guid><pubDate>Mon, 19 Jan 2026 20:56:32 +0000</pubDate><dc:creator>Yong Wang</dc:creator><dc:creator>Xuguang Liu</dc:creator><dc:creator>Xiaoqing Wang</dc:creator><dc:creator>Doudou Wu</dc:creator><dc:creator>Wenming Yang</dc:creator><dc:creator>Hongbo Kang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3655768</prism:doi><description>Recently, two-stage 3D human pose estimation using monocular cameras has gained significant attention. However, the inherent uncertainty in the upscaling process from 2D to 3D often compromises the accuracy of deterministic methods. To address this, we propose a novel diffusion-based refinement framework (DRPose) which models the uncertainty during the upscaling process by introducing stochastic noise to the initially predicted 3D poses. This approach facilitates the generation of more realistic predictions through iterative refinement with multiple noise samples, ultimately producing multi-hypothesis predictions that better align with ground truth. Our framework incorporates two key components: a Graph Convolution Transformer module (SGCT), which integrates scaling and displacement adjustments based on conditional information with a joint temporal-spatial feature separation mechanism, and a Pose Refinement Module (PRM), which balances the initial and refined poses. This design allows DRPose to effectively refine pose estimation for both individual frames and sequential data. Furthermore, our framework establishes new benchmarks for performance in both frame2frame and seq2frame scenarios. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP datasets. Notably, when applied to the current state-of-the-art single-frame 3D pose extractor, our multi-hypothesis optimization achieves an 18.8% reduction in Mean Per Joint Position Error (MPJPE) and a 16.9% reduction in Procrustes MPJPE (P-MPJPE). Code is available at https://github.com/KHB1698/DRPose.
Published: 2026-01-19T20:56:32+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yong Wang; Xuguang Liu; Xiaoqing Wang; Doudou Wu; Wenming Yang; Hongbo Kang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3655768"&gt;10.1109/tcsvt.2026.3655768&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, two-stage 3D human pose estimation using monocular cameras has gained significant attention. However, the inherent uncertainty in the upscaling process from 2D to 3D often compromises the accuracy of deterministic methods. To address this, we propose a novel diffusion-based refinement framework (DRPose) which models the uncertainty during the upscaling process by introducing stochastic noise to the initially predicted 3D poses. This approach facilitates the generation of more realistic predictions through iterative refinement with multiple noise samples, ultimately producing multi-hypothesis predictions that better align with ground truth. Our framework incorporates two key components: a Graph Convolution Transformer module (SGCT), which integrates scaling and displacement adjustments based on conditional information with a joint temporal-spatial feature separation mechanism, and a Pose Refinement Module (PRM), which balances the initial and refined poses. This design allows DRPose to effectively refine pose estimation for both individual frames and sequential data. Furthermore, our framework establishes new benchmarks for performance in both frame2frame and seq2frame scenarios. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP datasets. Notably, when applied to the current state-of-the-art single-frame 3D pose extractor, our multi-hypothesis optimization achieves an 18.8% reduction in Mean Per Joint Position Error (MPJPE) and a 16.9% reduction in Procrustes MPJPE (P-MPJPE). Code is available at https://github.com/KHB1698/DRPose.&lt;/p&gt;</content:encoded></item><item><title>VectorLLM: Human-like extraction of structured building contours via multimodal LLMs</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.025</link><guid>10.1016/j.isprsjprs.2026.01.025</guid><pubDate>Mon, 19 Jan 2026 14:06:54 +0000</pubDate><dc:creator>Tao Zhang</dc:creator><dc:creator>Shiqing Wei</dc:creator><dc:creator>Shihao Chen</dc:creator><dc:creator>Wenling Yu</dc:creator><dc:creator>Muying Luo</dc:creator><dc:creator>Shunping Ji</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.025</prism:doi><description>Automatically extracting vectorized building contours from remote sensing imagery is crucial for urban planning, population estimation, and disaster assessment. Current state-of-the-art methods rely on complex multi-stage pipelines involving pixel segmentation, vectorization, and polygon refinement, which limits their scalability and real-world applicability. Inspired by the remarkable reasoning capabilities of Large Language Models (LLMs), we introduce VectorLLM, the first Multi-modal Large Language Model (MLLM) designed for regular building contour extraction from remote sensing images. Unlike existing approaches, VectorLLM performs corner-point by corner-point regression of building contours directly, mimicking human annotators’ labeling process. Our architecture consists of a vision foundation backbone, an MLP connector, and an LLM, enhanced with learnable position embeddings to improve spatial understanding capability. Through comprehensive exploration of training strategies including pretraining, supervised fine-tuning, and direct preference optimization across WHU, WHU-Mix, and CrowdAI datasets, VectorLLM outperforms the previous SOTA methods. Remarkably, VectorLLM exhibits strong zero-shot performance on unseen objects including aircraft, water bodies, and oil tanks, highlighting its potential for unified modeling of diverse remote sensing object contour extraction tasks. Overall, this work establishes a new paradigm for vector extraction in remote sensing, leveraging the topological reasoning capabilities of LLMs to achieve both high accuracy and exceptional generalization. All code and weights will be available at https://github.com/zhang-tao-whu/VectorLLM .
Published: 2026-01-19T14:06:54+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Zhang; Shiqing Wei; Shihao Chen; Wenling Yu; Muying Luo; Shunping Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.025"&gt;10.1016/j.isprsjprs.2026.01.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Automatically extracting vectorized building contours from remote sensing imagery is crucial for urban planning, population estimation, and disaster assessment. Current state-of-the-art methods rely on complex multi-stage pipelines involving pixel segmentation, vectorization, and polygon refinement, which limits their scalability and real-world applicability. Inspired by the remarkable reasoning capabilities of Large Language Models (LLMs), we introduce VectorLLM, the first Multi-modal Large Language Model (MLLM) designed for regular building contour extraction from remote sensing images. Unlike existing approaches, VectorLLM performs corner-point by corner-point regression of building contours directly, mimicking human annotators’ labeling process. Our architecture consists of a vision foundation backbone, an MLP connector, and an LLM, enhanced with learnable position embeddings to improve spatial understanding capability. Through comprehensive exploration of training strategies including pretraining, supervised fine-tuning, and direct preference optimization across WHU, WHU-Mix, and CrowdAI datasets, VectorLLM outperforms the previous SOTA methods. Remarkably, VectorLLM exhibits strong zero-shot performance on unseen objects including aircraft, water bodies, and oil tanks, highlighting its potential for unified modeling of diverse remote sensing object contour extraction tasks. Overall, this work establishes a new paradigm for vector extraction in remote sensing, leveraging the topological reasoning capabilities of LLMs to achieve both high accuracy and exceptional generalization. All code and weights will be available at https://github.com/zhang-tao-whu/VectorLLM .&lt;/p&gt;</content:encoded></item><item><title>水下图像分割方法综述</title><link>https://doi.org/10.11834/jig.250481</link><guid>10.11834/jig.250481</guid><pubDate>Tue, 20 Jan 2026 06:30:28 +0000</pubDate><dc:creator>Fang Hao</dc:creator><dc:creator>Yu Zongji</dc:creator><dc:creator>Chen Zhiyang</dc:creator><dc:creator>Cong Runmin</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250481</prism:doi><description>水下图像分割作为海洋工程、水下机器人导航、海洋生物监测及水下资源勘探等领域的核心技术，旨在从复杂且退化的水下图像中精准提取目标区域（如生物体、机器、海底地形等）。然而，水下环境特有的光照衰减、水体散射、颜色失真及低对比度等问题，为分割任务带来了严峻挑战。本文系统综述了水下图像分割领域的研究进展，将现有方法按照任务目标划分为水下显著性目标检测、水下语义分割和水下实例分割三大类。首先，阐述了水下图像分割的研究背景、核心挑战及应用价值；其次，分别详细梳理了三类分割任务的技术演进，从传统方法到基于深度学习的现代方法，深入分析了各类方法的核心思想、网络架构与优势；随后，总结了主流的水下图像分割数据集及对应的评价指标；进而，通过基准实验结果对比，直观展示了不同方法在典型数据集上的性能差异；最后， 对该领域面临的挑战及未来的发展趋势进行总结与展望。本文旨在为相关领域的研究人员提供全面的技术参考，推动水下图像分割技术的进一步发展与应用落地。
Published: 2026-01-20T06:30:28+00:00
Venue: Journal of Image and Graphics
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fang Hao; Yu Zongji; Chen Zhiyang; Cong Runmin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250481"&gt;10.11834/jig.250481&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;水下图像分割作为海洋工程、水下机器人导航、海洋生物监测及水下资源勘探等领域的核心技术，旨在从复杂且退化的水下图像中精准提取目标区域（如生物体、机器、海底地形等）。然而，水下环境特有的光照衰减、水体散射、颜色失真及低对比度等问题，为分割任务带来了严峻挑战。本文系统综述了水下图像分割领域的研究进展，将现有方法按照任务目标划分为水下显著性目标检测、水下语义分割和水下实例分割三大类。首先，阐述了水下图像分割的研究背景、核心挑战及应用价值；其次，分别详细梳理了三类分割任务的技术演进，从传统方法到基于深度学习的现代方法，深入分析了各类方法的核心思想、网络架构与优势；随后，总结了主流的水下图像分割数据集及对应的评价指标；进而，通过基准实验结果对比，直观展示了不同方法在典型数据集上的性能差异；最后， 对该领域面临的挑战及未来的发展趋势进行总结与展望。本文旨在为相关领域的研究人员提供全面的技术参考，推动水下图像分割技术的进一步发展与应用落地。&lt;/p&gt;</content:encoded></item><item><title>SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote Sensing Object Detection</title><link>https://arxiv.org/abs/2601.12507v1</link><guid>http://arxiv.org/abs/2601.12507v1</guid><pubDate>Sun, 18 Jan 2026 17:36:48 +0000</pubDate><dc:creator>Ruo Qi</dc:creator><dc:creator>Linhui Dai</dc:creator><dc:creator>Yusong Qin</dc:creator><dc:creator>Chaolei Yang</dc:creator><dc:creator>Yanshan Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.
Published: 2026-01-18T17:36:48+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruo Qi; Linhui Dai; Yusong Qin; Chaolei Yang; Yanshan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;In remote sensing images, complex backgrounds, weak object signals, and small object scales make accurate detection particularly challenging, especially under low-quality imaging conditions. A common strategy is to integrate single-image super-resolution (SR) before detection; however, such serial pipelines often suffer from misaligned optimization objectives, feature redundancy, and a lack of effective interaction between SR and detection. To address these issues, we propose a Saliency-Driven multi-task Collaborative Network (SDCoNet) that couples SR and detection through implicit feature sharing while preserving task specificity. SDCoNet employs the swin transformer-based shared encoder, where hierarchical window-shifted self-attention supports cross-task feature collaboration and adaptively balances the trade-off between texture refinement and semantic representation. In addition, a multi-scale saliency prediction module produces importance scores to select key tokens, enabling focused attention on weak object regions, suppression of background clutter, and suppression of adverse features introduced by multi-task coupling. Furthermore, a gradient routing strategy is introduced to mitigate optimization conflicts. It first stabilizes detection semantics and subsequently routes SR gradients along a detection-oriented direction, enabling the framework to guide the SR branch to generate high-frequency details that are explicitly beneficial for detection. Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split, and HRSSD-Split, demonstrate that the proposed method, while maintaining competitive computational efficiency, significantly outperforms existing mainstream algorithms in small object detection on low-quality remote sensing images. Our code is available at https://github.com/qiruo-ya/SDCoNet.&lt;/p&gt;</content:encoded></item><item><title>面向遥感图像解译的参数高效微调研究综述</title><link>https://doi.org/10.11834/jig.250105</link><guid>10.11834/jig.250105</guid><pubDate>Mon, 19 Jan 2026 01:26:13 +0000</pubDate><dc:creator>Chen Shiqi</dc:creator><dc:creator>Yang Xue</dc:creator><dc:creator>Zhu Rongqiang</dc:creator><dc:creator>Liao Ning</dc:creator><dc:creator>Zhao Weiwei</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250105</prism:doi><description>海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。
Published: 2026-01-19T01:26:13+00:00
Venue: Journal of Image and Graphics
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Shiqi; Yang Xue; Zhu Rongqiang; Liao Ning; Zhao Weiwei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250105"&gt;10.11834/jig.250105&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;海量遥感数据的获取和AI大模型的发展极大程度地推动了智能化遥感图像解译的下游应用落地。“预训练 + 微调”是视觉语言基础大模型适配下游领域的经典范式，能有效将基础模型的知识迁移至新任务中。尽管遥感大模型发展如火如荼且在下游任务中表现突出，扩展的模型规模和高昂的训练成本使其难以适用于资源受限、标签不足、需求动态的实际应用场景。为使模型快速适应特定下游任务且有效避免额外训练资源消耗，参数高效微调方法得以广泛研究，并逐渐应用于遥感图像解译当中，成为当下的研究热点。本文面向不同类型的参数高效微调方法和解译任务，对提示词微调、适配器微调和低秩自适应微调三大类方法展开调研并梳理了现有研究工作。此外，本文收集归纳并总结了多个代表性数据集上30余种用于遥感图像解译任务的参数高效微调方法的性能，并从模型精度、训练参数量和推理耗时角度综合评估了方法性能，有助于启发研究者提出新方法并进行公平比较。最后，本文结合当前现状从多模态生成式任务、模型可解释性、边缘端部署应用的角度，展望并讨论了该交叉领域的未来研究方向，旨在为打造“AI + 遥感”的下游应用生态提供理论参考与研究思路。&lt;/p&gt;</content:encoded></item><item><title>A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection</title><link>https://arxiv.org/abs/2601.11910v1</link><guid>http://arxiv.org/abs/2601.11910v1</guid><pubDate>Sat, 17 Jan 2026 05:14:42 +0000</pubDate><dc:creator>Guiying Zhu</dc:creator><dc:creator>Bowen Yang</dc:creator><dc:creator>Yin Zhuang</dc:creator><dc:creator>Tong Zhang</dc:creator><dc:creator>Guanqun Wang</dc:creator><dc:creator>Zhihao Che</dc:creator><dc:creator>He Chen</dc:creator><dc:creator>Lianlin Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of "guess what". Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.
Published: 2026-01-17T05:14:42+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guiying Zhu; Bowen Yang; Yin Zhuang; Tong Zhang; Guanqun Wang; Zhihao Che; He Chen; Lianlin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of &amp;quot;guess what&amp;quot;. Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.&lt;/p&gt;</content:encoded></item><item><title>SLNMapping: Super Lightweight Neural Mapping in Large-Scale Scenes</title><link>https://doi.org/10.1007/s11263-025-02581-6</link><guid>10.1007/s11263-025-02581-6</guid><pubDate>Mon, 19 Jan 2026 08:28:15 +0000</pubDate><dc:creator>Chenhui Shi</dc:creator><dc:creator>Fulin Tang</dc:creator><dc:creator>Hao Wei</dc:creator><dc:creator>Yihong Wu</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02581-6</prism:doi><description>We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.
Published: 2026-01-19T08:28:15+00:00
Venue: International Journal of Computer Vision
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenhui Shi; Fulin Tang; Hao Wei; Yihong Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02581-6"&gt;10.1007/s11263-025-02581-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;We propose SLNMapping, a novel neural mapping framework for super lightweight reconstruction in large-scale scenes. The core is a new ultra-compact neural map representation composed of a set of feature-independent local signed distance functions (SDFs) with outstanding expressiveness. To support efficient optimization, we introduce a novel parallel local SDF detection algorithm that enables real-time updates of local SDF states. Based on the excellent representation, we develop a three-stage mapping strategy for efficient, accurate, and lightweight large-scale reconstruction from streaming LiDAR frames. First, an incremental mapping module is introduced for accurate online pose estimation and simultaneous construction of a globally consistent neural map. Then, we perform offline global optimization to refine the reconstruction quality for the initial map. Finally, we propose an innovative neural map simplification method tailored for our representation, which aggregates the redundant local SDFs to further reduce the memory usage while preserving geometric fidelity. Extensive experiments demonstrate that our approach delivers superior localization accuracy and achieves state-of-the-art mapping performance with high efficiency and extremely low map memory consumption, especially requiring only about 1/10 the memory on the Oxford Spires dataset compared with existing advanced methods.&lt;/p&gt;</content:encoded></item><item><title>WEGLA-NormGAN: wavelet-enhanced Cycle-GAN with global-local attention for radiometric normalization of remote sensing images</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.020</link><guid>10.1016/j.isprsjprs.2026.01.020</guid><pubDate>Mon, 19 Jan 2026 11:01:56 +0000</pubDate><dc:creator>Wenxia Gan</dc:creator><dc:creator>Yu Feng</dc:creator><dc:creator>Jianhao Miao</dc:creator><dc:creator>Xinghua Li</dc:creator><dc:creator>Huanfeng Shen</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.020</prism:doi><description>The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .
Published: 2026-01-19T11:01:56+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxia Gan; Yu Feng; Jianhao Miao; Xinghua Li; Huanfeng Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.020"&gt;10.1016/j.isprsjprs.2026.01.020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;The diversity of satellite remote sensing images has significantly enhanced the capability to observe surface information on Earth. However, multi-temporal optical remote sensing images acquired from different sensor platforms often exhibit substantial radiometric discrepancies, and it is difficult to obtain overlapping reference images, which poses critical challenges for seamless large-scale mosaicking, including global radiometric inconsistency, unsmooth local transitions, and visible seamlines. Existing traditional and deep learning methods can achieve reasonable performance on paired datasets, but often face challenges in balancing spatial structural integrity with enhanced radiometric consistency and generalizing to unseen images. To address these issues, a wavelet-enhanced radiometric normalization network called WEGLA-NormGAN is proposed to generate radiometrically normalized imagery with sound radiometric consistency and spatial fidelity. This framework integrates frequency-domain and spatial-domain information to achieve consistent multi-scale radiometric feature modeling while ensuring spatial structural fidelity. Firstly, wavelet transform is introduced to effectively decouple radiometric information and structural features from images, explicitly enhancing radiometric feature representation and edge-texture preservation. Secondly, a U-Net architecture with multi-scale modeling advantages is fused with an adaptive attention mechanism incorporating residual structures. This hybrid design employs a statistical alignment strategy to efficiently extract global shallow features and local statistical information, adaptively adjust the dynamic attention of unseen data, and alleviate local distortions, improving radiometric consistency and achieving high-fidelity spatial structure preservation. The proposed framework generates radiometrically normalized imagery that harmonizes radiometric consistency with spatial fidelity, while achieving outstanding radiometric normalization even in unseen scenarios. Extensive experiments were conducted on two public datasets and a self-constructed dataset. The results demonstrate that WEGLA-NormGAN outperforms seven state-of-the-art methods in cross-temporal scenarios and five in cross-spatiotemporal scenarios in terms of radiometric consistency, structural fidelity, and robustness. The code is available at https://github.com/WITRS/WeGLA-Norm.git .&lt;/p&gt;</content:encoded></item><item><title>IGECNet: An Interaction-Guided Multimodal Fusion Network with Elevation Constraint for Remote Sensing Semantic Segmentation</title><link>https://doi.org/10.1109/tgrs.2026.3655357</link><guid>10.1109/tgrs.2026.3655357</guid><pubDate>Mon, 19 Jan 2026 20:54:55 +0000</pubDate><dc:creator>Haoxue Zhang</dc:creator><dc:creator>Gang Xie</dc:creator><dc:creator>Linjuan Li</dc:creator><dc:creator>Chenhao Chang</dc:creator><dc:creator>Xinlin Xie</dc:creator><dc:creator>Jinchang Ren</dc:creator><dc:creator>Heng Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3655357</prism:doi><description>Semantic segmentation of multimodal remote sensing images plays a crucial role in geospatial analysis. However, existing methods often struggle to effectively integrate spatial–spectral and geometric–geographic information, capture multiscale contextual features, and preserve the fine-grained boundaries of geo-objects. To address these challenges, we propose an interaction-guided multimodal fusion network with elevation constraint, referred to as IGECNet. The proposed network comprises four key components. First, a multimodal correlation interaction mechanism adaptively fuses features from high-resolution remote sensing images and digital surface models (DSMs) using multiscale dilated convolutions and cross-attention mechanisms, thereby enhancing complementary relationships while preserving modality-specific characteristics. Second, a channel reorder enhancement module hierarchically decomposes channel-wise features based on entropy and global average pooling values, improving spatial–spectral discriminability. Third, a frequency-guided context decoder refines segmentation outputs by leveraging global semantic context via Fourier transform and fine boundary details through local feature fusion. In addition, a new loss function, jointly constrained by semantic, boundary, and elevation consistency, enforces elevation consistency from DSMs gradients and boundary alignment. Extensive experiments on the ISPRS Vaihingen, ISPRS Potsdam, and DroneDeploy Segmentation benchmarks demonstrate that IGECNet achieves state-of-the-art performance. The source code will be publicly available upon publication.
Published: 2026-01-19T20:54:55+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoxue Zhang; Gang Xie; Linjuan Li; Chenhao Chang; Xinlin Xie; Jinchang Ren; Heng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3655357"&gt;10.1109/tgrs.2026.3655357&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of multimodal remote sensing images plays a crucial role in geospatial analysis. However, existing methods often struggle to effectively integrate spatial–spectral and geometric–geographic information, capture multiscale contextual features, and preserve the fine-grained boundaries of geo-objects. To address these challenges, we propose an interaction-guided multimodal fusion network with elevation constraint, referred to as IGECNet. The proposed network comprises four key components. First, a multimodal correlation interaction mechanism adaptively fuses features from high-resolution remote sensing images and digital surface models (DSMs) using multiscale dilated convolutions and cross-attention mechanisms, thereby enhancing complementary relationships while preserving modality-specific characteristics. Second, a channel reorder enhancement module hierarchically decomposes channel-wise features based on entropy and global average pooling values, improving spatial–spectral discriminability. Third, a frequency-guided context decoder refines segmentation outputs by leveraging global semantic context via Fourier transform and fine boundary details through local feature fusion. In addition, a new loss function, jointly constrained by semantic, boundary, and elevation consistency, enforces elevation consistency from DSMs gradients and boundary alignment. Extensive experiments on the ISPRS Vaihingen, ISPRS Potsdam, and DroneDeploy Segmentation benchmarks demonstrate that IGECNet achieves state-of-the-art performance. The source code will be publicly available upon publication.&lt;/p&gt;</content:encoded></item><item><title>Weakly Supervised Salient Object Detection with Text Supervision</title><link>https://doi.org/10.1007/s11263-025-02728-5</link><guid>10.1007/s11263-025-02728-5</guid><pubDate>Tue, 20 Jan 2026 09:30:03 +0000</pubDate><dc:creator>Zhihao Wu</dc:creator><dc:creator>Jie Wen</dc:creator><dc:creator>Linlin Shen</dc:creator><dc:creator>Xiaopeng Fan</dc:creator><dc:creator>Yong Xu</dc:creator><dc:creator>Jian Yang</dc:creator><dc:creator>David Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02728-5</prism:doi><description>Weakly supervised salient object detection using image-category supervision offers a cost-effective alternative to dense annotations, yet suffers from significant performance degradation. This is primarily attributed to the limitations of existing pseudo-label generation methods, which tend to either under- or over-activate object regions and indiscriminately label all non-activated pixels as background, introducing considerable label noise. Furthermore, these methods are restricted in the ability to capture objects beyond the pre-trained category set. To overcome these challenges, we propose a CLIP-based pseudo-label generation that exploits text prompts to jointly activate generic background and salient objects, breaking the dependency on specific categories. However, we find that this paradigm faces three challenges: optimal prompt uncertainty, background redundancy, and object-background conflict. To mitigate these, we propose three key modules. First, spatial distribution-guided prompt selection evaluates the spatial distribution of activation regions to identify the optimal prompt. Second, center and scale prior-guided activation refinement integrates self-attention and superpixel cues to suppress background noise. Third, learning feedback-guided pseudo-label update learns saliency knowledge from other pseudo-labels to resolve conflicting regions and iteratively refine supervision. Extensive experiments demonstrate that our method surpasses previous weakly supervised methods with image-category supervision and unsupervised approaches.
Published: 2026-01-20T09:30:03+00:00
Venue: International Journal of Computer Vision
Score: 0.785 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhihao Wu; Jie Wen; Linlin Shen; Xiaopeng Fan; Yong Xu; Jian Yang; David Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02728-5"&gt;10.1007/s11263-025-02728-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (consider)&lt;/p&gt;
&lt;p&gt;Weakly supervised salient object detection using image-category supervision offers a cost-effective alternative to dense annotations, yet suffers from significant performance degradation. This is primarily attributed to the limitations of existing pseudo-label generation methods, which tend to either under- or over-activate object regions and indiscriminately label all non-activated pixels as background, introducing considerable label noise. Furthermore, these methods are restricted in the ability to capture objects beyond the pre-trained category set. To overcome these challenges, we propose a CLIP-based pseudo-label generation that exploits text prompts to jointly activate generic background and salient objects, breaking the dependency on specific categories. However, we find that this paradigm faces three challenges: optimal prompt uncertainty, background redundancy, and object-background conflict. To mitigate these, we propose three key modules. First, spatial distribution-guided prompt selection evaluates the spatial distribution of activation regions to identify the optimal prompt. Second, center and scale prior-guided activation refinement integrates self-attention and superpixel cues to suppress background noise. Third, learning feedback-guided pseudo-label update learns saliency knowledge from other pseudo-labels to resolve conflicting regions and iteratively refine supervision. Extensive experiments demonstrate that our method surpasses previous weakly supervised methods with image-category supervision and unsupervised approaches.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote Sensing Image Classification</title><link>https://arxiv.org/abs/2601.12308v1</link><guid>http://arxiv.org/abs/2601.12308v1</guid><pubDate>Sun, 18 Jan 2026 08:21:51 +0000</pubDate><dc:creator>Anurag Kaushish</dc:creator><dc:creator>Ayan Sar</dc:creator><dc:creator>Sampurna Roy</dc:creator><dc:creator>Sudeshna Chakraborty</dc:creator><dc:creator>Prashant Trivedi</dc:creator><dc:creator>Tanupriya Choudhury</dc:creator><dc:creator>Kanav Gupta</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($&lt;50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.
Published: 2026-01-18T08:21:51+00:00
Venue: arXiv
Score: 0.784 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anurag Kaushish; Ayan Sar; Sampurna Roy; Sudeshna Chakraborty; Prashant Trivedi; Tanupriya Choudhury; Kanav Gupta&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (consider)&lt;/p&gt;
&lt;p&gt;Few-shot learning in remote sensing remains challenging due to three factors: the scarcity of labeled data, substantial domain shifts, and the multi-scale nature of geospatial objects. To address these issues, we introduce Adaptive Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful framework with three key innovations: (i) correlation-guided feature pyramids for capturing scale-invariant patterns, (ii) an adaptive channel correlation module (ACCM) for learning dynamic cross-scale relationships, and (iii) correlation-guided meta-learning that leverages correlation patterns instead of conventional prototype averaging. Unlike prior approaches that rely on heavy pre-trained models or transformers, AMC-MetaNet is trained from scratch with only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18 while maintaining high efficiency ($&amp;lt;50$ms per image inference). AMC-MetaNet achieves up to 86.65\% accuracy in 5-way 5-shot classification on various remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use, and AID. Our results establish AMC-MetaNet as a computationally efficient, scale-aware framework for real-world few-shot remote sensing.&lt;/p&gt;</content:encoded></item><item><title>A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery</title><link>https://doi.org/10.1109/jstars.2026.3655550</link><guid>10.1109/jstars.2026.3655550</guid><pubDate>Mon, 19 Jan 2026 20:55:15 +0000</pubDate><dc:creator>Ch Muhammad Awais</dc:creator><dc:creator>Marco Reggiannini</dc:creator><dc:creator>Davide Moroni</dc:creator><dc:creator>Oktay Karakus</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3655550</prism:doi><description>High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.
Published: 2026-01-19T20:55:15+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ch Muhammad Awais; Marco Reggiannini; Davide Moroni; Oktay Karakus&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3655550"&gt;10.1109/jstars.2026.3655550&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;High-resolution imagery plays a critical role in improving the performance of visual recognition tasks such as classification, detection, and segmentation. In many domains, including remote sensing and surveillance, low-resolution images can limit the accuracy of automated analysis. To address this, super-resolution (SR) techniques have been widely adopted to attempt to reconstruct high-resolution images from low-resolution inputs. Related traditional approaches focus solely on enhancing image quality based on pixel-level metrics, leaving the relationship between super-resolved image fidelity and downstream classification performance largely underexplored. This raises a key question: can integrating classification objectives directly into the super-resolution process further improve classification accuracy? In this paper, we try to respond to this question by investigating the relationship between super-resolution and classification through the deployment of a specialised algorithmic strategy. We propose a novel methodology that increases the resolution of synthetic aperture radar imagery by optimising loss functions that account for both image quality and classification performance. Our approach improves image quality, as measured by scientifically ascertained image quality indicators, while also enhancing classification accuracy.&lt;/p&gt;</content:encoded></item><item><title>Symmetria: A Synthetic Dataset for Learning in Point Clouds</title><link>https://doi.org/10.1007/s11263-025-02684-0</link><guid>10.1007/s11263-025-02684-0</guid><pubDate>Tue, 20 Jan 2026 03:44:22 +0000</pubDate><dc:creator>Ivan Sipiran</dc:creator><dc:creator>Gustavo Santelices</dc:creator><dc:creator>Lucas Oyarzún</dc:creator><dc:creator>Andrea Ranieri</dc:creator><dc:creator>Chiara Romanengo</dc:creator><dc:creator>Silvia Biasotti</dc:creator><dc:creator>Bianca Falcidieno</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02684-0</prism:doi><description>Unlike image or text domains that benefit from an abundance of large-scale datasets, point cloud learning techniques frequently encounter limitations due to the scarcity of extensive datasets. To overcome this limitation, we present Symmetria, a formula-driven dataset that can be generated at any arbitrary scale. By construction, it ensures the absolute availability of precise ground truth, promotes data-efficient experimentation by requiring fewer samples, enables broad generalization across diverse geometric settings, and offers easy extensibility to new tasks and modalities. Using the concept of symmetry, we create shapes with known structure and high variability, enabling neural networks to learn point cloud features effectively. Our results demonstrate that this dataset is highly effective for point cloud self-supervised pre-training, yielding models with strong performance in downstream tasks such as classification and segmentation, which also show good few-shot learning capabilities. Additionally, our dataset can support fine-tuning models to classify real-world objects, highlighting our approach’s practical utility and application. We also introduce a challenging task for symmetry detection and provide a benchmark for baseline comparisons. A significant advantage of our approach is the public availability of the dataset, the accompanying code, and the ability to generate very large collections, promoting further research and innovation in point cloud learning.
Published: 2026-01-20T03:44:22+00:00
Venue: International Journal of Computer Vision
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ivan Sipiran; Gustavo Santelices; Lucas Oyarzún; Andrea Ranieri; Chiara Romanengo; Silvia Biasotti; Bianca Falcidieno&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02684-0"&gt;10.1007/s11263-025-02684-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Unlike image or text domains that benefit from an abundance of large-scale datasets, point cloud learning techniques frequently encounter limitations due to the scarcity of extensive datasets. To overcome this limitation, we present Symmetria, a formula-driven dataset that can be generated at any arbitrary scale. By construction, it ensures the absolute availability of precise ground truth, promotes data-efficient experimentation by requiring fewer samples, enables broad generalization across diverse geometric settings, and offers easy extensibility to new tasks and modalities. Using the concept of symmetry, we create shapes with known structure and high variability, enabling neural networks to learn point cloud features effectively. Our results demonstrate that this dataset is highly effective for point cloud self-supervised pre-training, yielding models with strong performance in downstream tasks such as classification and segmentation, which also show good few-shot learning capabilities. Additionally, our dataset can support fine-tuning models to classify real-world objects, highlighting our approach’s practical utility and application. We also introduce a challenging task for symmetry detection and provide a benchmark for baseline comparisons. A significant advantage of our approach is the public availability of the dataset, the accompanying code, and the ability to generate very large collections, promoting further research and innovation in point cloud learning.&lt;/p&gt;</content:encoded></item><item><title>RaCFusion: Improving Camera-based 3D Object Detection via Radar-assisted Hierarchical Refinement</title><link>https://doi.org/10.1109/lra.2026.3655290</link><guid>10.1109/lra.2026.3655290</guid><pubDate>Mon, 19 Jan 2026 20:56:21 +0000</pubDate><dc:creator>Yingjie Wang</dc:creator><dc:creator>Jiajun Deng</dc:creator><dc:creator>Yuenan Hou</dc:creator><dc:creator>Yao Li</dc:creator><dc:creator>Lidian Wang</dc:creator><dc:creator>Yanyong Zhang</dc:creator><prism:publicationName>IEEE Robotics and Automation Letters</prism:publicationName><prism:doi>10.1109/lra.2026.3655290</prism:doi><description>Cameras and radar sensors are complementary in 3D object detection in that cameras specialize in capturing an object's visual information while radar provides spatial information and velocity hints. Existing radar-camera fusion methods often employ a symmetrical architecture that processes inputs from cameras and radar indiscriminately, hindering the full leverage of each modality's distinct advantages. To this end, we propose RaCFusion, a radar-camera fusion framework that leverages the camera stream as the main detector and improves it via Radarassisted hierarchical refinement. Technically, the Radar-assisted refinement is performed via two specifically designed modules. Firstly, in the Radar-assisted Query Generation module, the initial object queries of the image branch are augmented with the spatial information obtained from radar data, formulating enhanced hybrid object queries. These hybrid object queries are used to interact with the image features in the transformer decoder to generate object-centric query features. Subsequently, within the Radar-assisted Velocity Aggregation module, these query features undergo further refinement through the incorporation of Radar-assisted velocity features. These velocity features are meticulously learned from the nuanced relationships between the queries and radar features, thereby diminishing the error in velocity estimation by utilizing the valuable velocity clues from the radar sensor. RaCFusion achieves competitive performance among radar-camera-fusion 3D detectors on the nuScenes benchmark. The project is at https://jessiew0806.github.io/RaCFusion/.
Published: 2026-01-19T20:56:21+00:00
Venue: IEEE Robotics and Automation Letters
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yingjie Wang; Jiajun Deng; Yuenan Hou; Yao Li; Lidian Wang; Yanyong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Robotics and Automation Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lra.2026.3655290"&gt;10.1109/lra.2026.3655290&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Cameras and radar sensors are complementary in 3D object detection in that cameras specialize in capturing an object&amp;#x27;s visual information while radar provides spatial information and velocity hints. Existing radar-camera fusion methods often employ a symmetrical architecture that processes inputs from cameras and radar indiscriminately, hindering the full leverage of each modality&amp;#x27;s distinct advantages. To this end, we propose RaCFusion, a radar-camera fusion framework that leverages the camera stream as the main detector and improves it via Radarassisted hierarchical refinement. Technically, the Radar-assisted refinement is performed via two specifically designed modules. Firstly, in the Radar-assisted Query Generation module, the initial object queries of the image branch are augmented with the spatial information obtained from radar data, formulating enhanced hybrid object queries. These hybrid object queries are used to interact with the image features in the transformer decoder to generate object-centric query features. Subsequently, within the Radar-assisted Velocity Aggregation module, these query features undergo further refinement through the incorporation of Radar-assisted velocity features. These velocity features are meticulously learned from the nuanced relationships between the queries and radar features, thereby diminishing the error in velocity estimation by utilizing the valuable velocity clues from the radar sensor. RaCFusion achieves competitive performance among radar-camera-fusion 3D detectors on the nuScenes benchmark. The project is at https://jessiew0806.github.io/RaCFusion/.&lt;/p&gt;</content:encoded></item><item><title>M-STEP: Multi-Scale Temporal Information Enhancement and Propagation for Hierarchical Visual Transformer Tracking</title><link>https://doi.org/10.1109/tmm.2026.3655505</link><guid>10.1109/tmm.2026.3655505</guid><pubDate>Mon, 19 Jan 2026 20:55:34 +0000</pubDate><dc:creator>Yang Fang</dc:creator><dc:creator>Yujie Wang</dc:creator><dc:creator>Bingbing Jiang</dc:creator><dc:creator>Zongyi Xu</dc:creator><dc:creator>Jiaxu Leng</dc:creator><dc:creator>Yan Zhang</dc:creator><dc:creator>Weisheng Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3655505</prism:doi><description>Temporal information, as an inherent attribute of video sequences, plays a crucial role in visual object tracking. It provides implicit cues to capture dynamic changes and invariant characteristics of both the tracking target and the environment. Adequately leveraging it enables tracking models to enhance their adaptability, robustness, and tracking performance. However, existing tracking methods often suffer from redundancy in temporal features, inadequate temporal context propagation, and reliance on empirical template updates, thus leading to suboptimal utilization of such cues. To address these limitations, this paper proposes M-STEP, a hierarchical visual Transformer tracking method that incorporates the shallow feature extraction (SFE) encoder, multi-scale template enhancement (MSTE) encoder, spatiotemporal information fusion (SIF) encoder, and temporal propagation enhancement (TPE) module into a unified framework to effectively enhance and propagate the multi-scale template and spatiotemporal information. Specifically, the SFE encoder employs a simple yet efficient MLP block to effectively preserve shallow spatial details while maintaining linear computational complexity. Second, the MSTE encoder utilizes a multi-scale pyramid pooling mechanism to capture the invariant target features under complex appearance and environmental changes, thereby maintaining the target-aware discriminative ability. Then, the TPE module combines Mamba and attention operations to fuse multi-scale temporal features, aiming to adaptively strengthen temporal correlations, reduce temporal redundancy, and ensure efficient propagation of temporal information. Finally, the SIF encoder integrates multi-scale spatiotemporal information to further improve the model's ability to capture motion and appearance diversity. Experimental results on eight benchmark datasets demonstrate that M-STEP achieves state-of-the-art performance, highlighting the critical role of multi-scale template aggregat...
Published: 2026-01-19T20:55:34+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Fang; Yujie Wang; Bingbing Jiang; Zongyi Xu; Jiaxu Leng; Yan Zhang; Weisheng Li; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3655505"&gt;10.1109/tmm.2026.3655505&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Temporal information, as an inherent attribute of video sequences, plays a crucial role in visual object tracking. It provides implicit cues to capture dynamic changes and invariant characteristics of both the tracking target and the environment. Adequately leveraging it enables tracking models to enhance their adaptability, robustness, and tracking performance. However, existing tracking methods often suffer from redundancy in temporal features, inadequate temporal context propagation, and reliance on empirical template updates, thus leading to suboptimal utilization of such cues. To address these limitations, this paper proposes M-STEP, a hierarchical visual Transformer tracking method that incorporates the shallow feature extraction (SFE) encoder, multi-scale template enhancement (MSTE) encoder, spatiotemporal information fusion (SIF) encoder, and temporal propagation enhancement (TPE) module into a unified framework to effectively enhance and propagate the multi-scale template and spatiotemporal information. Specifically, the SFE encoder employs a simple yet efficient MLP block to effectively preserve shallow spatial details while maintaining linear computational complexity. Second, the MSTE encoder utilizes a multi-scale pyramid pooling mechanism to capture the invariant target features under complex appearance and environmental changes, thereby maintaining the target-aware discriminative ability. Then, the TPE module combines Mamba and attention operations to fuse multi-scale temporal features, aiming to adaptively strengthen temporal correlations, reduce temporal redundancy, and ensure efficient propagation of temporal information. Finally, the SIF encoder integrates multi-scale spatiotemporal information to further improve the model&amp;#x27;s ability to capture motion and appearance diversity. Experimental results on eight benchmark datasets demonstrate that M-STEP achieves state-of-the-art performance, highlighting the critical role of multi-scale template aggregat...&lt;/p&gt;</content:encoded></item></channel></rss>