<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 27 Nov 2025 13:29:43 +0000</lastBuildDate><item><title>MMT: Multimodal meta-training for few-shot object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132197</link><guid>10.1016/j.neucom.2025.132197</guid><pubDate>Thu, 27 Nov 2025 07:57:02 +0000</pubDate><category>Neurocomputing</category><description>Few-Shot Object Detection (FSOD) aims to detect objects from novel classes using only a few labeled instances per class. Recently, several FSOD approaches have incorporated vision-language models (VLMs) to leverage textual semantics for improving visual representations. However, VLM-based FSOD methods still face two major challenges: (1) the alignment bias between textual and regional features, which leads to unstable or suboptimal performance on novel categories; and (2) the lack of efficient training strategies, as most methods rely on repeatedly fine-tuning models on limited novel samples, which contradicts the few-shot learning paradigm and incurs substantial computational cost. To address these issues, we propose a Multimodal Meta-Training (MMT) framework that enhances both semantic alignment and training efficiency in FSOD. MMT consists of two core components: (1) a Region Feature Enhancement Module (RFEM), which refines visual region representations through cross-modal fusion with textual features to alleviate semantic misalignment; and (2) a Meta-Training Strategy, which adopts an inner–outer loop optimization scheme to improve model generalization and reduce training overhead. Extensive experiments on PASCAL VOC and MS COCO demonstrate that MMT achieves superior detection accuracy on novel classes while significantly reducing training time.
Published: 2025-11-27T07:57:02+00:00
Venue: Neurocomputing
Score: 0.886 (must_read)</description></item><item><title>PixelDiT: Pixel Diffusion Transformers for Image Generation</title><link>https://arxiv.org/abs/2511.20645v1</link><guid>http://arxiv.org/abs/2511.20645v1</guid><pubDate>Tue, 25 Nov 2025 18:59:25 +0000</pubDate><category>arXiv</category><description>Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.
Published: 2025-11-25T18:59:25+00:00
Venue: arXiv
Score: 0.880 (must_read)</description></item><item><title>IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection</title><link>https://arxiv.org/abs/2511.20319v1</link><guid>http://arxiv.org/abs/2511.20319v1</guid><pubDate>Tue, 25 Nov 2025 13:53:54 +0000</pubDate><category>arXiv</category><description>Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.
Published: 2025-11-25T13:53:54+00:00
Venue: arXiv
Score: 0.875 (must_read)</description></item><item><title>From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting</title><link>https://arxiv.org/abs/2511.21215v1</link><guid>http://arxiv.org/abs/2511.21215v1</guid><pubDate>Wed, 26 Nov 2025 09:44:51 +0000</pubDate><category>arXiv</category><description>We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.
Published: 2025-11-26T09:44:51+00:00
Venue: arXiv
Score: 0.871 (must_read)</description></item><item><title>Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models</title><link>https://arxiv.org/abs/2511.19822v1</link><guid>http://arxiv.org/abs/2511.19822v1</guid><pubDate>Tue, 25 Nov 2025 01:24:41 +0000</pubDate><category>arXiv</category><description>Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.
Published: 2025-11-25T01:24:41+00:00
Venue: arXiv
Score: 0.866 (must_read)</description></item><item><title>SAM-MI: A Mask-Injected Framework for Enhancing Open-Vocabulary Semantic Segmentation with SAM</title><link>https://arxiv.org/abs/2511.20027v1</link><guid>http://arxiv.org/abs/2511.20027v1</guid><pubDate>Tue, 25 Nov 2025 07:52:07 +0000</pubDate><category>arXiv</category><description>Open-vocabulary semantic segmentation (OVSS) aims to segment and recognize objects universally. Trained on extensive high-quality segmentation data, the segment anything model (SAM) has demonstrated remarkable universal segmentation capabilities, offering valuable support for OVSS. Although previous methods have made progress in leveraging SAM for OVSS, there are still some challenges: (1) SAM's tendency to over-segment and (2) hard combinations between fixed masks and labels. This paper introduces a novel mask-injected framework, SAM-MI, which effectively integrates SAM with OVSS models to address these challenges. Initially, SAM-MI employs a Text-guided Sparse Point Prompter to sample sparse prompts for SAM instead of previous dense grid-like prompts, thus significantly accelerating the mask generation process. The framework then introduces Shallow Mask Aggregation (SMAgg) to merge partial masks to mitigate the SAM's over-segmentation issue. Finally, Decoupled Mask Injection (DMI) incorporates SAM-generated masks for guidance at low-frequency and high-frequency separately, rather than directly combining them with labels. Extensive experiments on multiple benchmarks validate the superiority of SAM-MI. Notably, the proposed method achieves a 16.7% relative improvement in mIoU over Grounded-SAM on the MESS benchmark, along with a 1.6$\times$ speedup. We hope SAM-MI can serve as an alternative methodology to effectively equip the OVSS model with SAM.
Published: 2025-11-25T07:52:07+00:00
Venue: arXiv
Score: 0.865 (must_read)</description></item><item><title>PRTF: Polar Space Represented Multi-View 3D Object Detection With Temporal Fusion Enhancement</title><link>https://doi.org/10.1109/tits.2025.3633448</link><guid>10.1109/tits.2025.3633448</guid><pubDate>Wed, 26 Nov 2025 19:05:39 +0000</pubDate><category>IEEE Transactions on Intelligent Transportation Systems</category><description>Autonomous driving technology is becoming a significant trend in the development of public transportation. A critical task in autonomous driving perception is 3D object detection, which provides essential data support for downstream applications. Most mainstream 3D object detection methods rely on the Cartesian coordinate system, where they construct object queries to interact with image features and position embedding. However, these methods have the following problems: 1) Sensor-captured detail information diminishes with increasing distance, while pixels represent the same space in Cartesian coordinates, preventing the model from fully leveraging details in closer regions. 2) Multi-view images suffer from spatial misalignment due to overlapping fields of view. 3) The performance of existing single-branch depth prediction networks lacks the necessary accuracy. These issues hinder the feature interaction and affect detection performance. We propose an innovative framework PRTF. Based on Polar space, we design the Two-Stage Transformation Encoder: in the first stage, Dual-DepthNet is used to improve the accuracy of depth prediction. In the second stage, Polar points are generated to address spatial misalignment, enabling effective encoding of details at close distance. In the Temporal Decoder, object queries are leveraged to integrate temporal information, effectively compensating for ambiguous information. By enhancing spatial information at both near and far distances in Polar space, the overall performance of multi-view 3D object detection is significantly improved. PRTF achieves state-of-the-art performance on nuScenes Test with 56.1% mAP and 63.9% NDS, exceeding multi-modal frameworks that combine image and radar data.
Published: 2025-11-26T19:05:39+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.860 (must_read)</description></item><item><title>VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction</title><link>https://arxiv.org/abs/2511.19971v1</link><guid>http://arxiv.org/abs/2511.19971v1</guid><pubDate>Tue, 25 Nov 2025 06:30:22 +0000</pubDate><category>arXiv</category><description>Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.
Published: 2025-11-25T06:30:22+00:00
Venue: arXiv
Score: 0.853 (must_read)</description></item><item><title>DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs</title><link>https://arxiv.org/abs/2511.20468v1</link><guid>http://arxiv.org/abs/2511.20468v1</guid><pubDate>Tue, 25 Nov 2025 16:33:42 +0000</pubDate><category>arXiv</category><description>Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed
Published: 2025-11-25T16:33:42+00:00
Venue: arXiv
Score: 0.853 (must_read)</description></item><item><title>ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images</title><link>https://arxiv.org/abs/2511.21606v1</link><guid>http://arxiv.org/abs/2511.21606v1</guid><pubDate>Wed, 26 Nov 2025 17:26:00 +0000</pubDate><category>arXiv</category><description>Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.
Published: 2025-11-26T17:26:00+00:00
Venue: arXiv
Score: 0.852 (must_read)</description></item><item><title>Optical Remote Sensing Ship Detection Combining Channel Shuffling and Bilinear Interpolation</title><link>https://doi.org/10.3390/rs17233828</link><guid>10.3390/rs17233828</guid><pubDate>Wed, 26 Nov 2025 15:56:41 +0000</pubDate><category>Remote Sensing</category><description>Maritime remote sensing ship detection has long been plagued by two major issues: the failure of geometric priors due to the extreme length-to-width ratio of ships; and the sharp drop in edge signal-to-noise ratio caused by the overlapping chromaticity domain between ships and seawater, which leads to unsatisfactory accuracy of existing detectors in such scenarios. Therefore, this paper proposes an optical remote sensing ship detection model combining channel shuffling and bilinear interpolation, named CSBI-YOLO. The core innovations include three aspects: First, a group shuffling feature enhancement module is designed, embedding parallel group bottlenecks and channel shuffling mechanisms into the interface between the YOLOv8 backbone and neck to achieve multi-scale semantic information coupling with a small number of parameters. Second, an edge-gated upsampling unit is constructed, using separable Sobel magnitude as structural prior and a learnable gating mechanism to suppress low-contrast noise on the sea surface. Third, an R-IoU-Focal loss function is proposed, introducing logarithmic curvature penalty and adaptive weights to achieve joint optimization in three dimensions: location, shape, and scale. Dual validation was conducted on the self-built SlewSea-RS dataset and the public DOTA-ship dataset. The results show that on the SlewSea-RS dataset, the mAP50 and mAP50–95 values of the CSBI-YOLO model increased by 6% and 5.4%, respectively. On the DOTA-ship dataset, comparisons with various models demonstrate that the proposed model outperforms others, proving the excellent performance of the CSBI-YOLO model in detecting maritime ship targets.
Published: 2025-11-26T15:56:41+00:00
Venue: Remote Sensing
Score: 0.851 (must_read)</description></item><item><title>V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence</title><link>https://arxiv.org/abs/2511.20886v1</link><guid>http://arxiv.org/abs/2511.20886v1</guid><pubDate>Tue, 25 Nov 2025 22:06:30 +0000</pubDate><category>arXiv</category><description>Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).
Published: 2025-11-25T22:06:30+00:00
Venue: arXiv
Score: 0.850 (must_read)</description></item><item><title>Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits</title><link>https://arxiv.org/abs/2511.20273v1</link><guid>http://arxiv.org/abs/2511.20273v1</guid><pubDate>Tue, 25 Nov 2025 12:59:15 +0000</pubDate><category>arXiv</category><description>Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.
Published: 2025-11-25T12:59:15+00:00
Venue: arXiv
Score: 0.850 (must_read)</description></item><item><title>HTTM: Head-wise Temporal Token Merging for Faster VGGT</title><link>https://arxiv.org/abs/2511.21317v1</link><guid>http://arxiv.org/abs/2511.21317v1</guid><pubDate>Wed, 26 Nov 2025 12:04:03 +0000</pubDate><category>arXiv</category><description>The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.
Published: 2025-11-26T12:04:03+00:00
Venue: arXiv
Score: 0.847 (must_read)</description></item><item><title>AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend</title><link>https://arxiv.org/abs/2511.20343v1</link><guid>http://arxiv.org/abs/2511.20343v1</guid><pubDate>Tue, 25 Nov 2025 14:23:04 +0000</pubDate><category>arXiv</category><description>We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.
Published: 2025-11-25T14:23:04+00:00
Venue: arXiv
Score: 0.847 (must_read)</description></item><item><title>Adam Simplified: Bias Correction Debunked</title><link>https://arxiv.org/abs/2511.20516v2</link><guid>http://arxiv.org/abs/2511.20516v2</guid><pubDate>Tue, 25 Nov 2025 17:20:40 +0000</pubDate><category>arXiv</category><description>The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.
Published: 2025-11-25T17:20:40+00:00
Venue: arXiv
Score: 0.845 (must_read)</description></item><item><title>Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models</title><link>https://arxiv.org/abs/2511.21320v1</link><guid>http://arxiv.org/abs/2511.21320v1</guid><pubDate>Wed, 26 Nov 2025 12:05:44 +0000</pubDate><category>arXiv</category><description>Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.
Published: 2025-11-26T12:05:44+00:00
Venue: arXiv
Score: 0.843 (must_read)</description></item><item><title>MTD-YOLO: A Multi-Scale Perception Framework with Task Decoupling and Dynamic Alignment for UAV Small Object Detection</title><link>https://doi.org/10.3390/rs17233823</link><guid>10.3390/rs17233823</guid><pubDate>Wed, 26 Nov 2025 11:43:22 +0000</pubDate><category>Remote Sensing</category><description>Unmanned aerial vehicles (UAVs) have been widely used in aerial photography and target detection tasks due to their flexibility and unique perspective. However, small targets often suffer from insufficient resolution, uneven scale distribution, and complex background clutter, which are constrained by imaging conditions such as high-altitude imaging, long-distance capture, and wide field of view. These factors weaken the feature representation and generalization ability of the model, becoming the key bottleneck that restricts the improvement of small target detection accuracy in UAV scenarios. To address the above issues, this paper proposes a small target detection algorithm for UAV perspective, namely MTD-YOLO. First, a Parallel Multi-Scale Receptive Field Unit (PMSRFU) is designed. This unit effectively enhances the receptive field range of feature extraction and the fusion ability of multi-scale contextual information by introducing parallel branches with different-sized convolutional kernels. Second, we embed PMSRFU into a C2f block to form C2f-PMSRFU, which reuses shallow details and fuses multi-scale features to clarify edges and textures in small targets, yielding stronger fine-grained representations. Finally, an efficient detection head with task decoupling, dynamic alignment, and adaptive scale adjustment capabilities, namely SDIDA-Head, is proposed, which significantly improves the model’s small target detection accuracy. Extensive experiments on the VisDrone2019 and HazyDet datasets demonstrate that MTD-YOLO achieves a 7.6% and 6.6% increase in mAP@0.5 compared to the baseline YOLOv8n, respectively. Meanwhile, the Precision is improved by 6.0% and 1.1%, and the Recall is enhanced by 7.5% and 6.9%, respectively. These results fully validate the effectiveness and superiority of the proposed method in UAV small target detection tasks.
Published: 2025-11-26T11:43:22+00:00
Venue: Remote Sensing
Score: 0.843 (must_read)</description></item><item><title>RYOLO-LWMD-Lite: A Lightweight Rotating Ship Target Detection Model for Optical Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2025.3637224</link><guid>10.1109/jstars.2025.3637224</guid><pubDate>Wed, 26 Nov 2025 18:49:02 +0000</pubDate><category>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</category><description>Combining optical remote sensing images for ship monitoring is a practical approach for maritime surveillance. However, existing research lacks sufficient detection accuracy and fails to consider computational resource constraints in ship detection processing. This paper proposes a novel lightweight rotating ship target detection model. First, we enhance the detection accuracy by expanding the YOLOv8n-obb model with Large Selective Kernel (LSK) attention mechanism, Weight-Fusion Multi-Branch Auxiliary FPN (WFMAFPN), and Dynamic Task-Aligned Detection Head (DTAH). Specifically, the LSK attention mechanism dynamically adjusts the receptive field, effectively capturing multi-scale features. The WFMAFPN improves the capacity of feature fusion by the multi-directional paths and adaptive weight assignment to individual feature maps. The DTAH further enhances detection performance by improving task interaction between classification and localization. Second, we reduce the computational resource consumption of our model. This technique is developed by pruning based on layer adaptive magnitude on the enhanced architecture and designing the DTAH module with shared parameters. Considering the above improvement, we name our model RYOLO-LWMD-Lite. Finally, we constructed a large-scale dataset for rotating ships, named AShipClass9, with diverse ship categories to evaluate our model. Experimental results indicate that the RYOLO-LWMD-Lite model achieves higher detection accuracy while maintaining a lower parameter count. Specifically, the model's parameter count is approximately 2/3 that of YOLOv8n-obb, and the test accuracy on AShipClass9 reaches 48.2% (in terms of AP50), a 6% improvement over the baseline. In addition, experiments conducted on the DOTA1.5 dataset validate the generalization capability of the proposed model.The source code is available at https://github.com/QSuser/RYOLO-LWMD.
Published: 2025-11-26T18:49:02+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.842 (must_read)</description></item><item><title>Escaping the Verifier: Learning to Reason via Demonstrations</title><link>https://arxiv.org/abs/2511.21667v1</link><guid>http://arxiv.org/abs/2511.21667v1</guid><pubDate>Wed, 26 Nov 2025 18:42:52 +0000</pubDate><category>arXiv</category><description>Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
Published: 2025-11-26T18:42:52+00:00
Venue: arXiv
Score: 0.842 (must_read)</description></item><item><title>Seeing without Pixels: Perception from Camera Trajectories</title><link>https://arxiv.org/abs/2511.21681v1</link><guid>http://arxiv.org/abs/2511.21681v1</guid><pubDate>Wed, 26 Nov 2025 18:57:01 +0000</pubDate><category>arXiv</category><description>Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.
Published: 2025-11-26T18:57:01+00:00
Venue: arXiv
Score: 0.839 (must_read)</description></item><item><title>OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection</title><link>https://arxiv.org/abs/2511.21064v1</link><guid>http://arxiv.org/abs/2511.21064v1</guid><pubDate>Wed, 26 Nov 2025 05:08:26 +0000</pubDate><category>arXiv</category><description>Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.
Published: 2025-11-26T05:08:26+00:00
Venue: arXiv
Score: 0.839 (must_read)</description></item><item><title>MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning</title><link>https://doi.org/10.1016/j.inffus.2025.104006</link><guid>10.1016/j.inffus.2025.104006</guid><pubDate>Wed, 26 Nov 2025 16:17:24 +0000</pubDate><category>Information Fusion</category><description>High-resolution multi-modal remote sensing (RS) images provide rich complementary information for Earth observation, yet the scarcity of high-quality annotated data remains a major obstacle for effective model training. To address this challenge, we propose a Modality-Shared Self-supervised Distillation Framework (MSSDF) that learns discriminative multi-modal representations with minimal reliance on labeled data. Specifically, MSSDF integrates information-aware and cross-modal masking strategies with multi-objective self-supervised learning, enabling the model to capture modality-shared semantics and compensate for missing or weakly labeled modalities. This design substantially reduces the dependence on large-scale annotations and enhances robustness under limited-label regimes. Extensive experiments on scene classification, semantic segmentation, and change detection tasks demonstrate that MSSDF consistently outperforms state-of-the-art methods, particularly when labeled data are scarce. Specifically, on the Potsdam and Vaihingen semantic segmentation tasks, our method achieved mIoU scores of 78.30% and 76.50%, with only 50% train-set. For the US3D depth estimation task, the RMSE error is reduced to 0.182, and for the binary change detection task in SECOND dataset, our method achieved mIoU scores of 47.51%, surpassing the second by 3 percentage points. In addition, we construct a high-resolution multi-modal remote sensing image dataset named HR-Pairs, which contains 640,000 DOM (Digital Orthophoto Map) -DSM(Digital Surface Model) pairs with a spatial resolution of 0.05 meters, providing a new high-quality dataset for multi-modal remote sensing research. Our pretrain code, checkpoints, and HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF .
Published: 2025-11-26T16:17:24+00:00
Venue: Information Fusion
Score: 0.838 (must_read)</description></item><item><title>MLPMoE: Zero-Shot Architectural Metamorphosis of Dense LLM MLPs into Static Mixture-of-Experts</title><link>https://arxiv.org/abs/2511.21089v1</link><guid>http://arxiv.org/abs/2511.21089v1</guid><pubDate>Wed, 26 Nov 2025 06:14:26 +0000</pubDate><category>arXiv</category><description>Large Language Models (LLMs) are predominantly deployed as dense transformers, where every parameter in every feed-forward block is activated for every token. While architecturally simple, this is computationally inefficient, since inference costs scale linearly with parameter count. Recent upcycling methods such as MoEfication, CMoE, ToMoE, and MoORE reveal that much of the useful computation lives in sparse, semi-modular substructures inside dense feed-forward networks, but these approaches typically rely on clustering, activation profiling, singular value decomposition, or custom routing that requires calibration data. This paper introduces MLPMoE (MLP Mixture-of-Experts), a training-free, deterministic transformation that restructures the dense MLP in transformer blocks into a static, high-cardinality mixture of experts. The transformation uses simple tensor slicing and summation, reinterpreting the algebra of tensor parallelism as a topological conversion rather than a distributed training pattern. We further introduce Fractal Fade (differential branch sparsity) and Compensated Pruning (variance-preserving branch reduction) as lightweight mechanisms for structured sparsity. On Qwen2.5-0.5B-Instruct and DeepSeek-R1-Distill-Llama-8B, the zero-shot MLPMoE transform changes a proxy perplexity metric by less than 0.05 percent while keeping the parameter count effectively constant. On the 8B model, differential sparsity removes about 20 percent of MLP parameters while keeping perplexity within about 2 percent of the dense baseline. The method operates entirely post hoc on existing checkpoints and does not require gradients, calibration sets, or router training. Code is available at https://gist.github.com/iwallarm/fc2ef1eddf226ca7814f9e5e2ae9bad1
Published: 2025-11-26T06:14:26+00:00
Venue: arXiv
Score: 0.838 (must_read)</description></item><item><title>Exploring State-of-the-art models for Early Detection of Forest Fires</title><link>https://arxiv.org/abs/2511.20096v1</link><guid>http://arxiv.org/abs/2511.20096v1</guid><pubDate>Tue, 25 Nov 2025 09:13:07 +0000</pubDate><category>arXiv</category><description>There have been many recent developments in the use of Deep Learning Neural Networks for fire detection. In this paper, we explore an early warning system for detection of forest fires. Due to the lack of sizeable datasets and models tuned for this task, existing methods suffer from missed detection. In this work, we first propose a dataset for early identification of forest fires through visual analysis. Unlike existing image corpuses that contain images of wide-spread fire, our dataset consists of multiple instances of smoke plumes and fire that indicates the initiation of fire. We obtained this dataset synthetically by utilising game simulators such as Red Dead Redemption 2. We also combined our dataset with already published images to obtain a more comprehensive set. Finally, we compared image classification and localisation methods on the proposed dataset. More specifically we used YOLOv7 (You Only Look Once) and different models of detection transformer.
Published: 2025-11-25T09:13:07+00:00
Venue: arXiv
Score: 0.837 (must_read)</description></item><item><title>Breaking the Safety-Capability Tradeoff: Reinforcement Learning with Verifiable Rewards Maintains Safety Guardrails in LLMs</title><link>https://arxiv.org/abs/2511.21050v1</link><guid>http://arxiv.org/abs/2511.21050v1</guid><pubDate>Wed, 26 Nov 2025 04:36:34 +0000</pubDate><category>arXiv</category><description>Fine-tuning large language models (LLMs) for downstream tasks typically exhibit a fundamental safety-capability tradeoff, where improving task performance degrades safety alignment even on benign datasets. This degradation persists across standard approaches including supervised finetuning (SFT) and reinforcement learning from human feedback (RLHF). While reinforcement learning with verifiable rewards (RLVR) has emerged as a promising alternative that optimizes models on objectively measurable tasks, its safety implications remain unexplored. We present the first comprehensive theoretical and empirical analysis of safety properties in RLVR. Theoretically, we derive upper bounds on safety drift under KL-constrained optimization and prove conditions under which safety degradation is eliminated. Empirically, we conduct extensive experiments across five adversarial safety benchmarks, demonstrating that RLVR can simultaneously enhance reasoning capabilities while maintaining or improving safety guardrails. Our comprehensive ablation studies examine the effects of optimization algorithms, model scale, and task domains. Our findings challenge the prevailing assumption of an inevitable safety capability trade-off, and establish that a specific training methodology can achieve both objectives simultaneously, providing insights for the safe deployment of reasoning-capable LLMs.
Published: 2025-11-26T04:36:34+00:00
Venue: arXiv
Score: 0.837 (must_read)</description></item><item><title>Scaling Foundation Models for Radar Scene Understanding</title><link>https://arxiv.org/abs/2511.21105v1</link><guid>http://arxiv.org/abs/2511.21105v1</guid><pubDate>Wed, 26 Nov 2025 06:41:00 +0000</pubDate><category>arXiv</category><description>Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.
Published: 2025-11-26T06:41:00+00:00
Venue: arXiv
Score: 0.836 (must_read)</description></item><item><title>The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment</title><link>https://arxiv.org/abs/2511.21331v1</link><guid>http://arxiv.org/abs/2511.21331v1</guid><pubDate>Wed, 26 Nov 2025 12:25:55 +0000</pubDate><category>arXiv</category><description>Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.
Published: 2025-11-26T12:25:55+00:00
Venue: arXiv
Score: 0.835 (must_read)</description></item><item><title>Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO</title><link>https://arxiv.org/abs/2511.21638v1</link><guid>http://arxiv.org/abs/2511.21638v1</guid><pubDate>Wed, 26 Nov 2025 18:12:16 +0000</pubDate><category>arXiv</category><description>Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.
Published: 2025-11-26T18:12:16+00:00
Venue: arXiv
Score: 0.835 (must_read)</description></item><item><title>ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning</title><link>https://arxiv.org/abs/2511.21005v1</link><guid>http://arxiv.org/abs/2511.21005v1</guid><pubDate>Wed, 26 Nov 2025 03:10:15 +0000</pubDate><category>arXiv</category><description>Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies, thereby facilitating more thorough exploration. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.
Published: 2025-11-26T03:10:15+00:00
Venue: arXiv
Score: 0.835 (must_read)</description></item></channel></rss>