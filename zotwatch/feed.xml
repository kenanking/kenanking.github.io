<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 08 Dec 2025 02:27:01 +0000</lastBuildDate><item><title>EquivFisheye: A Spherical Fusion Framework for Panoramic 3D Perception with Surround-View Fisheye Cameras</title><link>https://doi.org/10.1016/j.inffus.2025.104024</link><guid>10.1016/j.inffus.2025.104024</guid><pubDate>Sat, 06 Dec 2025 07:56:17 +0000</pubDate><dc:creator>Zhao Yang</dc:creator><dc:creator>Xinglin Pu</dc:creator><dc:creator>Weixiang Xu</dc:creator><dc:creator>Zezhong Qian</dc:creator><dc:creator>Kang Ke</dc:creator><dc:creator>Haonan Zhang</dc:creator><dc:creator>Longjun Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104024</prism:doi><description>Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.
Published: 2025-12-06T07:56:17+00:00
Venue: Information Fusion
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Yang; Xinglin Pu; Weixiang Xu; Zezhong Qian; Kang Ke; Haonan Zhang; Longjun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104024"&gt;10.1016/j.inffus.2025.104024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Language Models in Agriculture: A Tutorial and Survey</title><link>https://doi.org/10.1016/j.inffus.2025.104042</link><guid>10.1016/j.inffus.2025.104042</guid><pubDate>Sun, 07 Dec 2025 15:19:43 +0000</pubDate><dc:creator>Mohammadreza Haghighat</dc:creator><dc:creator>Alzayat Saleh</dc:creator><dc:creator>Mostafa Rahimi Azghadi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104042</prism:doi><description>The integration of artificial intelligence (AI) in agriculture is rapidly evolving, marked by increasing adoption of machine learning (ML), deep learning (DL), and the recent emergence of large language models (LLMs) and multimodal language models (MLMs). These technologies are transforming traditional agricultural practices through advanced data analysis and offering innovative solutions for smart agriculture. Although earlier methods often relied on unimodal data, primarily images, current research is shifting toward information fusion with multimodal AI systems that fuse text, imagery, and other types of data, such as agricultural knowledge graphs and videos of animal activities, for enhanced decision making. General-purpose MLMs struggle with agriculture-specific nuances due to domain gaps and limited multimodal datasets. The high computational demands of foundation models (FM) and domain-specific needs also restrict broader adoption. This study presents a tutorial on applying MLMs in agriculture, covering their main concepts. Building on this foundational knowledge, the tutorial surveys key developments in the literature and provides a comprehensive step-by-step guide for implementing and tailoring MLMs to agricultural applications. This will address the domain gaps by knowledge integration, synthetic multimodal data generation and efficient learning methods, thereby expanding their practical applications.
Published: 2025-12-07T15:19:43+00:00
Venue: Information Fusion
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohammadreza Haghighat; Alzayat Saleh; Mostafa Rahimi Azghadi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104042"&gt;10.1016/j.inffus.2025.104042&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;The integration of artificial intelligence (AI) in agriculture is rapidly evolving, marked by increasing adoption of machine learning (ML), deep learning (DL), and the recent emergence of large language models (LLMs) and multimodal language models (MLMs). These technologies are transforming traditional agricultural practices through advanced data analysis and offering innovative solutions for smart agriculture. Although earlier methods often relied on unimodal data, primarily images, current research is shifting toward information fusion with multimodal AI systems that fuse text, imagery, and other types of data, such as agricultural knowledge graphs and videos of animal activities, for enhanced decision making. General-purpose MLMs struggle with agriculture-specific nuances due to domain gaps and limited multimodal datasets. The high computational demands of foundation models (FM) and domain-specific needs also restrict broader adoption. This study presents a tutorial on applying MLMs in agriculture, covering their main concepts. Building on this foundational knowledge, the tutorial surveys key developments in the literature and provides a comprehensive step-by-step guide for implementing and tailoring MLMs to agricultural applications. This will address the domain gaps by knowledge integration, synthetic multimodal data generation and efficient learning methods, thereby expanding their practical applications.&lt;/p&gt;</content:encoded></item><item><title>Rethinking Infrared Small Target Detection: A Foundation-Driven Efficient Paradigm</title><link>https://arxiv.org/abs/2512.05511v1</link><guid>http://arxiv.org/abs/2512.05511v1</guid><pubDate>Fri, 05 Dec 2025 08:12:35 +0000</pubDate><dc:creator>Chuang Yu</dc:creator><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Yaokun Li</dc:creator><dc:creator>Xiujun Shu</dc:creator><dc:creator>Yuanhao Feng</dc:creator><dc:creator>Bo Wang</dc:creator><dc:creator>Yimian Dai</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework
Published: 2025-12-05T08:12:35+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuang Yu; Jinmiao Zhao; Yunpeng Liu; Yaokun Li; Xiujun Shu; Yuanhao Feng; Bo Wang; Yimian Dai; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework&lt;/p&gt;</content:encoded></item><item><title>Classifier Guidance and Domain Cooperation for Multisource Unsupervised Domain Adaptation</title><link>https://doi.org/10.1016/j.knosys.2025.115057</link><guid>10.1016/j.knosys.2025.115057</guid><pubDate>Sun, 07 Dec 2025 23:04:51 +0000</pubDate><dc:creator>Ming Zhao</dc:creator><dc:creator>Yifan Lan</dc:creator><dc:creator>Yuwu Lu</dc:creator><dc:creator>Leyao Yuan</dc:creator><dc:creator>Wenmeng Zhang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115057</prism:doi><description>Traditional deep learning models usually rely on identical data distributions for their training and testing sets. Unfortunately, this assumption rarely holds in real-world applications. This challenge has led to the proposal of unsupervised domain adaptation (UDA). While centred on transfers between single source and target domains, most UDA methods do not capitalize on the wealth of labelled resources that are present across multiple source domains. This gap is addressed herein by focusing on multisource unsupervised domain adaptation (MUDA), where knowledge is transferred from multiple source domains to an unlabelled target domain. The existing MUDA methods focus primarily on reducing discrepancies between the source and target domains and often overlook the goal of improving the performance of the utilized classifier across different domains, which is equally crucial for effectively implementing domain adaptation. To overcome this limitation, we propose a novel approach called classifier guidance and domain cooperation (CGDC). Our method calculates the accuracy of a classifier in the target domain using a probability-inspired approach that considers data transformation costs and classifier precision in the high-level feature space of the target domain. This enables high-performance classifiers to guide classifiers with lower performance. Additionally, we introduce a mixup-based multi-information fusion strategy to create enriched domains by combining information derived from the source and target domains. A comparison with many methods on public datasets shows the effectiveness of our CGDC approach in image classification tasks. Specifically, CGDC achieves improvements of 4.3% on Office-Home and 1.9% on Office-31 over the best baseline methods.
Published: 2025-12-07T23:04:51+00:00
Venue: Knowledge-Based Systems
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Zhao; Yifan Lan; Yuwu Lu; Leyao Yuan; Wenmeng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115057"&gt;10.1016/j.knosys.2025.115057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Traditional deep learning models usually rely on identical data distributions for their training and testing sets. Unfortunately, this assumption rarely holds in real-world applications. This challenge has led to the proposal of unsupervised domain adaptation (UDA). While centred on transfers between single source and target domains, most UDA methods do not capitalize on the wealth of labelled resources that are present across multiple source domains. This gap is addressed herein by focusing on multisource unsupervised domain adaptation (MUDA), where knowledge is transferred from multiple source domains to an unlabelled target domain. The existing MUDA methods focus primarily on reducing discrepancies between the source and target domains and often overlook the goal of improving the performance of the utilized classifier across different domains, which is equally crucial for effectively implementing domain adaptation. To overcome this limitation, we propose a novel approach called classifier guidance and domain cooperation (CGDC). Our method calculates the accuracy of a classifier in the target domain using a probability-inspired approach that considers data transformation costs and classifier precision in the high-level feature space of the target domain. This enables high-performance classifiers to guide classifiers with lower performance. Additionally, we introduce a mixup-based multi-information fusion strategy to create enriched domains by combining information derived from the source and target domains. A comparison with many methods on public datasets shows the effectiveness of our CGDC approach in image classification tasks. Specifically, CGDC achieves improvements of 4.3% on Office-Home and 1.9% on Office-31 over the best baseline methods.&lt;/p&gt;</content:encoded></item><item><title>MSG-CLIP: Enhancing CLIP’s Ability to Learn Fine-grained Structural Associations through Multi-modal Scene Graph Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112794</link><guid>10.1016/j.patcog.2025.112794</guid><pubDate>Sat, 06 Dec 2025 23:14:20 +0000</pubDate><dc:creator>Xiaotian Lv</dc:creator><dc:creator>Yue Zhao</dc:creator><dc:creator>Hanlong Yin</dc:creator><dc:creator>Yifei Chen</dc:creator><dc:creator>Jianxing Liu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112794</prism:doi><description>As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.
Published: 2025-12-06T23:14:20+00:00
Venue: Pattern Recognition
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaotian Lv; Yue Zhao; Hanlong Yin; Yifei Chen; Jianxing Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112794"&gt;10.1016/j.patcog.2025.112794&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.&lt;/p&gt;</content:encoded></item><item><title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title><link>https://arxiv.org/abs/2512.04581v1</link><guid>http://arxiv.org/abs/2512.04581v1</guid><pubDate>Thu, 04 Dec 2025 08:49:23 +0000</pubDate><dc:creator>Houzhang Fang</dc:creator><dc:creator>Chenxing Wu</dc:creator><dc:creator>Kun Bai</dc:creator><dc:creator>Tianqi Chen</dc:creator><dc:creator>Xiaolin Wang</dc:creator><dc:creator>Xiyang Liu</dc:creator><dc:creator>Yi Chang</dc:creator><dc:creator>Luxin Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
Published: 2025-12-04T08:49:23+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Houzhang Fang; Chenxing Wu; Kun Bai; Tianqi Chen; Xiaolin Wang; Xiyang Liu; Yi Chang; Luxin Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network&amp;#x27;s focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.&lt;/p&gt;</content:encoded></item><item><title>Lightweight Image Super-Resolution Network with Adaptive Token Selection and Feature Enhancement</title><link>https://doi.org/10.1016/j.knosys.2025.115055</link><guid>10.1016/j.knosys.2025.115055</guid><pubDate>Sat, 06 Dec 2025 16:09:36 +0000</pubDate><dc:creator>Detian Huang</dc:creator><dc:creator>Mingxin Lin</dc:creator><dc:creator>Xinwei Gan</dc:creator><dc:creator>Luanyuan Dai</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115055</prism:doi><description>Transformer has demonstrated remarkable performance in image super-resolution due to its powerful long-range dependency modeling. However, existing transformer-based methods face two challenges. First, their self-attention mechanisms often treat all tokens equally, introducing redundant correlations and noise. Second, the use of window-based attention restricts the receptive field, hindering effective long-range dependency modeling. To address these issues, we introduce TSFE, a lightweight super-resolution network with adaptive Token Selection and Feature Enhancement. Specifically, we design a Token Selection Channel self-Attention (TSCA) that adaptively selects the top- K most relevant tokens along the channel dimension, preserving critical features while reducing redundant computations. Then, we present a Sparse Spatial Self-Attention (SSSA) that enhances critical features by suppressing low-attention weights and amplifying high-attention ones in the spatial dimension. To expand the receptive field, SSSA integrates a prior dictionary that guides the attention distribution with external prior, improving its dependency modeling capability. The collaboration of TSCA and SSSA components enables TSFE to achieve precise self-attention computation with low complexity. Extensive experiments validate that the proposed TSFE outperforms state-of-the-art methods across benchmarks.
Published: 2025-12-06T16:09:36+00:00
Venue: Knowledge-Based Systems
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Detian Huang; Mingxin Lin; Xinwei Gan; Luanyuan Dai; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115055"&gt;10.1016/j.knosys.2025.115055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer has demonstrated remarkable performance in image super-resolution due to its powerful long-range dependency modeling. However, existing transformer-based methods face two challenges. First, their self-attention mechanisms often treat all tokens equally, introducing redundant correlations and noise. Second, the use of window-based attention restricts the receptive field, hindering effective long-range dependency modeling. To address these issues, we introduce TSFE, a lightweight super-resolution network with adaptive Token Selection and Feature Enhancement. Specifically, we design a Token Selection Channel self-Attention (TSCA) that adaptively selects the top- K most relevant tokens along the channel dimension, preserving critical features while reducing redundant computations. Then, we present a Sparse Spatial Self-Attention (SSSA) that enhances critical features by suppressing low-attention weights and amplifying high-attention ones in the spatial dimension. To expand the receptive field, SSSA integrates a prior dictionary that guides the attention distribution with external prior, improving its dependency modeling capability. The collaboration of TSCA and SSSA components enables TSFE to achieve precise self-attention computation with low complexity. Extensive experiments validate that the proposed TSFE outperforms state-of-the-art methods across benchmarks.&lt;/p&gt;</content:encoded></item><item><title>LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</title><link>https://arxiv.org/abs/2512.05663v1</link><guid>http://arxiv.org/abs/2512.05663v1</guid><pubDate>Fri, 05 Dec 2025 12:08:18 +0000</pubDate><dc:creator>Johannes Meier</dc:creator><dc:creator>Jonathan Michel</dc:creator><dc:creator>Oussema Dhaouadi</dc:creator><dc:creator>Yung-Hsu Yang</dc:creator><dc:creator>Christoph Reich</dc:creator><dc:creator>Zuria Bauer</dc:creator><dc:creator>Stefan Roth</dc:creator><dc:creator>Marc Pollefeys</dc:creator><dc:creator>Jacques Kaiser</dc:creator><dc:creator>Daniel Cremers</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.
Published: 2025-12-05T12:08:18+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Johannes Meier; Jonathan Michel; Oussema Dhaouadi; Yung-Hsu Yang; Christoph Reich; Zuria Bauer; Stefan Roth; Marc Pollefeys; Jacques Kaiser; Daniel Cremers&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.&lt;/p&gt;</content:encoded></item><item><title>Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection</title><link>https://arxiv.org/abs/2512.04413v1</link><guid>http://arxiv.org/abs/2512.04413v1</guid><pubDate>Thu, 04 Dec 2025 03:18:42 +0000</pubDate><dc:creator>Xiangyi Gao</dc:creator><dc:creator>Danpei Zhao</dc:creator><dc:creator>Bo Yuan</dc:creator><dc:creator>Wentao Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TGRS.2025.3600098</prism:doi><description>Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.
Published: 2025-12-04T03:18:42+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyi Gao; Danpei Zhao; Bo Yuan; Wentao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TGRS.2025.3600098"&gt;10.1109/TGRS.2025.3600098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.&lt;/p&gt;</content:encoded></item><item><title>RTFormer: radiative transfer model-coupled transformer for cloud removal in optical remote sensing imagery</title><link>https://doi.org/10.1080/15481603.2025.2584294</link><guid>10.1080/15481603.2025.2584294</guid><pubDate>Sat, 06 Dec 2025 07:53:13 +0000</pubDate><dc:creator>Shiyao Meng</dc:creator><dc:creator>Siwei Li</dc:creator><dc:creator>Xinyu Wang</dc:creator><dc:creator>Ge Song</dc:creator><dc:creator>Jie Yang</dc:creator><dc:creator>Yu Ding</dc:creator><dc:creator>Wei Gong</dc:creator><prism:publicationName>GIScience &amp;amp; Remote Sensing</prism:publicationName><prism:doi>10.1080/15481603.2025.2584294</prism:doi><description>Clouds cover approximately 70% of the Earth’s surface, severely degrading optical satellite imagery and limiting its use in critical applications. Although recent transformer-based cloud removal methods have shown promise, they face two key challenges: (1) empirical models are limited in representing the complex radiative transfer properties of clouds, leading to physical inconsistencies; and (2) the high computational demands of self-attention typically constrain receptive fields, hindering the exploitation of global context for reconstructing cloud-contaminated regions. To address these issues, we propose RTFormer, a novel Radiative Transfer Model-coupled Transformer framework that integrates physical modeling into deep learning for efficient and accurate cloud removal. The core contribution lies in the incorporation of radiative transfer principles into the transformer architecture, enabling global context perception without additional computational overhead. To bridge the gap between simulated and real-world scenarios, we furhter developed a radiative transfer model-based dataset for model training. RTFormer extends the perception field beyond local constraints and incorporates a residual learning mechanism to preserve intact information in clear regions while accurately reconstructing cloud-affected areas. Experimental results demonstrate that RTFormer outperforms state-of-the-art methods in accuracy. Moreover, it substantially benefits downstream applications; for instance, when applied as a preprocessing step for land-cover classification, the overall accuracy improved by 4.38% compared with competing methods. These findings highlight the value of integrating physical models with transformer-based deep learning for cloud removal and demonstrate its potential for advancing remote sensing applications.
Published: 2025-12-06T07:53:13+00:00
Venue: GIScience &amp;amp; Remote Sensing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiyao Meng; Siwei Li; Xinyu Wang; Ge Song; Jie Yang; Yu Ding; Wei Gong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; GIScience &amp;amp;amp; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/15481603.2025.2584294"&gt;10.1080/15481603.2025.2584294&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Clouds cover approximately 70% of the Earth’s surface, severely degrading optical satellite imagery and limiting its use in critical applications. Although recent transformer-based cloud removal methods have shown promise, they face two key challenges: (1) empirical models are limited in representing the complex radiative transfer properties of clouds, leading to physical inconsistencies; and (2) the high computational demands of self-attention typically constrain receptive fields, hindering the exploitation of global context for reconstructing cloud-contaminated regions. To address these issues, we propose RTFormer, a novel Radiative Transfer Model-coupled Transformer framework that integrates physical modeling into deep learning for efficient and accurate cloud removal. The core contribution lies in the incorporation of radiative transfer principles into the transformer architecture, enabling global context perception without additional computational overhead. To bridge the gap between simulated and real-world scenarios, we furhter developed a radiative transfer model-based dataset for model training. RTFormer extends the perception field beyond local constraints and incorporates a residual learning mechanism to preserve intact information in clear regions while accurately reconstructing cloud-affected areas. Experimental results demonstrate that RTFormer outperforms state-of-the-art methods in accuracy. Moreover, it substantially benefits downstream applications; for instance, when applied as a preprocessing step for land-cover classification, the overall accuracy improved by 4.38% compared with competing methods. These findings highlight the value of integrating physical models with transformer-based deep learning for cloud removal and demonstrate its potential for advancing remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>PreciseVideo: A Dual-Process Framework for Zero-Shot Text-to-Video Generation with Quantitative Content Control</title><link>https://doi.org/10.1016/j.inffus.2025.104030</link><guid>10.1016/j.inffus.2025.104030</guid><pubDate>Sat, 06 Dec 2025 00:36:18 +0000</pubDate><dc:creator>Lizhi Dang</dc:creator><dc:creator>Ting Liang</dc:creator><dc:creator>Huixin Zhang</dc:creator><dc:creator>Ruihao Zhang</dc:creator><dc:creator>Yingping Hong</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104030</prism:doi><description>Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .
Published: 2025-12-06T00:36:18+00:00
Venue: Information Fusion
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lizhi Dang; Ting Liang; Huixin Zhang; Ruihao Zhang; Yingping Hong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104030"&gt;10.1016/j.inffus.2025.104030&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .&lt;/p&gt;</content:encoded></item><item><title>DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance</title><link>https://arxiv.org/abs/2512.04511v1</link><guid>http://arxiv.org/abs/2512.04511v1</guid><pubDate>Thu, 04 Dec 2025 06:45:20 +0000</pubDate><dc:creator>Yinghui Xing</dc:creator><dc:creator>Xiaoting Su</dc:creator><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Donghao Chu</dc:creator><dc:creator>Di Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.
Published: 2025-12-04T06:45:20+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinghui Xing; Xiaoting Su; Shizhou Zhang; Donghao Chu; Di Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.&lt;/p&gt;</content:encoded></item><item><title>Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects</title><link>https://arxiv.org/abs/2512.05006v1</link><guid>http://arxiv.org/abs/2512.05006v1</guid><pubDate>Thu, 04 Dec 2025 17:17:47 +0000</pubDate><dc:creator>Xianghui Fan</dc:creator><dc:creator>Zhaoyu Chen</dc:creator><dc:creator>Mengyang Pan</dc:creator><dc:creator>Anping Deng</dc:creator><dc:creator>Hang Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.
Published: 2025-12-04T17:17:47+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xianghui Fan; Zhaoyu Chen; Mengyang Pan; Anping Deng; Hang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.&lt;/p&gt;</content:encoded></item><item><title>Deep learning detection and analysis of eddies in the East Greenland marginal ice zone from Sentinel-1 SAR imagery</title><link>https://doi.org/10.1016/j.rse.2025.115177</link><guid>10.1016/j.rse.2025.115177</guid><pubDate>Sun, 07 Dec 2025 05:01:05 +0000</pubDate><dc:creator>Fei Jiang</dc:creator><dc:creator>Xiaofeng Li</dc:creator><dc:creator>Yingjie Liu</dc:creator><dc:creator>Yibin Ren</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115177</prism:doi><description>Ocean eddies in the marginal ice zone (MIZ) play a crucial role in sea-ice dynamics and polar ocean-atmosphere interactions; however, their detection remains challenging due to their complex surface signatures. In this study, we developed MIZ-EDYOLO, a deep learning model customized for detecting MIZ eddies from dual-polarized Sentinel-1 SAR imagery. Built upon the YOLOv9-t architecture and enhanced with specialized modifications, the model was trained on a dataset containing over 20,000 slices extracted from 1370 SAR images. The MIZ-EDYOLO model achieves high detection accuracy (∼80 % F1-score) on the test set and performs reliably on 200 full-scene images, enabling efficient and automated eddy identification. Using this model, we constructed the first six-year (2018–2023) SAR-based MIZ eddy dataset for the East Greenland region, comprising over 10,000 eddy instances. Analysis of this dataset reveals that eddy distributions are related to boundary currents, topographic forcing, and seasonal variations of the MIZ. Cyclonic eddies (CEs) outnumber anticyclonic eddies (AEs) by a factor of 8.4, while AEs exhibited an average radius about 1.8 times larger than CEs. The observed asymmetries between CEs and AEs are linked to their rotational dynamics and the associated sea-ice responses. This study presents a scalable and operational framework for efficient eddy monitoring in the MIZ, providing new insights into multi-scale oceanographic processes in climate-sensitive polar regions.
Published: 2025-12-07T05:01:05+00:00
Venue: Remote Sensing of Environment
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fei Jiang; Xiaofeng Li; Yingjie Liu; Yibin Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115177"&gt;10.1016/j.rse.2025.115177&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Ocean eddies in the marginal ice zone (MIZ) play a crucial role in sea-ice dynamics and polar ocean-atmosphere interactions; however, their detection remains challenging due to their complex surface signatures. In this study, we developed MIZ-EDYOLO, a deep learning model customized for detecting MIZ eddies from dual-polarized Sentinel-1 SAR imagery. Built upon the YOLOv9-t architecture and enhanced with specialized modifications, the model was trained on a dataset containing over 20,000 slices extracted from 1370 SAR images. The MIZ-EDYOLO model achieves high detection accuracy (∼80 % F1-score) on the test set and performs reliably on 200 full-scene images, enabling efficient and automated eddy identification. Using this model, we constructed the first six-year (2018–2023) SAR-based MIZ eddy dataset for the East Greenland region, comprising over 10,000 eddy instances. Analysis of this dataset reveals that eddy distributions are related to boundary currents, topographic forcing, and seasonal variations of the MIZ. Cyclonic eddies (CEs) outnumber anticyclonic eddies (AEs) by a factor of 8.4, while AEs exhibited an average radius about 1.8 times larger than CEs. The observed asymmetries between CEs and AEs are linked to their rotational dynamics and the associated sea-ice responses. This study presents a scalable and operational framework for efficient eddy monitoring in the MIZ, providing new insights into multi-scale oceanographic processes in climate-sensitive polar regions.&lt;/p&gt;</content:encoded></item><item><title>Source-Free Domain Adaptation via Multimodal Space-Guided Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112827</link><guid>10.1016/j.patcog.2025.112827</guid><pubDate>Sun, 07 Dec 2025 15:13:39 +0000</pubDate><dc:creator>Lijuan Chen</dc:creator><dc:creator>Yunxiang Bai</dc:creator><dc:creator>Ying Hu</dc:creator><dc:creator>Qiong Wang</dc:creator><dc:creator>Xiaozhi Qi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112827</prism:doi><description>Conventional UDA requires access to the source domain, invalidating it in the information security and privacy protection scenarios. In contrast, Source-free Domain Adaptation (SFDA) involves transferring a pre-trained source model to an unlabeled target domain while the source data is absent. However, prior methods based on self-supervised learning have struggled to find a quality domain invariant representation space due to the lack of source data. To address this challenge, in this work, we propose leveraging the success of vision-language pre-trained (ViL) models (e.g., CLIP). To integrate the domain generality of the ViL model and the task specificity of source model more effectively, we introduce a novel M ulti M odal Space- G uided A lignment ( MMGA ) approach. Specifically, we start with a multimodal feature calibration for achieving coarse alignment between the target visual domain and the multimodal space. However, this ViL space is still not the domain invariant space, being trained on a large number of samples. To achieve further fine-grained alignment towards the domain invariant space, we have designed two methods: Potential category consistency and prediction consistency alignment. These methods push the potential categories distribution and the prediction distribution closer to the fused pseudo-supervision by the ViL model and the adapted source model, respectively. This strategy corrects the errors of feature alignment to the ViL space. Extensive experiments show that our MMGA approach significantly outperforms current state-of-the-art alternatives. The code and data are available at https://github.com/YunxiangBai0/MMGA/
Published: 2025-12-07T15:13:39+00:00
Venue: Pattern Recognition
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lijuan Chen; Yunxiang Bai; Ying Hu; Qiong Wang; Xiaozhi Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112827"&gt;10.1016/j.patcog.2025.112827&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Conventional UDA requires access to the source domain, invalidating it in the information security and privacy protection scenarios. In contrast, Source-free Domain Adaptation (SFDA) involves transferring a pre-trained source model to an unlabeled target domain while the source data is absent. However, prior methods based on self-supervised learning have struggled to find a quality domain invariant representation space due to the lack of source data. To address this challenge, in this work, we propose leveraging the success of vision-language pre-trained (ViL) models (e.g., CLIP). To integrate the domain generality of the ViL model and the task specificity of source model more effectively, we introduce a novel M ulti M odal Space- G uided A lignment ( MMGA ) approach. Specifically, we start with a multimodal feature calibration for achieving coarse alignment between the target visual domain and the multimodal space. However, this ViL space is still not the domain invariant space, being trained on a large number of samples. To achieve further fine-grained alignment towards the domain invariant space, we have designed two methods: Potential category consistency and prediction consistency alignment. These methods push the potential categories distribution and the prediction distribution closer to the fused pseudo-supervision by the ViL model and the adapted source model, respectively. This strategy corrects the errors of feature alignment to the ViL space. Extensive experiments show that our MMGA approach significantly outperforms current state-of-the-art alternatives. The code and data are available at https://github.com/YunxiangBai0/MMGA/&lt;/p&gt;</content:encoded></item><item><title>Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.04520v1</link><guid>http://arxiv.org/abs/2512.04520v1</guid><pubDate>Thu, 04 Dec 2025 07:08:21 +0000</pubDate><dc:creator>Chenlin Xu</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Lituan Wang</dc:creator><dc:creator>Xinyu Pu</dc:creator><dc:creator>Pengfei Ma</dc:creator><dc:creator>Guangwu Qian</dc:creator><dc:creator>Zizhou Wang</dc:creator><dc:creator>Yan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM's zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.
Published: 2025-12-04T07:08:21+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenlin Xu; Lei Zhang; Lituan Wang; Xinyu Pu; Pengfei Ma; Guangwu Qian; Zizhou Wang; Yan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM&amp;#x27;s zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.&lt;/p&gt;</content:encoded></item><item><title>Variance Matters: Improving Domain Adaptation via Stratified Sampling</title><link>https://arxiv.org/abs/2512.05226v1</link><guid>http://arxiv.org/abs/2512.05226v1</guid><pubDate>Thu, 04 Dec 2025 20:01:04 +0000</pubDate><dc:creator>Andrea Napoli</dc:creator><dc:creator>Paul White</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.
Published: 2025-12-04T20:01:04+00:00
Venue: arXiv
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Andrea Napoli; Paul White&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.&lt;/p&gt;</content:encoded></item><item><title>RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation</title><link>https://arxiv.org/abs/2512.05025v1</link><guid>http://arxiv.org/abs/2512.05025v1</guid><pubDate>Thu, 04 Dec 2025 17:40:17 +0000</pubDate><dc:creator>Nicolas Houdré</dc:creator><dc:creator>Diego Marcos</dc:creator><dc:creator>Hugo Riffaud de Turckheim</dc:creator><dc:creator>Dino Ienco</dc:creator><dc:creator>Laurent Wendling</dc:creator><dc:creator>Camille Kurtz</dc:creator><dc:creator>Sylvain Lobry</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.
Published: 2025-12-04T17:40:17+00:00
Venue: arXiv
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nicolas Houdré; Diego Marcos; Hugo Riffaud de Turckheim; Dino Ienco; Laurent Wendling; Camille Kurtz; Sylvain Lobry&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.&lt;/p&gt;</content:encoded></item><item><title>Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models</title><link>https://arxiv.org/abs/2512.04395v1</link><guid>http://arxiv.org/abs/2512.04395v1</guid><pubDate>Thu, 04 Dec 2025 02:32:55 +0000</pubDate><dc:creator>Hieu Dinh Trung Pham</dc:creator><dc:creator>Huy Minh Nhat Nguyen</dc:creator><dc:creator>Cuong Tuan Nguyen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.
Published: 2025-12-04T02:32:55+00:00
Venue: arXiv
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hieu Dinh Trung Pham; Huy Minh Nhat Nguyen; Cuong Tuan Nguyen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image&amp;#x27;s domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image&amp;#x27;s structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.&lt;/p&gt;</content:encoded></item><item><title>Mitigating Modal Discrepancies for Visible-Infrared Person Re-Identification via High-order Nonlinear Constraint</title><link>https://doi.org/10.1016/j.knosys.2025.115052</link><guid>10.1016/j.knosys.2025.115052</guid><pubDate>Sat, 06 Dec 2025 07:47:47 +0000</pubDate><dc:creator>Junyu Liu</dc:creator><dc:creator>Yanzhen Xiong</dc:creator><dc:creator>Jinjia Peng</dc:creator><dc:creator>Huibing Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115052</prism:doi><description>One of the principal challenges in the visible-infrared person re-identification (VI-ReID) task is bridging the modality gap between visible and infrared images. Most existing methods transfer single-modal feature extractors to VI-ReID without sufficient consideration of cross-modal characteristics, and apply metric learning losses with Euclidean distance as the common metric. However, the substantial differences between visible and infrared modalities make it difficult for such designs to capture complex nonlinear cross-modal relations. To overcome these issues, this paper proposes the Reproducing Kernel Hilbert Space-Based Modal Discrepancy Reduction Network (RMDR-Net), which optimizes cross-modal discrepancies by employing Reproducing Kernel Hilbert Space (RKHS) similarity metrics. RMDR-Net includes a novel High-order Nonlinear Discriminative loss, which initially mitigates cross-modal discrepancies by capturing the high-order nonlinear relationships between features in RKHS. Moreover, Gram Matrix Consistency loss is designed to enhance distributional consistency both within and between modalities, which revealing underlying feature relationships and further promoting multimodal alignment. Additionally, the Multi-Scale Enhanced Dual Attention module is proposed to capture cross-modal fine-grained differences and obtain discriminative features. Extensive experiments on several public datasets demonstrate that our network surpasses other state-of-the-art methods.
Published: 2025-12-06T07:47:47+00:00
Venue: Knowledge-Based Systems
Score: 0.767 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junyu Liu; Yanzhen Xiong; Jinjia Peng; Huibing Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115052"&gt;10.1016/j.knosys.2025.115052&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (must_read)&lt;/p&gt;
&lt;p&gt;One of the principal challenges in the visible-infrared person re-identification (VI-ReID) task is bridging the modality gap between visible and infrared images. Most existing methods transfer single-modal feature extractors to VI-ReID without sufficient consideration of cross-modal characteristics, and apply metric learning losses with Euclidean distance as the common metric. However, the substantial differences between visible and infrared modalities make it difficult for such designs to capture complex nonlinear cross-modal relations. To overcome these issues, this paper proposes the Reproducing Kernel Hilbert Space-Based Modal Discrepancy Reduction Network (RMDR-Net), which optimizes cross-modal discrepancies by employing Reproducing Kernel Hilbert Space (RKHS) similarity metrics. RMDR-Net includes a novel High-order Nonlinear Discriminative loss, which initially mitigates cross-modal discrepancies by capturing the high-order nonlinear relationships between features in RKHS. Moreover, Gram Matrix Consistency loss is designed to enhance distributional consistency both within and between modalities, which revealing underlying feature relationships and further promoting multimodal alignment. Additionally, the Multi-Scale Enhanced Dual Attention module is proposed to capture cross-modal fine-grained differences and obtain discriminative features. Extensive experiments on several public datasets demonstrate that our network surpasses other state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>MT-Depth: Multi-task Instance feature analysis for the Depth Completion</title><link>https://arxiv.org/abs/2512.04734v1</link><guid>http://arxiv.org/abs/2512.04734v1</guid><pubDate>Thu, 04 Dec 2025 12:17:33 +0000</pubDate><dc:creator>Abdul Haseeb Nizamani</dc:creator><dc:creator>Dandi Zhou</dc:creator><dc:creator>Xinhai Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.
Published: 2025-12-04T12:17:33+00:00
Venue: arXiv
Score: 0.767 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Abdul Haseeb Nizamani; Dandi Zhou; Xinhai Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (must_read)&lt;/p&gt;
&lt;p&gt;Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.&lt;/p&gt;</content:encoded></item><item><title>GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis</title><link>https://arxiv.org/abs/2512.04456v1</link><guid>http://arxiv.org/abs/2512.04456v1</guid><pubDate>Thu, 04 Dec 2025 05:00:00 +0000</pubDate><dc:creator>Changjin Kim</dc:creator><dc:creator>HyeokJun Lee</dc:creator><dc:creator>YoungJoon Yoo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.
Published: 2025-12-04T05:00:00+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changjin Kim; HyeokJun Lee; YoungJoon Yoo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model&amp;#x27;s backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.&lt;/p&gt;</content:encoded></item><item><title>Concept-based Explainable Data Mining with VLM for 3D Detection</title><link>https://arxiv.org/abs/2512.05482v1</link><guid>http://arxiv.org/abs/2512.05482v1</guid><pubDate>Fri, 05 Dec 2025 07:18:45 +0000</pubDate><dc:creator>Mai Tsujimoto</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.
Published: 2025-12-05T07:18:45+00:00
Venue: arXiv
Score: 0.763 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mai Tsujimoto&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (must_read)&lt;/p&gt;
&lt;p&gt;Rare-object detection remains a challenging task in autonomous driving systems, particularly when relying solely on point cloud data. Although Vision-Language Models (VLMs) exhibit strong capabilities in image understanding, their potential to enhance 3D object detection through intelligent data mining has not been fully explored. This paper proposes a novel cross-modal framework that leverages 2D VLMs to identify and mine rare objects from driving scenes, thereby improving 3D object detection performance. Our approach synthesizes complementary techniques such as object detection, semantic feature extraction, dimensionality reduction, and multi-faceted outlier detection into a cohesive, explainable pipeline that systematically identifies rare but critical objects in driving scenes. By combining Isolation Forest and t-SNE-based outlier detection methods with concept-based filtering, the framework effectively identifies semantically meaningful rare objects. A key strength of this approach lies in its ability to extract and annotate targeted rare object concepts such as construction vehicles, motorcycles, and barriers. This substantially reduces the annotation burden and focuses only on the most valuable training samples. Experiments on the nuScenes dataset demonstrate that this concept-guided data mining strategy enhances the performance of 3D object detection models while utilizing only a fraction of the training data, with particularly notable improvements for challenging object categories such as trailers and bicycles compared with the same amount of random data. This finding has substantial implications for the efficient curation of datasets in safety-critical autonomous systems.&lt;/p&gt;</content:encoded></item><item><title>MGC-Net: Learning Feature Matching with Multi-Geometry Cooperation</title><link>https://doi.org/10.1016/j.knosys.2025.115062</link><guid>10.1016/j.knosys.2025.115062</guid><pubDate>Sat, 06 Dec 2025 00:20:13 +0000</pubDate><dc:creator>Luxia Ai</dc:creator><dc:creator>Kun Sun</dc:creator><dc:creator>Chen Zhang</dc:creator><dc:creator>Nanjun Yuan</dc:creator><dc:creator>Qun Jiang</dc:creator><dc:creator>Wenbing Tao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115062</prism:doi><description>Feature matching is the task of determining correspondences between images, which is important for many downstream vision applications. Although learning-based methods have made great progress, it is challenging for effective and efficient feature matching in changing scenes. The existing sparse learning feature matching method is efficient but not suitable for various changing scenes, especially weak textures. Meanwhile, the dense learning feature matching method is effective in various changing scenes but inefficient. Therefore, we aim to leverage strengths of both sparse and dense methods to achieve effective and efficient feature matching. To achieve this, we introduce MGC-Net to learn sparse-to-dense matching via multi-geometry cooperation. MGC-Net includes affine-enhanced module (AEM), pose-guided module (PGM), and extra homography supervision. AEM can expand dense points around sparse keypoints and extract affine-invariant features, enhancing local features by embedding affine geometry. PGM can find outliers in putative correspondences and prune them out, guiding better matching by embedding epipolar geometry. Moreover, MGC-Net is supervised by homography geometry to determine correspondences. With multi-geometry cooperation, MGC-Net learns more useful priors through end-to-end training, making inference accurately and quickly. Experiments show that MGC-Net has a better AUC than sparse methods. Also, its running time is better than dense methods. Our method can achieve efficient and effective feature matching in various changing scenes, which other methods can’t balance. MGC-Net is the first to learn sparse-to-dense matching and achieve sota.
Published: 2025-12-06T00:20:13+00:00
Venue: Knowledge-Based Systems
Score: 0.762 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Luxia Ai; Kun Sun; Chen Zhang; Nanjun Yuan; Qun Jiang; Wenbing Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115062"&gt;10.1016/j.knosys.2025.115062&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (must_read)&lt;/p&gt;
&lt;p&gt;Feature matching is the task of determining correspondences between images, which is important for many downstream vision applications. Although learning-based methods have made great progress, it is challenging for effective and efficient feature matching in changing scenes. The existing sparse learning feature matching method is efficient but not suitable for various changing scenes, especially weak textures. Meanwhile, the dense learning feature matching method is effective in various changing scenes but inefficient. Therefore, we aim to leverage strengths of both sparse and dense methods to achieve effective and efficient feature matching. To achieve this, we introduce MGC-Net to learn sparse-to-dense matching via multi-geometry cooperation. MGC-Net includes affine-enhanced module (AEM), pose-guided module (PGM), and extra homography supervision. AEM can expand dense points around sparse keypoints and extract affine-invariant features, enhancing local features by embedding affine geometry. PGM can find outliers in putative correspondences and prune them out, guiding better matching by embedding epipolar geometry. Moreover, MGC-Net is supervised by homography geometry to determine correspondences. With multi-geometry cooperation, MGC-Net learns more useful priors through end-to-end training, making inference accurately and quickly. Experiments show that MGC-Net has a better AUC than sparse methods. Also, its running time is better than dense methods. Our method can achieve efficient and effective feature matching in various changing scenes, which other methods can’t balance. MGC-Net is the first to learn sparse-to-dense matching and achieve sota.&lt;/p&gt;</content:encoded></item><item><title>Heterogeneous Feature Knowledge Distillation based on Enhanced Feature Projector Correlation</title><link>https://doi.org/10.1016/j.neunet.2025.108409</link><guid>10.1016/j.neunet.2025.108409</guid><pubDate>Sat, 06 Dec 2025 16:08:34 +0000</pubDate><dc:creator>Hong Zhao</dc:creator><dc:creator>Kangping Chen</dc:creator><dc:creator>Qiaoyin Jin</dc:creator><dc:creator>Dailin Huang</dc:creator><dc:creator>Zhaobin Chang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108409</prism:doi><description>Knowledge Distillation (KD) is a widely used technique to enhance model performance. However, most existing methods are designed under the assumption that the teacher and student models belong to the homogeneous architecture —particularly those relying on intermediate feature KD. In practice, mainstream model architectures exhibit significant differences in feature structure and representation, making feature alignment challenging. To address this issue, we propose a heterogeneous feature knowledge distillation based on enhanced feature projector correlation. Specifically, we first project both teacher and student features into the structurally consistent latent space to measure their semantic distribution. To mitigate the potential loss of semantic relevance caused by feature decorrelation during projection, we introduce a cross-space fusion mechanism to model the correlation between the original and latent features. Furthermore, to better leverage the rich representations from deep teacher layers, we design a multi-level feature knowledge distillation loss that guides the student by regressing features from multiple semantic levels. Finally, to reduce the noise introduced by the inherent limitations of student features and the projection process, we incorporate a denoising mechanism based on diffusion models to enhance class-wise discriminability in the student feature space. We conducted extensive experiments on the CIFAR-100 and ImageNet datasets to validate the effectiveness of the proposed distillation method across various mainstream architectures, including Convolutional Neural Networks (CNNs), Transformers, and Multilayer Perceptrons (MLPs) for image classification. In addition, we performed semantic segmentation experiments on the Cityscapes dataset to further verify the performance of our method. Code is available at https://github.com/chenKP/HFKD-Diff .
Published: 2025-12-06T16:08:34+00:00
Venue: Neural Networks
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hong Zhao; Kangping Chen; Qiaoyin Jin; Dailin Huang; Zhaobin Chang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108409"&gt;10.1016/j.neunet.2025.108409&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Knowledge Distillation (KD) is a widely used technique to enhance model performance. However, most existing methods are designed under the assumption that the teacher and student models belong to the homogeneous architecture —particularly those relying on intermediate feature KD. In practice, mainstream model architectures exhibit significant differences in feature structure and representation, making feature alignment challenging. To address this issue, we propose a heterogeneous feature knowledge distillation based on enhanced feature projector correlation. Specifically, we first project both teacher and student features into the structurally consistent latent space to measure their semantic distribution. To mitigate the potential loss of semantic relevance caused by feature decorrelation during projection, we introduce a cross-space fusion mechanism to model the correlation between the original and latent features. Furthermore, to better leverage the rich representations from deep teacher layers, we design a multi-level feature knowledge distillation loss that guides the student by regressing features from multiple semantic levels. Finally, to reduce the noise introduced by the inherent limitations of student features and the projection process, we incorporate a denoising mechanism based on diffusion models to enhance class-wise discriminability in the student feature space. We conducted extensive experiments on the CIFAR-100 and ImageNet datasets to validate the effectiveness of the proposed distillation method across various mainstream architectures, including Convolutional Neural Networks (CNNs), Transformers, and Multilayer Perceptrons (MLPs) for image classification. In addition, we performed semantic segmentation experiments on the Cityscapes dataset to further verify the performance of our method. Code is available at https://github.com/chenKP/HFKD-Diff .&lt;/p&gt;</content:encoded></item><item><title>Prompt-oriented and Frequency-regularized Schrödinger Bridge for Unpaired Rain Streaks and Raindrops Removal</title><link>https://doi.org/10.1016/j.patcog.2025.112862</link><guid>10.1016/j.patcog.2025.112862</guid><pubDate>Sat, 06 Dec 2025 00:07:54 +0000</pubDate><dc:creator>Yuanbo Wen</dc:creator><dc:creator>Jing Qin</dc:creator><dc:creator>Ting Chen</dc:creator><dc:creator>Tao Gao</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112862</prism:doi><description>The removal of rain streaks and raindrops is essential for improving image visibility. However, most existing methods rely on paired rainy and clean images, which are difficult to acquire in real-world scenarios. To this end, we propose prior-oriented and frequency-regularized Schrödinger bridge (PFSB) for rain streaks and raindrops removal with unpaired training. Specifically, we initially formulate unpaired image deraining as a Schrödinger bridge problem. Furthermore, we demonstrate the locally quasi-convexity of structural similarity, and employ the multi-scale structural similarity constraint (MSSC) to minimize the duality gap between the primal and dual problems, ensuring linear convergence of gradient flow while preserving textural details. Meanwhile, we develop a context-preserving consistency modulator (CCM) guide the derained output toward clean content, thereby retaining rain-irrelevant features. Moreover, we propose a domain-representative prompt protocol (DPP), which enforces the generated sample to eliminate rain-relevant information and maintain alignment with the clean domain. Additionally, we utilize Bayesian frequency-domain regularization (BFR) to balance spectral consistency with clean references and repulsion from rainy patterns. Extensive experiments demonstrate that our method surpasses the existing well-performing unpaired learning approaches in both fidelity and photo-realism.
Published: 2025-12-06T00:07:54+00:00
Venue: Pattern Recognition
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanbo Wen; Jing Qin; Ting Chen; Tao Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112862"&gt;10.1016/j.patcog.2025.112862&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;The removal of rain streaks and raindrops is essential for improving image visibility. However, most existing methods rely on paired rainy and clean images, which are difficult to acquire in real-world scenarios. To this end, we propose prior-oriented and frequency-regularized Schrödinger bridge (PFSB) for rain streaks and raindrops removal with unpaired training. Specifically, we initially formulate unpaired image deraining as a Schrödinger bridge problem. Furthermore, we demonstrate the locally quasi-convexity of structural similarity, and employ the multi-scale structural similarity constraint (MSSC) to minimize the duality gap between the primal and dual problems, ensuring linear convergence of gradient flow while preserving textural details. Meanwhile, we develop a context-preserving consistency modulator (CCM) guide the derained output toward clean content, thereby retaining rain-irrelevant features. Moreover, we propose a domain-representative prompt protocol (DPP), which enforces the generated sample to eliminate rain-relevant information and maintain alignment with the clean domain. Additionally, we utilize Bayesian frequency-domain regularization (BFR) to balance spectral consistency with clean references and repulsion from rainy patterns. Extensive experiments demonstrate that our method surpasses the existing well-performing unpaired learning approaches in both fidelity and photo-realism.&lt;/p&gt;</content:encoded></item><item><title>Rethinking the Use of Vision Transformers for AI-Generated Image Detection</title><link>https://arxiv.org/abs/2512.04969v1</link><guid>http://arxiv.org/abs/2512.04969v1</guid><pubDate>Thu, 04 Dec 2025 16:37:47 +0000</pubDate><dc:creator>NaHyeon Park</dc:creator><dc:creator>Kunhee Kim</dc:creator><dc:creator>Junsuk Choe</dc:creator><dc:creator>Hyunjung Shim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.
Published: 2025-12-04T16:37:47+00:00
Venue: arXiv
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; NaHyeon Park; Kunhee Kim; Junsuk Choe; Hyunjung Shim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.&lt;/p&gt;</content:encoded></item><item><title>PoolNet: Deep Learning for 2D to 3D Video Process Validation</title><link>https://arxiv.org/abs/2512.05362v1</link><guid>http://arxiv.org/abs/2512.05362v1</guid><pubDate>Fri, 05 Dec 2025 02:00:28 +0000</pubDate><dc:creator>Sanchit Kaul</dc:creator><dc:creator>Joseph Luna</dc:creator><dc:creator>Shray Arora</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.
Published: 2025-12-05T02:00:28+00:00
Venue: arXiv
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sanchit Kaul; Joseph Luna; Shray Arora&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.&lt;/p&gt;</content:encoded></item><item><title>OWL: Unsupervised 3D Object Detection by Occupancy Guided Warm-up and Large Model Priors Reasoning</title><link>https://arxiv.org/abs/2512.05698v1</link><guid>http://arxiv.org/abs/2512.05698v1</guid><pubDate>Fri, 05 Dec 2025 13:24:42 +0000</pubDate><dc:creator>Xusheng Guo</dc:creator><dc:creator>Wanfa Zhang</dc:creator><dc:creator>Shijia Zhao</dc:creator><dc:creator>Qiming Xia</dc:creator><dc:creator>Xiaolong Xie</dc:creator><dc:creator>Mingming Wang</dc:creator><dc:creator>Hai Wu</dc:creator><dc:creator>Chenglu Wen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.
Published: 2025-12-05T13:24:42+00:00
Venue: arXiv
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xusheng Guo; Wanfa Zhang; Shijia Zhao; Qiming Xia; Xiaolong Xie; Mingming Wang; Hai Wu; Chenglu Wen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;Unsupervised 3D object detection leverages heuristic algorithms to discover potential objects, offering a promising route to reduce annotation costs in autonomous driving. Existing approaches mainly generate pseudo labels and refine them through self-training iterations. However, these pseudo-labels are often incorrect at the beginning of training, resulting in misleading the optimization process. Moreover, effectively filtering and refining them remains a critical challenge. In this paper, we propose OWL for unsupervised 3D object detection by occupancy guided warm-up and large-model priors reasoning. OWL first employs an Occupancy Guided Warm-up (OGW) strategy to initialize the backbone weight with spatial perception capabilities, mitigating the interference of incorrect pseudo-labels on network convergence. Furthermore, OWL introduces an Instance-Cued Reasoning (ICR) module that leverages the prior knowledge of large models to assess pseudo-label quality, enabling precise filtering and refinement. Finally, we design a Weight-adapted Self-training (WAS) strategy to dynamically re-weight pseudo-labels, improving the performance through self-training. Extensive experiments on Waymo Open Dataset (WOD) and KITTI demonstrate that OWL outperforms state-of-the-art unsupervised methods by over 15.0% mAP, revealing the effectiveness of our method.&lt;/p&gt;</content:encoded></item><item><title>DANIM: Domain Adaptation Network with Intermediate Domain Masking for Night-time Scene Parsing</title><link>https://doi.org/10.1016/j.patcog.2025.112796</link><guid>10.1016/j.patcog.2025.112796</guid><pubDate>Sun, 07 Dec 2025 15:13:38 +0000</pubDate><dc:creator>Qijian Tian</dc:creator><dc:creator>Sen Wang</dc:creator><dc:creator>Ran Yi</dc:creator><dc:creator>Zufeng Zhang</dc:creator><dc:creator>Bin Sheng</dc:creator><dc:creator>Xin Tan</dc:creator><dc:creator>Lizhuang Ma</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112796</prism:doi><description>Night-time scene parsing is important for practical applications such as autonomous driving and robot vision. Since annotating is time-consuming, Unsupervised Domain Adaptation (UDA) is an effective solution for night-time scene parsing. Due to the low illumination, over/under-exposure, and motion blur in night-time scenes, existing methods can not connect daytime scenes and night-time scenes well, limiting their performance. Some methods rely on day-night paired images, which are costly to collect and therefore impractical. In this paper, we propose DANIM, a self-training UDA network for night-time scene parsing. We introduce an intermediate domain that explicitly models the connection between daytime scenes and night-time scenes from lighting and structure. The intermediate domain shares similar structure information with the night-time target domain and similar lighting information with the daytime source domain. By harnessing the rich prior knowledge of a pre-trained text-driven generative model, the intermediate domain can be generated and we propose a scoring mechanism for selecting the high-quality one for training. Besides, we propose intermediate domain masking to address the inconsistency between the intermediate domain and the target domain. We further design a coupled mask strategy to make the mask more effective. Extensive experiments show that DANIM has achieved first place on the DarkZurich leaderboard and outperforms state-of-the-art methods on other widely used night-time scene parsing benchmarks, i.e. ACDC-night, NightCity, and NighttimeDriving.
Published: 2025-12-07T15:13:38+00:00
Venue: Pattern Recognition
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qijian Tian; Sen Wang; Ran Yi; Zufeng Zhang; Bin Sheng; Xin Tan; Lizhuang Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112796"&gt;10.1016/j.patcog.2025.112796&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;Night-time scene parsing is important for practical applications such as autonomous driving and robot vision. Since annotating is time-consuming, Unsupervised Domain Adaptation (UDA) is an effective solution for night-time scene parsing. Due to the low illumination, over/under-exposure, and motion blur in night-time scenes, existing methods can not connect daytime scenes and night-time scenes well, limiting their performance. Some methods rely on day-night paired images, which are costly to collect and therefore impractical. In this paper, we propose DANIM, a self-training UDA network for night-time scene parsing. We introduce an intermediate domain that explicitly models the connection between daytime scenes and night-time scenes from lighting and structure. The intermediate domain shares similar structure information with the night-time target domain and similar lighting information with the daytime source domain. By harnessing the rich prior knowledge of a pre-trained text-driven generative model, the intermediate domain can be generated and we propose a scoring mechanism for selecting the high-quality one for training. Besides, we propose intermediate domain masking to address the inconsistency between the intermediate domain and the target domain. We further design a coupled mask strategy to make the mask more effective. Extensive experiments show that DANIM has achieved first place on the DarkZurich leaderboard and outperforms state-of-the-art methods on other widely used night-time scene parsing benchmarks, i.e. ACDC-night, NightCity, and NighttimeDriving.&lt;/p&gt;</content:encoded></item></channel></rss>