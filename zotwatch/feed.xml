<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 02 Feb 2026 03:34:12 +0000</lastBuildDate><item><title>Multi-object tracking of vehicles and anomalous states in remote sensing videos: Joint learning of historical trajectory guidance and ID prediction</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.038</link><guid>10.1016/j.isprsjprs.2026.01.038</guid><pubDate>Sat, 31 Jan 2026 14:09:30 +0000</pubDate><dc:creator>Bin Wang</dc:creator><dc:creator>Yuan Zhou</dc:creator><dc:creator>Haigang Sui</dc:creator><dc:creator>Guorui Ma</dc:creator><dc:creator>Peng Cheng</dc:creator><dc:creator>Di Wang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.038</prism:doi><description>Research on multi-object tracking (MOT) of vehicles based on remote sensing video data has achieved breakthrough progress. However, MOT of vehicles in complex scenarios and their anomalous states after being subjected to strong deformation interference remains a huge challenge. This is of great significance for military defense, traffic flow management, vehicle damage assessment, etc. To address this problem, this study proposes an end-to-end MOT method that integrates a joint learning paradigm of historical trajectory guidance and identity (ID) prediction, aiming to bridge the gap between vehicle detection and continuous tracking after anomalous states occurrence. The proposed network framework primarily consists of a Frame Feature Aggregation Module (FFAM) that enhances spatial consistency of objects across consecutive video frames, a Historical Tracklets Flow Encoder (HTFE) that employs Mamba blocks to guide object embedding within potential motion flows based on historical frames, and a Semantic-Consistent Clustering Module (SCM) constructed via sparse attention computation to capture global semantic information. The discriminative features extracted by these modules are fused by a Dual-branch Modulation Fusion Unit (DMFU) to maximize the performance of the model. This study also constructs a new dataset for MOT of vehicles and anomalous states in videos, termed the VAS-MOT dataset. Extensive validation experiments conducted on this dataset demonstrate that the method achieves the highest level of performance, with HOTA and MOTA reaching 68.2% and 71.5%, respectively. Additional validation on the open-source dataset IRTS-AG confirms the strong robustness of the proposed method, showing excellent performance in long-term tracking of small vehicles in infrared videos under complex scenarios, where HOTA and MOTA reached 70.9% and 91.6%, respectively. The proposed method provides valuable insights for capturing moving objects and their anomalous states, laying a foundation for further damage assessment.
Published: 2026-01-31T14:09:30+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bin Wang; Yuan Zhou; Haigang Sui; Guorui Ma; Peng Cheng; Di Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.038"&gt;10.1016/j.isprsjprs.2026.01.038&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Research on multi-object tracking (MOT) of vehicles based on remote sensing video data has achieved breakthrough progress. However, MOT of vehicles in complex scenarios and their anomalous states after being subjected to strong deformation interference remains a huge challenge. This is of great significance for military defense, traffic flow management, vehicle damage assessment, etc. To address this problem, this study proposes an end-to-end MOT method that integrates a joint learning paradigm of historical trajectory guidance and identity (ID) prediction, aiming to bridge the gap between vehicle detection and continuous tracking after anomalous states occurrence. The proposed network framework primarily consists of a Frame Feature Aggregation Module (FFAM) that enhances spatial consistency of objects across consecutive video frames, a Historical Tracklets Flow Encoder (HTFE) that employs Mamba blocks to guide object embedding within potential motion flows based on historical frames, and a Semantic-Consistent Clustering Module (SCM) constructed via sparse attention computation to capture global semantic information. The discriminative features extracted by these modules are fused by a Dual-branch Modulation Fusion Unit (DMFU) to maximize the performance of the model. This study also constructs a new dataset for MOT of vehicles and anomalous states in videos, termed the VAS-MOT dataset. Extensive validation experiments conducted on this dataset demonstrate that the method achieves the highest level of performance, with HOTA and MOTA reaching 68.2% and 71.5%, respectively. Additional validation on the open-source dataset IRTS-AG confirms the strong robustness of the proposed method, showing excellent performance in long-term tracking of small vehicles in infrared videos under complex scenarios, where HOTA and MOTA reached 70.9% and 91.6%, respectively. The proposed method provides valuable insights for capturing moving objects and their anomalous states, laying a foundation for further damage assessment.&lt;/p&gt;</content:encoded></item><item><title>Infrared and Visible Image Fusion Based on Multi-modal and Multi-scale Cross-compensation</title><link>https://doi.org/10.1016/j.knosys.2026.115441</link><guid>10.1016/j.knosys.2026.115441</guid><pubDate>Sat, 31 Jan 2026 23:22:34 +0000</pubDate><dc:creator>Meitian Li</dc:creator><dc:creator>Jing Sun</dc:creator><dc:creator>Heng Ma</dc:creator><dc:creator>Fasheng Wang</dc:creator><dc:creator>Fuming Sun</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115441</prism:doi><description>In the task of infrared and visible image fusion, fully preserving the complementary information from different modalities while avoiding detail loss and redundant information superposition has been a core challenge in recent research. Most existing methods primarily focus on feature processing at a single level or for a single modality, leading to insufficient cross-level information interaction and inadequate cross-modal feature fusion. This deficiency typically results in two types of issues: firstly, the lack of effective compensation between adjacent-level features prevents the synergistic utilization of low-level details and high-level semantics; secondly, the differences between features from different modalities are not explicitly modeled, where direct concatenation or weighted summation often introduces redundancy or even artifacts, thereby compromising the overall quality of the fused image. To address these challenges, this paper proposes a novel infrared and visible image fusion network based on a Multi-modal and Multi-scale Cross-compensation referred to as MMCFusion. The proposed network incorporates an Upper-Lower-level Cross-Compensation (ULCC) module that integrates features from adjacent levels to enhance the richness and diversity of feature representations. Additionally, we introduce a Feature-Difference Cross-Compensation (FDCC) module to facilitate cross-compensation of upper-lower-level information through a differential approach. This design enhances the complementarity between features and effectively mitigates the problem of detail information loss prevalent in conventional methods. To further augment the model’s ability to detect and represent objects across various scales, we also devise the Multi-Scale Fusion Module (MSFM) that effectively integrates feature information from multiple scales, thereby improving the model’s adaptability to diverse objects. Furthermore, we design a Texture Enhancement Module (TEM) to capture and retain local structures and texture information in the image, thereby providing richer detail representation after processing. Finally, to comprehensively capture multi-modal information and perform remote modeling, we employ Pyramid Vision Transformer (PVTv2) to construct a dual-stream Transformer encoder, which can capture valuable information at multiple scales and provide robust global modeling capabilities, thereby improving the fusion results. The efficacy of the proposed method is rigorously evaluated on several datasets, including infrared and visible datasets such as MSRS, TNO, and RoadScene, as well as medical imaging datasets, such as PET-MRI. Experimental results demonstrate that MMCFusion significantly outperforms current state-of-the-art methods in terms of both visual quality and quantitative metrics, while also exhibiting strong generalization capability across different datasets, thereby validating its effectiveness and robustness in practical applications. The source code is available at https://github.com/leemt0127/MMCFusion .
Published: 2026-01-31T23:22:34+00:00
Venue: Knowledge-Based Systems
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meitian Li; Jing Sun; Heng Ma; Fasheng Wang; Fuming Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115441"&gt;10.1016/j.knosys.2026.115441&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;In the task of infrared and visible image fusion, fully preserving the complementary information from different modalities while avoiding detail loss and redundant information superposition has been a core challenge in recent research. Most existing methods primarily focus on feature processing at a single level or for a single modality, leading to insufficient cross-level information interaction and inadequate cross-modal feature fusion. This deficiency typically results in two types of issues: firstly, the lack of effective compensation between adjacent-level features prevents the synergistic utilization of low-level details and high-level semantics; secondly, the differences between features from different modalities are not explicitly modeled, where direct concatenation or weighted summation often introduces redundancy or even artifacts, thereby compromising the overall quality of the fused image. To address these challenges, this paper proposes a novel infrared and visible image fusion network based on a Multi-modal and Multi-scale Cross-compensation referred to as MMCFusion. The proposed network incorporates an Upper-Lower-level Cross-Compensation (ULCC) module that integrates features from adjacent levels to enhance the richness and diversity of feature representations. Additionally, we introduce a Feature-Difference Cross-Compensation (FDCC) module to facilitate cross-compensation of upper-lower-level information through a differential approach. This design enhances the complementarity between features and effectively mitigates the problem of detail information loss prevalent in conventional methods. To further augment the model’s ability to detect and represent objects across various scales, we also devise the Multi-Scale Fusion Module (MSFM) that effectively integrates feature information from multiple scales, thereby improving the model’s adaptability to diverse objects. Furthermore, we design a Texture Enhancement Module (TEM) to capture and retain local structures and texture information in the image, thereby providing richer detail representation after processing. Finally, to comprehensively capture multi-modal information and perform remote modeling, we employ Pyramid Vision Transformer (PVTv2) to construct a dual-stream Transformer encoder, which can capture valuable information at multiple scales and provide robust global modeling capabilities, thereby improving the fusion results. The efficacy of the proposed method is rigorously evaluated on several datasets, including infrared and visible datasets such as MSRS, TNO, and RoadScene, as well as medical imaging datasets, such as PET-MRI. Experimental results demonstrate that MMCFusion significantly outperforms current state-of-the-art methods in terms of both visual quality and quantitative metrics, while also exhibiting strong generalization capability across different datasets, thereby validating its effectiveness and robustness in practical applications. The source code is available at https://github.com/leemt0127/MMCFusion .&lt;/p&gt;</content:encoded></item><item><title>Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion</title><link>https://arxiv.org/abs/2601.22045v1</link><guid>http://arxiv.org/abs/2601.22045v1</guid><pubDate>Thu, 29 Jan 2026 17:47:07 +0000</pubDate><dc:creator>Da Li</dc:creator><dc:creator>Chen Yao</dc:creator><dc:creator>Tong Mao</dc:creator><dc:creator>Jiacheng Bao</dc:creator><dc:creator>Houjun Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.
Published: 2026-01-29T17:47:07+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Da Li; Chen Yao; Tong Mao; Jiacheng Bao; Houjun Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.&lt;/p&gt;</content:encoded></item><item><title>Lane Detection for Autonomous Driving: A Comprehensive Review</title><link>https://doi.org/10.1016/j.neucom.2026.132864</link><guid>10.1016/j.neucom.2026.132864</guid><pubDate>Sat, 31 Jan 2026 07:30:02 +0000</pubDate><dc:creator>Hongrui Kou</dc:creator><dc:creator>Ziyu Wang</dc:creator><dc:creator>Zhouhang Lv</dc:creator><dc:creator>Cheng Wang</dc:creator><dc:creator>Zixuan Guo</dc:creator><dc:creator>Yuxin Zhang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132864</prism:doi><description>Lane Detection plays a fundamental and critical role in autonomous driving systems, which can provide accurate road structure information for vehicles and lay a visual foundation for downstream trajectory prediction and planning control. Despite its significance, few papers survey existing lane detection algorithms, leading to unclear research gaps and technical challenges. To this end, this paper reviews lane detection comprehensively, ranging from datasets, loss functions and evaluation metrics to 2D and more advanced 3D lane detection, with the aim of presenting a clear and complete technical chain for developing lane detection algorithms. Specifically, the paper proposes a taxonomy for lane detection and analyzes the technical principles, advantages, and limitations of each category. Benchmark experiments are introduced to reveal the trade-off relationships between complexity and performance. Finally, we identify seven promising research directions that address current limitations in the field, charting a path toward safer, more efficient, and more reliable autonomous driving systems.
Published: 2026-01-31T07:30:02+00:00
Venue: Neurocomputing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongrui Kou; Ziyu Wang; Zhouhang Lv; Cheng Wang; Zixuan Guo; Yuxin Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132864"&gt;10.1016/j.neucom.2026.132864&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Lane Detection plays a fundamental and critical role in autonomous driving systems, which can provide accurate road structure information for vehicles and lay a visual foundation for downstream trajectory prediction and planning control. Despite its significance, few papers survey existing lane detection algorithms, leading to unclear research gaps and technical challenges. To this end, this paper reviews lane detection comprehensively, ranging from datasets, loss functions and evaluation metrics to 2D and more advanced 3D lane detection, with the aim of presenting a clear and complete technical chain for developing lane detection algorithms. Specifically, the paper proposes a taxonomy for lane detection and analyzes the technical principles, advantages, and limitations of each category. Benchmark experiments are introduced to reveal the trade-off relationships between complexity and performance. Finally, we identify seven promising research directions that address current limitations in the field, charting a path toward safer, more efficient, and more reliable autonomous driving systems.&lt;/p&gt;</content:encoded></item><item><title>Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation</title><link>https://arxiv.org/abs/2601.21315v1</link><guid>http://arxiv.org/abs/2601.21315v1</guid><pubDate>Thu, 29 Jan 2026 06:23:14 +0000</pubDate><dc:creator>Seonghwi Kim</dc:creator><dc:creator>Sung Ho Jo</dc:creator><dc:creator>Wooseok Ha</dc:creator><dc:creator>Minwoo Chae</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.
Published: 2026-01-29T06:23:14+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seonghwi Kim; Sung Ho Jo; Wooseok Ha; Minwoo Chae&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.&lt;/p&gt;</content:encoded></item><item><title>Semi-MedSAM: Adapting SAM-assisted Semi-supervised Multi-modality Learning for Medical Endoscopic Image Segmentation</title><link>https://doi.org/10.1016/j.patcog.2026.113206</link><guid>10.1016/j.patcog.2026.113206</guid><pubDate>Sat, 31 Jan 2026 15:44:04 +0000</pubDate><dc:creator>Junhao Li</dc:creator><dc:creator>Yun Li</dc:creator><dc:creator>Junhao Wu</dc:creator><dc:creator>Chaojie Ji</dc:creator><dc:creator>Zhijie Chen</dc:creator><dc:creator>Wenbin Lei</dc:creator><dc:creator>Ruxin Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113206</prism:doi><description>Accurate recognition of lesions in endoscopic images is essential for effective diagnosis and treatment. Multi-modal learning effectively utilizes complementary clues derived from multiple modalities, which can promote performance in lesion area detection. However, accessing the amount of annotated paired images for multi-modal learning is time-consuming and costly. Segment Anything Model (SAM) is a powerful vision foundation model that excels in natural image segmentation, but it encounters performance degradation in endoscopic scenes due to a lack of medical-specific knowledge. Besides, the simple structure of the SAM decoder fails to effectively capture fine-grained details among complex lesion structures and low-contrast tissue organs in endoscopic images. To utilize the powerful feature extraction capability of the foundation model and address the scarcity dilemma in medical image annotation, we present a novel prompt-free SAM-assisted framework, Semi-MedSAM, for semi-supervised multi-modal learning. The proposed Semi-MedSAM integrates an effective SAM-based backbone comprising a designed multi-expert-instructed encoder as well as a hierarchical prototypical decoder into a prompt-free semi-supervised framework. Extensive experiments on three multi-modal endoscopic datasets demonstrate the superior segmentation performance of our Semi-MedSAM.
Published: 2026-01-31T15:44:04+00:00
Venue: Pattern Recognition
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junhao Li; Yun Li; Junhao Wu; Chaojie Ji; Zhijie Chen; Wenbin Lei; Ruxin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113206"&gt;10.1016/j.patcog.2026.113206&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate recognition of lesions in endoscopic images is essential for effective diagnosis and treatment. Multi-modal learning effectively utilizes complementary clues derived from multiple modalities, which can promote performance in lesion area detection. However, accessing the amount of annotated paired images for multi-modal learning is time-consuming and costly. Segment Anything Model (SAM) is a powerful vision foundation model that excels in natural image segmentation, but it encounters performance degradation in endoscopic scenes due to a lack of medical-specific knowledge. Besides, the simple structure of the SAM decoder fails to effectively capture fine-grained details among complex lesion structures and low-contrast tissue organs in endoscopic images. To utilize the powerful feature extraction capability of the foundation model and address the scarcity dilemma in medical image annotation, we present a novel prompt-free SAM-assisted framework, Semi-MedSAM, for semi-supervised multi-modal learning. The proposed Semi-MedSAM integrates an effective SAM-based backbone comprising a designed multi-expert-instructed encoder as well as a hierarchical prototypical decoder into a prompt-free semi-supervised framework. Extensive experiments on three multi-modal endoscopic datasets demonstrate the superior segmentation performance of our Semi-MedSAM.&lt;/p&gt;</content:encoded></item><item><title>OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection</title><link>https://arxiv.org/abs/2601.22685v1</link><guid>http://arxiv.org/abs/2601.22685v1</guid><pubDate>Fri, 30 Jan 2026 07:59:35 +0000</pubDate><dc:creator>Binyi Su</dc:creator><dc:creator>Chenghao Huang</dc:creator><dc:creator>Haiyong Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model's lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.
Published: 2026-01-30T07:59:35+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Binyi Su; Chenghao Huang; Haiyong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model&amp;#x27;s lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.&lt;/p&gt;</content:encoded></item><item><title>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</title><link>https://arxiv.org/abs/2601.22054v1</link><guid>http://arxiv.org/abs/2601.22054v1</guid><pubDate>Thu, 29 Jan 2026 17:52:41 +0000</pubDate><dc:creator>Baorui Ma</dc:creator><dc:creator>Jiahui Yang</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Xuancheng Zhang</dc:creator><dc:creator>Jianxun Cui</dc:creator><dc:creator>Hao Li</dc:creator><dc:creator>Yan Xie</dc:creator><dc:creator>Wei Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.
Published: 2026-01-29T17:52:41+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baorui Ma; Jiahui Yang; Donglin Di; Xuancheng Zhang; Jianxun Cui; Hao Li; Yan Xie; Wei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.&lt;/p&gt;</content:encoded></item><item><title>Soft Quantization: Model Compression Via Weight Coupling</title><link>https://arxiv.org/abs/2601.21219v1</link><guid>http://arxiv.org/abs/2601.21219v1</guid><pubDate>Thu, 29 Jan 2026 03:34:06 +0000</pubDate><dc:creator>Daniel T. Bernstein</dc:creator><dc:creator>Luca Di Carlo</dc:creator><dc:creator>David Schwab</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model's weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our "soft quantization'' scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.
Published: 2026-01-29T03:34:06+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daniel T. Bernstein; Luca Di Carlo; David Schwab&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model&amp;#x27;s weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our &amp;quot;soft quantization&amp;#x27;&amp;#x27; scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.&lt;/p&gt;</content:encoded></item><item><title>Improved prediction of winter wheat yield at regional scale with limited ground samples by unmanned aerial vehicle and satellite synergy</title><link>https://doi.org/10.1016/j.rse.2026.115271</link><guid>10.1016/j.rse.2026.115271</guid><pubDate>Sat, 31 Jan 2026 10:59:02 +0000</pubDate><dc:creator>Yuan Xiong</dc:creator><dc:creator>Gaoxiang Yang</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Weiguo Yu</dc:creator><dc:creator>Yapeng Wu</dc:creator><dc:creator>Jun Lu</dc:creator><dc:creator>Chongya Jiang</dc:creator><dc:creator>Xia Yao</dc:creator><dc:creator>Yan Zhu</dc:creator><dc:creator>Weixing Cao</dc:creator><dc:creator>Tao Cheng</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115271</prism:doi><description>Rapid, accurate, and large-scale in-season prediction of winter wheat yield is essential for enhancing food security and guiding agricultural policies. Traditional data-driven methods with satellite imagery face challenges in large-scale prediction of winter wheat yield because of the limited ground sampling data available for model training. Although unmanned aerial vehicle (UAV) images have been integrated with satellite imagery for generating reference data in monitoring vegetation dynamics, the UAV and satellite synergy has not yet been investigated for cross-scale sample augmentation and information fusion in large-scale prediction of winter wheat yield. To address these issues, this study proposed a novel framework integrating ground, UAV, and satellite data with data-driven algorithms to improve regional-scale yield prediction without the need of adding field measured yield samples. The potential contributions of UAV data to yield sample augmentation were examined for compensating the lack of ground samples and improving regional-scale wheat yield prediction. Subsequently, an optimal yield prediction strategy was developed through augmented sample quality and spatial variability analysis with cross-scale information fusion. The proposed framework was evaluated with extensive field-level yield measurements over three consecutive seasons of winter wheat across Jiangsu Province, China.
Published: 2026-01-31T10:59:02+00:00
Venue: Remote Sensing of Environment
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuan Xiong; Gaoxiang Yang; Lei Zhang; Weiguo Yu; Yapeng Wu; Jun Lu; Chongya Jiang; Xia Yao; Yan Zhu; Weixing Cao; Tao Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115271"&gt;10.1016/j.rse.2026.115271&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Rapid, accurate, and large-scale in-season prediction of winter wheat yield is essential for enhancing food security and guiding agricultural policies. Traditional data-driven methods with satellite imagery face challenges in large-scale prediction of winter wheat yield because of the limited ground sampling data available for model training. Although unmanned aerial vehicle (UAV) images have been integrated with satellite imagery for generating reference data in monitoring vegetation dynamics, the UAV and satellite synergy has not yet been investigated for cross-scale sample augmentation and information fusion in large-scale prediction of winter wheat yield. To address these issues, this study proposed a novel framework integrating ground, UAV, and satellite data with data-driven algorithms to improve regional-scale yield prediction without the need of adding field measured yield samples. The potential contributions of UAV data to yield sample augmentation were examined for compensating the lack of ground samples and improving regional-scale wheat yield prediction. Subsequently, an optimal yield prediction strategy was developed through augmented sample quality and spatial variability analysis with cross-scale information fusion. The proposed framework was evaluated with extensive field-level yield measurements over three consecutive seasons of winter wheat across Jiangsu Province, China.&lt;/p&gt;</content:encoded></item><item><title>Diff-GDAformer: A Diffusion-Guided Dynamic Attention Transformer for Image Inpainting</title><link>https://doi.org/10.1016/j.knosys.2026.115443</link><guid>10.1016/j.knosys.2026.115443</guid><pubDate>Sat, 31 Jan 2026 07:30:05 +0000</pubDate><dc:creator>Hao Wu</dc:creator><dc:creator>Shuzhen Xu</dc:creator><dc:creator>Cuicui Lv</dc:creator><dc:creator>Yuanwei Bi</dc:creator><dc:creator>Zhizhong Liu</dc:creator><dc:creator>Shuo Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115443</prism:doi><description>Diffusion model (DM) has shown great promise in image inpainting by modeling complex data distributions and generating high-quality reconstructions. However, current diffusion-based methods often face challenges such as excessive iterative steps and limited adaptability to both local and global features, resulting in high computational costs and suboptimal restoration quality. To address these issues, we propose Diff-GDAformer, a novel image inpainting framework that combines diffusion-based prior feature generation with guided dynamic attention Transformer (GDAformer) for robust and efficient restoration. In our approach, the DM iteratively refines Gaussian noise in a compressed latent space to generate high-quality prior features, which guide the restoration process. These prior features are injected into GDAformer, which innovatively adopts a dynamic recursive local attention (DRLA) module. DRLA makes use of two complementary attention mechanisms: guided local self-attention (GL-SA) and guided recursive-generalized self-attention (GRG-SA). GL-SA enhances the model’s ability to capture fine-grained local details, while GRG-SA focuses on aggregating global contextual information efficiently. To bridge the gap between local and global features, we introduce the hybrid feature integration (HFI) module, which effectively fuses features from different attention layers, enabling a more comprehensive understanding of image contexts. The two-stage training strategy combines GDAformer with DM optimization, ensuring that the extracted prior features are accurate and seamlessly integrated into the restoration pipeline. Extensive experiments demonstrate that Diff-GDAformer achieves state-of-the-art performance on standard benchmarks, delivering superior visual quality and computational efficiency compared to existing methods.
Published: 2026-01-31T07:30:05+00:00
Venue: Knowledge-Based Systems
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wu; Shuzhen Xu; Cuicui Lv; Yuanwei Bi; Zhizhong Liu; Shuo Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115443"&gt;10.1016/j.knosys.2026.115443&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion model (DM) has shown great promise in image inpainting by modeling complex data distributions and generating high-quality reconstructions. However, current diffusion-based methods often face challenges such as excessive iterative steps and limited adaptability to both local and global features, resulting in high computational costs and suboptimal restoration quality. To address these issues, we propose Diff-GDAformer, a novel image inpainting framework that combines diffusion-based prior feature generation with guided dynamic attention Transformer (GDAformer) for robust and efficient restoration. In our approach, the DM iteratively refines Gaussian noise in a compressed latent space to generate high-quality prior features, which guide the restoration process. These prior features are injected into GDAformer, which innovatively adopts a dynamic recursive local attention (DRLA) module. DRLA makes use of two complementary attention mechanisms: guided local self-attention (GL-SA) and guided recursive-generalized self-attention (GRG-SA). GL-SA enhances the model’s ability to capture fine-grained local details, while GRG-SA focuses on aggregating global contextual information efficiently. To bridge the gap between local and global features, we introduce the hybrid feature integration (HFI) module, which effectively fuses features from different attention layers, enabling a more comprehensive understanding of image contexts. The two-stage training strategy combines GDAformer with DM optimization, ensuring that the extracted prior features are accurate and seamlessly integrated into the restoration pipeline. Extensive experiments demonstrate that Diff-GDAformer achieves state-of-the-art performance on standard benchmarks, delivering superior visual quality and computational efficiency compared to existing methods.&lt;/p&gt;</content:encoded></item><item><title>BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation</title><link>https://arxiv.org/abs/2601.22061v1</link><guid>http://arxiv.org/abs/2601.22061v1</guid><pubDate>Thu, 29 Jan 2026 17:58:55 +0000</pubDate><dc:creator>Li Zhang</dc:creator><dc:creator>Pengtao Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.
Published: 2026-01-29T17:58:55+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Zhang; Pengtao Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.&lt;/p&gt;</content:encoded></item><item><title>Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models</title><link>https://arxiv.org/abs/2601.23253v1</link><guid>http://arxiv.org/abs/2601.23253v1</guid><pubDate>Fri, 30 Jan 2026 18:21:45 +0000</pubDate><dc:creator>Yi Zhang</dc:creator><dc:creator>Chun-Wun Cheng</dc:creator><dc:creator>Angelica I. Aviles-Rivero</dc:creator><dc:creator>Zhihai He</dc:creator><dc:creator>Liang-Jie Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.
Published: 2026-01-30T18:21:45+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Zhang; Chun-Wun Cheng; Angelica I. Aviles-Rivero; Zhihai He; Liang-Jie Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.&lt;/p&gt;</content:encoded></item><item><title>A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions</title><link>https://arxiv.org/abs/2601.22830v1</link><guid>http://arxiv.org/abs/2601.22830v1</guid><pubDate>Fri, 30 Jan 2026 10:58:24 +0000</pubDate><dc:creator>Ji Zhou</dc:creator><dc:creator>Yilin Ding</dc:creator><dc:creator>Yongqi Zhao</dc:creator><dc:creator>Jiachen Xu</dc:creator><dc:creator>Arno Eichberger</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.
Published: 2026-01-30T10:58:24+00:00
Venue: arXiv
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ji Zhou; Yilin Ding; Yongqi Zhao; Jiachen Xu; Arno Eichberger&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.&lt;/p&gt;</content:encoded></item><item><title>Hypersolid: Emergent Vision Representations via Short-Range Repulsion</title><link>https://arxiv.org/abs/2601.21255v1</link><guid>http://arxiv.org/abs/2601.21255v1</guid><pubDate>Thu, 29 Jan 2026 04:25:43 +0000</pubDate><dc:creator>Esteban Rodríguez-Betancourt</dc:creator><dc:creator>Edgar Casasola-Murillo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.
Published: 2026-01-29T04:25:43+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Esteban Rodríguez-Betancourt; Edgar Casasola-Murillo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.&lt;/p&gt;</content:encoded></item><item><title>FlexMap: Generalized HD Map Construction from Flexible Camera Configurations</title><link>https://arxiv.org/abs/2601.22376v1</link><guid>http://arxiv.org/abs/2601.22376v1</guid><pubDate>Thu, 29 Jan 2026 22:41:11 +0000</pubDate><dc:creator>Run Wang</dc:creator><dc:creator>Chaoyi Zhou</dc:creator><dc:creator>Amir Salarpour</dc:creator><dc:creator>Xi Liu</dc:creator><dc:creator>Zhi-Qi Cheng</dc:creator><dc:creator>Feng Luo</dc:creator><dc:creator>Mert D. Pesé</dc:creator><dc:creator>Siyu Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.
Published: 2026-01-29T22:41:11+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Run Wang; Chaoyi Zhou; Amir Salarpour; Xi Liu; Zhi-Qi Cheng; Feng Luo; Mert D. Pesé; Siyu Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.&lt;/p&gt;</content:encoded></item><item><title>L$^3$: Large Lookup Layers</title><link>https://arxiv.org/abs/2601.21461v2</link><guid>http://arxiv.org/abs/2601.21461v2</guid><pubDate>Thu, 29 Jan 2026 09:37:31 +0000</pubDate><dc:creator>Albert Tseng</dc:creator><dc:creator>Christopher De Sa</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP "experts." However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.
Published: 2026-01-29T09:37:31+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Albert Tseng; Christopher De Sa&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Modern sparse language models typically achieve sparsity through Mixture-of-Experts (MoE) layers, which dynamically route tokens to dense MLP &amp;quot;experts.&amp;quot; However, dynamic hard routing has a number of drawbacks, such as potentially poor hardware efficiency and needing auxiliary losses for stable training. In contrast, the tokenizer embedding table, which is natively sparse, largely avoids these issues by selecting a single embedding per token at the cost of not having contextual information. In this work, we introduce the Large Lookup Layer (L$^3$), which unlocks a new axis of sparsity by generalizing embedding tables to model decoder layers. L$^3$ layers use static token-based routing to aggregate a set of learned embeddings per token in a context-dependent way, allowing the model to efficiently balance memory and compute by caching information in embeddings. L$^3$ has two main components: (1) a systems-friendly architecture that allows for fast training and CPU-offloaded inference with no overhead, and (2) an information-theoretic embedding allocation algorithm that effectively balances speed and quality. We empirically test L$^3$ by training transformers with up to 2.6B active parameters and find that L$^3$ strongly outperforms both dense models and iso-sparse MoEs in both language modeling and downstream tasks.&lt;/p&gt;</content:encoded></item><item><title>OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</title><link>https://arxiv.org/abs/2601.21639v1</link><guid>http://arxiv.org/abs/2601.21639v1</guid><pubDate>Thu, 29 Jan 2026 12:43:02 +0000</pubDate><dc:creator>Yufeng Zhong</dc:creator><dc:creator>Lei Chen</dc:creator><dc:creator>Xuanle Zhao</dc:creator><dc:creator>Wenkang Han</dc:creator><dc:creator>Liming Zheng</dc:creator><dc:creator>Jing Huang</dc:creator><dc:creator>Deyang Jiang</dc:creator><dc:creator>Yilin Cao</dc:creator><dc:creator>Lin Ma</dc:creator><dc:creator>Zhixiong Zeng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (\textbf{Text-centric OCR}), neglecting the identification of visual elements from visually information-dense image sources (\textbf{Vision-centric OCR}), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose \textbf{OCRVerse}, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.
Published: 2026-01-29T12:43:02+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yufeng Zhong; Lei Chen; Xuanle Zhao; Wenkang Han; Liming Zheng; Jing Huang; Deyang Jiang; Yilin Cao; Lin Ma; Zhixiong Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (\textbf{Text-centric OCR}), neglecting the identification of visual elements from visually information-dense image sources (\textbf{Vision-centric OCR}), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose \textbf{OCRVerse}, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.&lt;/p&gt;</content:encoded></item><item><title>One-step Latent-free Image Generation with Pixel Mean Flows</title><link>https://arxiv.org/abs/2601.22158v1</link><guid>http://arxiv.org/abs/2601.22158v1</guid><pubDate>Thu, 29 Jan 2026 18:59:56 +0000</pubDate><dc:creator>Yiyang Lu</dc:creator><dc:creator>Susie Lu</dc:creator><dc:creator>Qiao Sun</dc:creator><dc:creator>Hanhong Zhao</dc:creator><dc:creator>Zhicheng Jiang</dc:creator><dc:creator>Xianbang Wang</dc:creator><dc:creator>Tianhong Li</dc:creator><dc:creator>Zhengyang Geng</dc:creator><dc:creator>Kaiming He</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose "pixel MeanFlow" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.
Published: 2026-01-29T18:59:56+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiyang Lu; Susie Lu; Qiao Sun; Hanhong Zhao; Zhicheng Jiang; Xianbang Wang; Tianhong Li; Zhengyang Geng; Kaiming He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose &amp;quot;pixel MeanFlow&amp;quot; (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.&lt;/p&gt;</content:encoded></item><item><title>Mitigating Overthinking in Large Reasoning Models via Difficulty-aware Reinforcement Learning</title><link>https://arxiv.org/abs/2601.21418v1</link><guid>http://arxiv.org/abs/2601.21418v1</guid><pubDate>Thu, 29 Jan 2026 08:56:45 +0000</pubDate><dc:creator>Qian Wan</dc:creator><dc:creator>Ziao Xu</dc:creator><dc:creator>Luona Wei</dc:creator><dc:creator>Xiaoxuan Shen</dc:creator><dc:creator>Jianwen Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.
Published: 2026-01-29T08:56:45+00:00
Venue: arXiv
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Wan; Ziao Xu; Luona Wei; Xiaoxuan Shen; Jianwen Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) achieve explicit chain-of-thought expansion by imitating deep thinking behaviors of humans, demonstrating excellent performance in complex task scenarios. However, the deep-thinking mode often leads to unnecessarily lengthy reasoning and resource inefficiency when handling simple tasks. This overthinking phenomenon may arise from the generation preference triggered by the reward function during post-training. Existing research attempts to mitigate overthinking from the perspective of prompt design or model training, but generally underestimates the importance of task difficulty awareness, which makes it difficult for LRMs to effectively allocate reasoning resources. In this paper, we propose Difficulty-aware Policy Optimization (DiPO), a reinforcement learning-based LRM training framework. DiPO encourages LRM to spontaneously model task complexity, and integrates them into reinforcement learning framework to adjust the generation preferences introduced by post-training. A difficulty modeling method based on model self-reasoning is proposed, which significantly reduces the dependence on manual annotation and formalize task complexity. We further develop a difficulty-signal-enhanced reward function that incorporates a penalty for lengthy reasoning while considering reasoning performance and output format. Experimental results indicate that DiPO enables the model to spontaneously adjust inference overhead, significantly reducing redundant tokens without losing performance due to thought compression.&lt;/p&gt;</content:encoded></item><item><title>UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating</title><link>https://arxiv.org/abs/2601.22616v1</link><guid>http://arxiv.org/abs/2601.22616v1</guid><pubDate>Fri, 30 Jan 2026 06:15:50 +0000</pubDate><dc:creator>Xing Yi</dc:creator><dc:creator>Jinyang Huang</dc:creator><dc:creator>Feng-Qi Cui</dc:creator><dc:creator>Anyang Tong</dc:creator><dc:creator>Ruimin Wang</dc:creator><dc:creator>Liu Liu</dc:creator><dc:creator>Dan Guo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.
Published: 2026-01-30T06:15:50+00:00
Venue: arXiv
Score: 0.762 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xing Yi; Jinyang Huang; Feng-Qi Cui; Anyang Tong; Ruimin Wang; Liu Liu; Dan Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (must_read)&lt;/p&gt;
&lt;p&gt;The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.&lt;/p&gt;</content:encoded></item><item><title>How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models</title><link>https://arxiv.org/abs/2601.22841v1</link><guid>http://arxiv.org/abs/2601.22841v1</guid><pubDate>Fri, 30 Jan 2026 11:08:48 +0000</pubDate><dc:creator>Leonard Hackel</dc:creator><dc:creator>Tom Burgert</dc:creator><dc:creator>Begüm Demir</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.
Published: 2026-01-30T11:08:48+00:00
Venue: arXiv
Score: 0.762 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Leonard Hackel; Tom Burgert; Begüm Demir&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.&lt;/p&gt;</content:encoded></item><item><title>Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning</title><link>https://arxiv.org/abs/2601.22231v1</link><guid>http://arxiv.org/abs/2601.22231v1</guid><pubDate>Thu, 29 Jan 2026 19:04:30 +0000</pubDate><dc:creator>Jian Shi</dc:creator><dc:creator>Michael Birsak</dc:creator><dc:creator>Wenqing Cui</dc:creator><dc:creator>Zhenyu Li</dc:creator><dc:creator>Peter Wonka</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes
Published: 2026-01-29T19:04:30+00:00
Venue: arXiv
Score: 0.762 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Shi; Michael Birsak; Wenqing Cui; Zhenyu Li; Peter Wonka&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (must_read)&lt;/p&gt;
&lt;p&gt;This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes&lt;/p&gt;</content:encoded></item><item><title>Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model</title><link>https://arxiv.org/abs/2601.22581v1</link><guid>http://arxiv.org/abs/2601.22581v1</guid><pubDate>Fri, 30 Jan 2026 05:24:51 +0000</pubDate><dc:creator>Naeem Paeedeh</dc:creator><dc:creator>Mahardhika Pratama</dc:creator><dc:creator>Ary Shiddiqi</dc:creator><dc:creator>Zehong Cao</dc:creator><dc:creator>Mukesh Prasad</dc:creator><dc:creator>Wisnu Jatmiko</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.
Published: 2026-01-30T05:24:51+00:00
Venue: arXiv
Score: 0.762 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Naeem Paeedeh; Mahardhika Pratama; Ary Shiddiqi; Zehong Cao; Mukesh Prasad; Wisnu Jatmiko&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (must_read)&lt;/p&gt;
&lt;p&gt;Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.&lt;/p&gt;</content:encoded></item><item><title>Non-negative transfer space learning based on label release and graph embedding for small sample face recognition</title><link>https://doi.org/10.1016/j.ins.2026.123169</link><guid>10.1016/j.ins.2026.123169</guid><pubDate>Sat, 31 Jan 2026 23:20:13 +0000</pubDate><dc:creator>Mengmeng Liao</dc:creator><dc:creator>Jiahao Qin</dc:creator><dc:creator>Yuwei Du</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2026.123169</prism:doi><description>This paper proposes NTLG, a novel method for small-sample facial recognition, addressing two key limitations of traditional approaches: sensitivity to data bias and ineffective use of label information. NTLG introduces three innovations: (1) decomposing complex parameter optimization into simpler subtasks, (2) enhancing inter-class discrimination via label propagation, and (3) improving robustness through feature extraction and data reconstruction. Experiments demonstrate that NTLG significantly boosts accuracy while maintaining efficiency, outperforming state-of-the-art methods in small-sample scenarios.
Published: 2026-01-31T23:20:13+00:00
Venue: Information Sciences
Score: 0.759 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengmeng Liao; Jiahao Qin; Yuwei Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2026.123169"&gt;10.1016/j.ins.2026.123169&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (must_read)&lt;/p&gt;
&lt;p&gt;This paper proposes NTLG, a novel method for small-sample facial recognition, addressing two key limitations of traditional approaches: sensitivity to data bias and ineffective use of label information. NTLG introduces three innovations: (1) decomposing complex parameter optimization into simpler subtasks, (2) enhancing inter-class discrimination via label propagation, and (3) improving robustness through feature extraction and data reconstruction. Experiments demonstrate that NTLG significantly boosts accuracy while maintaining efficiency, outperforming state-of-the-art methods in small-sample scenarios.&lt;/p&gt;</content:encoded></item><item><title>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</title><link>https://arxiv.org/abs/2601.22060v1</link><guid>http://arxiv.org/abs/2601.22060v1</guid><pubDate>Thu, 29 Jan 2026 17:58:40 +0000</pubDate><dc:creator>Wenxuan Huang</dc:creator><dc:creator>Yu Zeng</dc:creator><dc:creator>Qiuchen Wang</dc:creator><dc:creator>Zhen Fang</dc:creator><dc:creator>Shaosheng Cao</dc:creator><dc:creator>Zheng Chu</dc:creator><dc:creator>Qingyu Yin</dc:creator><dc:creator>Shuang Chen</dc:creator><dc:creator>Zhenfei Yin</dc:creator><dc:creator>Lin Chen</dc:creator><dc:creator>Zehui Chen</dc:creator><dc:creator>Yao Hu</dc:creator><dc:creator>Philip Torr</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Wanli Ouyang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call'' for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.
Published: 2026-01-29T17:58:40+00:00
Venue: arXiv
Score: 0.759 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenxuan Huang; Yu Zeng; Qiuchen Wang; Zhen Fang; Shaosheng Cao; Zheng Chu; Qingyu Yin; Shuang Chen; Zhenfei Yin; Lin Chen; Zehui Chen; Yao Hu; Philip Torr; Feng Zhao; Wanli Ouyang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call&amp;#x27;&amp;#x27; for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches typically define multimodal search in a naive setting, assuming that a single full-level or entity-level image query and few text query suffices to retrieve the key evidence needed to answer the question, which is unrealistic in real-world scenarios with substantial visual noise. Moreover, they are often limited in the reasoning depth and search breadth, making it difficult to solve complex questions that require aggregating evidence from diverse visual and textual sources. Building on this, we propose Vision-DeepResearch, which proposes one new multimodal deep-research paradigm, i.e., performs multi-turn, multi-entity and multi-scale visual and textual search to robustly hit real-world search engines under heavy noise. Our Vision-DeepResearch supports dozens of reasoning steps and hundreds of engine interactions, while internalizing deep-research capabilities into the MLLM via cold-start supervision and RL training, resulting in a strong end-to-end multimodal deep-research MLLM. It substantially outperforming existing multimodal deep-research MLLMs, and workflows built on strong closed-source foundation model such as GPT-5, Gemini-2.5-pro and Claude-4-Sonnet. The code will be released in https://github.com/Osilly/Vision-DeepResearch.&lt;/p&gt;</content:encoded></item><item><title>Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector</title><link>https://arxiv.org/abs/2601.22468v1</link><guid>http://arxiv.org/abs/2601.22468v1</guid><pubDate>Fri, 30 Jan 2026 02:29:54 +0000</pubDate><dc:creator>Wenqiang Zu</dc:creator><dc:creator>Shenghao Xie</dc:creator><dc:creator>Bo Lei</dc:creator><dc:creator>Lei Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.
Published: 2026-01-30T02:29:54+00:00
Venue: arXiv
Score: 0.758 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenqiang Zu; Shenghao Xie; Bo Lei; Lei Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (must_read)&lt;/p&gt;
&lt;p&gt;Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.&lt;/p&gt;</content:encoded></item><item><title>Synthetic-to-Real Domain Bridging for Single-View 3D Reconstruction of Ships for Maritime Monitoring</title><link>https://arxiv.org/abs/2601.21786v1</link><guid>http://arxiv.org/abs/2601.21786v1</guid><pubDate>Thu, 29 Jan 2026 14:34:01 +0000</pubDate><dc:creator>Borja Carrillo-Perez</dc:creator><dc:creator>Felix Sattler</dc:creator><dc:creator>Angel Bueno Rodriguez</dc:creator><dc:creator>Maurice Stephan</dc:creator><dc:creator>Sarah Barnes</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1117/12.3063784</prism:doi><description>Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.
Published: 2026-01-29T14:34:01+00:00
Venue: arXiv
Score: 0.758 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Borja Carrillo-Perez; Felix Sattler; Angel Bueno Rodriguez; Maurice Stephan; Sarah Barnes&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1117/12.3063784"&gt;10.1117/12.3063784&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (must_read)&lt;/p&gt;
&lt;p&gt;Three-dimensional (3D) reconstruction of ships is an important part of maritime monitoring, allowing improved visualization, inspection, and decision-making in real-world monitoring environments. However, most state-ofthe-art 3D reconstruction methods require multi-view supervision, annotated 3D ground truth, or are computationally intensive, making them impractical for real-time maritime deployment. In this work, we present an efficient pipeline for single-view 3D reconstruction of real ships by training entirely on synthetic data and requiring only a single view at inference. Our approach uses the Splatter Image network, which represents objects as sparse sets of 3D Gaussians for rapid and accurate reconstruction from single images. The model is first fine-tuned on synthetic ShapeNet vessels and further refined with a diverse custom dataset of 3D ships, bridging the domain gap between synthetic and real-world imagery. We integrate a state-of-the-art segmentation module based on YOLOv8 and custom preprocessing to ensure compatibility with the reconstruction network. Postprocessing steps include real-world scaling, centering, and orientation alignment, followed by georeferenced placement on an interactive web map using AIS metadata and homography-based mapping. Quantitative evaluation on synthetic validation data demonstrates strong reconstruction fidelity, while qualitative results on real maritime images from the ShipSG dataset confirm the potential for transfer to operational maritime settings. The final system provides interactive 3D inspection of real ships without requiring real-world 3D annotations. This pipeline provides an efficient, scalable solution for maritime monitoring and highlights a path toward real-time 3D ship visualization in practical applications. Interactive demo: https://dlr-mi.github.io/ship3d-demo/.&lt;/p&gt;</content:encoded></item><item><title>PSR: Proactive Soft-Orthogonal Regulation for Long-Tailed Class-Incremental Learning</title><link>https://doi.org/10.1016/j.patcog.2026.113207</link><guid>10.1016/j.patcog.2026.113207</guid><pubDate>Sat, 31 Jan 2026 07:26:02 +0000</pubDate><dc:creator>Zhihan Fu</dc:creator><dc:creator>Zhiqi Zhang</dc:creator><dc:creator>Shipeng Liao</dc:creator><dc:creator>Zhengyu Huang</dc:creator><dc:creator>Zerun Chen</dc:creator><dc:creator>Tianyu Shen</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113207</prism:doi><description>Long-tailed class-incremental learning (LT-CIL) faces the challenge of imbalanced data streams that can weaken tail-class representations due to the double impact of inherent bias toward head classes and catastrophic forgetting. Existing methods typically employ passive adjustments to the feature space to alleviate conflicts, while tail classes often suffer from inadequate learning capacity, particularly under conditions of extreme imbalance. To address this limitation, this paper proposes a proactive soft-orthogonal regulation strategy, which reserves embedding space for future classes during the base phase and guides new classes to occupy these spaces while maintaining clear inter-class boundaries in the incremental phases. In contrast to hard-orthogonal or rigid constraints, our proposed soft-orthogonal strategy preserves semantic continuity of the feature space while enforcing necessary separation between old and new classes, thereby facilitating the natural embedding of tail classes. The proposed method demonstrates state-of-the-art performance across multiple benchmarks, exhibiting notable robustness, strong generalization capabilities, and scalability to varying task complexities.
Published: 2026-01-31T07:26:02+00:00
Venue: Pattern Recognition
Score: 0.758 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhihan Fu; Zhiqi Zhang; Shipeng Liao; Zhengyu Huang; Zerun Chen; Tianyu Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113207"&gt;10.1016/j.patcog.2026.113207&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (must_read)&lt;/p&gt;
&lt;p&gt;Long-tailed class-incremental learning (LT-CIL) faces the challenge of imbalanced data streams that can weaken tail-class representations due to the double impact of inherent bias toward head classes and catastrophic forgetting. Existing methods typically employ passive adjustments to the feature space to alleviate conflicts, while tail classes often suffer from inadequate learning capacity, particularly under conditions of extreme imbalance. To address this limitation, this paper proposes a proactive soft-orthogonal regulation strategy, which reserves embedding space for future classes during the base phase and guides new classes to occupy these spaces while maintaining clear inter-class boundaries in the incremental phases. In contrast to hard-orthogonal or rigid constraints, our proposed soft-orthogonal strategy preserves semantic continuity of the feature space while enforcing necessary separation between old and new classes, thereby facilitating the natural embedding of tail classes. The proposed method demonstrates state-of-the-art performance across multiple benchmarks, exhibiting notable robustness, strong generalization capabilities, and scalability to varying task complexities.&lt;/p&gt;</content:encoded></item><item><title>WDCGAN-GSMR: A more accurate framework for small-sample radar signal modulation recognition</title><link>https://doi.org/10.1016/j.dsp.2026.105971</link><guid>10.1016/j.dsp.2026.105971</guid><pubDate>Sat, 31 Jan 2026 00:27:15 +0000</pubDate><dc:creator>Qinghui Zhang</dc:creator><dc:creator>Wenzheng Li</dc:creator><dc:creator>Chenxia Wan</dc:creator><prism:publicationName>Digital Signal Processing</prism:publicationName><prism:doi>10.1016/j.dsp.2026.105971</prism:doi><description>Low Probability of Interception (LPI) radars feature strong anti-detection capabilities, rendering the acquisition of real signal samples extremely challenging. This severely restricts the performance of LPI radar signal modulation recognition under small-sample conditions. To address this issue, this paper proposes a novel Wasserstein Deep Convolutional Generative Adversarial Network integrated with Generative Spatial-Channel Synergistic Attention and Multi-Scale Asymmetric Convolutional Residual (WDCGAN-GSMR), to enhance recognition accuracy under small-sample conditions. The radar signals are first transformed into Time-Frequency Images (TFIs) using the Smoothed Pseudo Wigner–Ville Distribution (SPWVD). These limited TFIs are then augmented using WDCGAN-GSMR by combining real-world and simulated samples, and are finally fed into a convolutional neural network for model training and modulation recognition. Experimental results demonstrate that incorporating the MCR block into WDCGAN-GSMR model significantly reduces the computational complexity. When only 50 samples per class are available, combining the proposed WDCGAN-GSMR with MobileNetV1 improves recognition accuracy by 6.2%. When integrated with the ResNet18 model, the recognition accuracy of the WDCGAN-GSMR model achieves a 6.4% higher than the conventional DCGAN model. This proposed model effectively mitigates the issue of data scarcity and significantly enhances LPI radar signal modulation recognition under small-sample conditions, providing a novel and effective solution for enhancing radar signal modulation recognition.
Published: 2026-01-31T00:27:15+00:00
Venue: Digital Signal Processing
Score: 0.758 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qinghui Zhang; Wenzheng Li; Chenxia Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Digital Signal Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.dsp.2026.105971"&gt;10.1016/j.dsp.2026.105971&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (must_read)&lt;/p&gt;
&lt;p&gt;Low Probability of Interception (LPI) radars feature strong anti-detection capabilities, rendering the acquisition of real signal samples extremely challenging. This severely restricts the performance of LPI radar signal modulation recognition under small-sample conditions. To address this issue, this paper proposes a novel Wasserstein Deep Convolutional Generative Adversarial Network integrated with Generative Spatial-Channel Synergistic Attention and Multi-Scale Asymmetric Convolutional Residual (WDCGAN-GSMR), to enhance recognition accuracy under small-sample conditions. The radar signals are first transformed into Time-Frequency Images (TFIs) using the Smoothed Pseudo Wigner–Ville Distribution (SPWVD). These limited TFIs are then augmented using WDCGAN-GSMR by combining real-world and simulated samples, and are finally fed into a convolutional neural network for model training and modulation recognition. Experimental results demonstrate that incorporating the MCR block into WDCGAN-GSMR model significantly reduces the computational complexity. When only 50 samples per class are available, combining the proposed WDCGAN-GSMR with MobileNetV1 improves recognition accuracy by 6.2%. When integrated with the ResNet18 model, the recognition accuracy of the WDCGAN-GSMR model achieves a 6.4% higher than the conventional DCGAN model. This proposed model effectively mitigates the issue of data scarcity and significantly enhances LPI radar signal modulation recognition under small-sample conditions, providing a novel and effective solution for enhancing radar signal modulation recognition.&lt;/p&gt;</content:encoded></item></channel></rss>