<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 14 Feb 2026 03:27:14 +0000</lastBuildDate><item><title>SADW-Det: A Lightweight SAR Ship Detection Algorithm with Direction-Weighted Attention and Factorized-Parallel Structure Design</title><link>https://doi.org/10.3390/rs18040582</link><guid>10.3390/rs18040582</guid><pubDate>Fri, 13 Feb 2026 10:52:21 +0000</pubDate><dc:creator>Mengshan Gui</dc:creator><dc:creator>Hairui Zhu</dc:creator><dc:creator>Weixing Sheng</dc:creator><dc:creator>Renli Zhang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18040582</prism:doi><description>Synthetic Aperture Radar (SAR) is a powerful observation system capable of delivering high-resolution imagery under variable sea conditions to support target detection and tracking, such as for ships. However, conventional optical target detection models are typically engineered for complex optical imagery, leading to limitations in accuracy and high computational resource consumption when directly applied to SAR imagery. To address this, this paper proposes a lightweight shape-aware and direction-weighted algorithm for SAR ship detection, SADW-Det. First, a lightweight streamlined backbone network, LSFP-NET, is redesigned based on the YOLOX architecture. This achieves reduced parameter counts and computational burden by incorporating depthwise separable convolutions and factorized convolutions. Concurrently, a parallel fusion module is designed, leveraging multiple small-kernel depthwise separable convolutions to extract features in parallel. This approach maintains accuracy while achieving lightweight processing. Furthermore, addressing the differences between SAR imagery and other imaging modalities, a direction-weighted attention was devised. This enhances model performance with minimal computational overhead by incorporating positional information while preserving channel data. Experimental results demonstrate superior detection accuracy compared to existing methods on three representative SAR datasets, SSDD, HRSID and DSSDD, while achieving reduced parameter counts and computational complexity, indicating strong application potential and laying the foundation for cross-modal applications.
Published: 2026-02-13T10:52:21+00:00
Venue: Remote Sensing
Score: 0.832 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengshan Gui; Hairui Zhu; Weixing Sheng; Renli Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18040582"&gt;10.3390/rs18040582&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.832 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is a powerful observation system capable of delivering high-resolution imagery under variable sea conditions to support target detection and tracking, such as for ships. However, conventional optical target detection models are typically engineered for complex optical imagery, leading to limitations in accuracy and high computational resource consumption when directly applied to SAR imagery. To address this, this paper proposes a lightweight shape-aware and direction-weighted algorithm for SAR ship detection, SADW-Det. First, a lightweight streamlined backbone network, LSFP-NET, is redesigned based on the YOLOX architecture. This achieves reduced parameter counts and computational burden by incorporating depthwise separable convolutions and factorized convolutions. Concurrently, a parallel fusion module is designed, leveraging multiple small-kernel depthwise separable convolutions to extract features in parallel. This approach maintains accuracy while achieving lightweight processing. Furthermore, addressing the differences between SAR imagery and other imaging modalities, a direction-weighted attention was devised. This enhances model performance with minimal computational overhead by incorporating positional information while preserving channel data. Experimental results demonstrate superior detection accuracy compared to existing methods on three representative SAR datasets, SSDD, HRSID and DSSDD, while achieving reduced parameter counts and computational complexity, indicating strong application potential and laying the foundation for cross-modal applications.&lt;/p&gt;</content:encoded></item><item><title>DCGANet: Fusing Selective Variable Convolution and Dynamic Content-Guided Attention for Infrared Small Target Detection</title><link>https://doi.org/10.1016/j.knosys.2026.115546</link><guid>10.1016/j.knosys.2026.115546</guid><pubDate>Fri, 13 Feb 2026 16:44:54 +0000</pubDate><dc:creator>Yirui Chen</dc:creator><dc:creator>Yiming Zhu</dc:creator><dc:creator>Shuyan Min</dc:creator><dc:creator>Zhaoqi Qiu</dc:creator><dc:creator>Anglong Hu</dc:creator><dc:creator>Tuntun Wang</dc:creator><dc:creator>Tianpei Zhang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115546</prism:doi><description>Infrared Small Target Detection (IRSTD) presents significant difficulties due to the challenge of identifying minute targets within complex, cluttered backgrounds. Traditional Convolutional Neural Networks (CNNs) often struggle with this task, as their fixed, local receptive fields fail to capture global dependencies, while repeated down-sampling operations tend to submerge sparse target features in background noise. To address these limitations, we propose the Dynamic Content-Guided Attention Multiscale Feature Aggregation Network (DCGANet). Central to our method is the Selective Variable Convolution (SVC) module, which integrates standard, deformable, and multi-rate dilated convolutions. This design allows the network to dynamically adapt its receptive fields to capture diverse target structures effectively. To complement this, a novel two-stage content-guided attention mechanism simulates a “coarse-to-fine” search strategy; it first directs the network toward salient regions and subsequently refines the focus to precisely distinguish targets from background interference, thereby suppressing false alarms. Furthermore, we introduce the Adaptive Dynamic Feature Fusion (ADFF) module to facilitate information synergy across scales. Unlike static aggregation, ADFF adaptively integrates hierarchical contextual information, preventing the dilution of semantic-rich features by noise. Extensive experiments on the SIRST, IRSTD-1K, and NUDT-SIRST benchmarks demonstrate the effectiveness of DCGANet, achieving Intersection over Union (IoU) scores of 78.81%, 72.63%, and 91.25%, respectively. Finally, we discuss current limitations regarding extremely dim targets and suggest future directions for spatiotemporal modeling and model compression.
Published: 2026-02-13T16:44:54+00:00
Venue: Knowledge-Based Systems
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yirui Chen; Yiming Zhu; Shuyan Min; Zhaoqi Qiu; Anglong Hu; Tuntun Wang; Tianpei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115546"&gt;10.1016/j.knosys.2026.115546&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared Small Target Detection (IRSTD) presents significant difficulties due to the challenge of identifying minute targets within complex, cluttered backgrounds. Traditional Convolutional Neural Networks (CNNs) often struggle with this task, as their fixed, local receptive fields fail to capture global dependencies, while repeated down-sampling operations tend to submerge sparse target features in background noise. To address these limitations, we propose the Dynamic Content-Guided Attention Multiscale Feature Aggregation Network (DCGANet). Central to our method is the Selective Variable Convolution (SVC) module, which integrates standard, deformable, and multi-rate dilated convolutions. This design allows the network to dynamically adapt its receptive fields to capture diverse target structures effectively. To complement this, a novel two-stage content-guided attention mechanism simulates a “coarse-to-fine” search strategy; it first directs the network toward salient regions and subsequently refines the focus to precisely distinguish targets from background interference, thereby suppressing false alarms. Furthermore, we introduce the Adaptive Dynamic Feature Fusion (ADFF) module to facilitate information synergy across scales. Unlike static aggregation, ADFF adaptively integrates hierarchical contextual information, preventing the dilution of semantic-rich features by noise. Extensive experiments on the SIRST, IRSTD-1K, and NUDT-SIRST benchmarks demonstrate the effectiveness of DCGANet, achieving Intersection over Union (IoU) scores of 78.81%, 72.63%, and 91.25%, respectively. Finally, we discuss current limitations regarding extremely dim targets and suggest future directions for spatiotemporal modeling and model compression.&lt;/p&gt;</content:encoded></item><item><title>SP-KAN: Sparse-sine perception Kolmogorov–Arnold networks for infrared small target detection</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.019</link><guid>10.1016/j.isprsjprs.2026.02.019</guid><pubDate>Fri, 13 Feb 2026 08:08:53 +0000</pubDate><dc:creator>Shuai Yuan</dc:creator><dc:creator>Yu Liu</dc:creator><dc:creator>Xiaopei Zhang</dc:creator><dc:creator>Xiang Yan</dc:creator><dc:creator>Hanlin Qin</dc:creator><dc:creator>Naveed Akhtar</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.019</prism:doi><description>Infrared small target detection (IRSTD) plays a critical role in diverse complex remote sensing scenarios. However, existing IRSTD methods struggle to discriminate dim targets that are heavily entangled with complex interference due to their fixed activation representations. To tackle this issue, we reformulate IRSTD as a global context modulation problem driven by sparse nonlinear modules and propose a Sparse-sine Perception Kolmogorov–Arnold Network (SP-KAN). It marks a novel attempt to leverage the superior nonlinear capability of the Kolmogorov–Arnold theory for robust IRSTD. Specifically, a compressed vision transformer encoder is first employed to capture long-range spatial dependencies, while the proposed pattern complementarity module (PCM) constructs their essential nonlinear interactions. The PCM unifies channel-wise mappings of tokenized representations with local spatial saliency of structured features, enhancing target–background discrimination via multi-dimensional and multi-intensity nonlinear embedding. Within the PCM, a sparse-sine perception Kolmogorov–Arnold layer (SPKAL) is introduced to perceive the original nonlinear space and a sparse grid-based high-dimensional sinusoidal latent space at the pixel level, enabling fine-grained interactions among neurons and aligning with the inherent sparsity of small targets. Extensive experiments across four datasets demonstrate that SP-KAN consistently surpasses state-of-the-art IRSTD methods in accuracy, robustness, and generalization, verifying its superior capability in sparse nonlinear modeling. Code will be available at the author’s homepage https://github.com/xdFai .
Published: 2026-02-13T08:08:53+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Yuan; Yu Liu; Xiaopei Zhang; Xiang Yan; Hanlin Qin; Naveed Akhtar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.019"&gt;10.1016/j.isprsjprs.2026.02.019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) plays a critical role in diverse complex remote sensing scenarios. However, existing IRSTD methods struggle to discriminate dim targets that are heavily entangled with complex interference due to their fixed activation representations. To tackle this issue, we reformulate IRSTD as a global context modulation problem driven by sparse nonlinear modules and propose a Sparse-sine Perception Kolmogorov–Arnold Network (SP-KAN). It marks a novel attempt to leverage the superior nonlinear capability of the Kolmogorov–Arnold theory for robust IRSTD. Specifically, a compressed vision transformer encoder is first employed to capture long-range spatial dependencies, while the proposed pattern complementarity module (PCM) constructs their essential nonlinear interactions. The PCM unifies channel-wise mappings of tokenized representations with local spatial saliency of structured features, enhancing target–background discrimination via multi-dimensional and multi-intensity nonlinear embedding. Within the PCM, a sparse-sine perception Kolmogorov–Arnold layer (SPKAL) is introduced to perceive the original nonlinear space and a sparse grid-based high-dimensional sinusoidal latent space at the pixel level, enabling fine-grained interactions among neurons and aligning with the inherent sparsity of small targets. Extensive experiments across four datasets demonstrate that SP-KAN consistently surpasses state-of-the-art IRSTD methods in accuracy, robustness, and generalization, verifying its superior capability in sparse nonlinear modeling. Code will be available at the author’s homepage https://github.com/xdFai .&lt;/p&gt;</content:encoded></item><item><title>HLNet: A Lightweight Network for Ship Detection in Complex SAR Environments</title><link>https://doi.org/10.3390/rs18040577</link><guid>10.3390/rs18040577</guid><pubDate>Thu, 12 Feb 2026 15:28:11 +0000</pubDate><dc:creator>Xiaopeng Guo</dc:creator><dc:creator>Fan Deng</dc:creator><dc:creator>Jie Gong</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Jiajia Guo</dc:creator><dc:creator>Yong Wang</dc:creator><dc:creator>Yinmei Zeng</dc:creator><dc:creator>Gongquan Li</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18040577</prism:doi><description>The coherent speckle noise in synthetic aperture radar (SAR) imagery, together with complex sea clutter and large variations in ship target scales, poses significant challenges to accurate and robust ship detection, particularly under strict lightweight constraints required by satellite-borne and airborne platforms. To address this issue, this paper proposes a high-precision lightweight detection network, termed High-Lightweight Net (HLNet), specifically designed for SAR ship detection. The network incorporates a novel multi-scale backbone, Multi-Scale Net (MSNet), which integrates dynamic feature completion and multi-core parallel convolutions to alleviate small-target feature loss and suppress background interference. To further enhance multi-scale feature fusion while reducing model complexity, a lightweight path aggregation feature pyramid network, High-Lightweight Feature Pyramid (HLPAFPN), is introduced by reconstructing fusion pathways and removing redundant channels. In addition, a lightweight detection head, High-Lightweight Head (HLHead), is designed by combining grouped convolutions with distribution focal loss to improve localization robustness under low signal-to-noise ratio conditions. Extensive experiments conducted on the public SSDD and HRSID datasets demonstrate that HLNet achieves mAP50 scores of 98.3% and 91.7%, respectively, with only 0.66 M parameters. Extensive evaluations on the more challenging CSID subset, composed of complex scenes selected from SSDD and HRSID, demonstrate that HLNet attains an mAP50 of 75.9%, outperforming the baseline by 4.3%. These results indicate that HLNet achieves an effective balance between detection accuracy and computational efficiency, making it well-suited for deployment on resource-constrained SAR platforms.
Published: 2026-02-12T15:28:11+00:00
Venue: Remote Sensing
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaopeng Guo; Fan Deng; Jie Gong; Jing Zhang; Jiajia Guo; Yong Wang; Yinmei Zeng; Gongquan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18040577"&gt;10.3390/rs18040577&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;The coherent speckle noise in synthetic aperture radar (SAR) imagery, together with complex sea clutter and large variations in ship target scales, poses significant challenges to accurate and robust ship detection, particularly under strict lightweight constraints required by satellite-borne and airborne platforms. To address this issue, this paper proposes a high-precision lightweight detection network, termed High-Lightweight Net (HLNet), specifically designed for SAR ship detection. The network incorporates a novel multi-scale backbone, Multi-Scale Net (MSNet), which integrates dynamic feature completion and multi-core parallel convolutions to alleviate small-target feature loss and suppress background interference. To further enhance multi-scale feature fusion while reducing model complexity, a lightweight path aggregation feature pyramid network, High-Lightweight Feature Pyramid (HLPAFPN), is introduced by reconstructing fusion pathways and removing redundant channels. In addition, a lightweight detection head, High-Lightweight Head (HLHead), is designed by combining grouped convolutions with distribution focal loss to improve localization robustness under low signal-to-noise ratio conditions. Extensive experiments conducted on the public SSDD and HRSID datasets demonstrate that HLNet achieves mAP50 scores of 98.3% and 91.7%, respectively, with only 0.66 M parameters. Extensive evaluations on the more challenging CSID subset, composed of complex scenes selected from SSDD and HRSID, demonstrate that HLNet attains an mAP50 of 75.9%, outperforming the baseline by 4.3%. These results indicate that HLNet achieves an effective balance between detection accuracy and computational efficiency, making it well-suited for deployment on resource-constrained SAR platforms.&lt;/p&gt;</content:encoded></item><item><title>SARDet-MIM: Enhancing SAR Target Detection via a Structural and Scattering Masked Autoencoder</title><link>https://doi.org/10.3390/rs18040580</link><guid>10.3390/rs18040580</guid><pubDate>Fri, 13 Feb 2026 09:09:19 +0000</pubDate><dc:creator>Peiling Zhou</dc:creator><dc:creator>Ben Niu</dc:creator><dc:creator>Lijia Huang</dc:creator><dc:creator>Qiantong Wang</dc:creator><dc:creator>Yongchao Zhao</dc:creator><dc:creator>Guangyao Zhou</dc:creator><dc:creator>Yuxin Hu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18040580</prism:doi><description>The performance of deep learning approaches for Synthetic Aperture Radar (SAR) target detection is often limited by the scarcity of annotated data. While Self-Supervised Learning (SSL) has emerged as a powerful paradigm to mitigate data dependence, its potential in SAR target detection remains largely underexplored. In this study, we propose SARDet-MIM, a comprehensive framework based on Masked Image Modeling (MIM), to enhance SAR target detection. The approach consists of two stages. In the self-supervised pre-training stage, we propose an innovative Structural and Scattering Masked Autoencoder (SSMAE) method for SAR imagery. Unlike conventional MIM methods, which typically reconstruct raw pixels, SSMAE employs a physics-aware reconstruction target comprising multi-scale gradient and SAR-Harris features. This strategy explicitly guides the network to capture discriminative structural contexts and intrinsic scattering features that benefit SAR target detection. For downstream detection, we construct a Maximally Pre-trained Detector (MPD), which integrally transfers the pre-trained ViT encoder–decoder architecture to the detection network to fully exploit pre-trained representations. Extensive experiments on three SAR target detection datasets demonstrate that SARDet-MIM consistently outperforms competing methods.
Published: 2026-02-13T09:09:19+00:00
Venue: Remote Sensing
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peiling Zhou; Ben Niu; Lijia Huang; Qiantong Wang; Yongchao Zhao; Guangyao Zhou; Yuxin Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18040580"&gt;10.3390/rs18040580&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;The performance of deep learning approaches for Synthetic Aperture Radar (SAR) target detection is often limited by the scarcity of annotated data. While Self-Supervised Learning (SSL) has emerged as a powerful paradigm to mitigate data dependence, its potential in SAR target detection remains largely underexplored. In this study, we propose SARDet-MIM, a comprehensive framework based on Masked Image Modeling (MIM), to enhance SAR target detection. The approach consists of two stages. In the self-supervised pre-training stage, we propose an innovative Structural and Scattering Masked Autoencoder (SSMAE) method for SAR imagery. Unlike conventional MIM methods, which typically reconstruct raw pixels, SSMAE employs a physics-aware reconstruction target comprising multi-scale gradient and SAR-Harris features. This strategy explicitly guides the network to capture discriminative structural contexts and intrinsic scattering features that benefit SAR target detection. For downstream detection, we construct a Maximally Pre-trained Detector (MPD), which integrally transfers the pre-trained ViT encoder–decoder architecture to the detection network to fully exploit pre-trained representations. Extensive experiments on three SAR target detection datasets demonstrate that SARDet-MIM consistently outperforms competing methods.&lt;/p&gt;</content:encoded></item><item><title>GIC-FAFNet: Global-Local Information Coordination and Feature Alignment Fusion Network for Remote Sensing Object Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113292</link><guid>10.1016/j.patcog.2026.113292</guid><pubDate>Thu, 12 Feb 2026 07:53:33 +0000</pubDate><dc:creator>Yinggan Tang</dc:creator><dc:creator>Ziteng Zhao</dc:creator><dc:creator>Quansheng Xu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113292</prism:doi><description>The detection of small and multi-scale objects in remote sensing images (RSIs) remains a challenging task due to limited feature representation of small targets and insufficient use of spatial information across scales. To address these issues, we propose a novel Global-Local Information Coordination and Feature Alignment Fusion Network (GIC-FAFNet). First, we propose a Multi-level Feature Information Aggregation Module (MFIAM) that integrates local and global contextual cues, enriching small-object feature representation and partially mitigating the weakening or loss of small-object features caused by repeated down-sampling in deep networks. Second, we introduce a Feature Alignment Pyramid Network (FAPN) that effectively combines precise spatial details with high-level semantic information, improving localization accuracy for multi-scale objects. Additionally, a Detail Extraction Module (DEM) is developed to adaptively enhance features for objects of diverse scales and shapes. Extensive experiments on four public remote sensing datasets demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches. The code is available at: https://github.com/woshio/GIC-FAFNet .
Published: 2026-02-12T07:53:33+00:00
Venue: Pattern Recognition
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinggan Tang; Ziteng Zhao; Quansheng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113292"&gt;10.1016/j.patcog.2026.113292&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;The detection of small and multi-scale objects in remote sensing images (RSIs) remains a challenging task due to limited feature representation of small targets and insufficient use of spatial information across scales. To address these issues, we propose a novel Global-Local Information Coordination and Feature Alignment Fusion Network (GIC-FAFNet). First, we propose a Multi-level Feature Information Aggregation Module (MFIAM) that integrates local and global contextual cues, enriching small-object feature representation and partially mitigating the weakening or loss of small-object features caused by repeated down-sampling in deep networks. Second, we introduce a Feature Alignment Pyramid Network (FAPN) that effectively combines precise spatial details with high-level semantic information, improving localization accuracy for multi-scale objects. Additionally, a Detail Extraction Module (DEM) is developed to adaptively enhance features for objects of diverse scales and shapes. Extensive experiments on four public remote sensing datasets demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches. The code is available at: https://github.com/woshio/GIC-FAFNet .&lt;/p&gt;</content:encoded></item><item><title>ASDTracker: Adaptively Sparse Detection with Attention-Guided Refinement for Efficient Multi-Object Tracking</title><link>https://doi.org/10.1109/tip.2026.3662594</link><guid>10.1109/tip.2026.3662594</guid><pubDate>Fri, 13 Feb 2026 20:52:17 +0000</pubDate><dc:creator>Yueying Wang</dc:creator><dc:creator>Chenyang Yan</dc:creator><dc:creator>Cairong Zhao</dc:creator><dc:creator>Weidong Zhang</dc:creator><dc:creator>Dan Zeng</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3662594</prism:doi><description>Tracking-by-Detection paradigms shine in generic multi-object tracking (MOT), while their compact construction hinders the real-time applications. In this work, we attribute the substantial computational burden to two expensive components, i.e. detection and re-identification. Building upon the principle of adaptively maintaining acceptable inference efficiency, we present Adaptively Sparse Detection with attention-guided refinement (ASDTracker) for efficient tracking. In specific, our ASDTracker rapidly assess the short-term and long-term occlusion, dynamically determining the usage of the expensive detector. For non-key frames, we efficiently refine small-size crops out of Kalman Filter predictions and introduce the noisy shadow labels to robustly train this refinement network. Additionally, we substitute the lightweight appearance representation for the heavy ReID network, which efficiently extracts sufficient appearance cues in the coarsely quantized color spaces. Extensive experiments on four benchmarks demonstrate that ASDTracker achieves competitive performance in generalization and robustness under favorable inference speed. Moreover, the efficient tracking deployment is further implemented to an unmanned surface vehicle with high accuracy and low latency in real-world scenarios.
Published: 2026-02-13T20:52:17+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yueying Wang; Chenyang Yan; Cairong Zhao; Weidong Zhang; Dan Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3662594"&gt;10.1109/tip.2026.3662594&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Tracking-by-Detection paradigms shine in generic multi-object tracking (MOT), while their compact construction hinders the real-time applications. In this work, we attribute the substantial computational burden to two expensive components, i.e. detection and re-identification. Building upon the principle of adaptively maintaining acceptable inference efficiency, we present Adaptively Sparse Detection with attention-guided refinement (ASDTracker) for efficient tracking. In specific, our ASDTracker rapidly assess the short-term and long-term occlusion, dynamically determining the usage of the expensive detector. For non-key frames, we efficiently refine small-size crops out of Kalman Filter predictions and introduce the noisy shadow labels to robustly train this refinement network. Additionally, we substitute the lightweight appearance representation for the heavy ReID network, which efficiently extracts sufficient appearance cues in the coarsely quantized color spaces. Extensive experiments on four benchmarks demonstrate that ASDTracker achieves competitive performance in generalization and robustness under favorable inference speed. Moreover, the efficient tracking deployment is further implemented to an unmanned surface vehicle with high accuracy and low latency in real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>LMCNet: Lightweight Modality Compensation Network via Knowledge Distillation for Salient Ship Detection under Missing Modality Conditions</title><link>https://doi.org/10.1109/taes.2026.3664356</link><guid>10.1109/taes.2026.3664356</guid><pubDate>Thu, 12 Feb 2026 21:02:31 +0000</pubDate><dc:creator>Weibao Xue</dc:creator><dc:creator>Jiaqiu Ai</dc:creator><dc:creator>Yanan Zhu</dc:creator><dc:creator>Xinyu Sun</dc:creator><dc:creator>Yong Zhang</dc:creator><dc:creator>Gui Gao</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2026.3664356</prism:doi><description>Salient ship detection is critical for maritime applications that require accurate localization. Although the fusion of Synthetic Aperture Radar (SAR) and Automatic Identification System (AIS) data has proven effective in enhancing saliency and suppressing background clutter, practical deployment faces two major limitations: the limited unavailability of AIS data and the high computational overhead of existing multimodal models. These limitations pose a fundamental challenge in balancing detection accuracy and efficiency under missing modality and resource-constrained environments. To address this issue, a Lightweight Modality Compensation Network (LMCNet) is proposed. A multimodal teacher network is trained with SAR and AIS inputs to learn rich and complementary representations. Meanwhile, a compact, single-modality student network that relies only on SAR is designed to support low-cost, real-time deployment. To enable robust knowledge compensation and transfer, this paper designs a knowledge distillation strategy consisting of three modules: structure-aware attention distillation for spatial alignment, cross-head teacher distillation for semantic enhancement, and adaptive loss scheduling for dynamic optimization. This unified design allows the student model to inherit spatial precision and semantic awareness from the teacher, achieving strong performance even with limited input modalities. Extensive experiments on two datasets show that our distilled student model improves Eξ E by 1.39% and Fwβ by 4.37% compared to state-of-the-art methods, demonstrating its superior balance of accuracy and efficiency in real-world deployment scenarios.
Published: 2026-02-12T21:02:31+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weibao Xue; Jiaqiu Ai; Yanan Zhu; Xinyu Sun; Yong Zhang; Gui Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2026.3664356"&gt;10.1109/taes.2026.3664356&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Salient ship detection is critical for maritime applications that require accurate localization. Although the fusion of Synthetic Aperture Radar (SAR) and Automatic Identification System (AIS) data has proven effective in enhancing saliency and suppressing background clutter, practical deployment faces two major limitations: the limited unavailability of AIS data and the high computational overhead of existing multimodal models. These limitations pose a fundamental challenge in balancing detection accuracy and efficiency under missing modality and resource-constrained environments. To address this issue, a Lightweight Modality Compensation Network (LMCNet) is proposed. A multimodal teacher network is trained with SAR and AIS inputs to learn rich and complementary representations. Meanwhile, a compact, single-modality student network that relies only on SAR is designed to support low-cost, real-time deployment. To enable robust knowledge compensation and transfer, this paper designs a knowledge distillation strategy consisting of three modules: structure-aware attention distillation for spatial alignment, cross-head teacher distillation for semantic enhancement, and adaptive loss scheduling for dynamic optimization. This unified design allows the student model to inherit spatial precision and semantic awareness from the teacher, achieving strong performance even with limited input modalities. Extensive experiments on two datasets show that our distilled student model improves Eξ E by 1.39% and Fwβ by 4.37% compared to state-of-the-art methods, demonstrating its superior balance of accuracy and efficiency in real-world deployment scenarios.&lt;/p&gt;</content:encoded></item><item><title>Cross-Modal Attention-Modulated Feature Enhancement Network for Visible-Infrared Ship Detection</title><link>https://doi.org/10.1109/jstars.2026.3664123</link><guid>10.1109/jstars.2026.3664123</guid><pubDate>Thu, 12 Feb 2026 20:59:57 +0000</pubDate><dc:creator>Yaxin Lei</dc:creator><dc:creator>Wuxia Zhang</dc:creator><dc:creator>Xiaochen Niu</dc:creator><dc:creator>Hailong Ning</dc:creator><dc:creator>Xiaoqiang Lu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3664123</prism:doi><description>Ship detection is crucial for tasks such as harbor dynamic surveillance and maritime traffic management, aiming to identify and locate ships. Multimodal object detection based on visible-infrared data is well suited for ship detection because it enables all-weather object detection. However, it lacks paired visible-infrared ship datasets, and there still exist problems such as insufficient consistency and complementarity of cross-modal semantic information, as well as a large amount of redundant information when fusing different modalities. To address these problems, two pseudo-infrared datasets are established, and a Cross-modal Attention-modulated Feature Enhancement Network (CAMFEN) approach for ship detection is proposed. CAMFEN mainly utilizes the Cross-modal Collaborative-Differential Enhancement Feature Fusion module ( m{C}^{2}m{DEFF} m{C}^{2}m{DEFF} ) to effectively fuse the information of different modalities, which consists of Collaborative Attention Modulation Block (CAMB), Differential Attention Modulation Block (DAMB), and Global Feature Guided Fusion Block (GFGFB). The CAMB achieves cross-modal semantic alignment through channel attention modulation, eliminates geometric offset with spatial adaptive calibration, strengthens common semantics representation from the perspective of collaborative integrity, and equalizes modal contribution. The DAMB selects discriminant difference features through channel attention screening, uses spatial attention to focus on target regions, and mines complementary or contradictory information from the perspective of difference specificity, enhancing modal specificity and suppressing redundant noise. Finally, the enhanced single modality features obtained by CAMB and DAMB are fed into GFGFB, which guides the optimized fusion of enhanced single modality features from a global perspective. The proposed method has been validated on HRSC2016 and DOTAv1.0 datasets, and the experimental results show that CAMFEN outperforms existing ship d...
Published: 2026-02-12T20:59:57+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaxin Lei; Wuxia Zhang; Xiaochen Niu; Hailong Ning; Xiaoqiang Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3664123"&gt;10.1109/jstars.2026.3664123&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection is crucial for tasks such as harbor dynamic surveillance and maritime traffic management, aiming to identify and locate ships. Multimodal object detection based on visible-infrared data is well suited for ship detection because it enables all-weather object detection. However, it lacks paired visible-infrared ship datasets, and there still exist problems such as insufficient consistency and complementarity of cross-modal semantic information, as well as a large amount of redundant information when fusing different modalities. To address these problems, two pseudo-infrared datasets are established, and a Cross-modal Attention-modulated Feature Enhancement Network (CAMFEN) approach for ship detection is proposed. CAMFEN mainly utilizes the Cross-modal Collaborative-Differential Enhancement Feature Fusion module ( m{C}^{2}m{DEFF} m{C}^{2}m{DEFF} ) to effectively fuse the information of different modalities, which consists of Collaborative Attention Modulation Block (CAMB), Differential Attention Modulation Block (DAMB), and Global Feature Guided Fusion Block (GFGFB). The CAMB achieves cross-modal semantic alignment through channel attention modulation, eliminates geometric offset with spatial adaptive calibration, strengthens common semantics representation from the perspective of collaborative integrity, and equalizes modal contribution. The DAMB selects discriminant difference features through channel attention screening, uses spatial attention to focus on target regions, and mines complementary or contradictory information from the perspective of difference specificity, enhancing modal specificity and suppressing redundant noise. Finally, the enhanced single modality features obtained by CAMB and DAMB are fed into GFGFB, which guides the optimized fusion of enhanced single modality features from a global perspective. The proposed method has been validated on HRSC2016 and DOTAv1.0 datasets, and the experimental results show that CAMFEN outperforms existing ship d...&lt;/p&gt;</content:encoded></item><item><title>Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers</title><link>https://doi.org/10.1109/tpami.2026.3664227</link><guid>10.1109/tpami.2026.3664227</guid><pubDate>Thu, 12 Feb 2026 20:59:14 +0000</pubDate><dc:creator>Xin Ma</dc:creator><dc:creator>Yaohui Wang</dc:creator><dc:creator>Genyun Jia</dc:creator><dc:creator>Xinyuan Chen</dc:creator><dc:creator>Tien-Tsin Wong</dc:creator><dc:creator>Cunjian Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3664227</prism:doi><description>Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. The project page is available at https://maxin-cn.github.io/miramo_project.
Published: 2026-02-12T20:59:14+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Ma; Yaohui Wang; Genyun Jia; Xinyuan Chen; Tien-Tsin Wong; Cunjian Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3664227"&gt;10.1109/tpami.2026.3664227&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Image animation has seen significant progress, driven by the powerful generative capabilities of diffusion models. However, maintaining appearance consistency with static input images and mitigating abrupt motion transitions in generated animations remain persistent challenges. While text-to-video (T2V) generation has demonstrated impressive performance with diffusion transformer models, the image animation field still largely relies on U-Net-based diffusion models, which lag behind the latest T2V approaches. Moreover, the quadratic complexity of vanilla self-attention mechanisms in Transformers imposes heavy computational demands, making image animation particularly resource-intensive. To address these issues, we propose MiraMo, a framework designed to enhance efficiency, appearance consistency, and motion smoothness in image animation. Specifically, MiraMo introduces three key elements: (1) A foundational text-to-video architecture replacing vanilla self-attention with efficient linear attention to reduce computational overhead while preserving generation quality; (2) A novel motion residual learning paradigm that focuses on modeling motion dynamics rather than directly predicting frames, improving temporal consistency; and (3) A DCT-based noise refinement strategy during inference to suppress sudden motion artifacts, complemented by a dynamics control module to balance motion smoothness and expressiveness. Extensive experiments against state-of-the-art methods validate the superiority of MiraMo in generating consistent, smooth, and controllable animations with accelerated inference speed. Additionally, we demonstrate the versatility of MiraMo through applications in motion transfer and video editing tasks. The project page is available at https://maxin-cn.github.io/miramo_project.&lt;/p&gt;</content:encoded></item><item><title>Reusability Report: Evaluating the performance of a meta-learning foundation model on predicting the antibacterial activity of natural products</title><link>https://doi.org/10.1038/s42256-026-01187-y</link><guid>10.1038/s42256-026-01187-y</guid><pubDate>Thu, 12 Feb 2026 10:03:34 +0000</pubDate><dc:creator>Caitlin M. Butt</dc:creator><dc:creator>Allison S. Walker</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-026-01187-y</prism:doi><description>Abstract Deep learning foundation models are becoming increasingly popular for use in bioactivity prediction. Recently, Feng et al. developed ActFound, a bioactive foundation model that jointly uses pairwise learning and meta-learning. By utilizing these techniques, the model is capable of being fine-tuned to a more specific bioactivity task with only a small amount of new data. Here, to investigate the generalizability of the model, we looked to fine-tune the foundation model on an antibacterial natural products (NPs) dataset. Large, labelled NPs datasets, which are needed to train traditional deep learning methods, are scarce. Therefore, the bioactivity prediction of NPs is an ideal task for foundation models. We studied the performance of ActFound on the NPs dataset using a range of few-shot settings. Additionally, we compared ActFound’s performance with those of other state-of-the-art models in the field. We found ActFound was unable to reach the same level of accuracy on the antibacterial NPs dataset as it did on other cross-domain tasks reported in the original publication. However, ActFound displayed comparable or better performance compared to the other models studied, especially at the low-shot settings. Our results establish ActFound as a useful foundation model for the bioactivity prediction of tasks with limited data, particularly for datasets that contain the bioactivities of similar compounds.
Published: 2026-02-12T10:03:34+00:00
Venue: Nature Machine Intelligence
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Caitlin M. Butt; Allison S. Walker&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-026-01187-y"&gt;10.1038/s42256-026-01187-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract Deep learning foundation models are becoming increasingly popular for use in bioactivity prediction. Recently, Feng et al. developed ActFound, a bioactive foundation model that jointly uses pairwise learning and meta-learning. By utilizing these techniques, the model is capable of being fine-tuned to a more specific bioactivity task with only a small amount of new data. Here, to investigate the generalizability of the model, we looked to fine-tune the foundation model on an antibacterial natural products (NPs) dataset. Large, labelled NPs datasets, which are needed to train traditional deep learning methods, are scarce. Therefore, the bioactivity prediction of NPs is an ideal task for foundation models. We studied the performance of ActFound on the NPs dataset using a range of few-shot settings. Additionally, we compared ActFound’s performance with those of other state-of-the-art models in the field. We found ActFound was unable to reach the same level of accuracy on the antibacterial NPs dataset as it did on other cross-domain tasks reported in the original publication. However, ActFound displayed comparable or better performance compared to the other models studied, especially at the low-shot settings. Our results establish ActFound as a useful foundation model for the bioactivity prediction of tasks with limited data, particularly for datasets that contain the bioactivities of similar compounds.&lt;/p&gt;</content:encoded></item><item><title>A
                    &lt;sup&gt;2&lt;/sup&gt;
                    -MAE: A Spatial-temporal-spectral Unified Remote Sensing Pre-training Method Based on Anchor-aware Masked Autoencoder</title><link>https://doi.org/10.1109/tgrs.2026.3664662</link><guid>10.1109/tgrs.2026.3664662</guid><pubDate>Fri, 13 Feb 2026 20:49:47 +0000</pubDate><dc:creator>Lixian Zhang</dc:creator><dc:creator>Yi Zhao</dc:creator><dc:creator>Runmin Dong</dc:creator><dc:creator>Jinxiao Zhang</dc:creator><dc:creator>Shuai Yuan</dc:creator><dc:creator>Shilei Cao</dc:creator><dc:creator>Mengxuan Chen</dc:creator><dc:creator>Juepeng Zheng</dc:creator><dc:creator>Weijia Li</dc:creator><dc:creator>Wayne Zhang</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Litong Feng</dc:creator><dc:creator>Jianxi Huang</dc:creator><dc:creator>Haohuan Fu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3664662</prism:doi><description>Vast amounts of remote sensing (RS) data provide Earth observations across multiple dimensions, encompassing critical spatial, temporal, and spectral information which is essential for addressing global-scale challenges such as land use monitoring, disaster prevention, and environmental change mitigation. Despite various pre-training methods tailored to the characteristics of RS data, a key limitation persists: the inability to effectively integrate spatial, temporal, and spectral information within a single unified model. To unlock the potential of RS data, we construct a Spatial-Temporal-Spectral Structured Dataset (STSSD) characterized by the incorporation of multiple RS sources, diverse coverage, unified locations within image sets, and heterogeneity within images. Building upon this structured dataset, we propose an Anchor-Aware Masked AutoEncoder method (A2-MAE), leveraging intrinsic complementary information from the different kinds of images (featuring different resolutions, spectral compositions, and acquisition times) and geo-information to reconstruct the masked patches during the pre-training phase. Moreover, A2-MAE integrates an anchor-aware masking strategy and a geographic encoding module to comprehensively exploit the properties of RS images. Specifically, the proposed anchor-aware masking strategy dynamically adapts the masking process based on the meta-information of a pre selected anchor image, thereby facilitating the training on images captured by diverse types of RS sources within one model. Furthermore, we propose a geographic encoding method to leverage accurate spatial patterns, enhancing the model generalization capabilities for downstream applications that are generally location-related. Extensive experiments demonstrate our method achieves comprehensive improvements across various downstream tasks compared with existing RS pre-training methods, including image classification, semantic segmentation, and change detection tasks. The dataset ...
Published: 2026-02-13T20:49:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lixian Zhang; Yi Zhao; Runmin Dong; Jinxiao Zhang; Shuai Yuan; Shilei Cao; Mengxuan Chen; Juepeng Zheng; Weijia Li; Wayne Zhang; Wei Liu; Litong Feng; Jianxi Huang; Haohuan Fu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3664662"&gt;10.1109/tgrs.2026.3664662&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Vast amounts of remote sensing (RS) data provide Earth observations across multiple dimensions, encompassing critical spatial, temporal, and spectral information which is essential for addressing global-scale challenges such as land use monitoring, disaster prevention, and environmental change mitigation. Despite various pre-training methods tailored to the characteristics of RS data, a key limitation persists: the inability to effectively integrate spatial, temporal, and spectral information within a single unified model. To unlock the potential of RS data, we construct a Spatial-Temporal-Spectral Structured Dataset (STSSD) characterized by the incorporation of multiple RS sources, diverse coverage, unified locations within image sets, and heterogeneity within images. Building upon this structured dataset, we propose an Anchor-Aware Masked AutoEncoder method (A2-MAE), leveraging intrinsic complementary information from the different kinds of images (featuring different resolutions, spectral compositions, and acquisition times) and geo-information to reconstruct the masked patches during the pre-training phase. Moreover, A2-MAE integrates an anchor-aware masking strategy and a geographic encoding module to comprehensively exploit the properties of RS images. Specifically, the proposed anchor-aware masking strategy dynamically adapts the masking process based on the meta-information of a pre selected anchor image, thereby facilitating the training on images captured by diverse types of RS sources within one model. Furthermore, we propose a geographic encoding method to leverage accurate spatial patterns, enhancing the model generalization capabilities for downstream applications that are generally location-related. Extensive experiments demonstrate our method achieves comprehensive improvements across various downstream tasks compared with existing RS pre-training methods, including image classification, semantic segmentation, and change detection tasks. The dataset ...&lt;/p&gt;</content:encoded></item><item><title>DNGaussian++: Improving Sparse-View Gaussian Radiance Fields with Depth Normalization</title><link>https://doi.org/10.1109/tpami.2026.3664307</link><guid>10.1109/tpami.2026.3664307</guid><pubDate>Thu, 12 Feb 2026 20:59:14 +0000</pubDate><dc:creator>Jiahe Li</dc:creator><dc:creator>Jiawei Zhang</dc:creator><dc:creator>Xiaohan Yu</dc:creator><dc:creator>Xiao Bai</dc:creator><dc:creator>Jin Zheng</dc:creator><dc:creator>Xin Ning</dc:creator><dc:creator>Lin Gu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3664307</prism:doi><description>Synthesizing novel views from sparse views has achieved impressive advances with radiance fields, yet prevailing methods suffer from high consumption or insufficient refinement capability. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian Splatting, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the remarkable advancement of recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Although DNGaussian shows impressive performance, its patch-wise regularization obscures the inconsistency in cross-patch errors. Additionally, primitives can still be irreversibly trapped in local minima under sparse views, even if depth regularization is applied. In this paper, we propose an extended version, DNGaussian++. First, a Geometry Instance Regularizer is developed to enable depth regularization for continuous consistency by exploiting reliable instance-level depth cues. Leveraging the depth gradient guidance, we then propose a Depth-Guided Geometry Reorganization to address the aforementioned local minima problem with high representation efficiency. Extensive experiments show that DNGaussian++ exhibits state-of-the-art performance in multiple datasets and scenarios with high efficiency, and the broad applicability and effectiveness are verified on various backbones and tasks.
Published: 2026-02-12T20:59:14+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahe Li; Jiawei Zhang; Xiaohan Yu; Xiao Bai; Jin Zheng; Xin Ning; Lin Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3664307"&gt;10.1109/tpami.2026.3664307&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Synthesizing novel views from sparse views has achieved impressive advances with radiance fields, yet prevailing methods suffer from high consumption or insufficient refinement capability. This paper introduces DNGaussian, a depth-regularized framework based on 3D Gaussian Splatting, offering real-time and high-quality few-shot novel view synthesis at low costs. Our motivation stems from the remarkable advancement of recent 3D Gaussian Splatting, despite it will encounter a geometry degradation when input views decrease. In the Gaussian radiance fields, we find this degradation in scene geometry primarily lined to the positioning of Gaussian primitives and can be mitigated by depth constraint. Consequently, we propose a Hard and Soft Depth Regularization to restore accurate scene geometry under coarse monocular depth supervision while maintaining a fine-grained color appearance. To further refine detailed geometry, we introduce Global-Local Depth Normalization, enhancing the focus on small local depth changes. Although DNGaussian shows impressive performance, its patch-wise regularization obscures the inconsistency in cross-patch errors. Additionally, primitives can still be irreversibly trapped in local minima under sparse views, even if depth regularization is applied. In this paper, we propose an extended version, DNGaussian++. First, a Geometry Instance Regularizer is developed to enable depth regularization for continuous consistency by exploiting reliable instance-level depth cues. Leveraging the depth gradient guidance, we then propose a Depth-Guided Geometry Reorganization to address the aforementioned local minima problem with high representation efficiency. Extensive experiments show that DNGaussian++ exhibits state-of-the-art performance in multiple datasets and scenarios with high efficiency, and the broad applicability and effectiveness are verified on various backbones and tasks.&lt;/p&gt;</content:encoded></item><item><title>FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection</title><link>https://arxiv.org/abs/2602.10710v1</link><guid>http://arxiv.org/abs/2602.10710v1</guid><pubDate>Wed, 11 Feb 2026 10:15:06 +0000</pubDate><dc:creator>Jialin Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.
Published: 2026-02-11T10:15:06+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jialin Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;With the increasing availability of high-resolution remote sensing and aerial imagery, oriented object detection has become a key capability for geographic information updating, maritime surveillance, and disaster response. However, it remains challenging due to cluttered backgrounds, severe scale variation, and large orientation changes. Existing approaches largely improve performance through multi-scale feature fusion with feature pyramid networks or contextual modeling with attention, but they often lack explicit foreground modeling and do not leverage geometric orientation priors, which limits feature discriminability. To overcome these limitations, we propose FGAA-FPN, a Foreground-Guided Angle-Aware Feature Pyramid Network for oriented object detection. FGAA-FPN is built on a hierarchical functional decomposition that accounts for the distinct spatial resolution and semantic abstraction across pyramid levels, thereby strengthening multi-scale representations. Concretely, a Foreground-Guided Feature Modulation module learns foreground saliency under weak supervision to enhance object regions and suppress background interference in low-level features. In parallel, an Angle-Aware Multi-Head Attention module encodes relative orientation relationships to guide global interactions among high-level semantic features. Extensive experiments on DOTA v1.0 and DOTA v1.5 demonstrate that FGAA-FPN achieves state-of-the-art results, reaching 75.5% and 68.3% mAP, respectively.&lt;/p&gt;</content:encoded></item><item><title>Multi-Expert Learning Framework with the State Space Model for Optical and SAR Image Registration</title><link>https://doi.org/10.1109/tgrs.2026.3664116</link><guid>10.1109/tgrs.2026.3664116</guid><pubDate>Thu, 12 Feb 2026 20:59:23 +0000</pubDate><dc:creator>Wei Wang</dc:creator><dc:creator>Dou Quan</dc:creator><dc:creator>Ning Huyan</dc:creator><dc:creator>Chonghua Lv</dc:creator><dc:creator>Shuang Wang</dc:creator><dc:creator>Yunan Li</dc:creator><dc:creator>Licheng Jiao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3664116</prism:doi><description>Optical and Synthetic Aperture Radar (SAR) image registration is crucial for multi-modal image fusion and applications. However, several challenges limit the performance of existing deep learning-based methods in cross-modal image registration: (i) significant nonlinear radiometric variations between optical and SAR images affect the shared feature learning and matching; (ii) limited textures in images hinder discriminative feature extraction; (iii) the local receptive field of Convolutional Neural Networks (CNNs) restricts the learning of contextual information, while the Transformer can capture long-range global features but with high computational complexity. To address these issues, this paper proposes a multi-expert learning framework with the State Space Model (ME-SSM) for optical and SAR image registration. Firstly, to improve the registration performance with limited textures, ME-SSM constructs a multi-expert learning framework to capture shared features from multi-modal images. Specifically, it extracts features from various transformations of the input image and employs a learnable soft router to dynamically fuse these features, thereby enriching feature representations and improving registration performance. Secondly, ME-SSM introduces a state space model, Mamba, for feature extraction, which employs a multi-directional cross-scanning strategy to efficiently capture global contextual relationships with linear complexity. ME-SSM can expand the receptive field, enhance image registration accuracy, and avoid incurring high computational costs. Additionally, ME-SSM uses a multi-level feature aggregation (MFA) module to enhance the multi-scale feature fusion and interaction. Extensive experiments have demonstrated the effectiveness and advantages of our proposed ME-SSM on optical and SAR image registration. Specifically, ME-SSM improves the correct matching rate (CMR) by 7.14% and 1.95% based on thresholds 1 and 3, respectively, on the SEN1-2 dataset, and incr...
Published: 2026-02-12T20:59:23+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Wang; Dou Quan; Ning Huyan; Chonghua Lv; Shuang Wang; Yunan Li; Licheng Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3664116"&gt;10.1109/tgrs.2026.3664116&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Optical and Synthetic Aperture Radar (SAR) image registration is crucial for multi-modal image fusion and applications. However, several challenges limit the performance of existing deep learning-based methods in cross-modal image registration: (i) significant nonlinear radiometric variations between optical and SAR images affect the shared feature learning and matching; (ii) limited textures in images hinder discriminative feature extraction; (iii) the local receptive field of Convolutional Neural Networks (CNNs) restricts the learning of contextual information, while the Transformer can capture long-range global features but with high computational complexity. To address these issues, this paper proposes a multi-expert learning framework with the State Space Model (ME-SSM) for optical and SAR image registration. Firstly, to improve the registration performance with limited textures, ME-SSM constructs a multi-expert learning framework to capture shared features from multi-modal images. Specifically, it extracts features from various transformations of the input image and employs a learnable soft router to dynamically fuse these features, thereby enriching feature representations and improving registration performance. Secondly, ME-SSM introduces a state space model, Mamba, for feature extraction, which employs a multi-directional cross-scanning strategy to efficiently capture global contextual relationships with linear complexity. ME-SSM can expand the receptive field, enhance image registration accuracy, and avoid incurring high computational costs. Additionally, ME-SSM uses a multi-level feature aggregation (MFA) module to enhance the multi-scale feature fusion and interaction. Extensive experiments have demonstrated the effectiveness and advantages of our proposed ME-SSM on optical and SAR image registration. Specifically, ME-SSM improves the correct matching rate (CMR) by 7.14% and 1.95% based on thresholds 1 and 3, respectively, on the SEN1-2 dataset, and incr...&lt;/p&gt;</content:encoded></item><item><title>Dual Adaptive Disentangled Representation Learning with Multimodal Data for Disease Diagnosis</title><link>https://doi.org/10.1109/tpami.2026.3664047</link><guid>10.1109/tpami.2026.3664047</guid><pubDate>Thu, 12 Feb 2026 20:59:14 +0000</pubDate><dc:creator>Xiumei Chen</dc:creator><dc:creator>Wenliang Pan</dc:creator><dc:creator>Tao Wang</dc:creator><dc:creator>Xinyue Zhang</dc:creator><dc:creator>Wei Xiong</dc:creator><dc:creator>Ting Tian</dc:creator><dc:creator>Qianjin Feng</dc:creator><dc:creator>Meiyan Huang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3664047</prism:doi><description>The use of imaging and genetic data for biomarker detection and disease diagnosis can deepen the understanding of disease pathogenesis and assist in clinical diagnosis. However, current methods face two major challenges: 1) the significant heterogeneity between multimodal data hampers modality fusion. 2) Effectively exploring consistency and variability information from similar diseases for enhancing model performance is difficult. In this paper, we propose a novel unified frame work, termed dual adaptive disentangled representation learning (DADRL), to simultaneously achieve disease-shared and disease specific biomarker detection as well as disease diagnosis. Our DADRL comprises three components: 1) a biology information constraints-based modality fusion strategy is applied to adaptively explore inter- and intra-modal correlations, thereby effectively fusing multimodal data. 2) A unified framework that integrates modality fusion and disease diagnosis is proposed to mine disease-related information for simultaneously accomplishing disease-related biomarker detection and disease diagnosis. 3) Disentangled representation learning and several adaptive metric constraints are incorporated into the unified framework to adaptively separate disease-specific information from disease shared feature representations for effectively identifying disease shared and disease-specific biomarkers, thereby deepening the understanding of disease pathogenesis. Extensive experiments on multiple real datasets and simulated data demonstrate that our method significantly improves performance of biomarker detection and disease diagnosis.
Published: 2026-02-12T20:59:14+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiumei Chen; Wenliang Pan; Tao Wang; Xinyue Zhang; Wei Xiong; Ting Tian; Qianjin Feng; Meiyan Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3664047"&gt;10.1109/tpami.2026.3664047&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;The use of imaging and genetic data for biomarker detection and disease diagnosis can deepen the understanding of disease pathogenesis and assist in clinical diagnosis. However, current methods face two major challenges: 1) the significant heterogeneity between multimodal data hampers modality fusion. 2) Effectively exploring consistency and variability information from similar diseases for enhancing model performance is difficult. In this paper, we propose a novel unified frame work, termed dual adaptive disentangled representation learning (DADRL), to simultaneously achieve disease-shared and disease specific biomarker detection as well as disease diagnosis. Our DADRL comprises three components: 1) a biology information constraints-based modality fusion strategy is applied to adaptively explore inter- and intra-modal correlations, thereby effectively fusing multimodal data. 2) A unified framework that integrates modality fusion and disease diagnosis is proposed to mine disease-related information for simultaneously accomplishing disease-related biomarker detection and disease diagnosis. 3) Disentangled representation learning and several adaptive metric constraints are incorporated into the unified framework to adaptively separate disease-specific information from disease shared feature representations for effectively identifying disease shared and disease-specific biomarkers, thereby deepening the understanding of disease pathogenesis. Extensive experiments on multiple real datasets and simulated data demonstrate that our method significantly improves performance of biomarker detection and disease diagnosis.&lt;/p&gt;</content:encoded></item><item><title>FineShip-CM: A Fine-Grained Ship Detection Method in UHR Remote Sensing Images Based on Textual Cognition and Graph Structure Under Complex Backgrounds</title><link>https://doi.org/10.1109/taes.2026.3664347</link><guid>10.1109/taes.2026.3664347</guid><pubDate>Fri, 13 Feb 2026 20:52:03 +0000</pubDate><dc:creator>Yantong Chen</dc:creator><dc:creator>Na Lin</dc:creator><dc:creator>Ming Qu</dc:creator><dc:creator>Guanming Cheng</dc:creator><dc:creator>Chengyong Shi</dc:creator><dc:creator>Mingchen Ge</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2026.3664347</prism:doi><description>With the rapid growth of maritime traffic, ship detection using high-resolution remote sensing images has become essential for intelligent ocean monitoring. Conventional methods are constrained by limited resolution, reducing accuracy in distinguishing ship categories. Ultra-high-resolution (UHR) remote sensing images provide richer visual details while simultaneously introducing additional challenges. These challenges include stronger background interference and redundant features, which blur interclass boundaries while amplifying intraclass variations. Such factors make it difficult for models to extract truly discriminative information. To address these issues, we propose a text-cognitive and graph-structured fine-grained detection method. The method introduces a cognitive diffusion guidance mechanism that decouples and enhances textual cues for improved semantic interpretation. This mechanism helps the model interpret ambiguous language and better guide visual attention. In addition, a multi-proposal graph structure mitigates localization errors caused by arbitrary ship orientations. It further enables more reliable feature propagation and optimization. A detail factor extractor is also designed to semantically reconstruct the text for refined modeling. This improves both semantic representation and detection performance. Experiments on ShipRSImageNet-SR, HRSC2016-SR, and V-ships datasets show superior classification accuracy and detection results. These findings confirm the robustness and applicability of the proposed approach in complex remote sensing scenarios.
Published: 2026-02-13T20:52:03+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yantong Chen; Na Lin; Ming Qu; Guanming Cheng; Chengyong Shi; Mingchen Ge&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2026.3664347"&gt;10.1109/taes.2026.3664347&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid growth of maritime traffic, ship detection using high-resolution remote sensing images has become essential for intelligent ocean monitoring. Conventional methods are constrained by limited resolution, reducing accuracy in distinguishing ship categories. Ultra-high-resolution (UHR) remote sensing images provide richer visual details while simultaneously introducing additional challenges. These challenges include stronger background interference and redundant features, which blur interclass boundaries while amplifying intraclass variations. Such factors make it difficult for models to extract truly discriminative information. To address these issues, we propose a text-cognitive and graph-structured fine-grained detection method. The method introduces a cognitive diffusion guidance mechanism that decouples and enhances textual cues for improved semantic interpretation. This mechanism helps the model interpret ambiguous language and better guide visual attention. In addition, a multi-proposal graph structure mitigates localization errors caused by arbitrary ship orientations. It further enables more reliable feature propagation and optimization. A detail factor extractor is also designed to semantically reconstruct the text for refined modeling. This improves both semantic representation and detection performance. Experiments on ShipRSImageNet-SR, HRSC2016-SR, and V-ships datasets show superior classification accuracy and detection results. These findings confirm the robustness and applicability of the proposed approach in complex remote sensing scenarios.&lt;/p&gt;</content:encoded></item><item><title>Progressive Multiscale Generator for Domain Generalization in Hyperspectral Image Classification with Small Sample</title><link>https://doi.org/10.1109/tcsvt.2026.3663555</link><guid>10.1109/tcsvt.2026.3663555</guid><pubDate>Fri, 13 Feb 2026 20:51:49 +0000</pubDate><dc:creator>Wen An</dc:creator><dc:creator>Zhixi Feng</dc:creator><dc:creator>Shuyuan Yang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3663555</prism:doi><description>Domain generalization-based hyperspectral image classification methods have achieved promising results in recent years. However, these studies seldom consider the issue of small sample in the source domain. In practical applications, manually annotating hyperspectral images is difficult, so labeled samples in the source domain may be scarce. Existing models have limited feature extraction capability and poor generalization performance in scenarios with limited labeled samples. To address the limitations of existing methods on small sample data of the source domain, a novel approach, Progressive Multiscale Generator for Domain Generalization (PMGDG), is proposed in this paper. The PMGDG employs a progressive multiscale generator comprising a series of sub-generators with paired sub-discriminators. The channel dimension of generated samples grows gradually from the first layer to the last layer. Then, the Classifier network is trained on both the original samples and the generated samples with different distributions to enhance its generalization performance. Additionally, we introduce a hierarchical optimization approach to stabilize the training process. Extensive experiments are conducted on three public hyperspectral image cross-domain datasets:Houston, Pavia, and HyRANK. The experimental results demonstrate that, compared to existing domain generalization methods for hyperspectral image classification, the proposed approach significantly improves classification performance under small sample. The code is available from the website: https://github.com/adwfdawd/PMGDG.
Published: 2026-02-13T20:51:49+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wen An; Zhixi Feng; Shuyuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3663555"&gt;10.1109/tcsvt.2026.3663555&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Domain generalization-based hyperspectral image classification methods have achieved promising results in recent years. However, these studies seldom consider the issue of small sample in the source domain. In practical applications, manually annotating hyperspectral images is difficult, so labeled samples in the source domain may be scarce. Existing models have limited feature extraction capability and poor generalization performance in scenarios with limited labeled samples. To address the limitations of existing methods on small sample data of the source domain, a novel approach, Progressive Multiscale Generator for Domain Generalization (PMGDG), is proposed in this paper. The PMGDG employs a progressive multiscale generator comprising a series of sub-generators with paired sub-discriminators. The channel dimension of generated samples grows gradually from the first layer to the last layer. Then, the Classifier network is trained on both the original samples and the generated samples with different distributions to enhance its generalization performance. Additionally, we introduce a hierarchical optimization approach to stabilize the training process. Extensive experiments are conducted on three public hyperspectral image cross-domain datasets:Houston, Pavia, and HyRANK. The experimental results demonstrate that, compared to existing domain generalization methods for hyperspectral image classification, the proposed approach significantly improves classification performance under small sample. The code is available from the website: https://github.com/adwfdawd/PMGDG.&lt;/p&gt;</content:encoded></item><item><title>AurigaNet: A Real-Time Multi-Task Network for Enhanced Urban Driving Perception</title><link>https://arxiv.org/abs/2602.10660v1</link><guid>http://arxiv.org/abs/2602.10660v1</guid><pubDate>Wed, 11 Feb 2026 09:04:29 +0000</pubDate><dc:creator>Kiarash Ghasemzadeh</dc:creator><dc:creator>Sedigheh Dehghani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet's potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.
Published: 2026-02-11T09:04:29+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kiarash Ghasemzadeh; Sedigheh Dehghani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Self-driving cars hold significant potential to reduce traffic accidents, alleviate congestion, and enhance urban mobility. However, developing reliable AI systems for autonomous vehicles remains a substantial challenge. Over the past decade, multi-task learning has emerged as a powerful approach to address complex problems in driving perception. Multi-task networks offer several advantages, including increased computational efficiency, real-time processing capabilities, optimized resource utilization, and improved generalization. In this study, we present AurigaNet, an advanced multi-task network architecture designed to push the boundaries of autonomous driving perception. AurigaNet integrates three critical tasks: object detection, lane detection, and drivable area instance segmentation. The system is trained and evaluated using the BDD100K dataset, renowned for its diversity in driving conditions. Key innovations of AurigaNet include its end-to-end instance segmentation capability, which significantly enhances both accuracy and efficiency in path estimation for autonomous vehicles. Experimental results demonstrate that AurigaNet achieves an 85.2% IoU in drivable area segmentation, outperforming its closest competitor by 0.7%. In lane detection, AurigaNet achieves a remarkable 60.8% IoU, surpassing other models by more than 30%. Furthermore, the network achieves an mAP@0.5:0.95 of 47.6% in traffic object detection, exceeding the next leading model by 2.9%. Additionally, we validate the practical feasibility of AurigaNet by deploying it on embedded devices such as the Jetson Orin NX, where it demonstrates competitive real-time performance. These results underscore AurigaNet&amp;#x27;s potential as a robust and efficient solution for autonomous driving perception systems. The code can be found here https://github.com/KiaRational/AurigaNet.&lt;/p&gt;</content:encoded></item><item><title>CDFIT: A Transformer Using Cross-Modal Dual-Stream Feature Interaction for Multispectral Pedestrian Detection</title><link>https://doi.org/10.1109/tits.2025.3649738</link><guid>10.1109/tits.2025.3649738</guid><pubDate>Thu, 12 Feb 2026 21:01:24 +0000</pubDate><dc:creator>Zihao Huang</dc:creator><dc:creator>Wenshi Li</dc:creator><dc:creator>Yuzhen Zhang</dc:creator><dc:creator>Jiaren Guo</dc:creator><dc:creator>Jianyin Zheng</dc:creator><dc:creator>Guang Ji</dc:creator><dc:creator>Yanyun Tao</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3649738</prism:doi><description>Modality imbalance is a significant challenge for multi-modal interaction at various depths in multispectral pedestrian detection under varying illumination environments. To overcome the limitations of current cross attention in addressing the modality imbalance, we propose the Cross-Modal Dual-Stream Feature Interaction Transformer (CDFIT). CDFIT capitalizes on the Transformer’s ability to learn long-range dependencies, extracting global intra-modal and inter-modal correlations during the feature interaction phase. Crucially, in order to effectively eliminate the interference of the self-attention within one modality to the alternative one, we propose horizontal and vertical correlation decoupling modes to divide and reassemble the attention maps in CDFIT. This facilitates more purified inter-modal attention while preserving relevant intra-modal self-attention, reducing the information interference. Meanwhile, in CDFIT, we expand Transformer into dual-stream pathways to align and assemble the information from RGB and thermal modalities across depths separately, thereby greatly enhancing the performance of multispectral object detection. Comprehensive experiments and ablation studies on benchmark datasets demonstrate that CDFIT achieves superior performance compared with state-of-the-art methods.
Published: 2026-02-12T21:01:24+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihao Huang; Wenshi Li; Yuzhen Zhang; Jiaren Guo; Jianyin Zheng; Guang Ji; Yanyun Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3649738"&gt;10.1109/tits.2025.3649738&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Modality imbalance is a significant challenge for multi-modal interaction at various depths in multispectral pedestrian detection under varying illumination environments. To overcome the limitations of current cross attention in addressing the modality imbalance, we propose the Cross-Modal Dual-Stream Feature Interaction Transformer (CDFIT). CDFIT capitalizes on the Transformer’s ability to learn long-range dependencies, extracting global intra-modal and inter-modal correlations during the feature interaction phase. Crucially, in order to effectively eliminate the interference of the self-attention within one modality to the alternative one, we propose horizontal and vertical correlation decoupling modes to divide and reassemble the attention maps in CDFIT. This facilitates more purified inter-modal attention while preserving relevant intra-modal self-attention, reducing the information interference. Meanwhile, in CDFIT, we expand Transformer into dual-stream pathways to align and assemble the information from RGB and thermal modalities across depths separately, thereby greatly enhancing the performance of multispectral object detection. Comprehensive experiments and ablation studies on benchmark datasets demonstrate that CDFIT achieves superior performance compared with state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>A flaw in using pretrained protein language models in protein–protein interaction inference models</title><link>https://doi.org/10.1038/s42256-025-01176-7</link><guid>10.1038/s42256-025-01176-7</guid><pubDate>Fri, 13 Feb 2026 10:02:54 +0000</pubDate><dc:creator>Joseph Szymborski</dc:creator><dc:creator>Amin Emad</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01176-7</prism:doi><description>With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task. Here we identify and confirm that existing pretrained pLMs are a source of data leakage for the downstream PPI task. We characterize the extent of the data leakage problem by training and comparing small and efficient pLMs on a dataset that controls for data leakage (strict) with one that does not (non-strict). Although data leakage from pretrained pLMs cause a measurable inflation of testing scores, we find that this does not necessarily extend to other, non-paired biological tasks such as protein keyword annotation. Further, we find no connection between the context lengths of pLMs and the performance of pLM-based PPI inference methods on proteins with sequence lengths that surpass it. Furthermore, we show that pLM-based and non-pLM-based models fail to generalize in tasks such as prediction of the human-SARS-CoV-2 PPIs or the effect of point mutations on binding affinities. This study demonstrates the importance of extending existing protocols for the evaluation of pLM-based models applied to paired biological datasets and identifies areas of weakness of current pLM models. The usage of pretrained protein language models (pLMs) is rapidly growing. However, Szymborski and Emad find that pretrained pLMs can be a source of data leakage in the task of protein–protein interaction inference, showing inflated performance scores.
Published: 2026-02-13T10:02:54+00:00
Venue: Nature Machine Intelligence
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Joseph Szymborski; Amin Emad&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01176-7"&gt;10.1038/s42256-025-01176-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;With the growing pervasiveness of pretrained protein language models (pLMs), pLM-based methods are increasingly being put forward for the protein–protein interaction (PPI) inference task. Here we identify and confirm that existing pretrained pLMs are a source of data leakage for the downstream PPI task. We characterize the extent of the data leakage problem by training and comparing small and efficient pLMs on a dataset that controls for data leakage (strict) with one that does not (non-strict). Although data leakage from pretrained pLMs cause a measurable inflation of testing scores, we find that this does not necessarily extend to other, non-paired biological tasks such as protein keyword annotation. Further, we find no connection between the context lengths of pLMs and the performance of pLM-based PPI inference methods on proteins with sequence lengths that surpass it. Furthermore, we show that pLM-based and non-pLM-based models fail to generalize in tasks such as prediction of the human-SARS-CoV-2 PPIs or the effect of point mutations on binding affinities. This study demonstrates the importance of extending existing protocols for the evaluation of pLM-based models applied to paired biological datasets and identifies areas of weakness of current pLM models. The usage of pretrained protein language models (pLMs) is rapidly growing. However, Szymborski and Emad find that pretrained pLMs can be a source of data leakage in the task of protein–protein interaction inference, showing inflated performance scores.&lt;/p&gt;</content:encoded></item><item><title>Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3-D Object Detection</title><link>https://doi.org/10.1109/tits.2026.3659175</link><guid>10.1109/tits.2026.3659175</guid><pubDate>Thu, 12 Feb 2026 21:01:24 +0000</pubDate><dc:creator>Yifan Wang</dc:creator><dc:creator>Xiaochen Yang</dc:creator><dc:creator>Fanqi Pu</dc:creator><dc:creator>Qingmin Liao</dc:creator><dc:creator>Wenming Yang</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2026.3659175</prism:doi><description>Monocular 3D object detection has received considerable attention for its simplicity and low cost. Existing methods typically follow conventional 2D detection paradigms, first locating object centers and then predicting 3D attributes via neighboring features. However, these approaches mainly focus on local information, which may limit the model’s global context awareness and result in missed detections, as the global context provides semantic and spatial dependencies essential for detecting small objects in cluttered or occluded environments. In addition, due to large variation in object scales across different scenes and depths, inaccurate receptive fields often lead to background noise and degraded feature representation. To address these issues, we introduce MonoASRH, a novel monocular 3D detection framework composed of Efficient Hybrid Feature Aggregation Module (EH-FAM) and Adaptive Scale-Aware 3D Regression Head (ASRH). Specifically, EH-FAM employs multi-head attention with a global receptive field to extract semantic features and leverages lightweight convolutional modules to efficiently aggregate visual features across different scales, enhancing small-scale object detection. The ASRH encodes 2D bounding box dimensions and then fuses scale features with the semantic features aggregated by EH-FAM through a scale-semantic feature fusion module. The scale-semantic feature fusion module guides ASRH in learning dynamic receptive field offsets, incorporating scale information into 3D position prediction for better scale-awareness. Extensive experiments on the KITTI and Waymo datasets demonstrate that MonoASRH achieves state-of-the-art performance. The code and model are released at https://github.com/WYFDUT/MonoASRH
Published: 2026-02-12T21:01:24+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Wang; Xiaochen Yang; Fanqi Pu; Qingmin Liao; Wenming Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2026.3659175"&gt;10.1109/tits.2026.3659175&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection has received considerable attention for its simplicity and low cost. Existing methods typically follow conventional 2D detection paradigms, first locating object centers and then predicting 3D attributes via neighboring features. However, these approaches mainly focus on local information, which may limit the model’s global context awareness and result in missed detections, as the global context provides semantic and spatial dependencies essential for detecting small objects in cluttered or occluded environments. In addition, due to large variation in object scales across different scenes and depths, inaccurate receptive fields often lead to background noise and degraded feature representation. To address these issues, we introduce MonoASRH, a novel monocular 3D detection framework composed of Efficient Hybrid Feature Aggregation Module (EH-FAM) and Adaptive Scale-Aware 3D Regression Head (ASRH). Specifically, EH-FAM employs multi-head attention with a global receptive field to extract semantic features and leverages lightweight convolutional modules to efficiently aggregate visual features across different scales, enhancing small-scale object detection. The ASRH encodes 2D bounding box dimensions and then fuses scale features with the semantic features aggregated by EH-FAM through a scale-semantic feature fusion module. The scale-semantic feature fusion module guides ASRH in learning dynamic receptive field offsets, incorporating scale information into 3D position prediction for better scale-awareness. Extensive experiments on the KITTI and Waymo datasets demonstrate that MonoASRH achieves state-of-the-art performance. The code and model are released at https://github.com/WYFDUT/MonoASRH&lt;/p&gt;</content:encoded></item><item><title>Multimodal Action Recognition for Manufacturing Assembly Task through Spatio-temporal Knowledge Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104225</link><guid>10.1016/j.inffus.2026.104225</guid><pubDate>Fri, 13 Feb 2026 00:47:50 +0000</pubDate><dc:creator>Mahdi Bonyani</dc:creator><dc:creator>Maryam Soleymani</dc:creator><dc:creator>Chao Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104225</prism:doi><description>This paper introduces a novel multimodal action recognition framework designed to address the complexities of human activity recognition in manufacturing assembly tasks through spatio-temporal knowledge fusion. Traditional unimodal and naive multimodal fusion methods often fail to capture the intricate dependencies across space, time, and modality, especially in real-world industrial environments where actions are subtle, repetitive, and context-dependent. To overcome these challenges, we propose a unified architecture that incorporates: (i) a Multi-Stage Hierarchical Reconnection module for robust spatial and temporal feature disentanglement and reintegration; (ii) a spatio-temporal regularization technique, Optimized-MixUp (OMU), that jointly augments data along spatial and temporal axes to improve generalization; and (iii) a Cross-Modal Auxiliary Feature Learning component to enhance late fusion by exploiting modality-specific complementary information. Extensive experiments conducted on four benchmark datasets, NTU RGB+D, NTU RGB+D120, HA4M, and Northwestern-UCLA, demonstrate that our method outperforms recent state-of-the-art approaches, achieving top-1 accuracy of 98.4%, 93.5%, 92.0%, and 97.3%, respectively. These results confirm the framework’s robustness, scalability, and suitability for high-precision human activity understanding in manufacturing environments. The proposed method advances the field of information fusion by offering a principled approach to integrating heterogeneous spatio-temporal data for real-world action recognition tasks.
Published: 2026-02-13T00:47:50+00:00
Venue: Information Fusion
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mahdi Bonyani; Maryam Soleymani; Chao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104225"&gt;10.1016/j.inffus.2026.104225&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;This paper introduces a novel multimodal action recognition framework designed to address the complexities of human activity recognition in manufacturing assembly tasks through spatio-temporal knowledge fusion. Traditional unimodal and naive multimodal fusion methods often fail to capture the intricate dependencies across space, time, and modality, especially in real-world industrial environments where actions are subtle, repetitive, and context-dependent. To overcome these challenges, we propose a unified architecture that incorporates: (i) a Multi-Stage Hierarchical Reconnection module for robust spatial and temporal feature disentanglement and reintegration; (ii) a spatio-temporal regularization technique, Optimized-MixUp (OMU), that jointly augments data along spatial and temporal axes to improve generalization; and (iii) a Cross-Modal Auxiliary Feature Learning component to enhance late fusion by exploiting modality-specific complementary information. Extensive experiments conducted on four benchmark datasets, NTU RGB+D, NTU RGB+D120, HA4M, and Northwestern-UCLA, demonstrate that our method outperforms recent state-of-the-art approaches, achieving top-1 accuracy of 98.4%, 93.5%, 92.0%, and 97.3%, respectively. These results confirm the framework’s robustness, scalability, and suitability for high-precision human activity understanding in manufacturing environments. The proposed method advances the field of information fusion by offering a principled approach to integrating heterogeneous spatio-temporal data for real-world action recognition tasks.&lt;/p&gt;</content:encoded></item><item><title>C-WOE: Clustering for Out-of-Distribution Detection Learning with Wild Outlier Exposure</title><link>https://doi.org/10.1109/tip.2026.3662593</link><guid>10.1109/tip.2026.3662593</guid><pubDate>Fri, 13 Feb 2026 20:52:17 +0000</pubDate><dc:creator>Long Lan</dc:creator><dc:creator>Zhaohui Hu</dc:creator><dc:creator>He Li</dc:creator><dc:creator>Tongliang Liu</dc:creator><dc:creator>Xinwang Liu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3662593</prism:doi><description>Out-of-distribution (OOD) detection plays a crucial role as a mechanism for handling anomalies in computer vision systems. Among existing approaches, outlier exposure (OE), which trains the model with an additional auxiliary OOD dataset, has demonstrated strong effectiveness. However, acquiring clean and well-curated auxiliary OOD data is often infeasible, particularly within large and complex systems. Alternatively, wild outliers, i.e., unlabeled samples collected directly in deployment environments, are abundant and easy to obtain, and recent studies have shown that they can substantially benefit OOD detection learning. Nevertheless, wild outliers typically contain a mixture of in-distribution (ID) and OOD samples. Directly using them as auxiliary OOD data unavoidably exposes the model to adverse supervision signals arising from the contained ID samples. Yet existing methods still lack an effective strategy that can fully leverage wild outliers while suppressing the negative influence introduced by their ID subset. To this end, we propose a simple yet effective method named Clustering for Wild Outlier Exposure (C-WOE), which alleviates the adverse effect of the ID samples contained within wild outliers by reweighting them. Specifically, C-WOE assigns higher weights to real OOD samples and lower weights to ID samples and dynamically updates these weights during training. Theoretically, we establish solid guarantees for the proposed method. Empirically, extensive experiments conducted on various real-world benchmarks and simulated datasets demonstrate that C-WOE notably achieves superior performance compared with state-of-the-art methods, validating its reliability in image processing applications.
Published: 2026-02-13T20:52:17+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Long Lan; Zhaohui Hu; He Li; Tongliang Liu; Xinwang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3662593"&gt;10.1109/tip.2026.3662593&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Out-of-distribution (OOD) detection plays a crucial role as a mechanism for handling anomalies in computer vision systems. Among existing approaches, outlier exposure (OE), which trains the model with an additional auxiliary OOD dataset, has demonstrated strong effectiveness. However, acquiring clean and well-curated auxiliary OOD data is often infeasible, particularly within large and complex systems. Alternatively, wild outliers, i.e., unlabeled samples collected directly in deployment environments, are abundant and easy to obtain, and recent studies have shown that they can substantially benefit OOD detection learning. Nevertheless, wild outliers typically contain a mixture of in-distribution (ID) and OOD samples. Directly using them as auxiliary OOD data unavoidably exposes the model to adverse supervision signals arising from the contained ID samples. Yet existing methods still lack an effective strategy that can fully leverage wild outliers while suppressing the negative influence introduced by their ID subset. To this end, we propose a simple yet effective method named Clustering for Wild Outlier Exposure (C-WOE), which alleviates the adverse effect of the ID samples contained within wild outliers by reweighting them. Specifically, C-WOE assigns higher weights to real OOD samples and lower weights to ID samples and dynamically updates these weights during training. Theoretically, we establish solid guarantees for the proposed method. Empirically, extensive experiments conducted on various real-world benchmarks and simulated datasets demonstrate that C-WOE notably achieves superior performance compared with state-of-the-art methods, validating its reliability in image processing applications.&lt;/p&gt;</content:encoded></item><item><title>VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization</title><link>https://arxiv.org/abs/2602.09934v1</link><guid>http://arxiv.org/abs/2602.09934v1</guid><pubDate>Tue, 10 Feb 2026 16:08:19 +0000</pubDate><dc:creator>Yikun Liu</dc:creator><dc:creator>Yuan Liu</dc:creator><dc:creator>Shangzhe Di</dc:creator><dc:creator>Haicheng Wang</dc:creator><dc:creator>Zhongyin Zhao</dc:creator><dc:creator>Le Tian</dc:creator><dc:creator>Xiao Zhou</dc:creator><dc:creator>Jie Zhou</dc:creator><dc:creator>Jiangchao Yao</dc:creator><dc:creator>Yanfeng Wang</dc:creator><dc:creator>Weidi Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.
Published: 2026-02-10T16:08:19+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yikun Liu; Yuan Liu; Shangzhe Di; Haicheng Wang; Zhongyin Zhao; Le Tian; Xiao Zhou; Jie Zhou; Jiangchao Yao; Yanfeng Wang; Weidi Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.&lt;/p&gt;</content:encoded></item><item><title>SAM-IAD: Injecting Specific Knowledge into SAM for Industrial Anomaly Detection</title><link>https://doi.org/10.1016/j.knosys.2026.115515</link><guid>10.1016/j.knosys.2026.115515</guid><pubDate>Fri, 13 Feb 2026 16:44:55 +0000</pubDate><dc:creator>Yichi Chen</dc:creator><dc:creator>Bin Chen</dc:creator><dc:creator>Weizhi Xian</dc:creator><dc:creator>Junjie Wang</dc:creator><dc:creator>Xinyi Gong</dc:creator><dc:creator>Jianwen Han</dc:creator><dc:creator>Xian Tao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115515</prism:doi><description>Unsupervised visual anomaly detection plays a crucial role in smart manufacturing, aiming to identify deviations from normal patterns by training solely on normal samples. Existing methods model the normal distribution from images or pre-trained features, neglecting the complementary between fine-grained and semantic information, which limits the precision and generalization of anomaly segmentation. To this end, a novel framework named SAM-IAD is proposed to effectively inject knowledge from both image and feature spaces into the Segment Anything Model (SAM) for high-quality anomaly segmentation and detection. Specifically, anomaly knowledge represented by image and feature residuals is first obtained through the proposed cross color space mapping. Subsequently, the image residuals are used as input to interact with the feature residuals, which serve as mask prompts, effectively injecting the specific anomaly knowledge into SAM. Additionally, learnable anomaly related adapters are introduced into the frozen image encoder of SAM for efficient fine-tuning, aiming to preserve the general prior knowledge in the SAM so that various abnormal objects can be precisely segmented. Extensive experimental results on the MVTec AD, VisA , and DAGM benchmarks demonstrate that the proposed method achieves state-of-the-art performance in anomaly detection, particularly excelling in anomaly segmentation.
Published: 2026-02-13T16:44:55+00:00
Venue: Knowledge-Based Systems
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yichi Chen; Bin Chen; Weizhi Xian; Junjie Wang; Xinyi Gong; Jianwen Han; Xian Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115515"&gt;10.1016/j.knosys.2026.115515&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised visual anomaly detection plays a crucial role in smart manufacturing, aiming to identify deviations from normal patterns by training solely on normal samples. Existing methods model the normal distribution from images or pre-trained features, neglecting the complementary between fine-grained and semantic information, which limits the precision and generalization of anomaly segmentation. To this end, a novel framework named SAM-IAD is proposed to effectively inject knowledge from both image and feature spaces into the Segment Anything Model (SAM) for high-quality anomaly segmentation and detection. Specifically, anomaly knowledge represented by image and feature residuals is first obtained through the proposed cross color space mapping. Subsequently, the image residuals are used as input to interact with the feature residuals, which serve as mask prompts, effectively injecting the specific anomaly knowledge into SAM. Additionally, learnable anomaly related adapters are introduced into the frozen image encoder of SAM for efficient fine-tuning, aiming to preserve the general prior knowledge in the SAM so that various abnormal objects can be precisely segmented. Extensive experimental results on the MVTec AD, VisA , and DAGM benchmarks demonstrate that the proposed method achieves state-of-the-art performance in anomaly detection, particularly excelling in anomaly segmentation.&lt;/p&gt;</content:encoded></item><item><title>A Transformer-Based Tracker Integrating Motion and Representation Information</title><link>https://doi.org/10.1109/tmm.2026.3660077</link><guid>10.1109/tmm.2026.3660077</guid><pubDate>Fri, 13 Feb 2026 20:50:39 +0000</pubDate><dc:creator>Yuanhui Wang</dc:creator><dc:creator>Ben Ye</dc:creator><dc:creator>Zhanchuan Cai</dc:creator><dc:creator>Hao Wu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660077</prism:doi><description>The appearance information of the target has been used as the only tracking cue for most trackers to locate the target in the video. However, when the surrounding environment changes drastically or there are similar interference targets, it usually causes target drift. We propose a tracker named MRTrack, a transformer-based spatiotemporal information de coupling network architecture to enhance the target tracking capability. We designed two training schemes to explore the effective integration of motion cues derived from optical flow with representation information. The first approach integrates the target's motion and representation information for training. The second scheme is a step-by-step training, where the target motion information is first learned, and the learned model is used for representation learning. We compare the two training methods on five generic tracking datasets. The experiment results indicate that the first training approach can better integrate motion and representation information, leading to more precise tracking results for MRTrack compared to solely relying on the appearance model. In addition, optical flow cues are used only in the training phase to guide the tracker in understanding motion information, and no additional cost is incurred during tracking inference.
Published: 2026-02-13T20:50:39+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanhui Wang; Ben Ye; Zhanchuan Cai; Hao Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660077"&gt;10.1109/tmm.2026.3660077&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;The appearance information of the target has been used as the only tracking cue for most trackers to locate the target in the video. However, when the surrounding environment changes drastically or there are similar interference targets, it usually causes target drift. We propose a tracker named MRTrack, a transformer-based spatiotemporal information de coupling network architecture to enhance the target tracking capability. We designed two training schemes to explore the effective integration of motion cues derived from optical flow with representation information. The first approach integrates the target&amp;#x27;s motion and representation information for training. The second scheme is a step-by-step training, where the target motion information is first learned, and the learned model is used for representation learning. We compare the two training methods on five generic tracking datasets. The experiment results indicate that the first training approach can better integrate motion and representation information, leading to more precise tracking results for MRTrack compared to solely relying on the appearance model. In addition, optical flow cues are used only in the training phase to guide the tracker in understanding motion information, and no additional cost is incurred during tracking inference.&lt;/p&gt;</content:encoded></item><item><title>RAM-Net: Expressive Linear Attention with Selectively Addressable Memory</title><link>https://arxiv.org/abs/2602.11958v1</link><guid>http://arxiv.org/abs/2602.11958v1</guid><pubDate>Thu, 12 Feb 2026 13:55:29 +0000</pubDate><dc:creator>Kaicheng Xiao</dc:creator><dc:creator>Haotian Li</dc:creator><dc:creator>Liran Dong</dc:creator><dc:creator>Guoliang Xing</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency of linear models. The core of RAM-Net maps inputs to high-dimensional sparse vectors serving as explicit addresses, allowing the model to selectively access a massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-the-art baselines in fine-grained long-range retrieval tasks and achieves competitive performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.
Published: 2026-02-12T13:55:29+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaicheng Xiao; Haotian Li; Liran Dong; Guoliang Xing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;While linear attention architectures offer efficient inference, compressing unbounded history into a fixed-size memory inherently limits expressivity and causes information loss. To address this limitation, we introduce Random Access Memory Network (RAM-Net), a novel architecture designed to bridge the gap between the representational capacity of full attention and the memory efficiency of linear models. The core of RAM-Net maps inputs to high-dimensional sparse vectors serving as explicit addresses, allowing the model to selectively access a massive memory state. This design enables exponential state size scaling without additional parameters, which significantly mitigates signal interference and enhances retrieval fidelity. Moreover, the inherent sparsity ensures exceptional computational efficiency, as state updates are confined to minimal entries. Extensive experiments demonstrate that RAM-Net consistently surpasses state-of-the-art baselines in fine-grained long-range retrieval tasks and achieves competitive performance in standard language modeling and zero-shot commonsense reasoning benchmarks, validating its superior capability to capture complex dependencies with significantly reduced computational overhead.&lt;/p&gt;</content:encoded></item><item><title>Toward Safe Driving: Efficient Detection of Small Blurred Signs in Real-World Scenarios</title><link>https://doi.org/10.1109/tits.2026.3657922</link><guid>10.1109/tits.2026.3657922</guid><pubDate>Thu, 12 Feb 2026 21:01:24 +0000</pubDate><dc:creator>Yibo Wang</dc:creator><dc:creator>Dekui Wang</dc:creator><dc:creator>Jun Feng</dc:creator><dc:creator>Qirong Bo</dc:creator><dc:creator>Yaqiong Xing</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Xingxing Hao</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2026.3657922</prism:doi><description>Accurate traffic sign recognition is critical for safe driving, as over half of traffic accidents stem from drivers’ negligence of traffic signs. Thus, developing robust traffic sign detection methods is essential to improve road safety. While existing object detection methods have achieved remarkable success, their performance in traffic sign detection is often limited by small object sizes and low-resolution appearances. To address these issues, this study proposes a novel traffic sign detection framework with three innovative components for precise localization and classification: 1) a hierarchical feature aggregation module that emphasizes high-level semantic information for traffic sign localization; 2) a cross-layer semantic residual network that enhances recognition of small and blurred signs via hierarchical feature interaction and fusion; 3) a lightweight feature alignment unit that bridges semantic gaps between cross-level representations. These components jointly tackle the challenges of detecting small and blurred traffic signs in real-world driving scenarios. Experiments were conducted on three public datasets (TT100K, CCTSDB2021, GTSDB). Results show the proposed model outperforms other methods with comparable parameters. Additionally, dynamic motion blur augmentation was applied to datasets to simulate real driving scenarios, and experiments confirm the proposed method achieves state-of-the-art performance under such challenging conditions. Code is publicly available at https://github.com/Mo7nex/SAttFusion-YOLO
Published: 2026-02-12T21:01:24+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yibo Wang; Dekui Wang; Jun Feng; Qirong Bo; Yaqiong Xing; Wei Zhou; Xingxing Hao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2026.3657922"&gt;10.1109/tits.2026.3657922&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate traffic sign recognition is critical for safe driving, as over half of traffic accidents stem from drivers’ negligence of traffic signs. Thus, developing robust traffic sign detection methods is essential to improve road safety. While existing object detection methods have achieved remarkable success, their performance in traffic sign detection is often limited by small object sizes and low-resolution appearances. To address these issues, this study proposes a novel traffic sign detection framework with three innovative components for precise localization and classification: 1) a hierarchical feature aggregation module that emphasizes high-level semantic information for traffic sign localization; 2) a cross-layer semantic residual network that enhances recognition of small and blurred signs via hierarchical feature interaction and fusion; 3) a lightweight feature alignment unit that bridges semantic gaps between cross-level representations. These components jointly tackle the challenges of detecting small and blurred traffic signs in real-world driving scenarios. Experiments were conducted on three public datasets (TT100K, CCTSDB2021, GTSDB). Results show the proposed model outperforms other methods with comparable parameters. Additionally, dynamic motion blur augmentation was applied to datasets to simulate real driving scenarios, and experiments confirm the proposed method achieves state-of-the-art performance under such challenging conditions. Code is publicly available at https://github.com/Mo7nex/SAttFusion-YOLO&lt;/p&gt;</content:encoded></item><item><title>Spatial Multimodal Knowledge Driven 3D Scene Graph Prediction with Vision-Language Model</title><link>https://doi.org/10.1109/tcsvt.2026.3664122</link><guid>10.1109/tcsvt.2026.3664122</guid><pubDate>Thu, 12 Feb 2026 21:02:14 +0000</pubDate><dc:creator>Haoran Hou</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Zijie Wu</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Yaonan Wang</dc:creator><dc:creator>Ajmal Mian</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3664122</prism:doi><description>In-depth understanding of 3D environments not only involves locating and recognizing individual objects but also requires inferring the relationships and interactions among them. However, most existing methods heavily rely on scene-specific contents, which leads to poor performance due to the noisy, cluttered, and partial nature of real-world 3D scenes. In this work, we find that the inherently hierarchical structures of 3D environments, derived from support relationships, aid in the automatic association of semantic and spatial arrangements of objects and provide rich geometric and topological information independent of specific scenarios. To this end, we propose a 3D scene graph generation model that leverages the hierarchical structures of 3D environments as spatial multimodal knowledge to enhance 3D scene graph generation. Specifically, we first devise a cross-modal tuning approach, where a visually-prompted vision language model is learned to infer the support relationships between objects in a low-resource way. Subsequently, we build a hierarchical visual graph and hierarchical symbolic knowledge graph using the fine-tuned vision language model to extract contextualized visual contents and relevant textual facts, respectively. Finally, we progressively accumulate 3D spatial multimodal knowledge about the hierarchical structures by correlating contextualized visual contents and textual facts using a novel graph reasoning network. In addition, to better evaluate the performance of 3D scene graph generation models, we propose a new benchmark 3DSSG-M by reorganizing the widely-used 3D scene graph generation dataset 3DSSG. This reorganization balances the predicate distribution of 3DSSG and reduces the influence of frequency bias. Extensive results and ablations attest to the effectiveness of the hierarchical structures in 3D environments and demonstrate the superiority of our proposed method over current state-of-the-art competitors.
Published: 2026-02-12T21:02:14+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoran Hou; Mingtao Feng; Zijie Wu; Yulan Guo; Yaonan Wang; Ajmal Mian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3664122"&gt;10.1109/tcsvt.2026.3664122&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;In-depth understanding of 3D environments not only involves locating and recognizing individual objects but also requires inferring the relationships and interactions among them. However, most existing methods heavily rely on scene-specific contents, which leads to poor performance due to the noisy, cluttered, and partial nature of real-world 3D scenes. In this work, we find that the inherently hierarchical structures of 3D environments, derived from support relationships, aid in the automatic association of semantic and spatial arrangements of objects and provide rich geometric and topological information independent of specific scenarios. To this end, we propose a 3D scene graph generation model that leverages the hierarchical structures of 3D environments as spatial multimodal knowledge to enhance 3D scene graph generation. Specifically, we first devise a cross-modal tuning approach, where a visually-prompted vision language model is learned to infer the support relationships between objects in a low-resource way. Subsequently, we build a hierarchical visual graph and hierarchical symbolic knowledge graph using the fine-tuned vision language model to extract contextualized visual contents and relevant textual facts, respectively. Finally, we progressively accumulate 3D spatial multimodal knowledge about the hierarchical structures by correlating contextualized visual contents and textual facts using a novel graph reasoning network. In addition, to better evaluate the performance of 3D scene graph generation models, we propose a new benchmark 3DSSG-M by reorganizing the widely-used 3D scene graph generation dataset 3DSSG. This reorganization balances the predicate distribution of 3DSSG and reduces the influence of frequency bias. Extensive results and ablations attest to the effectiveness of the hierarchical structures in 3D environments and demonstrate the superiority of our proposed method over current state-of-the-art competitors.&lt;/p&gt;</content:encoded></item></channel></rss>