<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 06 Feb 2026 03:32:19 +0000</lastBuildDate><item><title>Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization</title><link>https://doi.org/10.1109/tpami.2026.3661049</link><guid>10.1109/tpami.2026.3661049</guid><pubDate>Wed, 04 Feb 2026 20:47:02 +0000</pubDate><dc:creator>De Cheng</dc:creator><dc:creator>Zhipeng Xu</dc:creator><dc:creator>Xinyang Jiang</dc:creator><dc:creator>Dongsheng Li</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3661049</prism:doi><description>Domain Generalization (DG) seeks to develop models that perform well on unseen target domains by learning domain-invariant representations. Recent advances in pre-trained Visual Foundation Models (VFMs), such as CLIP, have shown strong potential for enhancing DG through prompt tuning. However, existing VFM-based prompt tuning methods often focus on task-specific adaptation rather than disentangling domain invariant features, leaving cross-domain generalization insufficiently explored. In this paper, we address this challenge by fully leveraging the controllable and flexible language prompt in VFMs. Observing that the text modality is inherently rich in semantics and easier to disentangle, we propose a novel frame work termed Prompt Disentanglement via Language Guidance and Representation Alignment (PADG). PADG first employs a large language model (LLM) to disentangle textual prompts into domain-invariant and domain-specific components, which then guide the learning of domain-invariant visual representations. To complement the limitations of text-only guidance, we further introduce the Worst Explicit Representation Alignment (WERA) module, which enhances visual invariance by simulating bounded domain shifts through learnable stylization prompts and aligning representations between original and perturbed samples. Extensive experiments on mainstream DG benchmarks, including PACS, VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that PADG consistently outperforms existing state of-the-art methods, validating its effectiveness in robust domain invariant representation learning. The code is available at: https://anonymous.4open.science/r/paper-5403/
Published: 2026-02-04T20:47:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; De Cheng; Zhipeng Xu; Xinyang Jiang; Dongsheng Li; Nannan Wang; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3661049"&gt;10.1109/tpami.2026.3661049&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Domain Generalization (DG) seeks to develop models that perform well on unseen target domains by learning domain-invariant representations. Recent advances in pre-trained Visual Foundation Models (VFMs), such as CLIP, have shown strong potential for enhancing DG through prompt tuning. However, existing VFM-based prompt tuning methods often focus on task-specific adaptation rather than disentangling domain invariant features, leaving cross-domain generalization insufficiently explored. In this paper, we address this challenge by fully leveraging the controllable and flexible language prompt in VFMs. Observing that the text modality is inherently rich in semantics and easier to disentangle, we propose a novel frame work termed Prompt Disentanglement via Language Guidance and Representation Alignment (PADG). PADG first employs a large language model (LLM) to disentangle textual prompts into domain-invariant and domain-specific components, which then guide the learning of domain-invariant visual representations. To complement the limitations of text-only guidance, we further introduce the Worst Explicit Representation Alignment (WERA) module, which enhances visual invariance by simulating bounded domain shifts through learnable stylization prompts and aligning representations between original and perturbed samples. Extensive experiments on mainstream DG benchmarks, including PACS, VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that PADG consistently outperforms existing state of-the-art methods, validating its effectiveness in robust domain invariant representation learning. The code is available at: https://anonymous.4open.science/r/paper-5403/&lt;/p&gt;</content:encoded></item><item><title>Adaptive image zoom-in with bounding box transformation for UAV object detection</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.036</link><guid>10.1016/j.isprsjprs.2026.01.036</guid><pubDate>Thu, 05 Feb 2026 11:10:41 +0000</pubDate><dc:creator>Tao Wang</dc:creator><dc:creator>Chenyu Lin</dc:creator><dc:creator>Chenwei Tang</dc:creator><dc:creator>Jizhe Zhou</dc:creator><dc:creator>Deng Xiong</dc:creator><dc:creator>Jianan Li</dc:creator><dc:creator>Jian Zhao</dc:creator><dc:creator>Jiancheng Lv</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.036</prism:doi><description>Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: (i) How to conduct non-uniform zooming on each image efficiently? (ii) How to enable object detection training and inference with the zoomed image space? Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code .
Published: 2026-02-05T11:10:41+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Wang; Chenyu Lin; Chenwei Tang; Jizhe Zhou; Deng Xiong; Jianan Li; Jian Zhao; Jiancheng Lv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.036"&gt;10.1016/j.isprsjprs.2026.01.036&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: (i) How to conduct non-uniform zooming on each image efficiently? (ii) How to enable object detection training and inference with the zoomed image space? Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code .&lt;/p&gt;</content:encoded></item><item><title>Cross-modal contrastive learning for 3D point cloud-text fusion via implicit semantic alignment</title><link>https://doi.org/10.1016/j.inffus.2026.104208</link><guid>10.1016/j.inffus.2026.104208</guid><pubDate>Thu, 05 Feb 2026 01:09:00 +0000</pubDate><dc:creator>Xiangtian Zheng</dc:creator><dc:creator>Chen Ji</dc:creator><dc:creator>Wei Cai</dc:creator><dc:creator>Xianghua Tang</dc:creator><dc:creator>Xiaolin Yang</dc:creator><dc:creator>Liang Cheng</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104208</prism:doi><description>While Transformers have historically dominated point cloud analysis, their quadratic computational complexity O ( N 2 ) poses a significant bottleneck for processing large-scale 3D data. Recently, State Space Models (SSMs), particularly Mamba, have emerged as a potent alternative due to their linear complexity and robust long-range modeling capabilities. In this work, we propose PST-Mamba, a novel multimodal SSM-based framework designed for efficient and holistic point cloud understanding. To bridge the dimensionality gap between unstructured 3D geometry and 1D sequential SSMs, we introduce Semantic-Guided Manifold Unfolding. Unlike rigid geometric scanning, this strategy utilizes implicit textual embeddings to guide the projection of point clouds into 1D sequences, effectively preserving both topological continuity and semantic coherence. Furthermore, we integrate implicit semantic prompts to enrich the geometric features with high-level contextual priors, facilitating a deep fusion of visual and linguistic information. Extensive experiments demonstrate that PST-Mamba achieves state-of-the-art performance across multiple benchmarks (e.g., 95.21% OA on ModelNet40 and 90.09% on ScanObjectNN PB-T50-RS). Remarkably, compared to leading Transformer-based models, PST-Mamba reduces parameters by 54.8% and FLOPs by 70.5%, showcasing its immense potential for large-scale, real-time 3D multimodal applications.
Published: 2026-02-05T01:09:00+00:00
Venue: Information Fusion
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangtian Zheng; Chen Ji; Wei Cai; Xianghua Tang; Xiaolin Yang; Liang Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104208"&gt;10.1016/j.inffus.2026.104208&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;While Transformers have historically dominated point cloud analysis, their quadratic computational complexity O ( N 2 ) poses a significant bottleneck for processing large-scale 3D data. Recently, State Space Models (SSMs), particularly Mamba, have emerged as a potent alternative due to their linear complexity and robust long-range modeling capabilities. In this work, we propose PST-Mamba, a novel multimodal SSM-based framework designed for efficient and holistic point cloud understanding. To bridge the dimensionality gap between unstructured 3D geometry and 1D sequential SSMs, we introduce Semantic-Guided Manifold Unfolding. Unlike rigid geometric scanning, this strategy utilizes implicit textual embeddings to guide the projection of point clouds into 1D sequences, effectively preserving both topological continuity and semantic coherence. Furthermore, we integrate implicit semantic prompts to enrich the geometric features with high-level contextual priors, facilitating a deep fusion of visual and linguistic information. Extensive experiments demonstrate that PST-Mamba achieves state-of-the-art performance across multiple benchmarks (e.g., 95.21% OA on ModelNet40 and 90.09% on ScanObjectNN PB-T50-RS). Remarkably, compared to leading Transformer-based models, PST-Mamba reduces parameters by 54.8% and FLOPs by 70.5%, showcasing its immense potential for large-scale, real-time 3D multimodal applications.&lt;/p&gt;</content:encoded></item><item><title>GADet: Geometry-Aware Oriented Object Detection for Remote Sensing</title><link>https://doi.org/10.1016/j.knosys.2026.115475</link><guid>10.1016/j.knosys.2026.115475</guid><pubDate>Wed, 04 Feb 2026 07:59:10 +0000</pubDate><dc:creator>Haodong Li</dc:creator><dc:creator>Yan Gong</dc:creator><dc:creator>Xinyu Zhang</dc:creator><dc:creator>Ziying Song</dc:creator><dc:creator>Lei Yang</dc:creator><dc:creator>Haicheng Qu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115475</prism:doi><description>Oriented object detection in remote sensing images is a key technology for accurately perceiving the geometric properties of objects on the Earth’s surface, playing a significant role in smart cities, national defense and security, and disaster emergency response. However, existing anchor-free methods have obvious limitations in geometric feature adaptation and orientation-aware modeling, and their large number of parameters makes real-time deployment difficult. To address these issues, we propose the geometry-aware detector GADet, a single-stage anchor-free detector comprising three key components: a geometrically structured adaptive convolution (GSA-Conv) module for enhanced feature extraction, a rotation-sensitive attention (RSA) module for robust orientation awareness, and a channel-isomorphic adaptive (CIA) pruning method for model compression. Comprehensive experiments demonstrate that GADet achieves mAP scores of 76.90%, 70.20%, and 97.47% on the DOTA-v1.0, DIOR-R, and UCAS-AOD datasets, respectively, while running at 56.5 FPS, achieving the optimal balance between accuracy and efficiency compared to recent state-of-the-art methods.
Published: 2026-02-04T07:59:10+00:00
Venue: Knowledge-Based Systems
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haodong Li; Yan Gong; Xinyu Zhang; Ziying Song; Lei Yang; Haicheng Qu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115475"&gt;10.1016/j.knosys.2026.115475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Oriented object detection in remote sensing images is a key technology for accurately perceiving the geometric properties of objects on the Earth’s surface, playing a significant role in smart cities, national defense and security, and disaster emergency response. However, existing anchor-free methods have obvious limitations in geometric feature adaptation and orientation-aware modeling, and their large number of parameters makes real-time deployment difficult. To address these issues, we propose the geometry-aware detector GADet, a single-stage anchor-free detector comprising three key components: a geometrically structured adaptive convolution (GSA-Conv) module for enhanced feature extraction, a rotation-sensitive attention (RSA) module for robust orientation awareness, and a channel-isomorphic adaptive (CIA) pruning method for model compression. Comprehensive experiments demonstrate that GADet achieves mAP scores of 76.90%, 70.20%, and 97.47% on the DOTA-v1.0, DIOR-R, and UCAS-AOD datasets, respectively, while running at 56.5 FPS, achieving the optimal balance between accuracy and efficiency compared to recent state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Dynamic High-frequency Convolution for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2602.02969v1</link><guid>http://arxiv.org/abs/2602.02969v1</guid><pubDate>Tue, 03 Feb 2026 01:07:55 +0000</pubDate><dc:creator>Ruojing Li</dc:creator><dc:creator>Chao Xiao</dc:creator><dc:creator>Qian Yin</dc:creator><dc:creator>Wei An</dc:creator><dc:creator>Nuo Chen</dc:creator><dc:creator>Xinyi Ying</dc:creator><dc:creator>Miao Li</dc:creator><dc:creator>Yingqian Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TCSVT.2026.3661285</prism:doi><description>Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.
Published: 2026-02-03T01:07:55+00:00
Venue: arXiv
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruojing Li; Chao Xiao; Qian Yin; Wei An; Nuo Chen; Xinyi Ying; Miao Li; Yingqian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TCSVT.2026.3661285"&gt;10.1109/TCSVT.2026.3661285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.&lt;/p&gt;</content:encoded></item><item><title>Relaxed Knowledge Distillation</title><link>https://doi.org/10.1007/s11263-025-02705-y</link><guid>10.1007/s11263-025-02705-y</guid><pubDate>Wed, 04 Feb 2026 06:42:54 +0000</pubDate><dc:creator>Zheng Qu</dc:creator><dc:creator>Xiwen Yao</dc:creator><dc:creator>Xuguang Yang</dc:creator><dc:creator>Jie Tang</dc:creator><dc:creator>Lang Li</dc:creator><dc:creator>Gong Cheng</dc:creator><dc:creator>Junwei Han</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02705-y</prism:doi><description>Knowledge distillation, aiming to improve a compact student model using supervision from another cumbersome teacher model, has been a quite prevalent technique for model compression on various computer vision tasks. Existing methods mainly adopt a one-to-one knowledge transfer, where the student model will be forced to achieve a specific result provided by the teacher model. However, the performance of this training paradigm will deteriorate as the model capacity gap expands, since high-level teacher knowledge is too abstract and difficult to understand for the student models with low capacity. Based on this, we propose a novel feature-based Knowledge distillation framework dubbed ReKD, which can provide the student model with multiple choices in feature distillation, thereby relaxing the alignment process in knowledge transfer. Specifically, we transform the teacher features into latent variables through variational inference, with the posterior following Gaussian distribution. It renders the feature knowledge into a region instead of a specific point in the distillation space, which enables the student features to select suitable distillation targets from learned distribution adaptively. Furthermore, to ensure the high quality of latent variables, we utilize the student features as prior to reversely regularize the posterior inspired by mutual learning. Experimental results on three typical visual recognition datasets i.e., CIFAR-100, ImageNet-1K, and MS-COCO, have significantly demonstrated the superiority of our proposed method.
Published: 2026-02-04T06:42:54+00:00
Venue: International Journal of Computer Vision
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheng Qu; Xiwen Yao; Xuguang Yang; Jie Tang; Lang Li; Gong Cheng; Junwei Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02705-y"&gt;10.1007/s11263-025-02705-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation, aiming to improve a compact student model using supervision from another cumbersome teacher model, has been a quite prevalent technique for model compression on various computer vision tasks. Existing methods mainly adopt a one-to-one knowledge transfer, where the student model will be forced to achieve a specific result provided by the teacher model. However, the performance of this training paradigm will deteriorate as the model capacity gap expands, since high-level teacher knowledge is too abstract and difficult to understand for the student models with low capacity. Based on this, we propose a novel feature-based Knowledge distillation framework dubbed ReKD, which can provide the student model with multiple choices in feature distillation, thereby relaxing the alignment process in knowledge transfer. Specifically, we transform the teacher features into latent variables through variational inference, with the posterior following Gaussian distribution. It renders the feature knowledge into a region instead of a specific point in the distillation space, which enables the student features to select suitable distillation targets from learned distribution adaptively. Furthermore, to ensure the high quality of latent variables, we utilize the student features as prior to reversely regularize the posterior inspired by mutual learning. Experimental results on three typical visual recognition datasets i.e., CIFAR-100, ImageNet-1K, and MS-COCO, have significantly demonstrated the superiority of our proposed method.&lt;/p&gt;</content:encoded></item><item><title>Complementary Mixture-of-Experts and Complementary Cross-Attention for Single Image Reflection Separation in the Wild</title><link>https://doi.org/10.1109/tip.2026.3659334</link><guid>10.1109/tip.2026.3659334</guid><pubDate>Wed, 04 Feb 2026 20:50:14 +0000</pubDate><dc:creator>Jonghyuk Park</dc:creator><dc:creator>Jae-Young Sim</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3659334</prism:doi><description>Single Image Reflection Separation (SIRS) aims to reconstruct both the transmitted and reflected images from a single image that contains a superimposition of both, captured through a glass-like reflective surface. Recent learning-based methods of SIRS have significantly improved performance on typical images with mild reflection artifacts; however, they often struggle with diverse images containing challenging reflections captured in the wild. In this paper, we propose a universal SIRS framework based on a flexible dual-stream architecture, capable of handling diverse reflection artifacts. Specifically, we incorporate a Mixture-of-Experts mechanism that dynamically assigns specialized experts to image patches based on spatially heterogeneous reflection characteristics. The assigned experts then cooperate to extract complementary features between the transmission and reflection streams in an adaptive manner. In addition, we leverage the multi-head attention mechanism of Transformers to simultaneously exploit both high and low cross-correlations, which are then complementarily used to facilitate adaptive inter-stream feature interactions. Experimental results evaluated on diverse real-world datasets demonstrate that the proposed method significantly outperforms existing state-of-the-art methods qualitatively and quantitatively.
Published: 2026-02-04T20:50:14+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jonghyuk Park; Jae-Young Sim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3659334"&gt;10.1109/tip.2026.3659334&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Single Image Reflection Separation (SIRS) aims to reconstruct both the transmitted and reflected images from a single image that contains a superimposition of both, captured through a glass-like reflective surface. Recent learning-based methods of SIRS have significantly improved performance on typical images with mild reflection artifacts; however, they often struggle with diverse images containing challenging reflections captured in the wild. In this paper, we propose a universal SIRS framework based on a flexible dual-stream architecture, capable of handling diverse reflection artifacts. Specifically, we incorporate a Mixture-of-Experts mechanism that dynamically assigns specialized experts to image patches based on spatially heterogeneous reflection characteristics. The assigned experts then cooperate to extract complementary features between the transmission and reflection streams in an adaptive manner. In addition, we leverage the multi-head attention mechanism of Transformers to simultaneously exploit both high and low cross-correlations, which are then complementarily used to facilitate adaptive inter-stream feature interactions. Experimental results evaluated on diverse real-world datasets demonstrate that the proposed method significantly outperforms existing state-of-the-art methods qualitatively and quantitatively.&lt;/p&gt;</content:encoded></item><item><title>SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?</title><link>https://arxiv.org/abs/2602.02765v1</link><guid>http://arxiv.org/abs/2602.02765v1</guid><pubDate>Mon, 02 Feb 2026 20:17:34 +0000</pubDate><dc:creator>Haruhiko Murata</dc:creator><dc:creator>Kazuhiro Hotta</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.
Published: 2026-02-02T20:17:34+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haruhiko Murata; Kazuhiro Hotta&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.&lt;/p&gt;</content:encoded></item><item><title>SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection</title><link>https://arxiv.org/abs/2602.01843v1</link><guid>http://arxiv.org/abs/2602.01843v1</guid><pubDate>Mon, 02 Feb 2026 09:15:29 +0000</pubDate><dc:creator>Qian Xu</dc:creator><dc:creator>Xi Li</dc:creator><dc:creator>Fei Gao</dc:creator><dc:creator>Jie Guo</dc:creator><dc:creator>Haojuan Yuan</dc:creator><dc:creator>Shuaipeng Fan</dc:creator><dc:creator>Mingjin Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.
Published: 2026-02-02T09:15:29+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Xu; Xi Li; Fei Gao; Jie Guo; Haojuan Yuan; Shuaipeng Fan; Mingjin Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.&lt;/p&gt;</content:encoded></item><item><title>SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation</title><link>https://arxiv.org/abs/2602.04712v1</link><guid>http://arxiv.org/abs/2602.04712v1</guid><pubDate>Wed, 04 Feb 2026 16:23:16 +0000</pubDate><dc:creator>David F. Ramirez</dc:creator><dc:creator>Tim Overman</dc:creator><dc:creator>Kristen Jaskie</dc:creator><dc:creator>Joe Marvin</dc:creator><dc:creator>Andreas Spanias</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.
Published: 2026-02-04T16:23:16+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David F. Ramirez; Tim Overman; Kristen Jaskie; Joe Marvin; Andreas Spanias&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.&lt;/p&gt;</content:encoded></item><item><title>Accurate Industrial Anomaly Detection and Localization using Weakly-Supervised Residual Transformers</title><link>https://doi.org/10.1109/tip.2026.3659337</link><guid>10.1109/tip.2026.3659337</guid><pubDate>Wed, 04 Feb 2026 20:50:14 +0000</pubDate><dc:creator>Hanxi Li</dc:creator><dc:creator>Jingqi Wu</dc:creator><dc:creator>Deyin Liu</dc:creator><dc:creator>Lin Yuanbo Wu</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Chunhua Shen</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3659337</prism:doi><description>Recent advancements in industrial anomaly detection (AD) have demonstrated that incorporating a small number of anomalous samples during training can significantly enhance accuracy. However, this improvement often comes at the cost of extensive annotation efforts, which are impractical for many real-world applications. In this paper, we introduce a novel framework, “Weakly-supervised RESidual Transformer” (WeakREST), designed to achieve high anomaly detection accuracy while minimizing the reliance on manual annotations. First, we reformulate the pixel-wise anomaly localization task into a block-wise classification problem. Second, we introduce a residual-based feature representation called “Positional Fast Anomaly Residuals” (PosFAR) which captures anomalous patterns more effectively. To leverage this feature, we adapt the Swin Transformer for enhanced anomaly detection and localization. Additionally, we propose a weak annotation approach utilizing bounding boxes and image tags to define anomalous regions. This approach establishes a semi-supervised learning context that reduces the dependency on precise pixel-level labels. To further improve the learning process, we develop a novel ResMixMatch algorithm, capable of handling the interplay between weak labels and residual-based representations. On the benchmark dataset MVTec-AD, our method achieves an Average Precision (AP) of 83:0%, surpassing the previous best result of 82:7% in the unsupervised setting. In the supervised AD setting, WeakREST attains an AP of 87:6%, outperforming the previous best of 86:0%. Notably, even when using weaker annotations such as bounding boxes, WeakREST exceeds the performance of leading methods relying on pixel-wise supervision, achieving an AP of 87:1% compared to the prior best of 86:0% on MVTec-AD. This superior performance is consistently replicated across other well-established AD datasets, including MVTec 3D, KSDD2 and Real-IAD. Code is available at: https://github.com/BeJane/Se...
Published: 2026-02-04T20:50:14+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanxi Li; Jingqi Wu; Deyin Liu; Lin Yuanbo Wu; Hao Chen; Chunhua Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3659337"&gt;10.1109/tip.2026.3659337&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in industrial anomaly detection (AD) have demonstrated that incorporating a small number of anomalous samples during training can significantly enhance accuracy. However, this improvement often comes at the cost of extensive annotation efforts, which are impractical for many real-world applications. In this paper, we introduce a novel framework, “Weakly-supervised RESidual Transformer” (WeakREST), designed to achieve high anomaly detection accuracy while minimizing the reliance on manual annotations. First, we reformulate the pixel-wise anomaly localization task into a block-wise classification problem. Second, we introduce a residual-based feature representation called “Positional Fast Anomaly Residuals” (PosFAR) which captures anomalous patterns more effectively. To leverage this feature, we adapt the Swin Transformer for enhanced anomaly detection and localization. Additionally, we propose a weak annotation approach utilizing bounding boxes and image tags to define anomalous regions. This approach establishes a semi-supervised learning context that reduces the dependency on precise pixel-level labels. To further improve the learning process, we develop a novel ResMixMatch algorithm, capable of handling the interplay between weak labels and residual-based representations. On the benchmark dataset MVTec-AD, our method achieves an Average Precision (AP) of 83:0%, surpassing the previous best result of 82:7% in the unsupervised setting. In the supervised AD setting, WeakREST attains an AP of 87:6%, outperforming the previous best of 86:0%. Notably, even when using weaker annotations such as bounding boxes, WeakREST exceeds the performance of leading methods relying on pixel-wise supervision, achieving an AP of 87:1% compared to the prior best of 86:0% on MVTec-AD. This superior performance is consistently replicated across other well-established AD datasets, including MVTec 3D, KSDD2 and Real-IAD. Code is available at: https://github.com/BeJane/Se...&lt;/p&gt;</content:encoded></item><item><title>ATARS: Adaptive Task-Aware Feature Learning for Few-Shot Fine-Grained Classification</title><link>https://doi.org/10.1016/j.knosys.2026.115485</link><guid>10.1016/j.knosys.2026.115485</guid><pubDate>Wed, 04 Feb 2026 07:59:18 +0000</pubDate><dc:creator>Xiaomei Long</dc:creator><dc:creator>Xinyue Wang</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Zongbo He</dc:creator><dc:creator>Qian He</dc:creator><dc:creator>Xiangdong Chen</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115485</prism:doi><description>Few-shot fine-grained classification is challenging due to subtle inter-class differences and limited annotations. Existing methods often fail to fully exploit task-level information, limiting adaptation to scarce samples. We present ATARS, a task-aware framework that organizes alignment, feature reconstruction, and task-conditioned channel selection into a coordinated pipeline. These components progressively refine task-adaptive feature representations, enhancing intra-class consistency and discriminative capacity. Extensive experiments on five fine-grained benchmarks demonstrate the effectiveness of this design: ATARS achieves 5-way 5-shot accuracies of 97.38% on Cars, 94.40% on CUB, and 89.78% on Dogs, consistently outperforming previous reconstruction-based and task-aware approaches. The results highlight the benefits of coordinated component design under task-aware guidance in few-shot scenarios. The source code is available at: https://github.com/lxm-hjk/ATARS-FSL .
Published: 2026-02-04T07:59:18+00:00
Venue: Knowledge-Based Systems
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaomei Long; Xinyue Wang; Cheng Yang; Zongbo He; Qian He; Xiangdong Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115485"&gt;10.1016/j.knosys.2026.115485&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot fine-grained classification is challenging due to subtle inter-class differences and limited annotations. Existing methods often fail to fully exploit task-level information, limiting adaptation to scarce samples. We present ATARS, a task-aware framework that organizes alignment, feature reconstruction, and task-conditioned channel selection into a coordinated pipeline. These components progressively refine task-adaptive feature representations, enhancing intra-class consistency and discriminative capacity. Extensive experiments on five fine-grained benchmarks demonstrate the effectiveness of this design: ATARS achieves 5-way 5-shot accuracies of 97.38% on Cars, 94.40% on CUB, and 89.78% on Dogs, consistently outperforming previous reconstruction-based and task-aware approaches. The results highlight the benefits of coordinated component design under task-aware guidance in few-shot scenarios. The source code is available at: https://github.com/lxm-hjk/ATARS-FSL .&lt;/p&gt;</content:encoded></item><item><title>FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion</title><link>https://arxiv.org/abs/2602.03137v1</link><guid>http://arxiv.org/abs/2602.03137v1</guid><pubDate>Tue, 03 Feb 2026 05:45:22 +0000</pubDate><dc:creator>Chen-Bin Feng</dc:creator><dc:creator>Youyang Sha</dc:creator><dc:creator>Longfei Liu</dc:creator><dc:creator>Yongjun Yu</dc:creator><dc:creator>Chi Man Vong</dc:creator><dc:creator>Xuanlong Yu</dc:creator><dc:creator>Xi Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.
Published: 2026-02-03T05:45:22+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen-Bin Feng; Youyang Sha; Longfei Liu; Yongjun Yu; Chi Man Vong; Xuanlong Yu; Xi Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.&lt;/p&gt;</content:encoded></item><item><title>Graph-guided Cross-image Correlation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual Representation</title><link>https://doi.org/10.1016/j.inffus.2026.104204</link><guid>10.1016/j.inffus.2026.104204</guid><pubDate>Wed, 04 Feb 2026 00:27:18 +0000</pubDate><dc:creator>Hongxing You</dc:creator><dc:creator>Yangtao Wang</dc:creator><dc:creator>Xiaocui Li</dc:creator><dc:creator>Yanzhao Xie</dc:creator><dc:creator>Da Chen</dc:creator><dc:creator>Xinyu Zhang</dc:creator><dc:creator>Wensheng Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104204</prism:doi><description>Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .
Published: 2026-02-04T00:27:18+00:00
Venue: Information Fusion
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongxing You; Yangtao Wang; Xiaocui Li; Yanzhao Xie; Da Chen; Xinyu Zhang; Wensheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104204"&gt;10.1016/j.inffus.2026.104204&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .&lt;/p&gt;</content:encoded></item><item><title>Quantization-Aware Regularizers for Deep Neural Networks Compression</title><link>https://arxiv.org/abs/2602.03614v1</link><guid>http://arxiv.org/abs/2602.03614v1</guid><pubDate>Tue, 03 Feb 2026 15:07:43 +0000</pubDate><dc:creator>Dario Malchiodi</dc:creator><dc:creator>Mattia Ferraretto</dc:creator><dc:creator>Marco Frasca</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.
Published: 2026-02-03T15:07:43+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dario Malchiodi; Mattia Ferraretto; Marco Frasca&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.&lt;/p&gt;</content:encoded></item><item><title>SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing</title><link>https://arxiv.org/abs/2602.05480v1</link><guid>http://arxiv.org/abs/2602.05480v1</guid><pubDate>Thu, 05 Feb 2026 09:39:49 +0000</pubDate><dc:creator>Peihao Wu</dc:creator><dc:creator>Yongxiang Yao</dc:creator><dc:creator>Yi Wan</dc:creator><dc:creator>Wenfei Zhang</dc:creator><dc:creator>Ruipeng Zhao</dc:creator><dc:creator>Jiayuan Li</dc:creator><dc:creator>Yongjun Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.
Published: 2026-02-05T09:39:49+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peihao Wu; Yongxiang Yao; Yi Wan; Wenfei Zhang; Ruipeng Zhao; Jiayuan Li; Yongjun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.&lt;/p&gt;</content:encoded></item><item><title>Panoptic-VSNet: Visual-Semantic Prior Knowledge-Driven Multimodal 3D Panoptic Segmentation</title><link>https://doi.org/10.1016/j.patcog.2026.113239</link><guid>10.1016/j.patcog.2026.113239</guid><pubDate>Thu, 05 Feb 2026 00:51:08 +0000</pubDate><dc:creator>Xiao Li</dc:creator><dc:creator>Hui Li</dc:creator><dc:creator>Xiangzhen Kong</dc:creator><dc:creator>Yuang Ji</dc:creator><dc:creator>Zhiyu Liu</dc:creator><dc:creator>Hao Liu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113239</prism:doi><description>Precise and robust perception is critical for ensuring the safe operation of autonomous vehicles. However, current methods are constrained by sparse image-LiDAR alignment, insufficient annotations, and ineffective structural discrepancy modeling, causing semantic degradation and generalization deficiency. Therefore, we propose Panoptic-VSNet, a visual-semantic prior knowledge-driven multimodal 3D panoptic segmentation network. Firstly, we design a progressive fusion semantic alignment module that effectively aggregates visual prior features obtained from the large Visual-Language model, establishing a point-semantic region association, thereby enhancing semantic awareness. Secondly, we propose an instance-aware superpixel cross-modal fusion module that incorporates instance prior knowledge, forming a unified representation with spatial precision and class consistency. Finally, we introduce a correlation-aware adaptive panoptic segmentation network that reduces parameter count while dynamically capturing contextual information and enhancing local details, thereby improving panoptic perception capabilities. Experimental evaluations on benchmark datasets show that Panoptic-VSNet outperforms state-of-the-art methods. Code is available at https://github.com/lixiao0125/panoptic-vsnet.git .
Published: 2026-02-05T00:51:08+00:00
Venue: Pattern Recognition
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiao Li; Hui Li; Xiangzhen Kong; Yuang Ji; Zhiyu Liu; Hao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113239"&gt;10.1016/j.patcog.2026.113239&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Precise and robust perception is critical for ensuring the safe operation of autonomous vehicles. However, current methods are constrained by sparse image-LiDAR alignment, insufficient annotations, and ineffective structural discrepancy modeling, causing semantic degradation and generalization deficiency. Therefore, we propose Panoptic-VSNet, a visual-semantic prior knowledge-driven multimodal 3D panoptic segmentation network. Firstly, we design a progressive fusion semantic alignment module that effectively aggregates visual prior features obtained from the large Visual-Language model, establishing a point-semantic region association, thereby enhancing semantic awareness. Secondly, we propose an instance-aware superpixel cross-modal fusion module that incorporates instance prior knowledge, forming a unified representation with spatial precision and class consistency. Finally, we introduce a correlation-aware adaptive panoptic segmentation network that reduces parameter count while dynamically capturing contextual information and enhancing local details, thereby improving panoptic perception capabilities. Experimental evaluations on benchmark datasets show that Panoptic-VSNet outperforms state-of-the-art methods. Code is available at https://github.com/lixiao0125/panoptic-vsnet.git .&lt;/p&gt;</content:encoded></item><item><title>Improving Unsupervised Ultrasonic Image Anomaly Detection via Frequency-Spatial Feature Filtering and Gaussian Mixture Modeling</title><link>https://doi.org/10.1109/tip.2026.3659292</link><guid>10.1109/tip.2026.3659292</guid><pubDate>Wed, 04 Feb 2026 20:50:14 +0000</pubDate><dc:creator>Wenjing Zhang</dc:creator><dc:creator>Ke Lu</dc:creator><dc:creator>Jinbao Wang</dc:creator><dc:creator>Hao Liang</dc:creator><dc:creator>Can Gao</dc:creator><dc:creator>Jian Xue</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3659292</prism:doi><description>Ultrasonic image anomaly detection faces significant challenges due to limited labeled data, strong structural and random noise, and highly diverse defect manifestations. To overcome these obstacles, we introduce UltraChip, a new large-scale C-scan benchmark containing about 8,000 real-world images from various chip packaging types, each meticulously annotated with pixel-level masks for cracks, holes, and layers. Building on this resource, we present FSGM-Net, a fully unsupervised framework tailored for anomaly detection. FSGM-Net leverages an adaptive Frequency-Spatial feature filtering mechanism: a learnable FFT-Spatial patch filter first suppresses noise and dynamically assigns normality weights to Vision Transformer (ViT) patch features. Subsequently, an Adaptive Gaussian Mixture Model (Ada-GMM) captures the distribution of normal features and guides a deep–shallow multi-scale interaction decoder for accurate, pixel-level anomaly inference. In addition, we propose a filter loss that enforces encoder–filter consistency and entropy-based sparse gating, together with a distributional loss that encourages both feature reconstruction and confident Gaussian mixture modeling. Extensive experiments demonstrate that FSGM-Net not only achieves state-of-the-art results on UltraChip but also exhibits superior cross-domain generalization to MVTec-AD and VisA, while supporting real-time inference on a single GPU. Together, the dataset and framework advance robust, annotation-free ultrasonic NDT in practical applications.
Published: 2026-02-04T20:50:14+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenjing Zhang; Ke Lu; Jinbao Wang; Hao Liang; Can Gao; Jian Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3659292"&gt;10.1109/tip.2026.3659292&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Ultrasonic image anomaly detection faces significant challenges due to limited labeled data, strong structural and random noise, and highly diverse defect manifestations. To overcome these obstacles, we introduce UltraChip, a new large-scale C-scan benchmark containing about 8,000 real-world images from various chip packaging types, each meticulously annotated with pixel-level masks for cracks, holes, and layers. Building on this resource, we present FSGM-Net, a fully unsupervised framework tailored for anomaly detection. FSGM-Net leverages an adaptive Frequency-Spatial feature filtering mechanism: a learnable FFT-Spatial patch filter first suppresses noise and dynamically assigns normality weights to Vision Transformer (ViT) patch features. Subsequently, an Adaptive Gaussian Mixture Model (Ada-GMM) captures the distribution of normal features and guides a deep–shallow multi-scale interaction decoder for accurate, pixel-level anomaly inference. In addition, we propose a filter loss that enforces encoder–filter consistency and entropy-based sparse gating, together with a distributional loss that encourages both feature reconstruction and confident Gaussian mixture modeling. Extensive experiments demonstrate that FSGM-Net not only achieves state-of-the-art results on UltraChip but also exhibits superior cross-domain generalization to MVTec-AD and VisA, while supporting real-time inference on a single GPU. Together, the dataset and framework advance robust, annotation-free ultrasonic NDT in practical applications.&lt;/p&gt;</content:encoded></item><item><title>Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification</title><link>https://arxiv.org/abs/2602.05218v1</link><guid>http://arxiv.org/abs/2602.05218v1</guid><pubDate>Thu, 05 Feb 2026 02:17:38 +0000</pubDate><dc:creator>Jiahao Nie</dc:creator><dc:creator>Yun Xing</dc:creator><dc:creator>Wenbin An</dc:creator><dc:creator>Qingsong Zhao</dc:creator><dc:creator>Jiawei Shao</dc:creator><dc:creator>Yap-Peng Tan</dc:creator><dc:creator>Alex C. Kot</dc:creator><dc:creator>Shijian Lu</dc:creator><dc:creator>Xuelong Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.
Published: 2026-02-05T02:17:38+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Nie; Yun Xing; Wenbin An; Qingsong Zhao; Jiawei Shao; Yap-Peng Tan; Alex C. Kot; Shijian Lu; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.&lt;/p&gt;</content:encoded></item><item><title>CUDiff: Consistency and Uncertainty Guided Conditional Diffusion for Infrared and Visible Image Fusion</title><link>https://doi.org/10.1016/j.patcog.2026.113174</link><guid>10.1016/j.patcog.2026.113174</guid><pubDate>Wed, 04 Feb 2026 16:27:01 +0000</pubDate><dc:creator>Yueying Luo</dc:creator><dc:creator>Kangjian He</dc:creator><dc:creator>Dan Xu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113174</prism:doi><description>Infrared and visible image fusion aims to integrate complementary information from both modalities to produce more informative and visually coherent images. Although many existing methods focus on incorporating enhancement modules to improve model efficiency, few effectively address the challenges of learning in complex or ambiguous regions. In this paper, we propose CUDiff, a novel framework that leverages the powerful generative capabilities of diffusion models to reformulate the fusion process as a conditional generation task. Specifically, we design a conditional diffusion model that extracts and integrates relevant features from infrared and visible modalities. A content-consistency constraint is introduced to preserve the structural integrity of the source images, ensuring that essential information is retained in the fused output. Moreover, an uncertainty-driven mechanism adaptively refines and enhances uncertain regions, improving the overall quality and expressiveness of the fused images. Extensive experiments demonstrate that CUDiff surpasses 12 state-of-the-art methods in both visual quality and quantitative evaluation. Furthermore, CUDiff achieves superior performance in object detection tasks. The source code is available at: https://github.com/VCMHE/CUDiff
Published: 2026-02-04T16:27:01+00:00
Venue: Pattern Recognition
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yueying Luo; Kangjian He; Dan Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113174"&gt;10.1016/j.patcog.2026.113174&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared and visible image fusion aims to integrate complementary information from both modalities to produce more informative and visually coherent images. Although many existing methods focus on incorporating enhancement modules to improve model efficiency, few effectively address the challenges of learning in complex or ambiguous regions. In this paper, we propose CUDiff, a novel framework that leverages the powerful generative capabilities of diffusion models to reformulate the fusion process as a conditional generation task. Specifically, we design a conditional diffusion model that extracts and integrates relevant features from infrared and visible modalities. A content-consistency constraint is introduced to preserve the structural integrity of the source images, ensuring that essential information is retained in the fused output. Moreover, an uncertainty-driven mechanism adaptively refines and enhances uncertain regions, improving the overall quality and expressiveness of the fused images. Extensive experiments demonstrate that CUDiff surpasses 12 state-of-the-art methods in both visual quality and quantitative evaluation. Furthermore, CUDiff achieves superior performance in object detection tasks. The source code is available at: https://github.com/VCMHE/CUDiff&lt;/p&gt;</content:encoded></item><item><title>Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images</title><link>https://arxiv.org/abs/2602.01954v1</link><guid>http://arxiv.org/abs/2602.01954v1</guid><pubDate>Mon, 02 Feb 2026 11:03:01 +0000</pubDate><dc:creator>Shuai Yang</dc:creator><dc:creator>Ziyue Huang</dc:creator><dc:creator>Jiaxin Chen</dc:creator><dc:creator>Qingjie Liu</dc:creator><dc:creator>Yunhong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.
Published: 2026-02-02T11:03:01+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Yang; Ziyue Huang; Jiaxin Chen; Qingjie Liu; Yunhong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.&lt;/p&gt;</content:encoded></item><item><title>Align-then-Generate: An Effective Cross-Modal Generation Paradigm for Multi-Label Zero-Shot Learning</title><link>https://doi.org/10.1016/j.patcog.2026.113229</link><guid>10.1016/j.patcog.2026.113229</guid><pubDate>Wed, 04 Feb 2026 07:51:54 +0000</pubDate><dc:creator>Peirong Ma</dc:creator><dc:creator>Wu Ran</dc:creator><dc:creator>Yanhui Gu</dc:creator><dc:creator>Huaqiu Chen</dc:creator><dc:creator>Zhiquan He</dc:creator><dc:creator>Hong Lu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113229</prism:doi><description>Multi-label zero-shot learning (MLZSL) aims to recognize multiple unseen class labels that may appear in an image, posing a significant challenge in the field of computer vision. While generative methods have achieved remarkable progress by synthesizing visual features of unseen categories, they often suffer from poor visual-semantic consistency and limited generative quality. To address these issues, this paper proposes a novel “Align-then-Generate” paradigm and introduces a unified framework named VLA-CMG, which integrates vision-language alignment with cross-modal feature generation. Specifically, a Language-aware multi-label image encoder (LMIE) is designed to extract both global and local visual features from images, which are aligned with multi-label semantic embeddings generated by the text encoder of the Vision-language pre-training (VLP) Model, thereby enhancing the consistency between semantic and visual representations. This alignment provides high-quality input for the training of the Dual-stream feature generation network (DSFGN), which synthesizes discriminative visual features for unseen classes. Finally, a robust multi-label zero-shot classifier is built upon the generated features. Extensive experiments on two large-scale benchmark datasets ( i.e. , NUS-WIDE and Open Images) demonstrate that VLA-CMG consistently outperforms existing state-of-the-art methods on both ZSL and GZSL tasks, validating its effectiveness and superiority.
Published: 2026-02-04T07:51:54+00:00
Venue: Pattern Recognition
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peirong Ma; Wu Ran; Yanhui Gu; Huaqiu Chen; Zhiquan He; Hong Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113229"&gt;10.1016/j.patcog.2026.113229&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-label zero-shot learning (MLZSL) aims to recognize multiple unseen class labels that may appear in an image, posing a significant challenge in the field of computer vision. While generative methods have achieved remarkable progress by synthesizing visual features of unseen categories, they often suffer from poor visual-semantic consistency and limited generative quality. To address these issues, this paper proposes a novel “Align-then-Generate” paradigm and introduces a unified framework named VLA-CMG, which integrates vision-language alignment with cross-modal feature generation. Specifically, a Language-aware multi-label image encoder (LMIE) is designed to extract both global and local visual features from images, which are aligned with multi-label semantic embeddings generated by the text encoder of the Vision-language pre-training (VLP) Model, thereby enhancing the consistency between semantic and visual representations. This alignment provides high-quality input for the training of the Dual-stream feature generation network (DSFGN), which synthesizes discriminative visual features for unseen classes. Finally, a robust multi-label zero-shot classifier is built upon the generated features. Extensive experiments on two large-scale benchmark datasets ( i.e. , NUS-WIDE and Open Images) demonstrate that VLA-CMG consistently outperforms existing state-of-the-art methods on both ZSL and GZSL tasks, validating its effectiveness and superiority.&lt;/p&gt;</content:encoded></item><item><title>Self-Supervised Learning from Structural Invariance</title><link>https://arxiv.org/abs/2602.02381v1</link><guid>http://arxiv.org/abs/2602.02381v1</guid><pubDate>Mon, 02 Feb 2026 17:44:44 +0000</pubDate><dc:creator>Yipeng Zhang</dc:creator><dc:creator>Hafez Ghaemi</dc:creator><dc:creator>Jungyoon Lee</dc:creator><dc:creator>Shahab Bakhtiari</dc:creator><dc:creator>Eilif B. Muller</dc:creator><dc:creator>Laurent Charlin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.
Published: 2026-02-02T17:44:44+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yipeng Zhang; Hafez Ghaemi; Jungyoon Lee; Shahab Bakhtiari; Eilif B. Muller; Laurent Charlin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.&lt;/p&gt;</content:encoded></item><item><title>Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation</title><link>https://arxiv.org/abs/2602.05217v1</link><guid>http://arxiv.org/abs/2602.05217v1</guid><pubDate>Thu, 05 Feb 2026 02:16:44 +0000</pubDate><dc:creator>Jiahao Nie</dc:creator><dc:creator>Guanqiao Fu</dc:creator><dc:creator>Wenbin An</dc:creator><dc:creator>Yap-Peng Tan</dc:creator><dc:creator>Alex C. Kot</dc:creator><dc:creator>Shijian Lu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model's initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).
Published: 2026-02-05T02:16:44+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Nie; Guanqiao Fu; Wenbin An; Yap-Peng Tan; Alex C. Kot; Shijian Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model&amp;#x27;s initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).&lt;/p&gt;</content:encoded></item><item><title>CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion</title><link>https://arxiv.org/abs/2602.05598v1</link><guid>http://arxiv.org/abs/2602.05598v1</guid><pubDate>Thu, 05 Feb 2026 12:33:09 +0000</pubDate><dc:creator>Aon Safdar</dc:creator><dc:creator>Mohamed Saadeldin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.
Published: 2026-02-05T12:33:09+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aon Safdar; Mohamed Saadeldin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce &amp;#x27;CAViT&amp;#x27;, a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.&lt;/p&gt;</content:encoded></item><item><title>Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images</title><link>https://arxiv.org/abs/2602.03669v1</link><guid>http://arxiv.org/abs/2602.03669v1</guid><pubDate>Tue, 03 Feb 2026 15:51:29 +0000</pubDate><dc:creator>Sandeep Patil</dc:creator><dc:creator>Yongqi Dong</dc:creator><dc:creator>Haneen Farah</dc:creator><dc:creator>Hans Hellendoorn</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.
Published: 2026-02-03T15:51:29+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sandeep Patil; Yongqi Dong; Haneen Farah; Hans Hellendoorn&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.&lt;/p&gt;</content:encoded></item><item><title>Fast-SAM3D: 3Dfy Anything in Images but Faster</title><link>https://arxiv.org/abs/2602.05293v1</link><guid>http://arxiv.org/abs/2602.05293v1</guid><pubDate>Thu, 05 Feb 2026 04:27:59 +0000</pubDate><dc:creator>Weilun Feng</dc:creator><dc:creator>Mingqiang Wu</dc:creator><dc:creator>Zhiliang Chen</dc:creator><dc:creator>Chuanguang Yang</dc:creator><dc:creator>Haotong Qin</dc:creator><dc:creator>Yuqi Li</dc:creator><dc:creator>Xiaokun Liu</dc:creator><dc:creator>Guoxin Fan</dc:creator><dc:creator>Zhulin An</dc:creator><dc:creator>Libo Huang</dc:creator><dc:creator>Yulun Zhang</dc:creator><dc:creator>Michele Magno</dc:creator><dc:creator>Yongjun Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.
Published: 2026-02-05T04:27:59+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weilun Feng; Mingqiang Wu; Zhiliang Chen; Chuanguang Yang; Haotong Qin; Yuqi Li; Xiaokun Liu; Guoxin Fan; Zhulin An; Libo Huang; Yulun Zhang; Michele Magno; Yongjun Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline&amp;#x27;s inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.&lt;/p&gt;</content:encoded></item><item><title>ObjEmbed: Towards Universal Multimodal Object Embeddings</title><link>https://arxiv.org/abs/2602.01753v2</link><guid>http://arxiv.org/abs/2602.01753v2</guid><pubDate>Mon, 02 Feb 2026 07:38:45 +0000</pubDate><dc:creator>Shenghao Fu</dc:creator><dc:creator>Yukun Su</dc:creator><dc:creator>Fengyun Rao</dc:creator><dc:creator>Jing Lyu</dc:creator><dc:creator>Xiaohua Xie</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.
Published: 2026-02-02T07:38:45+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shenghao Fu; Yukun Su; Fengyun Rao; Jing Lyu; Xiaohua Xie; Wei-Shi Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.&lt;/p&gt;</content:encoded></item><item><title>ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network</title><link>https://arxiv.org/abs/2602.05262v1</link><guid>http://arxiv.org/abs/2602.05262v1</guid><pubDate>Thu, 05 Feb 2026 03:43:29 +0000</pubDate><dc:creator>Junzhou Li</dc:creator><dc:creator>Manqi Zhao</dc:creator><dc:creator>Yilin Gao</dc:creator><dc:creator>Zhiheng Yu</dc:creator><dc:creator>Yin Li</dc:creator><dc:creator>Dongsheng Jiang</dc:creator><dc:creator>Li Xiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \textbf{80.85\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \textbf{3.1\%} AP on COCO object detection and \textbf{3.6\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.
Published: 2026-02-05T03:43:29+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junzhou Li; Manqi Zhao; Yilin Gao; Zhiheng Yu; Yin Li; Dongsheng Jiang; Li Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \textbf{80.85\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \textbf{3.1\%} AP on COCO object detection and \textbf{3.6\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.&lt;/p&gt;</content:encoded></item><item><title>PC-YOLO: Moving Target Detection in Video SAR via YOLO on Principal Components</title><link>https://doi.org/10.3390/rs18030510</link><guid>10.3390/rs18030510</guid><pubDate>Thu, 05 Feb 2026 11:13:01 +0000</pubDate><dc:creator>Yu Han</dc:creator><dc:creator>Xinrong Wang</dc:creator><dc:creator>Jiaqing Jiang</dc:creator><dc:creator>Chao Xue</dc:creator><dc:creator>Rui Qin</dc:creator><dc:creator>Ganggang Dong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030510</prism:doi><description>Video synthetic aperture radar could provide more valuable information than static images. However, it suffers from several difficulties, such as strong clutter, low signal-to-noise ratio, and variable target scale. The task of moving target detection is therefore difficult to achieve. To solve these problems, this paper proposes a model and data co-driven learning method called look once on principal components (PC-YOLO). Unlike preceding works, we regarded the imaging scenario as a combination of low-rank and sparse scenes in theory. The former models the global, slowly varying background information, while the latter expresses the localized anomalies. These were then separated using the principal component decomposition technique to reduce the clutter while simultaneously enhancing the moving targets. The resulting principal components were then handled by an improved version of the look once framework. Since the moving targets featured various scales and weak scattering coefficients, the hierarchical attention mechanism and the cross-scale feature fusion strategy were introduced to further improve the detection performance. Finally, multiple rounds of experiments were performed to verify the proposed method, with the results proving that it could achieve more than 30% improvement in mAP compared to classical methods.
Published: 2026-02-05T11:13:01+00:00
Venue: Remote Sensing
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Han; Xinrong Wang; Jiaqing Jiang; Chao Xue; Rui Qin; Ganggang Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030510"&gt;10.3390/rs18030510&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Video synthetic aperture radar could provide more valuable information than static images. However, it suffers from several difficulties, such as strong clutter, low signal-to-noise ratio, and variable target scale. The task of moving target detection is therefore difficult to achieve. To solve these problems, this paper proposes a model and data co-driven learning method called look once on principal components (PC-YOLO). Unlike preceding works, we regarded the imaging scenario as a combination of low-rank and sparse scenes in theory. The former models the global, slowly varying background information, while the latter expresses the localized anomalies. These were then separated using the principal component decomposition technique to reduce the clutter while simultaneously enhancing the moving targets. The resulting principal components were then handled by an improved version of the look once framework. Since the moving targets featured various scales and weak scattering coefficients, the hierarchical attention mechanism and the cross-scale feature fusion strategy were introduced to further improve the detection performance. Finally, multiple rounds of experiments were performed to verify the proposed method, with the results proving that it could achieve more than 30% improvement in mAP compared to classical methods.&lt;/p&gt;</content:encoded></item></channel></rss>