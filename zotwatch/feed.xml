<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 28 Dec 2025 02:51:24 +0000</lastBuildDate><item><title>SAM-I2V++: Efficiently Upgrading SAM for Promptable Video Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3648863</link><guid>10.1109/tpami.2025.3648863</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Haiyang Mei</dc:creator><dc:creator>Pengyu Zhang</dc:creator><dc:creator>Mike Zheng Shou</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648863</prism:doi><description>Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V++, a training-efficient image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM's static image encoder to enable spatiotemporal video perception, (ii) a memory selective associator that retrieves the most relevant past frames via similarity-driven selection and uses multiscale-enhanced cross-attention to associate selected memory features with the current frame, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves 93% of SAM 2's performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Project page: https://github.com/showlab/SAM-I2V.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.860 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haiyang Mei; Pengyu Zhang; Mike Zheng Shou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648863"&gt;10.1109/tpami.2025.3648863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.860 (must_read)&lt;/p&gt;
&lt;p&gt;Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V++, a training-efficient image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM&amp;#x27;s static image encoder to enable spatiotemporal video perception, (ii) a memory selective associator that retrieves the most relevant past frames via similarity-driven selection and uses multiscale-enhanced cross-attention to associate selected memory features with the current frame, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves 93% of SAM 2&amp;#x27;s performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Project page: https://github.com/showlab/SAM-I2V.&lt;/p&gt;</content:encoded></item><item><title>Data-Driven Bidirectional Spatial-Adaptive Network for Weakly Supervised Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tpami.2025.3646464</link><guid>10.1109/tpami.2025.3646464</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Zebin Wu</dc:creator><dc:creator>Shangdong Zheng</dc:creator><dc:creator>Yang Xu</dc:creator><dc:creator>Le Wang</dc:creator><dc:creator>Zhihui Wei</dc:creator><dc:creator>Gang Hua</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646464</prism:doi><description>Weakly-supervised object detection (WSOD) learns detectors with only image-level classification annotations. Without precise instance-level labels, most previous WSOD methods in remote sensing images (RSIs) select the highest-scoring proposals as the final detection results, which are confronted by two major challenges: (1) instances with small scale or rare poses are easily neglected; (2) optimizing network by the top-scoring region inevitably overlooks many valuable candidate proposals. To mitigate the above-mentioned challenges, we propose a data-driven bidirectional spatial-adaptive network (BSANet). It contains a forward-reverse spatial dropout (FRSD) module to reduce instance ambiguity induced from extreme scales and poses, as well as crowded scene, and to better excavate the entire instances. From attention learning perspective, the proposed FRSD is conceptually similar to a data-driven hard attention mechanism, which adaptively samples and reconstructs the spatially related regions for mining more latent feature responses. Meanwhile, our FRSD effectively alleviates the inherent problem that non-parametric hard attention learning fashion cannot adapt to different datasets. In addition, we build a soft attention branch to simultaneously model soft pixel-level and hard region-level attention information for exploring the complementary benefit between soft and hard attention learning. We evaluate our BSANet on the challenging NWPU VHR-10.v2 and DIOR datasets. Experimental results demonstrate that our method sets a new state-of-the-art.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.843 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zebin Wu; Shangdong Zheng; Yang Xu; Le Wang; Zhihui Wei; Gang Hua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646464"&gt;10.1109/tpami.2025.3646464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.843 (must_read)&lt;/p&gt;
&lt;p&gt;Weakly-supervised object detection (WSOD) learns detectors with only image-level classification annotations. Without precise instance-level labels, most previous WSOD methods in remote sensing images (RSIs) select the highest-scoring proposals as the final detection results, which are confronted by two major challenges: (1) instances with small scale or rare poses are easily neglected; (2) optimizing network by the top-scoring region inevitably overlooks many valuable candidate proposals. To mitigate the above-mentioned challenges, we propose a data-driven bidirectional spatial-adaptive network (BSANet). It contains a forward-reverse spatial dropout (FRSD) module to reduce instance ambiguity induced from extreme scales and poses, as well as crowded scene, and to better excavate the entire instances. From attention learning perspective, the proposed FRSD is conceptually similar to a data-driven hard attention mechanism, which adaptively samples and reconstructs the spatially related regions for mining more latent feature responses. Meanwhile, our FRSD effectively alleviates the inherent problem that non-parametric hard attention learning fashion cannot adapt to different datasets. In addition, we build a soft attention branch to simultaneously model soft pixel-level and hard region-level attention information for exploring the complementary benefit between soft and hard attention learning. We evaluate our BSANet on the challenging NWPU VHR-10.v2 and DIOR datasets. Experimental results demonstrate that our method sets a new state-of-the-art.&lt;/p&gt;</content:encoded></item><item><title>Synergistic Fusion of Multi-Temporal and Multi-Resolution SAR Data: A Hierarchical Prior Transfer Approach for Aircraft Detection</title><link>https://doi.org/10.1109/tgrs.2025.3648806</link><guid>10.1109/tgrs.2025.3648806</guid><pubDate>Fri, 26 Dec 2025 18:23:46 +0000</pubDate><dc:creator>Yipeng Zhang</dc:creator><dc:creator>Fengming Hu</dc:creator><dc:creator>Haipeng Wang</dc:creator><dc:creator>Likang Zhu</dc:creator><dc:creator>Feng Xu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648806</prism:doi><description>Current deep learning methods for synthetic aperture radar (SAR) aircraft detection typically rely on single-temporal, high-resolution imagery, requiring extensive labeled datasets and limiting generalization across sensors and resolutions. Meanwhile, abundant multi-temporal, coarse- and moderate-resolution SAR images from satellites such as Sentinel-1, offering valuable temporal cues about stable aircraft positions at designated parking stands, remain largely underutilized. Additionally, existing techniques inadequately exploit the complementary advantages of multi-resolution SAR data, overlooking the synergy between global context from coarse imagery and detailed localization from fine-resolution imagery. To bridge this gap, we propose the hierarchical prior transfer approach (HiPTA), a novel framework for synergistic fusion of multi-temporal and multi-resolution SAR data in a coarse-to-fine detection paradigm. First, the coarse-resolution multi-temporal prior extraction (CMPE) module mines low-resolution time series to identify robust parking-stand priors. Next, the intermediate-resolution prior fusion and refinement (IPFR) and hierarchical multi-resolution prior registration (HMPR) modules align and refine these priors across intermediate and high-resolution domains, drastically narrowing the search space. Finally, the Fine-Resolution Aircraft Detection (FRAD) module employs a domain-adaptive frequency-adaptive filtering network to classify candidate regions in high-resolution imagery. Extensive experiments on multi-source C- and Ku-band SAR datasets show that HiPTA consistently improves precision–recall trade-offs and maintains robustness across sensors and resolutions, delivering consistent gains over single-resolution detectors and airport-detection-aided methods.
Published: 2025-12-26T18:23:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yipeng Zhang; Fengming Hu; Haipeng Wang; Likang Zhu; Feng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648806"&gt;10.1109/tgrs.2025.3648806&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Current deep learning methods for synthetic aperture radar (SAR) aircraft detection typically rely on single-temporal, high-resolution imagery, requiring extensive labeled datasets and limiting generalization across sensors and resolutions. Meanwhile, abundant multi-temporal, coarse- and moderate-resolution SAR images from satellites such as Sentinel-1, offering valuable temporal cues about stable aircraft positions at designated parking stands, remain largely underutilized. Additionally, existing techniques inadequately exploit the complementary advantages of multi-resolution SAR data, overlooking the synergy between global context from coarse imagery and detailed localization from fine-resolution imagery. To bridge this gap, we propose the hierarchical prior transfer approach (HiPTA), a novel framework for synergistic fusion of multi-temporal and multi-resolution SAR data in a coarse-to-fine detection paradigm. First, the coarse-resolution multi-temporal prior extraction (CMPE) module mines low-resolution time series to identify robust parking-stand priors. Next, the intermediate-resolution prior fusion and refinement (IPFR) and hierarchical multi-resolution prior registration (HMPR) modules align and refine these priors across intermediate and high-resolution domains, drastically narrowing the search space. Finally, the Fine-Resolution Aircraft Detection (FRAD) module employs a domain-adaptive frequency-adaptive filtering network to classify candidate regions in high-resolution imagery. Extensive experiments on multi-source C- and Ku-band SAR datasets show that HiPTA consistently improves precision–recall trade-offs and maintains robustness across sensors and resolutions, delivering consistent gains over single-resolution detectors and airport-detection-aided methods.&lt;/p&gt;</content:encoded></item><item><title>GCEPANet: A Lightweight and Efficient Remote Sensing Image Cloud Removal Network Model for Optical-SAR Image Fusion</title><link>https://doi.org/10.1016/j.inffus.2025.104090</link><guid>10.1016/j.inffus.2025.104090</guid><pubDate>Sat, 27 Dec 2025 16:17:57 +0000</pubDate><dc:creator>Qinglong Zhou</dc:creator><dc:creator>Xing Wang</dc:creator><dc:creator>Jiahao Fang</dc:creator><dc:creator>Wenbo Wu</dc:creator><dc:creator>Bingxian Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104090</prism:doi><description>To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.
Published: 2025-12-27T16:17:57+00:00
Venue: Information Fusion
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qinglong Zhou; Xing Wang; Jiahao Fang; Wenbo Wu; Bingxian Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104090"&gt;10.1016/j.inffus.2025.104090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.&lt;/p&gt;</content:encoded></item><item><title>Deep learning-based astronomical multimodal data fusion: A comprehensive review</title><link>https://doi.org/10.1016/j.inffus.2025.104103</link><guid>10.1016/j.inffus.2025.104103</guid><pubDate>Sat, 27 Dec 2025 16:17:58 +0000</pubDate><dc:creator>Wujun Shao</dc:creator><dc:creator>Dongwei Fan</dc:creator><dc:creator>Chenzhou Cui</dc:creator><dc:creator>Yunfei Xu</dc:creator><dc:creator>Shirui Wei</dc:creator><dc:creator>Xin Lyu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104103</prism:doi><description>With the rapid advancements in observational technologies and the widespread implementation of large-scale sky surveys, diverse electromagnetic wave data (e.g., optical and infrared) and non-electromagnetic wave data (e.g., gravitational waves) have become increasingly accessible. Astronomy has thus entered an unprecedented era of data abundance and complexity. Astronomers have long relied on unimodal data analysis to perceive the universe, but these efforts often provide only limited insights when confronted with the current massive and heterogeneous astronomical data. In this context, multimodal data fusion (MDF), as an emerging method, provides new opportunities to enhance the value of astronomical data and deepening the understanding of the universe by integrating information from different modalities. Recent progress in artificial intelligence (AI), particularly in deep learning (DL), has greatly accelerated the development of multimodal research in astronomy. Therefore, a timely review of this field is essential. This paper begins by discussing the motivation and necessity of astronomical MDF, followed by an overview of astronomical data sources and major data modalities. It then introduces representative DL models commonly used in astronomical multimodal studies, the general fusion process as well as various fusion strategies, emphasizing their characteristics, applicability, advantages, and limitations. Subsequently, the paper surveys existing astronomical multimodal studies and datasets. Finally, the discussion section synthesizes key findings, identifies potential challenges, and suggests promising directions for future research. By offering a structured overview and critical analysis, this review aims to inspire and guide researchers engaged in DL-based MDF in astronomy.
Published: 2025-12-27T16:17:58+00:00
Venue: Information Fusion
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wujun Shao; Dongwei Fan; Chenzhou Cui; Yunfei Xu; Shirui Wei; Xin Lyu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104103"&gt;10.1016/j.inffus.2025.104103&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancements in observational technologies and the widespread implementation of large-scale sky surveys, diverse electromagnetic wave data (e.g., optical and infrared) and non-electromagnetic wave data (e.g., gravitational waves) have become increasingly accessible. Astronomy has thus entered an unprecedented era of data abundance and complexity. Astronomers have long relied on unimodal data analysis to perceive the universe, but these efforts often provide only limited insights when confronted with the current massive and heterogeneous astronomical data. In this context, multimodal data fusion (MDF), as an emerging method, provides new opportunities to enhance the value of astronomical data and deepening the understanding of the universe by integrating information from different modalities. Recent progress in artificial intelligence (AI), particularly in deep learning (DL), has greatly accelerated the development of multimodal research in astronomy. Therefore, a timely review of this field is essential. This paper begins by discussing the motivation and necessity of astronomical MDF, followed by an overview of astronomical data sources and major data modalities. It then introduces representative DL models commonly used in astronomical multimodal studies, the general fusion process as well as various fusion strategies, emphasizing their characteristics, applicability, advantages, and limitations. Subsequently, the paper surveys existing astronomical multimodal studies and datasets. Finally, the discussion section synthesizes key findings, identifies potential challenges, and suggests promising directions for future research. By offering a structured overview and critical analysis, this review aims to inspire and guide researchers engaged in DL-based MDF in astronomy.&lt;/p&gt;</content:encoded></item><item><title>Language Embedded 3D Gaussians for Open-Vocabulary Scene Querying</title><link>https://doi.org/10.1109/tpami.2025.3648837</link><guid>10.1109/tpami.2025.3648837</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Miao Wang</dc:creator><dc:creator>Jin-Chuan Shi</dc:creator><dc:creator>Shao-Hua Guan</dc:creator><dc:creator>Hao-Bin Duan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648837</prism:doi><description>Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Miao Wang; Jin-Chuan Shi; Shao-Hua Guan; Hao-Bin Duan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648837"&gt;10.1109/tpami.2025.3648837&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.&lt;/p&gt;</content:encoded></item><item><title>Scoping Review of Multimodal Sentiment Analysis and Summarization: State of the Art, Challenges and Future Directions</title><link>https://doi.org/10.1016/j.inffus.2025.104082</link><guid>10.1016/j.inffus.2025.104082</guid><pubDate>Sat, 27 Dec 2025 07:23:47 +0000</pubDate><dc:creator>Magaly Lika Fujimoto</dc:creator><dc:creator>Ricardo Marcondes Marcacini</dc:creator><dc:creator>Solange Oliveira Rezende</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104082</prism:doi><description>In recent decades, advancements in computing power and the widespread availability of multimodal data have significantly redirected research, shifting the primary focus from text based approaches. This paper presents a scoping review focusing on approaches that jointly perform Multimodal Sentiment Analysis and Multimodal Summarization within the same framework. Beyond this, the review comprehensively surveys each domain individually, highlighting state-of-the-art techniques, key methodologies, and commonly used datasets. It also provides key insights into current challenges and proposes future research directions.
Published: 2025-12-27T07:23:47+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Magaly Lika Fujimoto; Ricardo Marcondes Marcacini; Solange Oliveira Rezende&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104082"&gt;10.1016/j.inffus.2025.104082&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;In recent decades, advancements in computing power and the widespread availability of multimodal data have significantly redirected research, shifting the primary focus from text based approaches. This paper presents a scoping review focusing on approaches that jointly perform Multimodal Sentiment Analysis and Multimodal Summarization within the same framework. Beyond this, the review comprehensively surveys each domain individually, highlighting state-of-the-art techniques, key methodologies, and commonly used datasets. It also provides key insights into current challenges and proposes future research directions.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical cross-module knowledge transfer based on structural multi-view least squares support vector classification</title><link>https://doi.org/10.1016/j.inffus.2025.104099</link><guid>10.1016/j.inffus.2025.104099</guid><pubDate>Sat, 27 Dec 2025 16:17:55 +0000</pubDate><dc:creator>Siyuan Zhang</dc:creator><dc:creator>Shuangrui Jia</dc:creator><dc:creator>Jianying Feng</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104099</prism:doi><description>Multi-view learning has garnered significant attention in machine learning due to its ability to leverage complementary information from diverse data sources. However, multi-view least squares support vector machines (MvLSSVMs) suffer from two critical limitations. Firstly, their reliance on pairwise view comparisons hinders their ability to capture complex inter-view relationships. Secondly, the high computational costs associated with hyperparameter tuning impede their scalability. To address these challenges, this paper proposes hierarchical transfer-based structural multi-view least squares support vector classification (HT-SMLSSVC). Inspired by the previous work of the multi-view structural large margin classifier (MvSLMC), the proposed HT-SMLSSVC achieves complementarity and consensus principles in each layer through a weighting strategy and clustering, which is used to form structural regularization. This term can enhance within-class cohesion and between-class separability within each view. At the same time, different views provide complementary structural information to one another, thereby enriching classifier diversity and further avoiding reliance on pairwise view-comparison strategies. The difference lies in the adoption of least squares loss in each layer of the model, whereby the solution for the hyperplane is a set of linear equations rather than a standard quadratic programming problem. In addition, hierarchical knowledge transfer is achieved through a deep stacked architecture, which propagates cross-layer predictions to enhance generalization ability. At the same time, efficient learning is achieved through randomized hyperparameter assignment and adaptive validation, eliminating the need for manual tuning and thereby significantly reducing model training time. Extensive experiments on 17 UCI and 45 AWA datasets demonstrate that HT-SMLSSVC outperforms state-of-the-art methods in both computational efficiency and classification accuracy, offering a scalable solution for real-world multi-view tasks.
Published: 2025-12-27T16:17:55+00:00
Venue: Information Fusion
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siyuan Zhang; Shuangrui Jia; Jianying Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104099"&gt;10.1016/j.inffus.2025.104099&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-view learning has garnered significant attention in machine learning due to its ability to leverage complementary information from diverse data sources. However, multi-view least squares support vector machines (MvLSSVMs) suffer from two critical limitations. Firstly, their reliance on pairwise view comparisons hinders their ability to capture complex inter-view relationships. Secondly, the high computational costs associated with hyperparameter tuning impede their scalability. To address these challenges, this paper proposes hierarchical transfer-based structural multi-view least squares support vector classification (HT-SMLSSVC). Inspired by the previous work of the multi-view structural large margin classifier (MvSLMC), the proposed HT-SMLSSVC achieves complementarity and consensus principles in each layer through a weighting strategy and clustering, which is used to form structural regularization. This term can enhance within-class cohesion and between-class separability within each view. At the same time, different views provide complementary structural information to one another, thereby enriching classifier diversity and further avoiding reliance on pairwise view-comparison strategies. The difference lies in the adoption of least squares loss in each layer of the model, whereby the solution for the hyperplane is a set of linear equations rather than a standard quadratic programming problem. In addition, hierarchical knowledge transfer is achieved through a deep stacked architecture, which propagates cross-layer predictions to enhance generalization ability. At the same time, efficient learning is achieved through randomized hyperparameter assignment and adaptive validation, eliminating the need for manual tuning and thereby significantly reducing model training time. Extensive experiments on 17 UCI and 45 AWA datasets demonstrate that HT-SMLSSVC outperforms state-of-the-art methods in both computational efficiency and classification accuracy, offering a scalable solution for real-world multi-view tasks.&lt;/p&gt;</content:encoded></item><item><title>Synthetic learning for primitive-based building model reconstruction from point clouds</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.012</link><guid>10.1016/j.isprsjprs.2025.12.012</guid><pubDate>Sat, 27 Dec 2025 19:51:14 +0000</pubDate><dc:creator>Zhixin Li</dc:creator><dc:creator>Jie Shan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.012</prism:doi><description>The rapid advancement of digital 3D environments has significantly increased the demand for geometrically accurate and semantically rich parametric building models. However, existing primitive- or model-based building reconstruction approaches often struggle with limited availability of labeled datasets and insufficient reconstruction accuracy. To address these challenges, we propose a novel learning-based method for building reconstruction from point clouds that leverages roof primitives and relies exclusively on synthetic data for supervision. Our approach begins with the generation of a large synthetic dataset comprising 100,000 buildings of varying scales based on a predefined library of 10 roof primitive classes. The synthetic point clouds are created by randomly sampling not only the interiors but also the edges and corners of the roof primitives. Two lightweight transformer-based neural networks are then trained to classify roof primitive classes and estimate their corresponding parameters. Compared to conventional learning-free fitting methods, our learning-based approach achieves higher parameter estimation accuracy and greater robustness when applied to six real-world point cloud datasets collected from drone, airborne, and spaceborne platforms. Notably, the synthetic learning approach reduces primitive parameter estimation errors from approximately 50% to 6% of the point ground spacing — demonstrating a distinctive advantage when trained effectively on synthetic data. Future work may explore generating synthetic data for irregular, complex buildings, expanding the library with additional roof primitive classes, and applying the proposed training strategy to such synthetic datasets.
Published: 2025-12-27T19:51:14+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhixin Li; Jie Shan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.012"&gt;10.1016/j.isprsjprs.2025.12.012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of digital 3D environments has significantly increased the demand for geometrically accurate and semantically rich parametric building models. However, existing primitive- or model-based building reconstruction approaches often struggle with limited availability of labeled datasets and insufficient reconstruction accuracy. To address these challenges, we propose a novel learning-based method for building reconstruction from point clouds that leverages roof primitives and relies exclusively on synthetic data for supervision. Our approach begins with the generation of a large synthetic dataset comprising 100,000 buildings of varying scales based on a predefined library of 10 roof primitive classes. The synthetic point clouds are created by randomly sampling not only the interiors but also the edges and corners of the roof primitives. Two lightweight transformer-based neural networks are then trained to classify roof primitive classes and estimate their corresponding parameters. Compared to conventional learning-free fitting methods, our learning-based approach achieves higher parameter estimation accuracy and greater robustness when applied to six real-world point cloud datasets collected from drone, airborne, and spaceborne platforms. Notably, the synthetic learning approach reduces primitive parameter estimation errors from approximately 50% to 6% of the point ground spacing — demonstrating a distinctive advantage when trained effectively on synthetic data. Future work may explore generating synthetic data for irregular, complex buildings, expanding the library with additional roof primitive classes, and applying the proposed training strategy to such synthetic datasets.&lt;/p&gt;</content:encoded></item><item><title>SAREval: A Multi-Dimensional and Multi-Task Benchmark for Evaluating Visual Language Models on SAR Image Understanding</title><link>https://doi.org/10.3390/rs18010082</link><guid>10.3390/rs18010082</guid><pubDate>Fri, 26 Dec 2025 03:06:02 +0000</pubDate><dc:creator>Ziyan Wang</dc:creator><dc:creator>Lei Liu</dc:creator><dc:creator>Gang Wan</dc:creator><dc:creator>Yuchen Lu</dc:creator><dc:creator>Fengjie Zheng</dc:creator><dc:creator>Guangde Sun</dc:creator><dc:creator>Yixiang Huang</dc:creator><dc:creator>Shihao Guo</dc:creator><dc:creator>Xinyi Li</dc:creator><dc:creator>Liang Yuan</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010082</prism:doi><description>Vision-Language Models (VLMs) demonstrate significant potential for remote sensing interpretation through multimodal fusion and semantic representation of imagery. However, their adaptation to Synthetic Aperture Radar (SAR) remains challenging due to fundamental differences in imaging mechanisms and physical properties compared to optical remote sensing. SAREval, the first comprehensive benchmark specifically designed for SAR image understanding, incorporates SAR-specific characteristics, including scattering mechanisms and polarization features, through a hierarchical framework spanning perception, reasoning, and robustness capabilities. It encompasses 20 tasks from image classification to physical-attribute inference with over 10,000 high-quality image–text pairs. Extensive experiments conducted on 11 mainstream VLMs reveal substantial limitations in SAR image interpretation. Models achieve merely 25.35% accuracy in fine-grained ship classification tasks and demonstrate significant difficulties in establishing mappings between visual features and physical parameters. Furthermore, certain models exhibit unexpected performance improvements under certain noise conditions that challenge conventional robustness understanding. SAREval establishes an essential foundation for developing and evaluating VLMs in SAR image interpretation, providing standardized assessment protocols and quality-controlled annotations for cross-modal remote sensing research.
Published: 2025-12-26T03:06:02+00:00
Venue: Remote Sensing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyan Wang; Lei Liu; Gang Wan; Yuchen Lu; Fengjie Zheng; Guangde Sun; Yixiang Huang; Shihao Guo; Xinyi Li; Liang Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010082"&gt;10.3390/rs18010082&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) demonstrate significant potential for remote sensing interpretation through multimodal fusion and semantic representation of imagery. However, their adaptation to Synthetic Aperture Radar (SAR) remains challenging due to fundamental differences in imaging mechanisms and physical properties compared to optical remote sensing. SAREval, the first comprehensive benchmark specifically designed for SAR image understanding, incorporates SAR-specific characteristics, including scattering mechanisms and polarization features, through a hierarchical framework spanning perception, reasoning, and robustness capabilities. It encompasses 20 tasks from image classification to physical-attribute inference with over 10,000 high-quality image–text pairs. Extensive experiments conducted on 11 mainstream VLMs reveal substantial limitations in SAR image interpretation. Models achieve merely 25.35% accuracy in fine-grained ship classification tasks and demonstrate significant difficulties in establishing mappings between visual features and physical parameters. Furthermore, certain models exhibit unexpected performance improvements under certain noise conditions that challenge conventional robustness understanding. SAREval establishes an essential foundation for developing and evaluating VLMs in SAR image interpretation, providing standardized assessment protocols and quality-controlled annotations for cross-modal remote sensing research.&lt;/p&gt;</content:encoded></item><item><title>PMM3D: a transformer-based monocular 3D detector with parallel multi-time inquiry and mixup enhancement</title><link>https://doi.org/10.1016/j.eswa.2025.131014</link><guid>10.1016/j.eswa.2025.131014</guid><pubDate>Sat, 27 Dec 2025 16:14:50 +0000</pubDate><dc:creator>Chao Lin</dc:creator><dc:creator>Tongzhou Zhang</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Yiou Wang</dc:creator><dc:creator>Wei Zhang</dc:creator><dc:creator>Gang Wang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131014</prism:doi><description>Monocular 3D object detection, which aims to infer 3D geometric properties and spatial locations from a single image, is critical for applications such as autonomous driving. However, the inherent depth ambiguity in projecting 3D space from 2D images makes this task particularly challenging. Existing methods often suffer from insufficient interaction between encoded features and object queries during decoding, limiting their ability to model complex 3D relationships. To address these issues, this paper proposes Parallel Multi-time Inquiry and Mixup-enhanced Monocular 3D Detector (PMM3D), a novel framework that enhances feature interaction and data diversity. The core of our method is a Parallel Multi-time Inquiries (PMI) mechanism integrated into the decoder, which allows object queries to interact multiple times in parallel with both visual and depth-aware features within a single decoding layer. This design significantly improves the modeling capacity for 3D structures. In addition, we introduce a conditionally constrained data augmentation strategy, MixDA3D, which synthesizes diverse training samples while maintaining geometric plausibility, thereby improving generalization. Extensive experiments on the KITTI benchmark demonstrate the effectiveness of PMM3D. It achieves competitive performance, especially in moderate and hard scenarios. Ablation studies confirm the complementary contributions of the PMI mechanism and MixDA3D. Moreover, qualitative visualizations reveal the adaptive behavior of the inquiry heads in different scenarios.
Published: 2025-12-27T16:14:50+00:00
Venue: Expert Systems with Applications
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Lin; Tongzhou Zhang; Wei Zhou; Yiou Wang; Wei Zhang; Gang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131014"&gt;10.1016/j.eswa.2025.131014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection, which aims to infer 3D geometric properties and spatial locations from a single image, is critical for applications such as autonomous driving. However, the inherent depth ambiguity in projecting 3D space from 2D images makes this task particularly challenging. Existing methods often suffer from insufficient interaction between encoded features and object queries during decoding, limiting their ability to model complex 3D relationships. To address these issues, this paper proposes Parallel Multi-time Inquiry and Mixup-enhanced Monocular 3D Detector (PMM3D), a novel framework that enhances feature interaction and data diversity. The core of our method is a Parallel Multi-time Inquiries (PMI) mechanism integrated into the decoder, which allows object queries to interact multiple times in parallel with both visual and depth-aware features within a single decoding layer. This design significantly improves the modeling capacity for 3D structures. In addition, we introduce a conditionally constrained data augmentation strategy, MixDA3D, which synthesizes diverse training samples while maintaining geometric plausibility, thereby improving generalization. Extensive experiments on the KITTI benchmark demonstrate the effectiveness of PMM3D. It achieves competitive performance, especially in moderate and hard scenarios. Ablation studies confirm the complementary contributions of the PMI mechanism and MixDA3D. Moreover, qualitative visualizations reveal the adaptive behavior of the inquiry heads in different scenarios.&lt;/p&gt;</content:encoded></item><item><title>Attention Reallocation: Towards Zero-cost and Controllable Hallucination Mitigation of MLLMs</title><link>https://doi.org/10.1007/s11263-025-02607-z</link><guid>10.1007/s11263-025-02607-z</guid><pubDate>Fri, 26 Dec 2025 18:57:08 +0000</pubDate><dc:creator>Chongjun Tu</dc:creator><dc:creator>Peng Ye</dc:creator><dc:creator>Dongzhan Zhou</dc:creator><dc:creator>Lei Bai</dc:creator><dc:creator>Gang Yu</dc:creator><dc:creator>Tao Chen</dc:creator><dc:creator>Wanli Ouyang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02607-z</prism:doi><description>Multi-Modal Large Language Models (MLLMs) stand out in various tasks but still struggle with hallucinations. While recent training-free mitigation methods mostly introduce additional inference overhead through a retrospection strategy and contrastive decoding, we propose attention reallocation (AttnReal) to mitigate MLLM hallucinations with nearly zero extra cost. Our approach is motivated by the key observations that, MLLM’s unreasonable attention distribution causes features to be dominated by historical output tokens, which further contributes to hallucinated responses because of the distribution gap between different token types. Based on the observations, AttnReal recycles excessive attention from output tokens and reallocates it to visual tokens, which reduces MLLM’s reliance on language priors and ensures the decoding process depends more on the visual inputs. Notably, by controlling the intensity of AttnReal, we can achieve a wide-range trade-off between response faithfulness and overall performance. Comprehensive results from four hallucination benchmarks validate the effectiveness of AttnReal across six open-source MLLMs and three decoding strategies. Further evaluations on four general vision-language tasks and generated text quality demonstrate that AttnReal improves general visual understanding capabilities and output quality of MLLMs. All the codes will be open-sourced soon.
Published: 2025-12-26T18:57:08+00:00
Venue: International Journal of Computer Vision
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chongjun Tu; Peng Ye; Dongzhan Zhou; Lei Bai; Gang Yu; Tao Chen; Wanli Ouyang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02607-z"&gt;10.1007/s11263-025-02607-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-Modal Large Language Models (MLLMs) stand out in various tasks but still struggle with hallucinations. While recent training-free mitigation methods mostly introduce additional inference overhead through a retrospection strategy and contrastive decoding, we propose attention reallocation (AttnReal) to mitigate MLLM hallucinations with nearly zero extra cost. Our approach is motivated by the key observations that, MLLM’s unreasonable attention distribution causes features to be dominated by historical output tokens, which further contributes to hallucinated responses because of the distribution gap between different token types. Based on the observations, AttnReal recycles excessive attention from output tokens and reallocates it to visual tokens, which reduces MLLM’s reliance on language priors and ensures the decoding process depends more on the visual inputs. Notably, by controlling the intensity of AttnReal, we can achieve a wide-range trade-off between response faithfulness and overall performance. Comprehensive results from four hallucination benchmarks validate the effectiveness of AttnReal across six open-source MLLMs and three decoding strategies. Further evaluations on four general vision-language tasks and generated text quality demonstrate that AttnReal improves general visual understanding capabilities and output quality of MLLMs. All the codes will be open-sourced soon.&lt;/p&gt;</content:encoded></item><item><title>Satellite Image Denoising Techniques using CSC Data Fidelity with Adaptive Total Variation Regularization</title><link>https://doi.org/10.1109/tgrs.2025.3648809</link><guid>10.1109/tgrs.2025.3648809</guid><pubDate>Fri, 26 Dec 2025 18:23:46 +0000</pubDate><dc:creator>Kalukuri Princy Niveditha</dc:creator><dc:creator>Amit Vishwakarma</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648809</prism:doi><description>Satellite image denoising is a fundamental preprocessing step for restoring the original visual and spectral quality of remotely sensed data, as noise can significantly distort spatial and spectral information. Conventional denoising methods often struggle to effectively handle high-resolution satellite imagery, leading to loss of fine textures and structural integrity. To address these challenges, this paper introduces a novel Convolutional Sparse Representation (CSR) based denoising framework that integrates adaptive Total Variation (TV) regularization with noise specific data fidelity terms. For Gaussian noise, an L2TV model is employed to ensure smooth restoration, while for Impulse noise, an L1TV model is utilized to robustly suppress sparse outliers. Experimental evaluations conducted on benchmark satellite datasets demonstrate that the proposed framework achieves superior quantitative performance in terms of Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), Visual Information Fidelity (VIF), Figure of Merit (FOM) as well as improved visual quality when compared with traditional and contemporary state-of-the-art techniques. The proposed approach thus provides an efficient and generalized solution for enhancing the quality and interpretability of satellite imagery in diverse remote sensing applications.
Published: 2025-12-26T18:23:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kalukuri Princy Niveditha; Amit Vishwakarma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648809"&gt;10.1109/tgrs.2025.3648809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Satellite image denoising is a fundamental preprocessing step for restoring the original visual and spectral quality of remotely sensed data, as noise can significantly distort spatial and spectral information. Conventional denoising methods often struggle to effectively handle high-resolution satellite imagery, leading to loss of fine textures and structural integrity. To address these challenges, this paper introduces a novel Convolutional Sparse Representation (CSR) based denoising framework that integrates adaptive Total Variation (TV) regularization with noise specific data fidelity terms. For Gaussian noise, an L2TV model is employed to ensure smooth restoration, while for Impulse noise, an L1TV model is utilized to robustly suppress sparse outliers. Experimental evaluations conducted on benchmark satellite datasets demonstrate that the proposed framework achieves superior quantitative performance in terms of Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), Visual Information Fidelity (VIF), Figure of Merit (FOM) as well as improved visual quality when compared with traditional and contemporary state-of-the-art techniques. The proposed approach thus provides an efficient and generalized solution for enhancing the quality and interpretability of satellite imagery in diverse remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>Fast SAM2 with Text-Driven Token Pruning</title><link>https://arxiv.org/abs/2512.21333v1</link><guid>http://arxiv.org/abs/2512.21333v1</guid><pubDate>Wed, 24 Dec 2025 18:59:05 +0000</pubDate><dc:creator>Avilasha Mandal</dc:creator><dc:creator>Chaoning Zhang</dc:creator><dc:creator>Fachrina Dewi Puspitasari</dc:creator><dc:creator>Xudong Wang</dc:creator><dc:creator>Jiaquan Zhang</dc:creator><dc:creator>Caiyan Qin</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Heng Tao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.
Published: 2025-12-24T18:59:05+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Avilasha Mandal; Chaoning Zhang; Fachrina Dewi Puspitasari; Xudong Wang; Jiaquan Zhang; Caiyan Qin; Guoqing Wang; Yang Yang; Heng Tao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.&lt;/p&gt;</content:encoded></item><item><title>Two-stage offline knowledge distillation for onboard registration of multispectral satellite images</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.009</link><guid>10.1016/j.isprsjprs.2025.12.009</guid><pubDate>Fri, 26 Dec 2025 14:26:48 +0000</pubDate><dc:creator>Darshana Priyasad</dc:creator><dc:creator>Tharindu Fernando</dc:creator><dc:creator>Maryam Haghighat</dc:creator><dc:creator>Harshala Gammulle</dc:creator><dc:creator>Roberto Del Prete</dc:creator><dc:creator>Clinton Fookes</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.009</prism:doi><description>Multi-band optical sensors onboard modern Earth observation satellites capture complementary spectral responses across varying spatial and spectral resolutions. To effectively fuse this information for downstream applications, accurate band co-registration is critical. However, for real-time processing, such registration must be performed onboard, where sensor distortions, platform-induced motion, and spectral disparities introduce significant challenges. Traditional feature matching algorithms struggle to cope with these variations or are often too computationally intensive for the constrained hardware typically found on small satellites. As a result, real-time onboard multimodal fusion has remained largely impractical in operational settings. With the emergence of next-generation satellites equipped with AI-enabled onboard processing, such as Australia’s Kanyini mission, there is now an opportunity to overcome these limitations. In this work, we introduce a deep learning-based, lightweight band registration framework specifically designed for real-time onboard deployment. Our approach features a band-independent teacher network that jointly leverages adversarial learning and supervised regression to estimate affine registration parameters across spectral bands. To meet hardware constraints, we employ a two-stage knowledge distillation strategy that produces a compact yet accurate student model. Experimental results demonstrate that our method delivers robust and efficient registration performance, enabling real-time spectral alignment and significantly enhancing the potential for onboard multimodal data fusion in Earth observation missions.
Published: 2025-12-26T14:26:48+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Darshana Priyasad; Tharindu Fernando; Maryam Haghighat; Harshala Gammulle; Roberto Del Prete; Clinton Fookes&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.009"&gt;10.1016/j.isprsjprs.2025.12.009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-band optical sensors onboard modern Earth observation satellites capture complementary spectral responses across varying spatial and spectral resolutions. To effectively fuse this information for downstream applications, accurate band co-registration is critical. However, for real-time processing, such registration must be performed onboard, where sensor distortions, platform-induced motion, and spectral disparities introduce significant challenges. Traditional feature matching algorithms struggle to cope with these variations or are often too computationally intensive for the constrained hardware typically found on small satellites. As a result, real-time onboard multimodal fusion has remained largely impractical in operational settings. With the emergence of next-generation satellites equipped with AI-enabled onboard processing, such as Australia’s Kanyini mission, there is now an opportunity to overcome these limitations. In this work, we introduce a deep learning-based, lightweight band registration framework specifically designed for real-time onboard deployment. Our approach features a band-independent teacher network that jointly leverages adversarial learning and supervised regression to estimate affine registration parameters across spectral bands. To meet hardware constraints, we employ a two-stage knowledge distillation strategy that produces a compact yet accurate student model. Experimental results demonstrate that our method delivers robust and efficient registration performance, enabling real-time spectral alignment and significantly enhancing the potential for onboard multimodal data fusion in Earth observation missions.&lt;/p&gt;</content:encoded></item><item><title>Latent Implicit Visual Reasoning</title><link>https://arxiv.org/abs/2512.21218v1</link><guid>http://arxiv.org/abs/2512.21218v1</guid><pubDate>Wed, 24 Dec 2025 14:59:49 +0000</pubDate><dc:creator>Kelvin Li</dc:creator><dc:creator>Chuyi Shang</dc:creator><dc:creator>Leonid Karlinsky</dc:creator><dc:creator>Rogerio Feris</dc:creator><dc:creator>Trevor Darrell</dc:creator><dc:creator>Roei Herzig</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what "useful" visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.
Published: 2025-12-24T14:59:49+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kelvin Li; Chuyi Shang; Leonid Karlinsky; Rogerio Feris; Trevor Darrell; Roei Herzig&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what &amp;quot;useful&amp;quot; visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks -- including those where intermediate abstractions are hard to specify -- while also generalizing to multi-task instruction tuning.&lt;/p&gt;</content:encoded></item><item><title>Embracing the Power of Known Class Bias in Open Set Recognition from A Reconstruction Perspective</title><link>https://doi.org/10.1109/tip.2025.3644791</link><guid>10.1109/tip.2025.3644791</guid><pubDate>Fri, 26 Dec 2025 18:25:24 +0000</pubDate><dc:creator>Heyang Sun</dc:creator><dc:creator>Chuanxing Geng</dc:creator><dc:creator>Songcan Chen</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644791</prism:doi><description>The open set known class bias is conventionally viewed as a fatal problem i.e., the models trained solely on known classes tend to fit unknown classes to known classes with high confidence in inference. Thus existing methods, without exception make a choice in two manners: most methods opt for eliminating the known class bias as much as possible with tireless efforts, while others circumvent the known class bias by employing a reconstruction method. However, in this paper, we challenge the two widely accepted approaches and present a novel proposition: the so-called harmful known class bias for most methods is, exactly conversely, beneficial for the reconstruction-based method and thus such known class bias can serve as a positive-incentive to the Open set recognition (OSR) models from a reconstruction perspective. Along this line, we propose the Bias Enhanced Reconstruction Learning (BERL) framework to enhance the known class bias respectively from the class level, model level and sample level. Specifically, at the class level, a specific representation is constructed in a supervised contrastive manner to avoid overgeneralization, while a diffusion model is employed by injecting the class prior to guide the biased reconstruction at the model level. Additionally, we leverage the advantages of the diffusion model to design a self-adaptive strategy, enabling effective sample-level biased sampling based on the information bottleneck theory. Experiments on various benchmarks demonstrate the effectiveness and performance superiority of the proposed method.
Published: 2025-12-26T18:25:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Heyang Sun; Chuanxing Geng; Songcan Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644791"&gt;10.1109/tip.2025.3644791&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (consider)&lt;/p&gt;
&lt;p&gt;The open set known class bias is conventionally viewed as a fatal problem i.e., the models trained solely on known classes tend to fit unknown classes to known classes with high confidence in inference. Thus existing methods, without exception make a choice in two manners: most methods opt for eliminating the known class bias as much as possible with tireless efforts, while others circumvent the known class bias by employing a reconstruction method. However, in this paper, we challenge the two widely accepted approaches and present a novel proposition: the so-called harmful known class bias for most methods is, exactly conversely, beneficial for the reconstruction-based method and thus such known class bias can serve as a positive-incentive to the Open set recognition (OSR) models from a reconstruction perspective. Along this line, we propose the Bias Enhanced Reconstruction Learning (BERL) framework to enhance the known class bias respectively from the class level, model level and sample level. Specifically, at the class level, a specific representation is constructed in a supervised contrastive manner to avoid overgeneralization, while a diffusion model is employed by injecting the class prior to guide the biased reconstruction at the model level. Additionally, we leverage the advantages of the diffusion model to design a self-adaptive strategy, enabling effective sample-level biased sampling based on the information bottleneck theory. Experiments on various benchmarks demonstrate the effectiveness and performance superiority of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Frequency-Guided Denoising Network for Semantic Segmentation of Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3648408</link><guid>10.1109/tgrs.2025.3648408</guid><pubDate>Fri, 26 Dec 2025 18:23:46 +0000</pubDate><dc:creator>Xin Li</dc:creator><dc:creator>Feng Xu</dc:creator><dc:creator>Jue Zhang</dc:creator><dc:creator>Hongsheng Zhang</dc:creator><dc:creator>Xin Lyu</dc:creator><dc:creator>Fan Liu</dc:creator><dc:creator>Hongmin Gao</dc:creator><dc:creator>André Kaup</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648408</prism:doi><description>Semantic segmentation of high-resolution remote sensing images remains challenging due to the degradation of high-frequency semantic cues during convolutional encoding and the lack of frequency consistency in multi-stage feature fusion. To address these issues, we propose FreDNet, a frequency-guided denoising network that explicitly enhances frequency-sensitive representations throughout the segmentation process. Specifically, we introduce the Dual-path Residual Block (DRB), which incorporates a Frequency-aware Denoising Module (FDM) and a Frequency-aware Fusion Module (FFM) to suppress frequency-domain noise while preserving edge structures. Furthermore, we design a Frequency-aware Cross-level Fusion Module (FCFM) that leverages frequency intensity response maps to adaptively fuse encoder and decoder features. These components work collaboratively to enhance the frequency robustness and spatial consistency of the segmentation predictions. Extensive experiments on three challenging benchmarks, ISPRS Vaihingen, ISPRS Potsdam, and LoveDA, demonstrate that FreDNet achieves superior performance, surpassing the latest state-of-the-art approaches by up to 0.8% in mean IoU and 0.9% in overall accuracy, while maintaining a lightweight inference cost. In addition, ablation study confirms the contribution of each component of FreDNet.
Published: 2025-12-26T18:23:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.786 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Li; Feng Xu; Jue Zhang; Hongsheng Zhang; Xin Lyu; Fan Liu; Hongmin Gao; André Kaup&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648408"&gt;10.1109/tgrs.2025.3648408&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of high-resolution remote sensing images remains challenging due to the degradation of high-frequency semantic cues during convolutional encoding and the lack of frequency consistency in multi-stage feature fusion. To address these issues, we propose FreDNet, a frequency-guided denoising network that explicitly enhances frequency-sensitive representations throughout the segmentation process. Specifically, we introduce the Dual-path Residual Block (DRB), which incorporates a Frequency-aware Denoising Module (FDM) and a Frequency-aware Fusion Module (FFM) to suppress frequency-domain noise while preserving edge structures. Furthermore, we design a Frequency-aware Cross-level Fusion Module (FCFM) that leverages frequency intensity response maps to adaptively fuse encoder and decoder features. These components work collaboratively to enhance the frequency robustness and spatial consistency of the segmentation predictions. Extensive experiments on three challenging benchmarks, ISPRS Vaihingen, ISPRS Potsdam, and LoveDA, demonstrate that FreDNet achieves superior performance, surpassing the latest state-of-the-art approaches by up to 0.8% in mean IoU and 0.9% in overall accuracy, while maintaining a lightweight inference cost. In addition, ablation study confirms the contribution of each component of FreDNet.&lt;/p&gt;</content:encoded></item><item><title>Contourlet Refinement Gate Framework for Thermal Spectrum Distribution Regularized Infrared Image Super-Resolution</title><link>https://doi.org/10.1007/s11263-025-02668-0</link><guid>10.1007/s11263-025-02668-0</guid><pubDate>Sat, 27 Dec 2025 07:49:37 +0000</pubDate><dc:creator>Yang Zou</dc:creator><dc:creator>Zhixin Chen</dc:creator><dc:creator>Zhipeng Zhang</dc:creator><dc:creator>Xingyuan Li</dc:creator><dc:creator>Long Ma</dc:creator><dc:creator>Jinyuan Liu</dc:creator><dc:creator>Peng Wang</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02668-0</prism:doi><description>Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at .
Published: 2025-12-27T07:49:37+00:00
Venue: International Journal of Computer Vision
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Zou; Zhixin Chen; Zhipeng Zhang; Xingyuan Li; Long Ma; Jinyuan Liu; Peng Wang; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02668-0"&gt;10.1007/s11263-025-02668-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at .&lt;/p&gt;</content:encoded></item><item><title>Closing the Training-Sampling Gap in Conditional Diffusion Models for Versatile Image Restoration</title><link>https://doi.org/10.1016/j.patcog.2025.112983</link><guid>10.1016/j.patcog.2025.112983</guid><pubDate>Fri, 26 Dec 2025 07:33:19 +0000</pubDate><dc:creator>Yue Lei</dc:creator><dc:creator>Jiati Cai</dc:creator><dc:creator>Wenxin Tai</dc:creator><dc:creator>Ting Zhong</dc:creator><dc:creator>Jin Yin</dc:creator><dc:creator>Kunpeng Zhang</dc:creator><dc:creator>Fan Zhou</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112983</prism:doi><description>Conditional diffusion models have emerged as powerful tools for image restoration, yet a significant bottleneck is the training-sampling discrepancy: models trained on forward diffusion samples are ill-equipped to handle backward diffusion noise encountered during sampling. To address this, we propose two new data augmentation strategies: Dynamic Condition Interpolation, which expands the training data distribution and Self-Generated Augmentation, which explicitly mitigates estimation errors. Our approach is model-agnostic and can be seamlessly integrated into existing diffusion models without introducing complex theoretical constraints. Experiments across four representative image restoration tasks demonstrate that our approach significantly outperforms fourteen advanced baselines, effectively closing the training-sampling gap and achieving state-of-the-art image restoration performance.
Published: 2025-12-26T07:33:19+00:00
Venue: Pattern Recognition
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Lei; Jiati Cai; Wenxin Tai; Ting Zhong; Jin Yin; Kunpeng Zhang; Fan Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112983"&gt;10.1016/j.patcog.2025.112983&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;Conditional diffusion models have emerged as powerful tools for image restoration, yet a significant bottleneck is the training-sampling discrepancy: models trained on forward diffusion samples are ill-equipped to handle backward diffusion noise encountered during sampling. To address this, we propose two new data augmentation strategies: Dynamic Condition Interpolation, which expands the training data distribution and Self-Generated Augmentation, which explicitly mitigates estimation errors. Our approach is model-agnostic and can be seamlessly integrated into existing diffusion models without introducing complex theoretical constraints. Experiments across four representative image restoration tasks demonstrate that our approach significantly outperforms fourteen advanced baselines, effectively closing the training-sampling gap and achieving state-of-the-art image restoration performance.&lt;/p&gt;</content:encoded></item><item><title>BACFormer: a robust boundary-aware transformer for medical image segmentation</title><link>https://doi.org/10.1016/j.knosys.2025.115209</link><guid>10.1016/j.knosys.2025.115209</guid><pubDate>Fri, 26 Dec 2025 16:36:15 +0000</pubDate><dc:creator>Zhiyong Huang</dc:creator><dc:creator>Mingyu Wang</dc:creator><dc:creator>Mingyang Hou</dc:creator><dc:creator>Zhi Yu</dc:creator><dc:creator>Shiwei Wang</dc:creator><dc:creator>Xiaoyu Li</dc:creator><dc:creator>Jiahong Wang</dc:creator><dc:creator>Yan Yan</dc:creator><dc:creator>Yushi Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115209</prism:doi><description>Recent advances in Transformer-based models have greatly improved the global context in medical image segmentation. However, these models often struggle to capture fine-grained local details, especially at organ boundaries. These details are critical for accurate segmentation in medical imaging tasks. To address this challenge, we propose the Boundary-Aware Convolutional Transformer (BACFormer), a novel U-shaped hierarchical Transformer architecture aimed at enhancing boundary and local detail segmentation, while maintaining long-range dependencies. BACFormer has two key innovations: (1) Hierarchical Attention Module (HAM). It combines the Boundary-Aware Convolutional Attention Module (BACAM) with Dilated Grid Attention (DGA). This improves boundary perception and multi-scale feature extraction. This module is highly portable, making it suitable for a wide range of vision tasks requiring robust multi-scale feature extraction. (2) Symmetric Convolutional Feed-Forward Network (SC-FFN), which facilitates local feature fusion and redistribution to improve segmentation accuracy, especially for small organs and blurred edges. BACFormer demonstrates the strong capacity to maintain long-range dependencies while simultaneously enhancing local boundary precision and detail capture. Extensive experiments on CT and MRI datasets show that BACFormer outperforms state-of-the-art methods, including those pre-trained on ImageNet. The code is publicly available at https://github.com/AmariJane/BACFormer .
Published: 2025-12-26T16:36:15+00:00
Venue: Knowledge-Based Systems
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiyong Huang; Mingyu Wang; Mingyang Hou; Zhi Yu; Shiwei Wang; Xiaoyu Li; Jiahong Wang; Yan Yan; Yushi Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115209"&gt;10.1016/j.knosys.2025.115209&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in Transformer-based models have greatly improved the global context in medical image segmentation. However, these models often struggle to capture fine-grained local details, especially at organ boundaries. These details are critical for accurate segmentation in medical imaging tasks. To address this challenge, we propose the Boundary-Aware Convolutional Transformer (BACFormer), a novel U-shaped hierarchical Transformer architecture aimed at enhancing boundary and local detail segmentation, while maintaining long-range dependencies. BACFormer has two key innovations: (1) Hierarchical Attention Module (HAM). It combines the Boundary-Aware Convolutional Attention Module (BACAM) with Dilated Grid Attention (DGA). This improves boundary perception and multi-scale feature extraction. This module is highly portable, making it suitable for a wide range of vision tasks requiring robust multi-scale feature extraction. (2) Symmetric Convolutional Feed-Forward Network (SC-FFN), which facilitates local feature fusion and redistribution to improve segmentation accuracy, especially for small organs and blurred edges. BACFormer demonstrates the strong capacity to maintain long-range dependencies while simultaneously enhancing local boundary precision and detail capture. Extensive experiments on CT and MRI datasets show that BACFormer outperforms state-of-the-art methods, including those pre-trained on ImageNet. The code is publicly available at https://github.com/AmariJane/BACFormer .&lt;/p&gt;</content:encoded></item><item><title>ICSD-YOLO: Intelligent Detection for Real-time Industrial Field Safety</title><link>https://doi.org/10.1016/j.eswa.2025.130994</link><guid>10.1016/j.eswa.2025.130994</guid><pubDate>Sat, 27 Dec 2025 00:07:00 +0000</pubDate><dc:creator>Cheng Shi</dc:creator><dc:creator>Yan Chen</dc:creator><dc:creator>Chong Zhang</dc:creator><dc:creator>Dong-Guo Chang</dc:creator><dc:creator>Yi-Jia Chen</dc:creator><dc:creator>Qian Wang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130994</prism:doi><description>Comprehensive and accurate object detection in industrial environments is crucial for ensuring operational safety. However, existing real-time detectors based on ConvNet such as YOLOv series face constraints in early-stage feature extraction capability and lack effective perception mechanisms to deal with occlusion, deformation, and scale variation. Transformer-based detectors strengthen global context modeling through self-attention and achieve better performance on complex benchmarks. However, their high computational cost and large model size limit their applicability, particularly in resource-constrained industrial environments. To address these issues, we propose a family of object detectors, named ICSD-YOLO (Information ConvStem FocusBlock Detector) , a lightweight detection framework that enhances features encode and hierarchical perception. We design a ConvStemBlock to improve low-level feature extraction and enlarge the receptive field, and a FocusBlock to perform multi-level semantic refinement.We implement ICSD-YOLO based on YOLO Series, and evaluate it across five model scales (Nano, Small, Medium, Large, Extra-Large) on the COCO benchmark and a industrial field dataset. Experimental results show that the mAP 50: 95 of our ICSD-YOLO-X rises from 66.9% to 68.1% (+1.2%), and the F1-score increases from 80.8% to 83.0% (+2.2%) compared to the original YOLOv12-X while reducing FLOPs by 41.3% (from 199G to 116.8G), demonstrating better perception under complex conditions and suitability for deployment in safety-critical scenarios.The code is available at https://github.com/PrintSC/code
Published: 2025-12-27T00:07:00+00:00
Venue: Expert Systems with Applications
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheng Shi; Yan Chen; Chong Zhang; Dong-Guo Chang; Yi-Jia Chen; Qian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130994"&gt;10.1016/j.eswa.2025.130994&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Comprehensive and accurate object detection in industrial environments is crucial for ensuring operational safety. However, existing real-time detectors based on ConvNet such as YOLOv series face constraints in early-stage feature extraction capability and lack effective perception mechanisms to deal with occlusion, deformation, and scale variation. Transformer-based detectors strengthen global context modeling through self-attention and achieve better performance on complex benchmarks. However, their high computational cost and large model size limit their applicability, particularly in resource-constrained industrial environments. To address these issues, we propose a family of object detectors, named ICSD-YOLO (Information ConvStem FocusBlock Detector) , a lightweight detection framework that enhances features encode and hierarchical perception. We design a ConvStemBlock to improve low-level feature extraction and enlarge the receptive field, and a FocusBlock to perform multi-level semantic refinement.We implement ICSD-YOLO based on YOLO Series, and evaluate it across five model scales (Nano, Small, Medium, Large, Extra-Large) on the COCO benchmark and a industrial field dataset. Experimental results show that the mAP 50: 95 of our ICSD-YOLO-X rises from 66.9% to 68.1% (+1.2%), and the F1-score increases from 80.8% to 83.0% (+2.2%) compared to the original YOLOv12-X while reducing FLOPs by 41.3% (from 199G to 116.8G), demonstrating better perception under complex conditions and suitability for deployment in safety-critical scenarios.The code is available at https://github.com/PrintSC/code&lt;/p&gt;</content:encoded></item><item><title>Learning from Next-Frame Prediction: Autoregressive Video Modeling Encodes Effective Representations</title><link>https://arxiv.org/abs/2512.21004v1</link><guid>http://arxiv.org/abs/2512.21004v1</guid><pubDate>Wed, 24 Dec 2025 07:07:08 +0000</pubDate><dc:creator>Jinghan Li</dc:creator><dc:creator>Yang Jin</dc:creator><dc:creator>Hao Jiang</dc:creator><dc:creator>Yadong Mu</dc:creator><dc:creator>Yang Song</dc:creator><dc:creator>Kun Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.
Published: 2025-12-24T07:07:08+00:00
Venue: arXiv
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinghan Li; Yang Jin; Hao Jiang; Yadong Mu; Yang Song; Kun Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in pretraining general foundation models have significantly improved performance across diverse downstream tasks. While autoregressive (AR) generative models like GPT have revolutionized NLP, most visual generative pretraining methods still rely on BERT-style masked modeling, which often disregards the temporal information essential for video analysis. The few existing autoregressive visual pretraining methods suffer from issues such as inaccurate semantic localization and poor generation quality, leading to poor semantics. In this work, we propose NExT-Vid, a novel autoregressive visual generative pretraining framework that utilizes masked next-frame prediction to jointly model images and videos. NExT-Vid introduces a context-isolated autoregressive predictor to decouple semantic representation from target decoding, and a conditioned flow-matching decoder to enhance generation quality and diversity. Through context-isolated flow-matching pretraining, our approach achieves strong representations. Extensive experiments on large-scale pretrained models demonstrate that our proposed method consistently outperforms previous generative pretraining methods for visual representation learning via attentive probing in downstream classification.&lt;/p&gt;</content:encoded></item><item><title>From Pixels to Meters: Ground Sampling Distance Priors for Remote Sensing Detection</title><link>https://doi.org/10.1109/lgrs.2025.3648944</link><guid>10.1109/lgrs.2025.3648944</guid><pubDate>Fri, 26 Dec 2025 18:25:46 +0000</pubDate><dc:creator>Shihao Yu</dc:creator><dc:creator>Yifan Dong</dc:creator><dc:creator>Yun Su</dc:creator><dc:creator>Yang Zhao</dc:creator><dc:creator>Xiaoqiang Jia</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3648944</prism:doi><description>Object detection in remote sensing imagery is challenging because targets often appear at markedly different scales and may become visually similar when the image resolution varies. A key advantage of remote sensing imagery is that each image provides a ground sample distance (GSD) value linking pixel units to real-world size, offering physical cues that can mitigate such scale-induced ambiguity. While recent methods have attempted to exploit GSD, many rely on additional subnetworks or heuristic weighting and still fail to capture category-specific size characteristics. To address this, we propose a lightweight module, Pixels-to-Meters Prior Fusion (P2M-PF), which integrates physical-size priors into the classification branch without altering the detection architecture. Experiments on the GSD-labelled subset of DOTA v1.0 demonstrate consistent improvements over strong baselines, with notable gains for categories susceptible to cross-scale confusion.
Published: 2025-12-26T18:25:46+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shihao Yu; Yifan Dong; Yun Su; Yang Zhao; Xiaoqiang Jia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3648944"&gt;10.1109/lgrs.2025.3648944&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Object detection in remote sensing imagery is challenging because targets often appear at markedly different scales and may become visually similar when the image resolution varies. A key advantage of remote sensing imagery is that each image provides a ground sample distance (GSD) value linking pixel units to real-world size, offering physical cues that can mitigate such scale-induced ambiguity. While recent methods have attempted to exploit GSD, many rely on additional subnetworks or heuristic weighting and still fail to capture category-specific size characteristics. To address this, we propose a lightweight module, Pixels-to-Meters Prior Fusion (P2M-PF), which integrates physical-size priors into the classification branch without altering the detection architecture. Experiments on the GSD-labelled subset of DOTA v1.0 demonstrate consistent improvements over strong baselines, with notable gains for categories susceptible to cross-scale confusion.&lt;/p&gt;</content:encoded></item><item><title>Shadow-DETR: Alleviating Matching Conflicts through Shadow Queries</title><link>https://doi.org/10.1016/j.neunet.2025.108524</link><guid>10.1016/j.neunet.2025.108524</guid><pubDate>Sat, 27 Dec 2025 22:56:36 +0000</pubDate><dc:creator>Yunfei Ma</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Lingfeng Yang</dc:creator><dc:creator>Yifei Su</dc:creator><dc:creator>Yingpeng Li</dc:creator><dc:creator>Wankou Yang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108524</prism:doi><description>Leveraging the end-to-end detection capability enabled by one-to-one matching, DETR has achieved state-of-the-art performance in simplified pipelines. However, the one-to-one matching mechanism also introduces certain limitations, such as slow convergence, which can be attributed to challenges like matching conflicts and limited supervision imposed by the matching process. This paper analyzes and identifies two forms of conflicts that arise from one-to-one matching: opposite optimization directions for similar samples and misalignment in query-object matching between different decoder layers. To mitigate the conflict while maintaining the end-to-end properties, we identify negative samples that closely resemble positive samples as shadow samples and ignore their classification loss during training. To address the issue of limited supervision, we compute the regression loss for these shadow samples, thereby providing additional localization supervision. By addressing these issues, our strategy enhances network training efficiency and improves overall performance under identical training configurations. Furthermore, we propose a loss-balancing strategy to enhance the effectiveness of shadow samples. Additionally, a feature-aware query initialization approach is proposed that offers the benefits of providing distinct features to shadow queries and strengthening the interaction between queries and image features. Experimental results demonstrate that our Shadow-DETR substantially boosts existing methods such as DAB-DETR, Deformable-DETR, and DINO while achieving comparable performance with SOTA methods.
Published: 2025-12-27T22:56:36+00:00
Venue: Neural Networks
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunfei Ma; Jie Li; Lingfeng Yang; Yifei Su; Yingpeng Li; Wankou Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108524"&gt;10.1016/j.neunet.2025.108524&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Leveraging the end-to-end detection capability enabled by one-to-one matching, DETR has achieved state-of-the-art performance in simplified pipelines. However, the one-to-one matching mechanism also introduces certain limitations, such as slow convergence, which can be attributed to challenges like matching conflicts and limited supervision imposed by the matching process. This paper analyzes and identifies two forms of conflicts that arise from one-to-one matching: opposite optimization directions for similar samples and misalignment in query-object matching between different decoder layers. To mitigate the conflict while maintaining the end-to-end properties, we identify negative samples that closely resemble positive samples as shadow samples and ignore their classification loss during training. To address the issue of limited supervision, we compute the regression loss for these shadow samples, thereby providing additional localization supervision. By addressing these issues, our strategy enhances network training efficiency and improves overall performance under identical training configurations. Furthermore, we propose a loss-balancing strategy to enhance the effectiveness of shadow samples. Additionally, a feature-aware query initialization approach is proposed that offers the benefits of providing distinct features to shadow queries and strengthening the interaction between queries and image features. Experimental results demonstrate that our Shadow-DETR substantially boosts existing methods such as DAB-DETR, Deformable-DETR, and DINO while achieving comparable performance with SOTA methods.&lt;/p&gt;</content:encoded></item><item><title>Multiple space transfer learning based on maximizing mean variance differences for soft sensor modeling</title><link>https://doi.org/10.1016/j.eswa.2025.130975</link><guid>10.1016/j.eswa.2025.130975</guid><pubDate>Fri, 26 Dec 2025 16:02:57 +0000</pubDate><dc:creator>Xuan Hu</dc:creator><dc:creator>Siying Zhang</dc:creator><dc:creator>Tianyu Zhang</dc:creator><dc:creator>Yongming Han</dc:creator><dc:creator>Ling Wang</dc:creator><dc:creator>Zhiqiang Geng</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130975</prism:doi><description>The scarcity of labeled data significantly affects the effectiveness of industrial soft sensing. Domain adaptation can transfer rich label information from the source domain to the sparsely labeled target domain. However, existing domain adaptation methods typically align source and target domain features into a single feature space, which may negatively impact the adaptation performance of soft sensor models. Therefore, the multiple space transfer learning based on maximizing mean variance discrepancy (MMVD-MSTL) is proposed. First, source and target domain data are mapped into a spatio-temporal-frequency feature space based on feature-disentangled encoder. Then, the maximizing mean–variance discrepancy is designed to align disentangled features distributions across multiple spaces between the source and target domains. Finally, the cycle adversarial loss constrains feature distributions in both domains, establishing a reciprocal feature relationship. Comparative experiments on public datasets and real-world industrial process data demonstrate that MMVD-MSTL outperforms state-of-the-art domain adaptation soft sensor models, showing superior adaptation performance in the target domain
Published: 2025-12-26T16:02:57+00:00
Venue: Expert Systems with Applications
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuan Hu; Siying Zhang; Tianyu Zhang; Yongming Han; Ling Wang; Zhiqiang Geng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130975"&gt;10.1016/j.eswa.2025.130975&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;The scarcity of labeled data significantly affects the effectiveness of industrial soft sensing. Domain adaptation can transfer rich label information from the source domain to the sparsely labeled target domain. However, existing domain adaptation methods typically align source and target domain features into a single feature space, which may negatively impact the adaptation performance of soft sensor models. Therefore, the multiple space transfer learning based on maximizing mean variance discrepancy (MMVD-MSTL) is proposed. First, source and target domain data are mapped into a spatio-temporal-frequency feature space based on feature-disentangled encoder. Then, the maximizing mean–variance discrepancy is designed to align disentangled features distributions across multiple spaces between the source and target domains. Finally, the cycle adversarial loss constrains feature distributions in both domains, establishing a reciprocal feature relationship. Comparative experiments on public datasets and real-world industrial process data demonstrate that MMVD-MSTL outperforms state-of-the-art domain adaptation soft sensor models, showing superior adaptation performance in the target domain&lt;/p&gt;</content:encoded></item><item><title>IceShipvsNet:A Joint Network for Ship Detection in Ice-Infested Waters Using Visible and SWIR Images</title><link>https://doi.org/10.1109/lgrs.2025.3648659</link><guid>10.1109/lgrs.2025.3648659</guid><pubDate>Fri, 26 Dec 2025 18:25:46 +0000</pubDate><dc:creator>Bingxin Liu</dc:creator><dc:creator>Yulong Du</dc:creator><dc:creator>Yikai Huang</dc:creator><dc:creator>Peilin Wang</dc:creator><dc:creator>Peixin Cai</dc:creator><dc:creator>Ying Li</dc:creator><dc:creator>Peng Chen</dc:creator><dc:creator>Peng Liu</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3648659</prism:doi><description>In ice-infested waters, the complexity of background makes it challenging to accurately detect ship targets using only visible (VIS) remote sensing images. To address this challenge, we introduce short-wave infrared (SWIR) images to enhance target visibility and propose a joint network for ship detection in ice-infested waters using both VIS and SWIR images, termed IceShipvsNet. The joint detection backbone of this network is implemented using C2Former, which is known for its strong performance in multi-modal feature learning. To improve the multimodal feature extraction, we introduce a spectral frequency augmentation (SFA) module, which adaptively enhances or suppresses high-frequency features from VIS and SWIR images to improve target response and suppress background interference. Specifically, the SFA module incorporates two components: Frequency-Aware Modulation for VIS (FAM-VI), which regulates high-frequency noise in VIS images and enhances ship characteristics; SWIR High-Frequency Guided Attention (FGA-S), which boosts the high-frequency information in SWIR images and suppresses irrelevant background noise. Given the lack of publicly available datasets for ship detection in ice-infested waters, we construct a VIS-SWIR dataset (VS-IceShip) and use it for experimental evaluation. The results demonstrate that IceShipvsNet achieves superior detection accuracy compared to single-modal baselines on various detectors. Ablation studies further validate the effectiveness of SFA, with both FAM-VI and FGA-S contributing significantly to performance improvement.
Published: 2025-12-26T18:25:46+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.778 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bingxin Liu; Yulong Du; Yikai Huang; Peilin Wang; Peixin Cai; Ying Li; Peng Chen; Peng Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3648659"&gt;10.1109/lgrs.2025.3648659&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (consider)&lt;/p&gt;
&lt;p&gt;In ice-infested waters, the complexity of background makes it challenging to accurately detect ship targets using only visible (VIS) remote sensing images. To address this challenge, we introduce short-wave infrared (SWIR) images to enhance target visibility and propose a joint network for ship detection in ice-infested waters using both VIS and SWIR images, termed IceShipvsNet. The joint detection backbone of this network is implemented using C2Former, which is known for its strong performance in multi-modal feature learning. To improve the multimodal feature extraction, we introduce a spectral frequency augmentation (SFA) module, which adaptively enhances or suppresses high-frequency features from VIS and SWIR images to improve target response and suppress background interference. Specifically, the SFA module incorporates two components: Frequency-Aware Modulation for VIS (FAM-VI), which regulates high-frequency noise in VIS images and enhances ship characteristics; SWIR High-Frequency Guided Attention (FGA-S), which boosts the high-frequency information in SWIR images and suppresses irrelevant background noise. Given the lack of publicly available datasets for ship detection in ice-infested waters, we construct a VIS-SWIR dataset (VS-IceShip) and use it for experimental evaluation. The results demonstrate that IceShipvsNet achieves superior detection accuracy compared to single-modal baselines on various detectors. Ablation studies further validate the effectiveness of SFA, with both FAM-VI and FGA-S contributing significantly to performance improvement.&lt;/p&gt;</content:encoded></item><item><title>Optical and SAR Image Fusion: A Review of Theories, Methods, and Applications</title><link>https://doi.org/10.3390/rs18010073</link><guid>10.3390/rs18010073</guid><pubDate>Fri, 26 Dec 2025 02:07:58 +0000</pubDate><dc:creator>Ruyi Zhang</dc:creator><dc:creator>Yi Yang</dc:creator><dc:creator>Zhuoxuan Li</dc:creator><dc:creator>Peixuan Li</dc:creator><dc:creator>Haipeng Wang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010073</prism:doi><description>Remote sensing technology has become an indispensable core means for Earth observation. As two of the most commonly used remote sensing modalities, the fusion of optical and synthetic aperture radar (SAR) (OPT-SAR fusion) can effectively overcome the limitations of a single data source, achieve information complementarity and synergistic enhancement, thereby significantly improving the interpretation capability of multi-source remote sensing data. This paper first discusses the necessity of OPT-SAR fusion, systematically reviews the historical development of fusion technologies, and summarizes open-source resources for various tasks, aiming to provide a reference for related research. Finally, building upon recent advances in OPT-SAR fusion research and cutting-edge developments in deep learning, this paper proposes that future fusion technologies should develop in the following directions: interpretable fusion models driven by both data and knowledge, general fusion perception driven by multimodal large models, and lightweight architectures with efficient deployment strategies.
Published: 2025-12-26T02:07:58+00:00
Venue: Remote Sensing
Score: 0.776 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruyi Zhang; Yi Yang; Zhuoxuan Li; Peixuan Li; Haipeng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010073"&gt;10.3390/rs18010073&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing technology has become an indispensable core means for Earth observation. As two of the most commonly used remote sensing modalities, the fusion of optical and synthetic aperture radar (SAR) (OPT-SAR fusion) can effectively overcome the limitations of a single data source, achieve information complementarity and synergistic enhancement, thereby significantly improving the interpretation capability of multi-source remote sensing data. This paper first discusses the necessity of OPT-SAR fusion, systematically reviews the historical development of fusion technologies, and summarizes open-source resources for various tasks, aiming to provide a reference for related research. Finally, building upon recent advances in OPT-SAR fusion research and cutting-edge developments in deep learning, this paper proposes that future fusion technologies should develop in the following directions: interpretable fusion models driven by both data and knowledge, general fusion perception driven by multimodal large models, and lightweight architectures with efficient deployment strategies.&lt;/p&gt;</content:encoded></item><item><title>Enhanced Visual Prompt Meets Low-Light Saliency Detection</title><link>https://doi.org/10.1016/j.patcog.2025.113008</link><guid>10.1016/j.patcog.2025.113008</guid><pubDate>Sat, 27 Dec 2025 07:16:40 +0000</pubDate><dc:creator>Nana Yu</dc:creator><dc:creator>Jie Wang</dc:creator><dc:creator>Yahong Han</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113008</prism:doi><description>The presence of low-light scenes poses significant challenges to salient object detection (SOD), including false positives, false negatives, and missed detections. Existing approaches to low-light SOD can be broadly categorized into two paradigms. The first employs a two-stage framework, where image enhancement precedes saliency detection. The second integrates enhancement and saliency detection within a unified end-to-end framework, typically trained with comprehensive fine-tuning. However, these approaches face two main issues. In the two-stage framework, enhancement and SOD are treated as largely independent tasks, resulting in poor adaptability of enhanced images for SOD. In fully fine-tuned end-to-end frameworks, an inherent optimization conflict exists between enhancement and SOD. To address these issues, we propose Enhancement Visual Prompt (EnVP), which adopts local fine-tuning for low-light SOD. The core idea of EnVP is to fine-tune only the enhancement module rather than performing full fine-tuning. Specifically, the Transformer backbone is frozen, and only the enhancement prompt is fine-tuned. The enhancement level is constrained through illumination estimation and grayscale threshold judgment, allowing the model to gradually adapt to diverse low-light conditions. This approach mitigates the adverse effects of uniform enhancement on SOD performance. Extensive experiments show that EnVP outperforms state-of-the-art fully fine-tuned methods on various low-light SOD datasets. Moreover, on the RGBD-385 and RGBT-621 sub-datasets, EnVP improves the MAE metric by 27% and 35% , respectively.
Published: 2025-12-27T07:16:40+00:00
Venue: Pattern Recognition
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nana Yu; Jie Wang; Yahong Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113008"&gt;10.1016/j.patcog.2025.113008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;The presence of low-light scenes poses significant challenges to salient object detection (SOD), including false positives, false negatives, and missed detections. Existing approaches to low-light SOD can be broadly categorized into two paradigms. The first employs a two-stage framework, where image enhancement precedes saliency detection. The second integrates enhancement and saliency detection within a unified end-to-end framework, typically trained with comprehensive fine-tuning. However, these approaches face two main issues. In the two-stage framework, enhancement and SOD are treated as largely independent tasks, resulting in poor adaptability of enhanced images for SOD. In fully fine-tuned end-to-end frameworks, an inherent optimization conflict exists between enhancement and SOD. To address these issues, we propose Enhancement Visual Prompt (EnVP), which adopts local fine-tuning for low-light SOD. The core idea of EnVP is to fine-tune only the enhancement module rather than performing full fine-tuning. Specifically, the Transformer backbone is frozen, and only the enhancement prompt is fine-tuned. The enhancement level is constrained through illumination estimation and grayscale threshold judgment, allowing the model to gradually adapt to diverse low-light conditions. This approach mitigates the adverse effects of uniform enhancement on SOD performance. Extensive experiments show that EnVP outperforms state-of-the-art fully fine-tuned methods on various low-light SOD datasets. Moreover, on the RGBD-385 and RGBT-621 sub-datasets, EnVP improves the MAE metric by 27% and 35% , respectively.&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Benchmark for Evaluating Night-time Visual Object Tracking</title><link>https://doi.org/10.1007/s11263-025-02661-7</link><guid>10.1007/s11263-025-02661-7</guid><pubDate>Fri, 26 Dec 2025 17:58:36 +0000</pubDate><dc:creator>Yu Liu</dc:creator><dc:creator>Arif Mahmood</dc:creator><dc:creator>Muhammad Haris Khan</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02661-7</prism:doi><description>Several existing visual object tracking benchmarks, including OTB100, NfS, UAV123, LaSOT, and GOT-10K, mostly feature day-time scenarios. However, the challenges posed by the night-time remain relatively underexplored. We attribute this primarily to the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. In this paper, we first introduce NT-VOT211, a novel benchmark specifically designed to thoroughly evaluate visual object tracking algorithms under a wide range of challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. After conducting a comprehensive analysis of the results from 42 distinct tracking algorithms on the NT-VOT211 dataset, we develop a simple yet effective zero-shot domain adaptation method to significantly enhance the tracking performance of state-of-the-art trackers under low-light conditions. In this method, a newly designed module aims to distinguish the background token from the target token before feeding into the MLP Head. Our module enables for more accurate position estimation. Remarkably, it accomplishes this enhancement with merely 11 epochs of fine-tuning on a standard daylight dataset. Our dataset, code and other assets can be found at: https://github.com/LiuYuML/NV-VOT211
Published: 2025-12-26T17:58:36+00:00
Venue: International Journal of Computer Vision
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Liu; Arif Mahmood; Muhammad Haris Khan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02661-7"&gt;10.1007/s11263-025-02661-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Several existing visual object tracking benchmarks, including OTB100, NfS, UAV123, LaSOT, and GOT-10K, mostly feature day-time scenarios. However, the challenges posed by the night-time remain relatively underexplored. We attribute this primarily to the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. In this paper, we first introduce NT-VOT211, a novel benchmark specifically designed to thoroughly evaluate visual object tracking algorithms under a wide range of challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. After conducting a comprehensive analysis of the results from 42 distinct tracking algorithms on the NT-VOT211 dataset, we develop a simple yet effective zero-shot domain adaptation method to significantly enhance the tracking performance of state-of-the-art trackers under low-light conditions. In this method, a newly designed module aims to distinguish the background token from the target token before feeding into the MLP Head. Our module enables for more accurate position estimation. Remarkably, it accomplishes this enhancement with merely 11 epochs of fine-tuning on a standard daylight dataset. Our dataset, code and other assets can be found at: https://github.com/LiuYuML/NV-VOT211&lt;/p&gt;</content:encoded></item></channel></rss>