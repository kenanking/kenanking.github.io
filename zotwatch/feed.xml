<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 05 Jan 2026 02:50:12 +0000</lastBuildDate><item><title>DAK-Pose: Dual-Augmentor Knowledge Fusion for Generalizable Video-Based 3D Human Pose Estimation</title><link>https://doi.org/10.1016/j.inffus.2025.104100</link><guid>10.1016/j.inffus.2025.104100</guid><pubDate>Sat, 03 Jan 2026 16:24:02 +0000</pubDate><dc:creator>Yachuan Wang</dc:creator><dc:creator>Bin Zhang</dc:creator><dc:creator>Hao Yuan</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104100</prism:doi><description>Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.
Published: 2026-01-03T16:24:02+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yachuan Wang; Bin Zhang; Hao Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104100"&gt;10.1016/j.inffus.2025.104100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Real-world deployment of video-based 3D human pose estimation remains challenging, as limited annotated data collected in constrained lab settings cannot fully capture the complexity of human motion. While motion synthesis for data augmentation has emerged as a mainstream solution to enhance generalization, existing synthesis methods suffer from inherent trade-offs: kinematics-based motion synthesis approaches preserve anatomical plausibility but sacrifice temporal coherence, while coordinate-based methods ensure motion smoothness but violate biomechanical constraints. This results in persistent domain gaps when synthetic data is directly used in the observation space to train pose estimation models. To overcome this, we propose DAK-Pose, which shifts augmentation to the feature space. We disentangle motion into structural and dynamic features, and design two complementary augmentors: (1) A structure-prioritized module enforces kinematic constraints for anatomical validity, and (2) a dynamic-prioritized module generates diverse temporal patterns. Auxiliary encoders trained on synthetic motions generated by these augmentors transfer domain-invariant knowledge to the pose estimator through adversarial alignment. Experiments on Human3.6M, MPI-INF-3DHP, and 3DPW datasets show that DAK-Pose achieves state-of-the-art cross-dataset performance.&lt;/p&gt;</content:encoded></item><item><title>Progressive Temporal Compensation and Semantic Enhancement for Exo-to-Ego Video Generation</title><link>https://doi.org/10.1016/j.inffus.2025.104117</link><guid>10.1016/j.inffus.2025.104117</guid><pubDate>Sat, 03 Jan 2026 16:24:00 +0000</pubDate><dc:creator>Xingyue Wang</dc:creator><dc:creator>Weipeng Hu</dc:creator><dc:creator>Jiun Tian Hoe</dc:creator><dc:creator>Jianhui Li</dc:creator><dc:creator>Ping Hu</dc:creator><dc:creator>Yap-Peng Tan</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104117</prism:doi><description>Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.
Published: 2026-01-03T16:24:00+00:00
Venue: Information Fusion
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyue Wang; Weipeng Hu; Jiun Tian Hoe; Jianhui Li; Ping Hu; Yap-Peng Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104117"&gt;10.1016/j.inffus.2025.104117&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Transforming video perspectives from exocentric (third-person) to egocentric (first-person) is challenging due to limited overlap between two perspectives. Existing approaches often neglect the temporal dynamics—critical for capturing motion cues and reappearing objects—and do not fully exploit source-view inferred semantics. To address these limitations, we propose a Progressive Temporal Compensation and Semantic Enhancement (PCSE) framework for Exocentric-to-Egocentric Video Generation. The Progressive Temporal Compensation (PTC) module focuses on long-term temporal dependencies, progressively aligning exocentric temporal patterns with egocentric representations. By employing a reliance-shifting mechanism with a progression mask, PTC gradually reduces dependence on egocentric supervision, enabling more robust target-view learning. Moreover, to leverage high-level scene context, we introduce a Hierarchical Dual-channel Transformer (HDT), which jointly generates egocentric frames and their corresponding semantic layouts via dual encoder–decoder architectures with hierarchically processed transformer blocks. To further enhance structural coherence and semantic consistency, the generated semantic layouts guide frame refinement through an Uncertainty-aware Semantic Enhancement (USE) module. USE dynamically estimates uncertainty masks to locate and refine ambiguous regions, yielding more coherent and visually accurate results. Extensive experiments demonstrate that PCSE achieves leading performance among cue-free methods.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Harmful Meme Detection via Self-adaption Mixture-of-Experts</title><link>https://doi.org/10.1016/j.inffus.2026.104122</link><guid>10.1016/j.inffus.2026.104122</guid><pubDate>Sat, 03 Jan 2026 16:23:50 +0000</pubDate><dc:creator>Zou Li</dc:creator><dc:creator>Jinzhi Liao</dc:creator><dc:creator>Jiting Li</dc:creator><dc:creator>Ji Wang</dc:creator><dc:creator>Xiang Zhao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104122</prism:doi><description>The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.
Published: 2026-01-03T16:23:50+00:00
Venue: Information Fusion
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zou Li; Jinzhi Liao; Jiting Li; Ji Wang; Xiang Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104122"&gt;10.1016/j.inffus.2026.104122&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;The automatic detection of harmful memes is essential for healthy online ecosystems but remains challenging due to the intricate interaction between visual and textual elements. Recently, the remarkable capabilities of multimodal large language models (MLLMs) have significantly enhanced the detection performance, yet scarce labeled data still limits their effectiveness. Although pioneering few-shot studies have explored this regime, they merely leverage surface-level capabilities while ignoring deeper complexities. To approach the core of the problem, we identify its notorious challenges: (1) heterogeneous multimodal features are complex and may exhibit negative correlations; (2) the semantic patterns underlying single modal are hard to uncover; and (3) the insufficient training samples render models more reliant on commonsense. To address the challenges, we propose a structural self-adaption mixture-of-experts framework (SSMoE) for few-shot harmful meme detection, including universal and specialized experts to foster more effective knowledge sharing, modal synergy, and expert specialization within the MLLM structure. Specifically, SSMoE integrates four novel components: (1) Semantic Data Clustering module aims to partition heterogeneous source data and mitigate negative transfer; (2) Targeted Prompt Injection module aims to employ a teacher model for providing cluster-specific external guidance; (3) Asymmetric Expert Specialization module aims to introduce shared and specialized experts for efficient parameter adaptation and knowledge specialization; and (4) Cluster-conditioned Routing module aims to dynamically direct inputs to the most relevant expert pathway based on semantic cluster identity. Extensive experiments on three benchmark datasets (FHM, MAMI, HarM) demonstrate that SSMoE significantly outperforms state-of-the-art baseline methods, particularly in extremely low-data scenarios.&lt;/p&gt;</content:encoded></item><item><title>LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving</title><link>https://doi.org/10.1016/j.patcog.2026.113046</link><guid>10.1016/j.patcog.2026.113046</guid><pubDate>Sat, 03 Jan 2026 23:24:25 +0000</pubDate><dc:creator>Carlo Sgaravatti</dc:creator><dc:creator>Riccardo Pieroni</dc:creator><dc:creator>Matteo Corno</dc:creator><dc:creator>Sergio M. Savaresi</dc:creator><dc:creator>Luca Magri</dc:creator><dc:creator>Giacomo Boracchi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113046</prism:doi><description>Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .
Published: 2026-01-03T23:24:25+00:00
Venue: Pattern Recognition
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Carlo Sgaravatti; Riccardo Pieroni; Matteo Corno; Sergio M. Savaresi; Luca Magri; Giacomo Boracchi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113046"&gt;10.1016/j.patcog.2026.113046&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion , to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion , to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D .&lt;/p&gt;</content:encoded></item><item><title>A Multi-Modal Knowledge-Driven Approach for Generalized Zero-shot Video Classification</title><link>https://doi.org/10.1007/s11263-025-02584-3</link><guid>10.1007/s11263-025-02584-3</guid><pubDate>Sun, 04 Jan 2026 02:39:12 +0000</pubDate><dc:creator>Mingyao Hong</dc:creator><dc:creator>Xinfeng Zhang</dc:creator><dc:creator>Guorong Li</dc:creator><dc:creator>Qingming Huang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02584-3</prism:doi><description>Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.
Published: 2026-01-04T02:39:12+00:00
Venue: International Journal of Computer Vision
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyao Hong; Xinfeng Zhang; Guorong Li; Qingming Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02584-3"&gt;10.1007/s11263-025-02584-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.&lt;/p&gt;</content:encoded></item><item><title>CPPO: Contrastive Perception for Vision Language Policy Optimization</title><link>https://arxiv.org/abs/2601.00501v1</link><guid>http://arxiv.org/abs/2601.00501v1</guid><pubDate>Thu, 01 Jan 2026 22:48:26 +0000</pubDate><dc:creator>Ahmad Rezaei</dc:creator><dc:creator>Mohsen Gholami</dc:creator><dc:creator>Saeed Ranjbar Alvar</dc:creator><dc:creator>Kevin Cannons</dc:creator><dc:creator>Mohammad Asiful Hossain</dc:creator><dc:creator>Zhou Weimin</dc:creator><dc:creator>Shunbo Zhou</dc:creator><dc:creator>Yong Zhang</dc:creator><dc:creator>Mohammad Akbari</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.
Published: 2026-01-01T22:48:26+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ahmad Rezaei; Mohsen Gholami; Saeed Ranjbar Alvar; Kevin Cannons; Mohammad Asiful Hossain; Zhou Weimin; Shunbo Zhou; Yong Zhang; Mohammad Akbari&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.&lt;/p&gt;</content:encoded></item><item><title>IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation</title><link>https://arxiv.org/abs/2601.00212v1</link><guid>http://arxiv.org/abs/2601.00212v1</guid><pubDate>Thu, 01 Jan 2026 05:04:36 +0000</pubDate><dc:creator>Han Liu</dc:creator><dc:creator>Yubo Fan</dc:creator><dc:creator>Hao Li</dc:creator><dc:creator>Dewei Hu</dc:creator><dc:creator>Daniel Moyer</dc:creator><dc:creator>Zhoubing Xu</dc:creator><dc:creator>Benoit M. Dawant</dc:creator><dc:creator>Ipek Oguz</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.
Published: 2026-01-01T05:04:36+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Liu; Yubo Fan; Hao Li; Dewei Hu; Daniel Moyer; Zhoubing Xu; Benoit M. Dawant; Ipek Oguz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.&lt;/p&gt;</content:encoded></item><item><title>From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning</title><link>https://arxiv.org/abs/2601.00215v1</link><guid>http://arxiv.org/abs/2601.00215v1</guid><pubDate>Thu, 01 Jan 2026 05:19:28 +0000</pubDate><dc:creator>Omar Sharif</dc:creator><dc:creator>Eftekhar Hossain</dc:creator><dc:creator>Patrick Ng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.
Published: 2026-01-01T05:19:28+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Omar Sharif; Eftekhar Hossain; Patrick Ng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.&lt;/p&gt;</content:encoded></item><item><title>A two-stage self-supervised learning framework for breast cancer detection with multi-scale vision transformers</title><link>https://doi.org/10.1016/j.ins.2025.123061</link><guid>10.1016/j.ins.2025.123061</guid><pubDate>Sat, 03 Jan 2026 23:23:29 +0000</pubDate><dc:creator>Shahriar Mohammadi</dc:creator><dc:creator>Mohammad Ahmadi Livani</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2025.123061</prism:doi><description>Breast cancer detection through mammography remains a cornerstone of early diagnosis, yet the limited availability of large, expertly annotated datasets poses a significant challenge for developing robust AI models. To address this data scarcity, we propose a novel Two-Stage Self-Supervised Learning (TSSL) framework named TSSL-MSViT, which utilizes a Multi-Scale Vision Transformer (MSViT) to learn data-efficient mammographic representations. In Stage 1, the MSViT backbone is pretrained using a dual-objective strategy that integrates Multi-Scale Masked Reconstruction (MS-MR) and Cross-Scale Contrastive Learning (CS-C). Unlike prior single-task SSL pipelines, MS-MR captures fine- and coarse-grained structures, while CS-C explicitly aligns multi-resolution and multi-view (CC/MLO) semantics, yielding representations that are simultaneously hierarchical and view-consistent. This synergistic design provides a principled foundation—beyond empirical gains—for learning stable and transferable mammographic features from unlabeled data. In Stage 2, the pretrained MSViT backbone is fine-tuned with limited labeled data for breast-level classification. Comprehensive experiments on the CBIS-DDSM and INbreast datasets demonstrate that TSSL-MSViT consistently outperforms both Convolutional Neural Network (CNN) and Vision Transformer baselines. The model achieves state-of-the-art AUCs of 0.967 (CBIS-DDSM) and 0.972 (INbreast), significantly surpassing the Swin Transformer and other leading architectures. These results highlight the effectiveness of combining multi-scale feature modeling with self-supervised representation learning for data-efficient, generalizable, and accurate mammographic analysis. The proposed framework establishes a strong foundation for future AI-driven diagnostic systems, reducing dependence on extensive expert annotations while enhancing clinical reliability.
Published: 2026-01-03T23:23:29+00:00
Venue: Information Sciences
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shahriar Mohammadi; Mohammad Ahmadi Livani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2025.123061"&gt;10.1016/j.ins.2025.123061&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Breast cancer detection through mammography remains a cornerstone of early diagnosis, yet the limited availability of large, expertly annotated datasets poses a significant challenge for developing robust AI models. To address this data scarcity, we propose a novel Two-Stage Self-Supervised Learning (TSSL) framework named TSSL-MSViT, which utilizes a Multi-Scale Vision Transformer (MSViT) to learn data-efficient mammographic representations. In Stage 1, the MSViT backbone is pretrained using a dual-objective strategy that integrates Multi-Scale Masked Reconstruction (MS-MR) and Cross-Scale Contrastive Learning (CS-C). Unlike prior single-task SSL pipelines, MS-MR captures fine- and coarse-grained structures, while CS-C explicitly aligns multi-resolution and multi-view (CC/MLO) semantics, yielding representations that are simultaneously hierarchical and view-consistent. This synergistic design provides a principled foundation—beyond empirical gains—for learning stable and transferable mammographic features from unlabeled data. In Stage 2, the pretrained MSViT backbone is fine-tuned with limited labeled data for breast-level classification. Comprehensive experiments on the CBIS-DDSM and INbreast datasets demonstrate that TSSL-MSViT consistently outperforms both Convolutional Neural Network (CNN) and Vision Transformer baselines. The model achieves state-of-the-art AUCs of 0.967 (CBIS-DDSM) and 0.972 (INbreast), significantly surpassing the Swin Transformer and other leading architectures. These results highlight the effectiveness of combining multi-scale feature modeling with self-supervised representation learning for data-efficient, generalizable, and accurate mammographic analysis. The proposed framework establishes a strong foundation for future AI-driven diagnostic systems, reducing dependence on extensive expert annotations while enhancing clinical reliability.&lt;/p&gt;</content:encoded></item><item><title>Innovative Tooth Segmentation Using Hierarchical Features and Bidirectional Sequence Modeling</title><link>https://doi.org/10.1016/j.patcog.2026.113045</link><guid>10.1016/j.patcog.2026.113045</guid><pubDate>Sat, 03 Jan 2026 07:24:56 +0000</pubDate><dc:creator>Xinxin Zhao</dc:creator><dc:creator>Jian Jiang</dc:creator><dc:creator>Yan Tian</dc:creator><dc:creator>Liqin Wu</dc:creator><dc:creator>Zhaocheng Xu</dc:creator><dc:creator>Teddy Yang</dc:creator><dc:creator>Yunuo Zou</dc:creator><dc:creator>Xun Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113045</prism:doi><description>Tooth image segmentation is a cornerstone of dental digitization. However, traditional image encoders relying on fixed-resolution feature maps often lead to discontinuous segmentation and poor discrimination between target regions and background, due to insufficient modeling of environmental and global context. Moreover, transformer-based self-attention introduces substantial computational overhead because of its quadratic complexity (O(n²)), making it inefficient for high-resolution dental images. To address these challenges, we introduce a three-stage encoder with hierarchical feature representation to capture scale-adaptive information in dental images. By jointly leveraging low-level details and high-level semantics through cross-scale feature fusion, the model effectively preserves fine structural information while maintaining strong contextual awareness. Furthermore, a bidirectional sequence modeling strategy is incorporated to enhance global spatial context understanding without incurring high computational cost. We validate our method on two dental datasets, with experimental results demonstrating its superiority over existing approaches. On the OralVision dataset, our model achieves a 1.1% improvement in mean intersection over union (mIoU).
Published: 2026-01-03T07:24:56+00:00
Venue: Pattern Recognition
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinxin Zhao; Jian Jiang; Yan Tian; Liqin Wu; Zhaocheng Xu; Teddy Yang; Yunuo Zou; Xun Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113045"&gt;10.1016/j.patcog.2026.113045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Tooth image segmentation is a cornerstone of dental digitization. However, traditional image encoders relying on fixed-resolution feature maps often lead to discontinuous segmentation and poor discrimination between target regions and background, due to insufficient modeling of environmental and global context. Moreover, transformer-based self-attention introduces substantial computational overhead because of its quadratic complexity (O(n²)), making it inefficient for high-resolution dental images. To address these challenges, we introduce a three-stage encoder with hierarchical feature representation to capture scale-adaptive information in dental images. By jointly leveraging low-level details and high-level semantics through cross-scale feature fusion, the model effectively preserves fine structural information while maintaining strong contextual awareness. Furthermore, a bidirectional sequence modeling strategy is incorporated to enhance global spatial context understanding without incurring high computational cost. We validate our method on two dental datasets, with experimental results demonstrating its superiority over existing approaches. On the OralVision dataset, our model achieves a 1.1% improvement in mean intersection over union (mIoU).&lt;/p&gt;</content:encoded></item><item><title>EPSO-Net: A Multi-Objective Evolutionary Neural Architecture Search with PSO-Guided Mutation Fusion for Explainable Brain Tumor Segmentation</title><link>https://doi.org/10.1016/j.inffus.2025.104119</link><guid>10.1016/j.inffus.2025.104119</guid><pubDate>Sat, 03 Jan 2026 07:35:20 +0000</pubDate><dc:creator>Farhana Yasmin</dc:creator><dc:creator>Yu Xue</dc:creator><dc:creator>Mahade Hasan</dc:creator><dc:creator>Ghulam Muhammad</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104119</prism:doi><description>Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .
Published: 2026-01-03T07:35:20+00:00
Venue: Information Fusion
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Farhana Yasmin; Yu Xue; Mahade Hasan; Ghulam Muhammad&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104119"&gt;10.1016/j.inffus.2025.104119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Accurate brain tumor segmentation from magnetic resonance imaging (MRI) remains a significant challenge due to early loss of spatial detail, inadequate contextual representation, and ineffective decoder fusion. In this paper, we propose EPSO-Net, a multi-objective evolutionary neural architecture search (NAS) framework that integrates three specialized modules: UTSA for preserving spatial encoding and enhancing low-level feature representation, Astra for capturing semantic abstraction and multi-scale context, and Revo for improving decoder refinement through attention-guided fusion of feature maps. These modules work synergistically within a flexible modular 3D search space, enabling dynamic architecture optimization during the evolutionary process. EPSO-Net utilizes a particle swarm optimization (PSO)-guided mutation fusion mechanism that enables efficient exploration of the search space, adjusting mutation behavior based on performance feedback. To the best of our knowledge, this is the first multi-objective evolutionary NAS framework employing PSO-guided mutation fusion to adapt mutation strategies, driving the search towards optimal solutions in a resource-efficient manner. Experiments on the BraTS 2021, BraTS 2020, and MSD Brain Tumor datasets demonstrate that EPSO-Net outperforms nine state-of-the-art methods, achieving high dice similarity coefficients (DSC) of 93.89%, 95.02%, and 91.25%, low Hausdorff distance (HD95) of 1.14 mm, 1.02 mm, and 1.44 mm, and strong Grad-CAM IoU (GIoU) of 89.32%, 90.12%, and 85.68%, respectively. EPSO-Net also demonstrates reliable generalization to the CHAOS, PROMISE12, and ACDC datasets. Furthermore, it significantly reduces model complexity, lowers FLOPS, accelerates inference, and enhances interpretability. The full code will be publicly available at: https://github.com/Farhana005/EPSO-Net .&lt;/p&gt;</content:encoded></item><item><title>Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation</title><link>https://arxiv.org/abs/2601.00344v1</link><guid>http://arxiv.org/abs/2601.00344v1</guid><pubDate>Thu, 01 Jan 2026 13:54:29 +0000</pubDate><dc:creator>Bruce Mugizi</dc:creator><dc:creator>Sudi Murindanyi</dc:creator><dc:creator>Olivia Nakacwa</dc:creator><dc:creator>Andrew Katumba</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.
Published: 2026-01-01T13:54:29+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bruce Mugizi; Sudi Murindanyi; Olivia Nakacwa; Andrew Katumba&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa&amp;#x27;s Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.&lt;/p&gt;</content:encoded></item><item><title>A Cascaded Information Interaction Network for Precise Image Segmentation</title><link>https://arxiv.org/abs/2601.00562v1</link><guid>http://arxiv.org/abs/2601.00562v1</guid><pubDate>Fri, 02 Jan 2026 04:33:03 +0000</pubDate><dc:creator>Hewen Xiao</dc:creator><dc:creator>Jie Mei</dc:creator><dc:creator>Guangfu Ma</dc:creator><dc:creator>Weiren Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.
Published: 2026-01-02T04:33:03+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hewen Xiao; Jie Mei; Guangfu Ma; Weiren Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.&lt;/p&gt;</content:encoded></item><item><title>Document Image Shadow Removal via Score-based Gradient-guided Generative Model</title><link>https://doi.org/10.1016/j.eswa.2025.131070</link><guid>10.1016/j.eswa.2025.131070</guid><pubDate>Sat, 03 Jan 2026 16:21:35 +0000</pubDate><dc:creator>Yang Yang</dc:creator><dc:creator>Shuai Luo</dc:creator><dc:creator>Lanling Zeng</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131070</prism:doi><description>Document digitization faces significant challenges due to shadow artifacts caused by non-uniform illumination and geometric distortions. These artifacts significantly degrade image quality and impede the performance of downstream tasks such as optical character recognition and document analysis. In this paper, we propose a Score-based Gradient-guided Generative model (SGGM) for document shadow image shadow removal. It integrates the stochastic diffusion process with the structure-preserving gradient prior. Notably, by incorporating a shadow detection mechanism, our method relaxes the common assumption that the degradation operator, i.e., the shadow mask in our context, is known a priori. Based on the model, we propose a Diffusive Gradient Posterior Sampling (DGPS) method, which iteratively refines the image toward shadow-free states, while preserving fine document structures. Quantitative evaluations on the RDD and Kligler datasets demonstrate superior performance across PSNR, SSIM, FID, and LPIPS metrics. Qualitative results further demonstrate that our method produces visually coherent outputs without noticeable artifacts or color distortion. Ablation studies validate the effectiveness of the posterior sampling and the gradient guidance.
Published: 2026-01-03T16:21:35+00:00
Venue: Expert Systems with Applications
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Yang; Shuai Luo; Lanling Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131070"&gt;10.1016/j.eswa.2025.131070&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Document digitization faces significant challenges due to shadow artifacts caused by non-uniform illumination and geometric distortions. These artifacts significantly degrade image quality and impede the performance of downstream tasks such as optical character recognition and document analysis. In this paper, we propose a Score-based Gradient-guided Generative model (SGGM) for document shadow image shadow removal. It integrates the stochastic diffusion process with the structure-preserving gradient prior. Notably, by incorporating a shadow detection mechanism, our method relaxes the common assumption that the degradation operator, i.e., the shadow mask in our context, is known a priori. Based on the model, we propose a Diffusive Gradient Posterior Sampling (DGPS) method, which iteratively refines the image toward shadow-free states, while preserving fine document structures. Quantitative evaluations on the RDD and Kligler datasets demonstrate superior performance across PSNR, SSIM, FID, and LPIPS metrics. Qualitative results further demonstrate that our method produces visually coherent outputs without noticeable artifacts or color distortion. Ablation studies validate the effectiveness of the posterior sampling and the gradient guidance.&lt;/p&gt;</content:encoded></item><item><title>Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers</title><link>https://arxiv.org/abs/2601.00359v1</link><guid>http://arxiv.org/abs/2601.00359v1</guid><pubDate>Thu, 01 Jan 2026 14:29:31 +0000</pubDate><dc:creator>Söhnke Benedikt Fischedick</dc:creator><dc:creator>Daniel Seichter</dc:creator><dc:creator>Benedict Stephan</dc:creator><dc:creator>Robin Schmidt</dc:creator><dc:creator>Horst-Michael Gross</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/IROS60139.2025.11245809</prism:doi><description>In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.
Published: 2026-01-01T14:29:31+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Söhnke Benedikt Fischedick; Daniel Seichter; Benedict Stephan; Robin Schmidt; Horst-Michael Gross&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/IROS60139.2025.11245809"&gt;10.1109/IROS60139.2025.11245809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.&lt;/p&gt;</content:encoded></item><item><title>Extreme Weakly Supervised Binary Semantic Image Segmentation via One-Pixel Supervision</title><link>https://doi.org/10.1016/j.patcog.2026.113048</link><guid>10.1016/j.patcog.2026.113048</guid><pubDate>Sat, 03 Jan 2026 07:24:55 +0000</pubDate><dc:creator>Matthaios Tzimas</dc:creator><dc:creator>Vasileios Mygdalis</dc:creator><dc:creator>Christos Papaioannidis</dc:creator><dc:creator>Ioannis Pitas</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113048</prism:doi><description>Despite recent advancements, Unsupervised Semantic Segmentation (USS) methods still exhibit a significant performance deficit compared to supervised approaches, particularly in binary semantic segmentation. This limitation arises because, without supervision, USS methods struggle to distinguish foreground from background image regions, particularly when the foreground contains small or uncommon objects. This issue is addressed by our proposed Extremely Weakly Supervised Binary Semantic Segmentation (EWS) framework. EWS expects minimal supervision, consisting only of a small set of one-pixel annotations explicitly belonging to the foreground class across the entire image dataset. Our approach leverages these one-pixel annotations and employs two contrastive losses to map visual transformer features into well-separated foreground and background feature clusters. Additionally, we propose a novel loss function to eliminate the need for hyperparameter tuning of the contrastive loss threshold, by dynamically computing it based on the similarity between the input image features. Even if we employ a single one-pixel annotation, EWS achieves competitive results in binary segmentation tasks while maintaining low computational costs, making it an efficient solution for critical segmentation applications. GitHub Repo: https://github.com/matJTzimas/EWS
Published: 2026-01-03T07:24:55+00:00
Venue: Pattern Recognition
Score: 0.762 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Matthaios Tzimas; Vasileios Mygdalis; Christos Papaioannidis; Ioannis Pitas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113048"&gt;10.1016/j.patcog.2026.113048&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (consider)&lt;/p&gt;
&lt;p&gt;Despite recent advancements, Unsupervised Semantic Segmentation (USS) methods still exhibit a significant performance deficit compared to supervised approaches, particularly in binary semantic segmentation. This limitation arises because, without supervision, USS methods struggle to distinguish foreground from background image regions, particularly when the foreground contains small or uncommon objects. This issue is addressed by our proposed Extremely Weakly Supervised Binary Semantic Segmentation (EWS) framework. EWS expects minimal supervision, consisting only of a small set of one-pixel annotations explicitly belonging to the foreground class across the entire image dataset. Our approach leverages these one-pixel annotations and employs two contrastive losses to map visual transformer features into well-separated foreground and background feature clusters. Additionally, we propose a novel loss function to eliminate the need for hyperparameter tuning of the contrastive loss threshold, by dynamically computing it based on the similarity between the input image features. Even if we employ a single one-pixel annotation, EWS achieves competitive results in binary segmentation tasks while maintaining low computational costs, making it an efficient solution for critical segmentation applications. GitHub Repo: https://github.com/matJTzimas/EWS&lt;/p&gt;</content:encoded></item><item><title>FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection</title><link>https://arxiv.org/abs/2601.00535v1</link><guid>http://arxiv.org/abs/2601.00535v1</guid><pubDate>Fri, 02 Jan 2026 02:36:48 +0000</pubDate><dc:creator>Ruiqiang Zhang</dc:creator><dc:creator>Hengyi Wang</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Guanjie Wang</dc:creator><dc:creator>Zehua Ma</dc:creator><dc:creator>Weiming Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.
Published: 2026-01-02T02:36:48+00:00
Venue: arXiv
Score: 0.762 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiqiang Zhang; Hengyi Wang; Chang Liu; Guanjie Wang; Zehua Ma; Weiming Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (consider)&lt;/p&gt;
&lt;p&gt;Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.&lt;/p&gt;</content:encoded></item><item><title>Prior-oriented Specific and Triple-view General Prompts for Multi-weather Degraded Image Restoration</title><link>https://doi.org/10.1016/j.eswa.2025.131042</link><guid>10.1016/j.eswa.2025.131042</guid><pubDate>Sun, 04 Jan 2026 22:53:11 +0000</pubDate><dc:creator>Yuanbo Wen</dc:creator><dc:creator>Tao Gao</dc:creator><dc:creator>Shan Liang</dc:creator><dc:creator>Ziqi Li</dc:creator><dc:creator>Ting Chen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131042</prism:doi><description>Image restoration aims to reconstruct high-quality images from their degraded observations. Recent advancements highlight the potential of all-in-one restoration models to address multiple degradations simultaneously. However, existing approaches struggle to adapt effectively to various degradations, leading to the sub-optimal performance across different weather conditions. To this end, we propose the prior-oriented specific and triple-view general prompts (PSTGP) to boost multi-weather degraded image restoration. Specifically, we utilize a simple condition diffusion model to generate the prior-oriented specific prompt (POSP) that directly aligns with the clean images, guiding the degradation elimination and image reconstruction procedure. Meanwhile, we establish the triple-view general prompt (TVGP) from multiple perspectives, overcoming the representation limitations of existing single-view prompts. We introduce two essential components, namely the directed prompt transposed attention (DPTA) and directed prompt partition network (DPPN), which function as specialized modules designed to integrate both the specific and general prompts. Extensive experiments on publicly available benchmarks demonstrate that our model outperforms existing well-performing approaches by 1.278 dB ∼ 9.480 dB in PSNR indicator.
Published: 2026-01-04T22:53:11+00:00
Venue: Expert Systems with Applications
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanbo Wen; Tao Gao; Shan Liang; Ziqi Li; Ting Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131042"&gt;10.1016/j.eswa.2025.131042&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Image restoration aims to reconstruct high-quality images from their degraded observations. Recent advancements highlight the potential of all-in-one restoration models to address multiple degradations simultaneously. However, existing approaches struggle to adapt effectively to various degradations, leading to the sub-optimal performance across different weather conditions. To this end, we propose the prior-oriented specific and triple-view general prompts (PSTGP) to boost multi-weather degraded image restoration. Specifically, we utilize a simple condition diffusion model to generate the prior-oriented specific prompt (POSP) that directly aligns with the clean images, guiding the degradation elimination and image reconstruction procedure. Meanwhile, we establish the triple-view general prompt (TVGP) from multiple perspectives, overcoming the representation limitations of existing single-view prompts. We introduce two essential components, namely the directed prompt transposed attention (DPTA) and directed prompt partition network (DPPN), which function as specialized modules designed to integrate both the specific and general prompts. Extensive experiments on publicly available benchmarks demonstrate that our model outperforms existing well-performing approaches by 1.278 dB ∼ 9.480 dB in PSNR indicator.&lt;/p&gt;</content:encoded></item><item><title>Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network</title><link>https://arxiv.org/abs/2601.00658v1</link><guid>http://arxiv.org/abs/2601.00658v1</guid><pubDate>Fri, 02 Jan 2026 11:34:35 +0000</pubDate><dc:creator>Zhaiyu Chen</dc:creator><dc:creator>Yuanyuan Wang</dc:creator><dc:creator>Yilei Shi</dc:creator><dc:creator>Xiao Xiang Zhu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.
Published: 2026-01-02T11:34:35+00:00
Venue: arXiv
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaiyu Chen; Yuanyuan Wang; Yilei Shi; Xiao Xiang Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.&lt;/p&gt;</content:encoded></item><item><title>ProSe: Decoupling Knowledge via Prototype-based Selection for Data-Incremental Object Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131092</link><guid>10.1016/j.eswa.2026.131092</guid><pubDate>Sun, 04 Jan 2026 22:53:35 +0000</pubDate><dc:creator>Zexuan Ji</dc:creator><dc:creator>Jian Zhang</dc:creator><dc:creator>Shule Yan</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131092</prism:doi><description>Incremental object detection continues to face catastrophic forgetting when models are updated with sequential data. Current approaches predominantly address either class-incremental or domain-incremental settings in isolation, often relying on complex knowledge distillation or data replay mechanisms. These strategies introduce training instability, scalability issues, and optimization difficulties. In this paper, we propose a unified data-incremental learning paradigm that jointly accommodates both category dynamics and domain shifts, offering a more realistic and general framework for evolving data streams. Within this paradigm, we introduce Prototype-based Selection (ProSe), a distillation-free and replay-free framework for DETR-based detectors that inserts learnable prototypes between the encoder and decoder to capture dataset-level semantics and uses an Attention-based Selector to dynamically route each input to the most suitable increment-specific branch during inference. By avoiding repeated overwriting of a single shared model and instead updating only the selected branch, ProSe better preserves previously learned representations and maintains stable performance over long-term incremental updates. Extensive experiments across nine diverse benchmarks demonstrate that ProSe achieves performance on par with or superior to state-of-the-art methods, while significantly simplifying the training process and ensuring stable long-term learning. Code is available at https://github.com/Kled-Skaarl/ProSe .
Published: 2026-01-04T22:53:35+00:00
Venue: Expert Systems with Applications
Score: 0.760 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zexuan Ji; Jian Zhang; Shule Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131092"&gt;10.1016/j.eswa.2026.131092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.760 (consider)&lt;/p&gt;
&lt;p&gt;Incremental object detection continues to face catastrophic forgetting when models are updated with sequential data. Current approaches predominantly address either class-incremental or domain-incremental settings in isolation, often relying on complex knowledge distillation or data replay mechanisms. These strategies introduce training instability, scalability issues, and optimization difficulties. In this paper, we propose a unified data-incremental learning paradigm that jointly accommodates both category dynamics and domain shifts, offering a more realistic and general framework for evolving data streams. Within this paradigm, we introduce Prototype-based Selection (ProSe), a distillation-free and replay-free framework for DETR-based detectors that inserts learnable prototypes between the encoder and decoder to capture dataset-level semantics and uses an Attention-based Selector to dynamically route each input to the most suitable increment-specific branch during inference. By avoiding repeated overwriting of a single shared model and instead updating only the selected branch, ProSe better preserves previously learned representations and maintains stable performance over long-term incremental updates. Extensive experiments across nine diverse benchmarks demonstrate that ProSe achieves performance on par with or superior to state-of-the-art methods, while significantly simplifying the training process and ensuring stable long-term learning. Code is available at https://github.com/Kled-Skaarl/ProSe .&lt;/p&gt;</content:encoded></item><item><title>Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection</title><link>https://arxiv.org/abs/2601.00141v1</link><guid>http://arxiv.org/abs/2601.00141v1</guid><pubDate>Thu, 01 Jan 2026 00:00:07 +0000</pubDate><dc:creator>Lawrence Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.
Published: 2026-01-01T00:00:07+00:00
Venue: arXiv
Score: 0.760 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lawrence Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.760 (consider)&lt;/p&gt;
&lt;p&gt;The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.&lt;/p&gt;</content:encoded></item><item><title>A Multi-Granularity Scene-Aware Graph Convolution Method for Weakly Supervised Person Search</title><link>https://doi.org/10.1007/s11263-025-02665-3</link><guid>10.1007/s11263-025-02665-3</guid><pubDate>Sat, 03 Jan 2026 09:25:59 +0000</pubDate><dc:creator>De Cheng</dc:creator><dc:creator>Haichun Tai</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Xiangqian Zhao</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02665-3</prism:doi><description>One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .
Published: 2026-01-03T09:25:59+00:00
Venue: International Journal of Computer Vision
Score: 0.759 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; De Cheng; Haichun Tai; Nannan Wang; Xiangqian Zhao; Jie Li; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02665-3"&gt;10.1007/s11263-025-02665-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (consider)&lt;/p&gt;
&lt;p&gt;One-step Weakly Supervised Person Search (WSPS) addresses person detection and re-identification (ReID) within a unified framework, relying solely on pedestrian bounding box annotations for training, without requiring annotated identity labels. This approach enhances the practicality and efficiency of person search in real-world applications. However, WSPS faces two primary challenges: (1) the significant feature discrepancy between ReID and pedestrian detection tasks complicates shared representation learning, and (2) accurately estimating pseudo identity for each person image is challenging due to unrefined detections and significant intra-class variation in complex scenes. To address these challenges, we introduce a multi-granularity scene-aware graph convolution framework, which jointly optimizes task-specific features, improves pseudo-label estimation, and reduces the effects of label noise. Specifically, the Multi-granularity Feature Alignment (MFA) module in our designed two-branch network leverages bi-directional cluster-level interactions across multiple granularities to address the feature discrepancy. Building on MFA, we develop the Graph-convolution-based feature enhancement for more reliable Scene-aware pseudo-label Estimation (GSE). Meanwhile, the Label Refinement module, with its global-local Collaborative Learning (LCL) mechanism, addresses label noise by refining labels at both global and local levels, ensuring more robust weakly supervised learning. Extensive experimental evaluations demonstrate the effectiveness of the proposed method, achieving significant performance improvements over state-of-the-art approaches on the CUHK-SYSU and PRW datasets. Code is available at https://github.com/haichuntai/MSGM-main .&lt;/p&gt;</content:encoded></item><item><title>Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception</title><link>https://arxiv.org/abs/2601.00598v1</link><guid>http://arxiv.org/abs/2601.00598v1</guid><pubDate>Fri, 02 Jan 2026 07:36:47 +0000</pubDate><dc:creator>Xianhui Liu</dc:creator><dc:creator>Siqi Jiang</dc:creator><dc:creator>Yi Xie</dc:creator><dc:creator>Yuqing Lin</dc:creator><dc:creator>Siao Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.
Published: 2026-01-02T07:36:47+00:00
Venue: arXiv
Score: 0.759 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xianhui Liu; Siqi Jiang; Yi Xie; Yuqing Lin; Siao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (consider)&lt;/p&gt;
&lt;p&gt;RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.&lt;/p&gt;</content:encoded></item><item><title>LTSTrack: Visual Tracking with Long-term Temporal Sequence</title><link>https://doi.org/10.1016/j.patcog.2026.113052</link><guid>10.1016/j.patcog.2026.113052</guid><pubDate>Sun, 04 Jan 2026 22:50:54 +0000</pubDate><dc:creator>Zhaochuan Zeng</dc:creator><dc:creator>Shilei Wang</dc:creator><dc:creator>Yidong Song</dc:creator><dc:creator>Zhenhua Wang</dc:creator><dc:creator>Jifeng Ning</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113052</prism:doi><description>The utilization of temporal sequences is crucial for tracking in complex scenarios, particularly when addressing challenges such as occlusion and deformation. However, existing methods are often constrained by limitations such as the use of unrefined raw images or computationally expensive temporal fusion modules, both of which restrict the scale of temporal sequences that can be utilized. This study proposes a novel appearance compression strategy and a temporal feature fusion module, which together significantly enhance the tracker’s ability to utilize long-term temporal sequences. Based on these designs, we propose a tracker that can leverage a L ong-term T emporal S equence that contains historical context across 300 frames, which we name LTSTrack. First, we present a simple yet effective appearance compression strategy to extract target appearance features from each frame and compress them into compact summary tokens, which constitute a long-term temporal sequence. Then, the Mamba block is introduced to efficiently fuse the long-term temporal sequence, generating a fusion token containing the historical representation of the target. Finally, this fusion token is used to enhance the search-region features, thereby achieving more accurate tracking. Extensive experiments demonstrate that the proposed method achieves significant performance improvements across the GOT-10K, TrackingNet, TNL2K, LaSOT, UAV123 and LaSOT ext datasets. Notably, it achieves remarkable scores of 75.1% AO on GOT-10K and 84.6% AUC on TrackingNet, substantially outperforming previous state-of-the-art methods.
Published: 2026-01-04T22:50:54+00:00
Venue: Pattern Recognition
Score: 0.759 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaochuan Zeng; Shilei Wang; Yidong Song; Zhenhua Wang; Jifeng Ning&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113052"&gt;10.1016/j.patcog.2026.113052&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (consider)&lt;/p&gt;
&lt;p&gt;The utilization of temporal sequences is crucial for tracking in complex scenarios, particularly when addressing challenges such as occlusion and deformation. However, existing methods are often constrained by limitations such as the use of unrefined raw images or computationally expensive temporal fusion modules, both of which restrict the scale of temporal sequences that can be utilized. This study proposes a novel appearance compression strategy and a temporal feature fusion module, which together significantly enhance the tracker’s ability to utilize long-term temporal sequences. Based on these designs, we propose a tracker that can leverage a L ong-term T emporal S equence that contains historical context across 300 frames, which we name LTSTrack. First, we present a simple yet effective appearance compression strategy to extract target appearance features from each frame and compress them into compact summary tokens, which constitute a long-term temporal sequence. Then, the Mamba block is introduced to efficiently fuse the long-term temporal sequence, generating a fusion token containing the historical representation of the target. Finally, this fusion token is used to enhance the search-region features, thereby achieving more accurate tracking. Extensive experiments demonstrate that the proposed method achieves significant performance improvements across the GOT-10K, TrackingNet, TNL2K, LaSOT, UAV123 and LaSOT ext datasets. Notably, it achieves remarkable scores of 75.1% AO on GOT-10K and 84.6% AUC on TrackingNet, substantially outperforming previous state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection</title><link>https://arxiv.org/abs/2601.00237v1</link><guid>http://arxiv.org/abs/2601.00237v1</guid><pubDate>Thu, 01 Jan 2026 07:01:47 +0000</pubDate><dc:creator>Chao Yang</dc:creator><dc:creator>Haoyuan Zheng</dc:creator><dc:creator>Yue Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.
Published: 2026-01-01T07:01:47+00:00
Venue: arXiv
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Yang; Haoyuan Zheng; Yue Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.&lt;/p&gt;</content:encoded></item><item><title>RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection</title><link>https://arxiv.org/abs/2601.00398v1</link><guid>http://arxiv.org/abs/2601.00398v1</guid><pubDate>Thu, 01 Jan 2026 17:22:44 +0000</pubDate><dc:creator>Tao Wu</dc:creator><dc:creator>Qing Xu</dc:creator><dc:creator>Xiangjian He</dc:creator><dc:creator>Oakleigh Weekes</dc:creator><dc:creator>James Brown</dc:creator><dc:creator>Wenting Duan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.
Published: 2026-01-01T17:22:44+00:00
Venue: arXiv
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Wu; Qing Xu; Xiangjian He; Oakleigh Weekes; James Brown; Wenting Duan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Semi-supervised Medical Image Segmentation via Semantic Transfer</title><link>https://doi.org/10.1016/j.patcog.2026.113039</link><guid>10.1016/j.patcog.2026.113039</guid><pubDate>Sat, 03 Jan 2026 16:19:14 +0000</pubDate><dc:creator>Shiyuan Huang</dc:creator><dc:creator>Shudong Wang</dc:creator><dc:creator>Kuijie Zhang</dc:creator><dc:creator>Wenhao Wu</dc:creator><dc:creator>Yingye Liu</dc:creator><dc:creator>Tiyao Liu</dc:creator><dc:creator>Shanchen Pang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113039</prism:doi><description>Semi-supervised learning has gained increasing attention in medical image segmentation due to its ability to alleviate the reliance on large-scale expert annotations. However, many existing SSL approaches focus on generic consistency constraints while lacking explicit mechanisms for semantic transfer between labeled and unlabeled data, limiting their effectiveness in regions with ambiguous or low-confidence predictions. To address this challenge, we propose STLU-Net, a dual-stream semi-supervised framework enhancing semantic interaction between labeled and unlabeled data via a fine-grained feature mixing module. This module performs channel-wise cross-sample fusion guided by feature similarity, encouraging the learning of transferable deep semantics while introducing controlled perturbations. Dual-stream supervision with structured feature perturbation penalizes predictions lacking consistent semantic support, mitigating confirmation bias on unlabeled data. Extensive experiments on multiple 3D medical image segmentation benchmarks demonstrate that STLU-Net achieves superior performance under limited supervision. Further analysis confirms that our method effectively extracts rich and generalizable semantic representations from limited annotations through hierarchical feature coordination, leading to notable performance gains in semi-supervised segmentation. Code is available at: https://github.com/Shiyuan-H/STLU-Net .
Published: 2026-01-03T16:19:14+00:00
Venue: Pattern Recognition
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiyuan Huang; Shudong Wang; Kuijie Zhang; Wenhao Wu; Yingye Liu; Tiyao Liu; Shanchen Pang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113039"&gt;10.1016/j.patcog.2026.113039&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;Semi-supervised learning has gained increasing attention in medical image segmentation due to its ability to alleviate the reliance on large-scale expert annotations. However, many existing SSL approaches focus on generic consistency constraints while lacking explicit mechanisms for semantic transfer between labeled and unlabeled data, limiting their effectiveness in regions with ambiguous or low-confidence predictions. To address this challenge, we propose STLU-Net, a dual-stream semi-supervised framework enhancing semantic interaction between labeled and unlabeled data via a fine-grained feature mixing module. This module performs channel-wise cross-sample fusion guided by feature similarity, encouraging the learning of transferable deep semantics while introducing controlled perturbations. Dual-stream supervision with structured feature perturbation penalizes predictions lacking consistent semantic support, mitigating confirmation bias on unlabeled data. Extensive experiments on multiple 3D medical image segmentation benchmarks demonstrate that STLU-Net achieves superior performance under limited supervision. Further analysis confirms that our method effectively extracts rich and generalizable semantic representations from limited annotations through hierarchical feature coordination, leading to notable performance gains in semi-supervised segmentation. Code is available at: https://github.com/Shiyuan-H/STLU-Net .&lt;/p&gt;</content:encoded></item><item><title>Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians</title><link>https://arxiv.org/abs/2601.00678v1</link><guid>http://arxiv.org/abs/2601.00678v1</guid><pubDate>Fri, 02 Jan 2026 13:04:47 +0000</pubDate><dc:creator>Melonie de Almeida</dc:creator><dc:creator>Daniela Ivanova</dc:creator><dc:creator>Tong Shi</dc:creator><dc:creator>John H. Williamson</dc:creator><dc:creator>Paul Henderson</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.
Published: 2026-01-02T13:04:47+00:00
Venue: arXiv
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Melonie de Almeida; Daniela Ivanova; Tong Shi; John H. Williamson; Paul Henderson&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.&lt;/p&gt;</content:encoded></item><item><title>COBRA: A Continual Learning Approach to Vision-Brain Understanding</title><link>https://doi.org/10.1007/s11263-025-02617-x</link><guid>10.1007/s11263-025-02617-x</guid><pubDate>Sun, 04 Jan 2026 10:02:31 +0000</pubDate><dc:creator>Xuan-Bac Nguyen</dc:creator><dc:creator>Manuel Serna-Aguilera</dc:creator><dc:creator>Arabinda Kumar Choudhary</dc:creator><dc:creator>Pawan Sinha</dc:creator><dc:creator>Xin Li</dc:creator><dc:creator>Khoa Luu</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02617-x</prism:doi><description>Vision-Brain Understanding (VBU) aims to extract visual information perceived by humans from brain activity recorded through functional Magnetic Resonance Imaging (fMRI). Despite notable advancements in recent years, existing studies in VBU continue to face the challenge of catastrophic forgetting, where models lose knowledge from prior subjects as they adapt to new ones. Addressing continual learning in this field is, therefore, essential. This paper introduces a novel framework called Continual Learning for Vision-Brain (COBRA) to address continual learning in VBU. Our approach includes three novel modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific (PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer module. The SC module captures shared vision-brain patterns across subjects, preserving this knowledge as the model encounters new subjects, thereby reducing the impact of catastrophic forgetting. On the other hand, the PSS module learns unique vision-brain patterns specific to each subject. Finally, the MRIFormer module comprises a transformer encoder and decoder that learn the fMRI features for VBU from both common and specific patterns. In a continual learning setup, COBRA is trained on new PSS and MRIFormer modules for new subjects, while the modules for previous subjects remain unaffected. As a result, COBRA effectively addresses catastrophic forgetting and achieves state-of-the-art performance in both continual learning and vision-brain reconstruction tasks, surpassing previous methods.
Published: 2026-01-04T10:02:31+00:00
Venue: International Journal of Computer Vision
Score: 0.755 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuan-Bac Nguyen; Manuel Serna-Aguilera; Arabinda Kumar Choudhary; Pawan Sinha; Xin Li; Khoa Luu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02617-x"&gt;10.1007/s11263-025-02617-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.755 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Brain Understanding (VBU) aims to extract visual information perceived by humans from brain activity recorded through functional Magnetic Resonance Imaging (fMRI). Despite notable advancements in recent years, existing studies in VBU continue to face the challenge of catastrophic forgetting, where models lose knowledge from prior subjects as they adapt to new ones. Addressing continual learning in this field is, therefore, essential. This paper introduces a novel framework called Continual Learning for Vision-Brain (COBRA) to address continual learning in VBU. Our approach includes three novel modules: a Subject Commonality (SC) module, a Prompt-based Subject Specific (PSS) module, and a transformer-based module for fMRI, denoted as MRIFormer module. The SC module captures shared vision-brain patterns across subjects, preserving this knowledge as the model encounters new subjects, thereby reducing the impact of catastrophic forgetting. On the other hand, the PSS module learns unique vision-brain patterns specific to each subject. Finally, the MRIFormer module comprises a transformer encoder and decoder that learn the fMRI features for VBU from both common and specific patterns. In a continual learning setup, COBRA is trained on new PSS and MRIFormer modules for new subjects, while the modules for previous subjects remain unaffected. As a result, COBRA effectively addresses catastrophic forgetting and achieves state-of-the-art performance in both continual learning and vision-brain reconstruction tasks, surpassing previous methods.&lt;/p&gt;</content:encoded></item><item><title>CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models</title><link>https://arxiv.org/abs/2601.00659v1</link><guid>http://arxiv.org/abs/2601.00659v1</guid><pubDate>Fri, 02 Jan 2026 11:39:00 +0000</pubDate><dc:creator>Neeraj Anand</dc:creator><dc:creator>Samyak Jha</dc:creator><dc:creator>Udbhav Bamba</dc:creator><dc:creator>Rahul Rahaman</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.
Published: 2026-01-02T11:39:00+00:00
Venue: arXiv
Score: 0.754 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Neeraj Anand; Samyak Jha; Udbhav Bamba; Rahul Rahaman&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.754 (consider)&lt;/p&gt;
&lt;p&gt;Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.&lt;/p&gt;</content:encoded></item></channel></rss>