<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 08 Dec 2025 01:23:33 +0000</lastBuildDate><item><title>EquivFisheye: A Spherical Fusion Framework for Panoramic 3D Perception with Surround-View Fisheye Cameras</title><link>https://doi.org/10.1016/j.inffus.2025.104024</link><guid>10.1016/j.inffus.2025.104024</guid><pubDate>Sat, 06 Dec 2025 07:56:17 +0000</pubDate><dc:creator>Zhao Yang</dc:creator><dc:creator>Xinglin Pu</dc:creator><dc:creator>Weixiang Xu</dc:creator><dc:creator>Zezhong Qian</dc:creator><dc:creator>Kang Ke</dc:creator><dc:creator>Haonan Zhang</dc:creator><dc:creator>Longjun Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104024</prism:doi><description>Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.
Published: 2025-12-06T07:56:17+00:00
Venue: Information Fusion
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Yang; Xinglin Pu; Weixiang Xu; Zezhong Qian; Kang Ke; Haonan Zhang; Longjun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104024"&gt;10.1016/j.inffus.2025.104024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Language Models in Agriculture: A Tutorial and Survey</title><link>https://doi.org/10.1016/j.inffus.2025.104042</link><guid>10.1016/j.inffus.2025.104042</guid><pubDate>Sun, 07 Dec 2025 15:19:43 +0000</pubDate><dc:creator>Mohammadreza Haghighat</dc:creator><dc:creator>Alzayat Saleh</dc:creator><dc:creator>Mostafa Rahimi Azghadi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104042</prism:doi><description>The integration of artificial intelligence (AI) in agriculture is rapidly evolving, marked by increasing adoption of machine learning (ML), deep learning (DL), and the recent emergence of large language models (LLMs) and multimodal language models (MLMs). These technologies are transforming traditional agricultural practices through advanced data analysis and offering innovative solutions for smart agriculture. Although earlier methods often relied on unimodal data, primarily images, current research is shifting toward information fusion with multimodal AI systems that fuse text, imagery, and other types of data, such as agricultural knowledge graphs and videos of animal activities, for enhanced decision making. General-purpose MLMs struggle with agriculture-specific nuances due to domain gaps and limited multimodal datasets. The high computational demands of foundation models (FM) and domain-specific needs also restrict broader adoption. This study presents a tutorial on applying MLMs in agriculture, covering their main concepts. Building on this foundational knowledge, the tutorial surveys key developments in the literature and provides a comprehensive step-by-step guide for implementing and tailoring MLMs to agricultural applications. This will address the domain gaps by knowledge integration, synthetic multimodal data generation and efficient learning methods, thereby expanding their practical applications.
Published: 2025-12-07T15:19:43+00:00
Venue: Information Fusion
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohammadreza Haghighat; Alzayat Saleh; Mostafa Rahimi Azghadi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104042"&gt;10.1016/j.inffus.2025.104042&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;The integration of artificial intelligence (AI) in agriculture is rapidly evolving, marked by increasing adoption of machine learning (ML), deep learning (DL), and the recent emergence of large language models (LLMs) and multimodal language models (MLMs). These technologies are transforming traditional agricultural practices through advanced data analysis and offering innovative solutions for smart agriculture. Although earlier methods often relied on unimodal data, primarily images, current research is shifting toward information fusion with multimodal AI systems that fuse text, imagery, and other types of data, such as agricultural knowledge graphs and videos of animal activities, for enhanced decision making. General-purpose MLMs struggle with agriculture-specific nuances due to domain gaps and limited multimodal datasets. The high computational demands of foundation models (FM) and domain-specific needs also restrict broader adoption. This study presents a tutorial on applying MLMs in agriculture, covering their main concepts. Building on this foundational knowledge, the tutorial surveys key developments in the literature and provides a comprehensive step-by-step guide for implementing and tailoring MLMs to agricultural applications. This will address the domain gaps by knowledge integration, synthetic multimodal data generation and efficient learning methods, thereby expanding their practical applications.&lt;/p&gt;</content:encoded></item><item><title>Classifier Guidance and Domain Cooperation for Multisource Unsupervised Domain Adaptation</title><link>https://doi.org/10.1016/j.knosys.2025.115057</link><guid>10.1016/j.knosys.2025.115057</guid><pubDate>Sun, 07 Dec 2025 23:04:51 +0000</pubDate><dc:creator>Ming Zhao</dc:creator><dc:creator>Yifan Lan</dc:creator><dc:creator>Yuwu Lu</dc:creator><dc:creator>Leyao Yuan</dc:creator><dc:creator>Wenmeng Zhang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115057</prism:doi><description>Traditional deep learning models usually rely on identical data distributions for their training and testing sets. Unfortunately, this assumption rarely holds in real-world applications. This challenge has led to the proposal of unsupervised domain adaptation (UDA). While centred on transfers between single source and target domains, most UDA methods do not capitalize on the wealth of labelled resources that are present across multiple source domains. This gap is addressed herein by focusing on multisource unsupervised domain adaptation (MUDA), where knowledge is transferred from multiple source domains to an unlabelled target domain. The existing MUDA methods focus primarily on reducing discrepancies between the source and target domains and often overlook the goal of improving the performance of the utilized classifier across different domains, which is equally crucial for effectively implementing domain adaptation. To overcome this limitation, we propose a novel approach called classifier guidance and domain cooperation (CGDC). Our method calculates the accuracy of a classifier in the target domain using a probability-inspired approach that considers data transformation costs and classifier precision in the high-level feature space of the target domain. This enables high-performance classifiers to guide classifiers with lower performance. Additionally, we introduce a mixup-based multi-information fusion strategy to create enriched domains by combining information derived from the source and target domains. A comparison with many methods on public datasets shows the effectiveness of our CGDC approach in image classification tasks. Specifically, CGDC achieves improvements of 4.3% on Office-Home and 1.9% on Office-31 over the best baseline methods.
Published: 2025-12-07T23:04:51+00:00
Venue: Knowledge-Based Systems
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Zhao; Yifan Lan; Yuwu Lu; Leyao Yuan; Wenmeng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115057"&gt;10.1016/j.knosys.2025.115057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Traditional deep learning models usually rely on identical data distributions for their training and testing sets. Unfortunately, this assumption rarely holds in real-world applications. This challenge has led to the proposal of unsupervised domain adaptation (UDA). While centred on transfers between single source and target domains, most UDA methods do not capitalize on the wealth of labelled resources that are present across multiple source domains. This gap is addressed herein by focusing on multisource unsupervised domain adaptation (MUDA), where knowledge is transferred from multiple source domains to an unlabelled target domain. The existing MUDA methods focus primarily on reducing discrepancies between the source and target domains and often overlook the goal of improving the performance of the utilized classifier across different domains, which is equally crucial for effectively implementing domain adaptation. To overcome this limitation, we propose a novel approach called classifier guidance and domain cooperation (CGDC). Our method calculates the accuracy of a classifier in the target domain using a probability-inspired approach that considers data transformation costs and classifier precision in the high-level feature space of the target domain. This enables high-performance classifiers to guide classifiers with lower performance. Additionally, we introduce a mixup-based multi-information fusion strategy to create enriched domains by combining information derived from the source and target domains. A comparison with many methods on public datasets shows the effectiveness of our CGDC approach in image classification tasks. Specifically, CGDC achieves improvements of 4.3% on Office-Home and 1.9% on Office-31 over the best baseline methods.&lt;/p&gt;</content:encoded></item><item><title>MSG-CLIP: Enhancing CLIP’s Ability to Learn Fine-grained Structural Associations through Multi-modal Scene Graph Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112794</link><guid>10.1016/j.patcog.2025.112794</guid><pubDate>Sat, 06 Dec 2025 23:14:20 +0000</pubDate><dc:creator>Xiaotian Lv</dc:creator><dc:creator>Yue Zhao</dc:creator><dc:creator>Hanlong Yin</dc:creator><dc:creator>Yifei Chen</dc:creator><dc:creator>Jianxing Liu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112794</prism:doi><description>As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.
Published: 2025-12-06T23:14:20+00:00
Venue: Pattern Recognition
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaotian Lv; Yue Zhao; Hanlong Yin; Yifei Chen; Jianxing Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112794"&gt;10.1016/j.patcog.2025.112794&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.&lt;/p&gt;</content:encoded></item><item><title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title><link>https://arxiv.org/abs/2512.04581v1</link><guid>http://arxiv.org/abs/2512.04581v1</guid><pubDate>Thu, 04 Dec 2025 08:49:23 +0000</pubDate><dc:creator>Houzhang Fang</dc:creator><dc:creator>Chenxing Wu</dc:creator><dc:creator>Kun Bai</dc:creator><dc:creator>Tianqi Chen</dc:creator><dc:creator>Xiaolin Wang</dc:creator><dc:creator>Xiyang Liu</dc:creator><dc:creator>Yi Chang</dc:creator><dc:creator>Luxin Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
Published: 2025-12-04T08:49:23+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Houzhang Fang; Chenxing Wu; Kun Bai; Tianqi Chen; Xiaolin Wang; Xiyang Liu; Yi Chang; Luxin Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network&amp;#x27;s focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.&lt;/p&gt;</content:encoded></item><item><title>Lightweight Image Super-Resolution Network with Adaptive Token Selection and Feature Enhancement</title><link>https://doi.org/10.1016/j.knosys.2025.115055</link><guid>10.1016/j.knosys.2025.115055</guid><pubDate>Sat, 06 Dec 2025 16:09:36 +0000</pubDate><dc:creator>Detian Huang</dc:creator><dc:creator>Mingxin Lin</dc:creator><dc:creator>Xinwei Gan</dc:creator><dc:creator>Luanyuan Dai</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115055</prism:doi><description>Transformer has demonstrated remarkable performance in image super-resolution due to its powerful long-range dependency modeling. However, existing transformer-based methods face two challenges. First, their self-attention mechanisms often treat all tokens equally, introducing redundant correlations and noise. Second, the use of window-based attention restricts the receptive field, hindering effective long-range dependency modeling. To address these issues, we introduce TSFE, a lightweight super-resolution network with adaptive Token Selection and Feature Enhancement. Specifically, we design a Token Selection Channel self-Attention (TSCA) that adaptively selects the top- K most relevant tokens along the channel dimension, preserving critical features while reducing redundant computations. Then, we present a Sparse Spatial Self-Attention (SSSA) that enhances critical features by suppressing low-attention weights and amplifying high-attention ones in the spatial dimension. To expand the receptive field, SSSA integrates a prior dictionary that guides the attention distribution with external prior, improving its dependency modeling capability. The collaboration of TSCA and SSSA components enables TSFE to achieve precise self-attention computation with low complexity. Extensive experiments validate that the proposed TSFE outperforms state-of-the-art methods across benchmarks.
Published: 2025-12-06T16:09:36+00:00
Venue: Knowledge-Based Systems
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Detian Huang; Mingxin Lin; Xinwei Gan; Luanyuan Dai; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115055"&gt;10.1016/j.knosys.2025.115055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer has demonstrated remarkable performance in image super-resolution due to its powerful long-range dependency modeling. However, existing transformer-based methods face two challenges. First, their self-attention mechanisms often treat all tokens equally, introducing redundant correlations and noise. Second, the use of window-based attention restricts the receptive field, hindering effective long-range dependency modeling. To address these issues, we introduce TSFE, a lightweight super-resolution network with adaptive Token Selection and Feature Enhancement. Specifically, we design a Token Selection Channel self-Attention (TSCA) that adaptively selects the top- K most relevant tokens along the channel dimension, preserving critical features while reducing redundant computations. Then, we present a Sparse Spatial Self-Attention (SSSA) that enhances critical features by suppressing low-attention weights and amplifying high-attention ones in the spatial dimension. To expand the receptive field, SSSA integrates a prior dictionary that guides the attention distribution with external prior, improving its dependency modeling capability. The collaboration of TSCA and SSSA components enables TSFE to achieve precise self-attention computation with low complexity. Extensive experiments validate that the proposed TSFE outperforms state-of-the-art methods across benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection</title><link>https://arxiv.org/abs/2512.04413v1</link><guid>http://arxiv.org/abs/2512.04413v1</guid><pubDate>Thu, 04 Dec 2025 03:18:42 +0000</pubDate><dc:creator>Xiangyi Gao</dc:creator><dc:creator>Danpei Zhao</dc:creator><dc:creator>Bo Yuan</dc:creator><dc:creator>Wentao Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TGRS.2025.3600098</prism:doi><description>Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.
Published: 2025-12-04T03:18:42+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyi Gao; Danpei Zhao; Bo Yuan; Wentao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TGRS.2025.3600098"&gt;10.1109/TGRS.2025.3600098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.&lt;/p&gt;</content:encoded></item><item><title>RTFormer: radiative transfer model-coupled transformer for cloud removal in optical remote sensing imagery</title><link>https://doi.org/10.1080/15481603.2025.2584294</link><guid>10.1080/15481603.2025.2584294</guid><pubDate>Sat, 06 Dec 2025 07:53:13 +0000</pubDate><dc:creator>Shiyao Meng</dc:creator><dc:creator>Siwei Li</dc:creator><dc:creator>Xinyu Wang</dc:creator><dc:creator>Ge Song</dc:creator><dc:creator>Jie Yang</dc:creator><dc:creator>Yu Ding</dc:creator><dc:creator>Wei Gong</dc:creator><prism:publicationName>GIScience &amp;amp; Remote Sensing</prism:publicationName><prism:doi>10.1080/15481603.2025.2584294</prism:doi><description>Clouds cover approximately 70% of the Earth’s surface, severely degrading optical satellite imagery and limiting its use in critical applications. Although recent transformer-based cloud removal methods have shown promise, they face two key challenges: (1) empirical models are limited in representing the complex radiative transfer properties of clouds, leading to physical inconsistencies; and (2) the high computational demands of self-attention typically constrain receptive fields, hindering the exploitation of global context for reconstructing cloud-contaminated regions. To address these issues, we propose RTFormer, a novel Radiative Transfer Model-coupled Transformer framework that integrates physical modeling into deep learning for efficient and accurate cloud removal. The core contribution lies in the incorporation of radiative transfer principles into the transformer architecture, enabling global context perception without additional computational overhead. To bridge the gap between simulated and real-world scenarios, we furhter developed a radiative transfer model-based dataset for model training. RTFormer extends the perception field beyond local constraints and incorporates a residual learning mechanism to preserve intact information in clear regions while accurately reconstructing cloud-affected areas. Experimental results demonstrate that RTFormer outperforms state-of-the-art methods in accuracy. Moreover, it substantially benefits downstream applications; for instance, when applied as a preprocessing step for land-cover classification, the overall accuracy improved by 4.38% compared with competing methods. These findings highlight the value of integrating physical models with transformer-based deep learning for cloud removal and demonstrate its potential for advancing remote sensing applications.
Published: 2025-12-06T07:53:13+00:00
Venue: GIScience &amp;amp; Remote Sensing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiyao Meng; Siwei Li; Xinyu Wang; Ge Song; Jie Yang; Yu Ding; Wei Gong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; GIScience &amp;amp;amp; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/15481603.2025.2584294"&gt;10.1080/15481603.2025.2584294&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Clouds cover approximately 70% of the Earth’s surface, severely degrading optical satellite imagery and limiting its use in critical applications. Although recent transformer-based cloud removal methods have shown promise, they face two key challenges: (1) empirical models are limited in representing the complex radiative transfer properties of clouds, leading to physical inconsistencies; and (2) the high computational demands of self-attention typically constrain receptive fields, hindering the exploitation of global context for reconstructing cloud-contaminated regions. To address these issues, we propose RTFormer, a novel Radiative Transfer Model-coupled Transformer framework that integrates physical modeling into deep learning for efficient and accurate cloud removal. The core contribution lies in the incorporation of radiative transfer principles into the transformer architecture, enabling global context perception without additional computational overhead. To bridge the gap between simulated and real-world scenarios, we furhter developed a radiative transfer model-based dataset for model training. RTFormer extends the perception field beyond local constraints and incorporates a residual learning mechanism to preserve intact information in clear regions while accurately reconstructing cloud-affected areas. Experimental results demonstrate that RTFormer outperforms state-of-the-art methods in accuracy. Moreover, it substantially benefits downstream applications; for instance, when applied as a preprocessing step for land-cover classification, the overall accuracy improved by 4.38% compared with competing methods. These findings highlight the value of integrating physical models with transformer-based deep learning for cloud removal and demonstrate its potential for advancing remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>PreciseVideo: A Dual-Process Framework for Zero-Shot Text-to-Video Generation with Quantitative Content Control</title><link>https://doi.org/10.1016/j.inffus.2025.104030</link><guid>10.1016/j.inffus.2025.104030</guid><pubDate>Sat, 06 Dec 2025 00:36:18 +0000</pubDate><dc:creator>Lizhi Dang</dc:creator><dc:creator>Ting Liang</dc:creator><dc:creator>Huixin Zhang</dc:creator><dc:creator>Ruihao Zhang</dc:creator><dc:creator>Yingping Hong</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104030</prism:doi><description>Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .
Published: 2025-12-06T00:36:18+00:00
Venue: Information Fusion
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lizhi Dang; Ting Liang; Huixin Zhang; Ruihao Zhang; Yingping Hong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104030"&gt;10.1016/j.inffus.2025.104030&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Text-to-video (T2V) generation has recently gained significant attention, yet existing methods primarily focus on global temporal consistency and lack fine-grained, element-wise control over background dynamics and character behaviors. We propose PreciseVideo , a zero-shot T2V framework that enables controllable video synthesis at both the background and foreground levels. PreciseVideo introduces a dual-stage generation paradigm, separating background and character synthesis, and incorporates three novel modules: the Region-Independent Noise Modulator for quantifiable, region-wise temporal dynamics, Sparse Fusion Attention for structured cross-frame coherence, and Optimal-Reference-Frame Attention to preserve full-body character identity and appearance. This modular design ensures high-fidelity, temporally coherent, and behaviorally consistent video generation, even in complex multi-character scenarios. Extensive experiments demonstrate that PreciseVideo excels in element-wise controllability, character quantity accuracy, and multi-character scene synthesis compared with both zero-shot and training-based baselines. Ablation studies validate the effectiveness of each proposed module, while additional evaluations on scene-to-character and inter-character occlusions highlight the framework’s robustness and flexibility. Collectively, our results establish PreciseVideo as a highly controllable and scalable T2V approach, filling a critical gap in fine-grained, element-wise controllable video generation and setting a foundation for future advances in complex scene synthesis. Our code and related experimental results are available at https://github.com/GG-Bond2023/PreciseVideo .&lt;/p&gt;</content:encoded></item><item><title>DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance</title><link>https://arxiv.org/abs/2512.04511v1</link><guid>http://arxiv.org/abs/2512.04511v1</guid><pubDate>Thu, 04 Dec 2025 06:45:20 +0000</pubDate><dc:creator>Yinghui Xing</dc:creator><dc:creator>Xiaoting Su</dc:creator><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Donghao Chu</dc:creator><dc:creator>Di Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.
Published: 2025-12-04T06:45:20+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinghui Xing; Xiaoting Su; Shizhou Zhang; Donghao Chu; Di Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.&lt;/p&gt;</content:encoded></item><item><title>Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects</title><link>https://arxiv.org/abs/2512.05006v1</link><guid>http://arxiv.org/abs/2512.05006v1</guid><pubDate>Thu, 04 Dec 2025 17:17:47 +0000</pubDate><dc:creator>Xianghui Fan</dc:creator><dc:creator>Zhaoyu Chen</dc:creator><dc:creator>Mengyang Pan</dc:creator><dc:creator>Anping Deng</dc:creator><dc:creator>Hang Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.
Published: 2025-12-04T17:17:47+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xianghui Fan; Zhaoyu Chen; Mengyang Pan; Anping Deng; Hang Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.&lt;/p&gt;</content:encoded></item><item><title>Deep learning detection and analysis of eddies in the East Greenland marginal ice zone from Sentinel-1 SAR imagery</title><link>https://doi.org/10.1016/j.rse.2025.115177</link><guid>10.1016/j.rse.2025.115177</guid><pubDate>Sun, 07 Dec 2025 05:01:05 +0000</pubDate><dc:creator>Fei Jiang</dc:creator><dc:creator>Xiaofeng Li</dc:creator><dc:creator>Yingjie Liu</dc:creator><dc:creator>Yibin Ren</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115177</prism:doi><description>Ocean eddies in the marginal ice zone (MIZ) play a crucial role in sea-ice dynamics and polar ocean-atmosphere interactions; however, their detection remains challenging due to their complex surface signatures. In this study, we developed MIZ-EDYOLO, a deep learning model customized for detecting MIZ eddies from dual-polarized Sentinel-1 SAR imagery. Built upon the YOLOv9-t architecture and enhanced with specialized modifications, the model was trained on a dataset containing over 20,000 slices extracted from 1370 SAR images. The MIZ-EDYOLO model achieves high detection accuracy (∼80 % F1-score) on the test set and performs reliably on 200 full-scene images, enabling efficient and automated eddy identification. Using this model, we constructed the first six-year (2018–2023) SAR-based MIZ eddy dataset for the East Greenland region, comprising over 10,000 eddy instances. Analysis of this dataset reveals that eddy distributions are related to boundary currents, topographic forcing, and seasonal variations of the MIZ. Cyclonic eddies (CEs) outnumber anticyclonic eddies (AEs) by a factor of 8.4, while AEs exhibited an average radius about 1.8 times larger than CEs. The observed asymmetries between CEs and AEs are linked to their rotational dynamics and the associated sea-ice responses. This study presents a scalable and operational framework for efficient eddy monitoring in the MIZ, providing new insights into multi-scale oceanographic processes in climate-sensitive polar regions.
Published: 2025-12-07T05:01:05+00:00
Venue: Remote Sensing of Environment
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fei Jiang; Xiaofeng Li; Yingjie Liu; Yibin Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115177"&gt;10.1016/j.rse.2025.115177&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Ocean eddies in the marginal ice zone (MIZ) play a crucial role in sea-ice dynamics and polar ocean-atmosphere interactions; however, their detection remains challenging due to their complex surface signatures. In this study, we developed MIZ-EDYOLO, a deep learning model customized for detecting MIZ eddies from dual-polarized Sentinel-1 SAR imagery. Built upon the YOLOv9-t architecture and enhanced with specialized modifications, the model was trained on a dataset containing over 20,000 slices extracted from 1370 SAR images. The MIZ-EDYOLO model achieves high detection accuracy (∼80 % F1-score) on the test set and performs reliably on 200 full-scene images, enabling efficient and automated eddy identification. Using this model, we constructed the first six-year (2018–2023) SAR-based MIZ eddy dataset for the East Greenland region, comprising over 10,000 eddy instances. Analysis of this dataset reveals that eddy distributions are related to boundary currents, topographic forcing, and seasonal variations of the MIZ. Cyclonic eddies (CEs) outnumber anticyclonic eddies (AEs) by a factor of 8.4, while AEs exhibited an average radius about 1.8 times larger than CEs. The observed asymmetries between CEs and AEs are linked to their rotational dynamics and the associated sea-ice responses. This study presents a scalable and operational framework for efficient eddy monitoring in the MIZ, providing new insights into multi-scale oceanographic processes in climate-sensitive polar regions.&lt;/p&gt;</content:encoded></item><item><title>Source-Free Domain Adaptation via Multimodal Space-Guided Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112827</link><guid>10.1016/j.patcog.2025.112827</guid><pubDate>Sun, 07 Dec 2025 15:13:39 +0000</pubDate><dc:creator>Lijuan Chen</dc:creator><dc:creator>Yunxiang Bai</dc:creator><dc:creator>Ying Hu</dc:creator><dc:creator>Qiong Wang</dc:creator><dc:creator>Xiaozhi Qi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112827</prism:doi><description>Conventional UDA requires access to the source domain, invalidating it in the information security and privacy protection scenarios. In contrast, Source-free Domain Adaptation (SFDA) involves transferring a pre-trained source model to an unlabeled target domain while the source data is absent. However, prior methods based on self-supervised learning have struggled to find a quality domain invariant representation space due to the lack of source data. To address this challenge, in this work, we propose leveraging the success of vision-language pre-trained (ViL) models (e.g., CLIP). To integrate the domain generality of the ViL model and the task specificity of source model more effectively, we introduce a novel M ulti M odal Space- G uided A lignment ( MMGA ) approach. Specifically, we start with a multimodal feature calibration for achieving coarse alignment between the target visual domain and the multimodal space. However, this ViL space is still not the domain invariant space, being trained on a large number of samples. To achieve further fine-grained alignment towards the domain invariant space, we have designed two methods: Potential category consistency and prediction consistency alignment. These methods push the potential categories distribution and the prediction distribution closer to the fused pseudo-supervision by the ViL model and the adapted source model, respectively. This strategy corrects the errors of feature alignment to the ViL space. Extensive experiments show that our MMGA approach significantly outperforms current state-of-the-art alternatives. The code and data are available at https://github.com/YunxiangBai0/MMGA/
Published: 2025-12-07T15:13:39+00:00
Venue: Pattern Recognition
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lijuan Chen; Yunxiang Bai; Ying Hu; Qiong Wang; Xiaozhi Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112827"&gt;10.1016/j.patcog.2025.112827&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Conventional UDA requires access to the source domain, invalidating it in the information security and privacy protection scenarios. In contrast, Source-free Domain Adaptation (SFDA) involves transferring a pre-trained source model to an unlabeled target domain while the source data is absent. However, prior methods based on self-supervised learning have struggled to find a quality domain invariant representation space due to the lack of source data. To address this challenge, in this work, we propose leveraging the success of vision-language pre-trained (ViL) models (e.g., CLIP). To integrate the domain generality of the ViL model and the task specificity of source model more effectively, we introduce a novel M ulti M odal Space- G uided A lignment ( MMGA ) approach. Specifically, we start with a multimodal feature calibration for achieving coarse alignment between the target visual domain and the multimodal space. However, this ViL space is still not the domain invariant space, being trained on a large number of samples. To achieve further fine-grained alignment towards the domain invariant space, we have designed two methods: Potential category consistency and prediction consistency alignment. These methods push the potential categories distribution and the prediction distribution closer to the fused pseudo-supervision by the ViL model and the adapted source model, respectively. This strategy corrects the errors of feature alignment to the ViL space. Extensive experiments show that our MMGA approach significantly outperforms current state-of-the-art alternatives. The code and data are available at https://github.com/YunxiangBai0/MMGA/&lt;/p&gt;</content:encoded></item><item><title>Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.04520v1</link><guid>http://arxiv.org/abs/2512.04520v1</guid><pubDate>Thu, 04 Dec 2025 07:08:21 +0000</pubDate><dc:creator>Chenlin Xu</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Lituan Wang</dc:creator><dc:creator>Xinyu Pu</dc:creator><dc:creator>Pengfei Ma</dc:creator><dc:creator>Guangwu Qian</dc:creator><dc:creator>Zizhou Wang</dc:creator><dc:creator>Yan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM's zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.
Published: 2025-12-04T07:08:21+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenlin Xu; Lei Zhang; Lituan Wang; Xinyu Pu; Pengfei Ma; Guangwu Qian; Zizhou Wang; Yan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM&amp;#x27;s zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.&lt;/p&gt;</content:encoded></item><item><title>RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation</title><link>https://arxiv.org/abs/2512.05025v1</link><guid>http://arxiv.org/abs/2512.05025v1</guid><pubDate>Thu, 04 Dec 2025 17:40:17 +0000</pubDate><dc:creator>Nicolas Houdré</dc:creator><dc:creator>Diego Marcos</dc:creator><dc:creator>Hugo Riffaud de Turckheim</dc:creator><dc:creator>Dino Ienco</dc:creator><dc:creator>Laurent Wendling</dc:creator><dc:creator>Camille Kurtz</dc:creator><dc:creator>Sylvain Lobry</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.
Published: 2025-12-04T17:40:17+00:00
Venue: arXiv
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nicolas Houdré; Diego Marcos; Hugo Riffaud de Turckheim; Dino Ienco; Laurent Wendling; Camille Kurtz; Sylvain Lobry&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.&lt;/p&gt;</content:encoded></item><item><title>Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models</title><link>https://arxiv.org/abs/2512.04395v1</link><guid>http://arxiv.org/abs/2512.04395v1</guid><pubDate>Thu, 04 Dec 2025 02:32:55 +0000</pubDate><dc:creator>Hieu Dinh Trung Pham</dc:creator><dc:creator>Huy Minh Nhat Nguyen</dc:creator><dc:creator>Cuong Tuan Nguyen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.
Published: 2025-12-04T02:32:55+00:00
Venue: arXiv
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hieu Dinh Trung Pham; Huy Minh Nhat Nguyen; Cuong Tuan Nguyen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image&amp;#x27;s domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image&amp;#x27;s structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.&lt;/p&gt;</content:encoded></item><item><title>Mitigating Modal Discrepancies for Visible-Infrared Person Re-Identification via High-order Nonlinear Constraint</title><link>https://doi.org/10.1016/j.knosys.2025.115052</link><guid>10.1016/j.knosys.2025.115052</guid><pubDate>Sat, 06 Dec 2025 07:47:47 +0000</pubDate><dc:creator>Junyu Liu</dc:creator><dc:creator>Yanzhen Xiong</dc:creator><dc:creator>Jinjia Peng</dc:creator><dc:creator>Huibing Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115052</prism:doi><description>One of the principal challenges in the visible-infrared person re-identification (VI-ReID) task is bridging the modality gap between visible and infrared images. Most existing methods transfer single-modal feature extractors to VI-ReID without sufficient consideration of cross-modal characteristics, and apply metric learning losses with Euclidean distance as the common metric. However, the substantial differences between visible and infrared modalities make it difficult for such designs to capture complex nonlinear cross-modal relations. To overcome these issues, this paper proposes the Reproducing Kernel Hilbert Space-Based Modal Discrepancy Reduction Network (RMDR-Net), which optimizes cross-modal discrepancies by employing Reproducing Kernel Hilbert Space (RKHS) similarity metrics. RMDR-Net includes a novel High-order Nonlinear Discriminative loss, which initially mitigates cross-modal discrepancies by capturing the high-order nonlinear relationships between features in RKHS. Moreover, Gram Matrix Consistency loss is designed to enhance distributional consistency both within and between modalities, which revealing underlying feature relationships and further promoting multimodal alignment. Additionally, the Multi-Scale Enhanced Dual Attention module is proposed to capture cross-modal fine-grained differences and obtain discriminative features. Extensive experiments on several public datasets demonstrate that our network surpasses other state-of-the-art methods.
Published: 2025-12-06T07:47:47+00:00
Venue: Knowledge-Based Systems
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junyu Liu; Yanzhen Xiong; Jinjia Peng; Huibing Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115052"&gt;10.1016/j.knosys.2025.115052&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;One of the principal challenges in the visible-infrared person re-identification (VI-ReID) task is bridging the modality gap between visible and infrared images. Most existing methods transfer single-modal feature extractors to VI-ReID without sufficient consideration of cross-modal characteristics, and apply metric learning losses with Euclidean distance as the common metric. However, the substantial differences between visible and infrared modalities make it difficult for such designs to capture complex nonlinear cross-modal relations. To overcome these issues, this paper proposes the Reproducing Kernel Hilbert Space-Based Modal Discrepancy Reduction Network (RMDR-Net), which optimizes cross-modal discrepancies by employing Reproducing Kernel Hilbert Space (RKHS) similarity metrics. RMDR-Net includes a novel High-order Nonlinear Discriminative loss, which initially mitigates cross-modal discrepancies by capturing the high-order nonlinear relationships between features in RKHS. Moreover, Gram Matrix Consistency loss is designed to enhance distributional consistency both within and between modalities, which revealing underlying feature relationships and further promoting multimodal alignment. Additionally, the Multi-Scale Enhanced Dual Attention module is proposed to capture cross-modal fine-grained differences and obtain discriminative features. Extensive experiments on several public datasets demonstrate that our network surpasses other state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>MT-Depth: Multi-task Instance feature analysis for the Depth Completion</title><link>https://arxiv.org/abs/2512.04734v1</link><guid>http://arxiv.org/abs/2512.04734v1</guid><pubDate>Thu, 04 Dec 2025 12:17:33 +0000</pubDate><dc:creator>Abdul Haseeb Nizamani</dc:creator><dc:creator>Dandi Zhou</dc:creator><dc:creator>Xinhai Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.
Published: 2025-12-04T12:17:33+00:00
Venue: arXiv
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Abdul Haseeb Nizamani; Dandi Zhou; Xinhai Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Depth completion plays a vital role in 3D perception systems, especially in scenarios where sparse depth data must be densified for tasks such as autonomous driving, robotics, and augmented reality. While many existing approaches rely on semantic segmentation to guide depth completion, they often overlook the benefits of object-level understanding. In this work, we introduce an instance-aware depth completion framework that explicitly integrates binary instance masks as spatial priors to refine depth predictions. Our model combines four main components: a frozen YOLO V11 instance segmentation branch, a U-Net-based depth completion backbone, a cross-attention fusion module, and an attention-guided prediction head. The instance segmentation branch generates per-image foreground masks that guide the depth branch via cross-attention, allowing the network to focus on object-centric regions during refinement. We validate our method on the Virtual KITTI 2 dataset, showing that it achieves lower RMSE compared to both a U-Net-only baseline and previous semantic-guided methods, while maintaining competitive MAE. Qualitative and quantitative results demonstrate that the proposed model effectively enhances depth accuracy near object boundaries, occlusions, and thin structures. Our findings suggest that incorporating instance-aware cues offers a promising direction for improving depth completion without relying on dense semantic labels.&lt;/p&gt;</content:encoded></item><item><title>GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis</title><link>https://arxiv.org/abs/2512.04456v1</link><guid>http://arxiv.org/abs/2512.04456v1</guid><pubDate>Thu, 04 Dec 2025 05:00:00 +0000</pubDate><dc:creator>Changjin Kim</dc:creator><dc:creator>HyeokJun Lee</dc:creator><dc:creator>YoungJoon Yoo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.
Published: 2025-12-04T05:00:00+00:00
Venue: arXiv
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changjin Kim; HyeokJun Lee; YoungJoon Yoo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model&amp;#x27;s backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.&lt;/p&gt;</content:encoded></item><item><title>MGC-Net: Learning Feature Matching with Multi-Geometry Cooperation</title><link>https://doi.org/10.1016/j.knosys.2025.115062</link><guid>10.1016/j.knosys.2025.115062</guid><pubDate>Sat, 06 Dec 2025 00:20:13 +0000</pubDate><dc:creator>Luxia Ai</dc:creator><dc:creator>Kun Sun</dc:creator><dc:creator>Chen Zhang</dc:creator><dc:creator>Nanjun Yuan</dc:creator><dc:creator>Qun Jiang</dc:creator><dc:creator>Wenbing Tao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115062</prism:doi><description>Feature matching is the task of determining correspondences between images, which is important for many downstream vision applications. Although learning-based methods have made great progress, it is challenging for effective and efficient feature matching in changing scenes. The existing sparse learning feature matching method is efficient but not suitable for various changing scenes, especially weak textures. Meanwhile, the dense learning feature matching method is effective in various changing scenes but inefficient. Therefore, we aim to leverage strengths of both sparse and dense methods to achieve effective and efficient feature matching. To achieve this, we introduce MGC-Net to learn sparse-to-dense matching via multi-geometry cooperation. MGC-Net includes affine-enhanced module (AEM), pose-guided module (PGM), and extra homography supervision. AEM can expand dense points around sparse keypoints and extract affine-invariant features, enhancing local features by embedding affine geometry. PGM can find outliers in putative correspondences and prune them out, guiding better matching by embedding epipolar geometry. Moreover, MGC-Net is supervised by homography geometry to determine correspondences. With multi-geometry cooperation, MGC-Net learns more useful priors through end-to-end training, making inference accurately and quickly. Experiments show that MGC-Net has a better AUC than sparse methods. Also, its running time is better than dense methods. Our method can achieve efficient and effective feature matching in various changing scenes, which other methods can’t balance. MGC-Net is the first to learn sparse-to-dense matching and achieve sota.
Published: 2025-12-06T00:20:13+00:00
Venue: Knowledge-Based Systems
Score: 0.762 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Luxia Ai; Kun Sun; Chen Zhang; Nanjun Yuan; Qun Jiang; Wenbing Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115062"&gt;10.1016/j.knosys.2025.115062&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (consider)&lt;/p&gt;
&lt;p&gt;Feature matching is the task of determining correspondences between images, which is important for many downstream vision applications. Although learning-based methods have made great progress, it is challenging for effective and efficient feature matching in changing scenes. The existing sparse learning feature matching method is efficient but not suitable for various changing scenes, especially weak textures. Meanwhile, the dense learning feature matching method is effective in various changing scenes but inefficient. Therefore, we aim to leverage strengths of both sparse and dense methods to achieve effective and efficient feature matching. To achieve this, we introduce MGC-Net to learn sparse-to-dense matching via multi-geometry cooperation. MGC-Net includes affine-enhanced module (AEM), pose-guided module (PGM), and extra homography supervision. AEM can expand dense points around sparse keypoints and extract affine-invariant features, enhancing local features by embedding affine geometry. PGM can find outliers in putative correspondences and prune them out, guiding better matching by embedding epipolar geometry. Moreover, MGC-Net is supervised by homography geometry to determine correspondences. With multi-geometry cooperation, MGC-Net learns more useful priors through end-to-end training, making inference accurately and quickly. Experiments show that MGC-Net has a better AUC than sparse methods. Also, its running time is better than dense methods. Our method can achieve efficient and effective feature matching in various changing scenes, which other methods can’t balance. MGC-Net is the first to learn sparse-to-dense matching and achieve sota.&lt;/p&gt;</content:encoded></item><item><title>Heterogeneous Feature Knowledge Distillation based on Enhanced Feature Projector Correlation</title><link>https://doi.org/10.1016/j.neunet.2025.108409</link><guid>10.1016/j.neunet.2025.108409</guid><pubDate>Sat, 06 Dec 2025 16:08:34 +0000</pubDate><dc:creator>Hong Zhao</dc:creator><dc:creator>Kangping Chen</dc:creator><dc:creator>Qiaoyin Jin</dc:creator><dc:creator>Dailin Huang</dc:creator><dc:creator>Zhaobin Chang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108409</prism:doi><description>Knowledge Distillation (KD) is a widely used technique to enhance model performance. However, most existing methods are designed under the assumption that the teacher and student models belong to the homogeneous architecture —particularly those relying on intermediate feature KD. In practice, mainstream model architectures exhibit significant differences in feature structure and representation, making feature alignment challenging. To address this issue, we propose a heterogeneous feature knowledge distillation based on enhanced feature projector correlation. Specifically, we first project both teacher and student features into the structurally consistent latent space to measure their semantic distribution. To mitigate the potential loss of semantic relevance caused by feature decorrelation during projection, we introduce a cross-space fusion mechanism to model the correlation between the original and latent features. Furthermore, to better leverage the rich representations from deep teacher layers, we design a multi-level feature knowledge distillation loss that guides the student by regressing features from multiple semantic levels. Finally, to reduce the noise introduced by the inherent limitations of student features and the projection process, we incorporate a denoising mechanism based on diffusion models to enhance class-wise discriminability in the student feature space. We conducted extensive experiments on the CIFAR-100 and ImageNet datasets to validate the effectiveness of the proposed distillation method across various mainstream architectures, including Convolutional Neural Networks (CNNs), Transformers, and Multilayer Perceptrons (MLPs) for image classification. In addition, we performed semantic segmentation experiments on the Cityscapes dataset to further verify the performance of our method. Code is available at https://github.com/chenKP/HFKD-Diff .
Published: 2025-12-06T16:08:34+00:00
Venue: Neural Networks
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hong Zhao; Kangping Chen; Qiaoyin Jin; Dailin Huang; Zhaobin Chang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108409"&gt;10.1016/j.neunet.2025.108409&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Knowledge Distillation (KD) is a widely used technique to enhance model performance. However, most existing methods are designed under the assumption that the teacher and student models belong to the homogeneous architecture —particularly those relying on intermediate feature KD. In practice, mainstream model architectures exhibit significant differences in feature structure and representation, making feature alignment challenging. To address this issue, we propose a heterogeneous feature knowledge distillation based on enhanced feature projector correlation. Specifically, we first project both teacher and student features into the structurally consistent latent space to measure their semantic distribution. To mitigate the potential loss of semantic relevance caused by feature decorrelation during projection, we introduce a cross-space fusion mechanism to model the correlation between the original and latent features. Furthermore, to better leverage the rich representations from deep teacher layers, we design a multi-level feature knowledge distillation loss that guides the student by regressing features from multiple semantic levels. Finally, to reduce the noise introduced by the inherent limitations of student features and the projection process, we incorporate a denoising mechanism based on diffusion models to enhance class-wise discriminability in the student feature space. We conducted extensive experiments on the CIFAR-100 and ImageNet datasets to validate the effectiveness of the proposed distillation method across various mainstream architectures, including Convolutional Neural Networks (CNNs), Transformers, and Multilayer Perceptrons (MLPs) for image classification. In addition, we performed semantic segmentation experiments on the Cityscapes dataset to further verify the performance of our method. Code is available at https://github.com/chenKP/HFKD-Diff .&lt;/p&gt;</content:encoded></item><item><title>Prompt-oriented and Frequency-regularized Schrödinger Bridge for Unpaired Rain Streaks and Raindrops Removal</title><link>https://doi.org/10.1016/j.patcog.2025.112862</link><guid>10.1016/j.patcog.2025.112862</guid><pubDate>Sat, 06 Dec 2025 00:07:54 +0000</pubDate><dc:creator>Yuanbo Wen</dc:creator><dc:creator>Jing Qin</dc:creator><dc:creator>Ting Chen</dc:creator><dc:creator>Tao Gao</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112862</prism:doi><description>The removal of rain streaks and raindrops is essential for improving image visibility. However, most existing methods rely on paired rainy and clean images, which are difficult to acquire in real-world scenarios. To this end, we propose prior-oriented and frequency-regularized Schrödinger bridge (PFSB) for rain streaks and raindrops removal with unpaired training. Specifically, we initially formulate unpaired image deraining as a Schrödinger bridge problem. Furthermore, we demonstrate the locally quasi-convexity of structural similarity, and employ the multi-scale structural similarity constraint (MSSC) to minimize the duality gap between the primal and dual problems, ensuring linear convergence of gradient flow while preserving textural details. Meanwhile, we develop a context-preserving consistency modulator (CCM) guide the derained output toward clean content, thereby retaining rain-irrelevant features. Moreover, we propose a domain-representative prompt protocol (DPP), which enforces the generated sample to eliminate rain-relevant information and maintain alignment with the clean domain. Additionally, we utilize Bayesian frequency-domain regularization (BFR) to balance spectral consistency with clean references and repulsion from rainy patterns. Extensive experiments demonstrate that our method surpasses the existing well-performing unpaired learning approaches in both fidelity and photo-realism.
Published: 2025-12-06T00:07:54+00:00
Venue: Pattern Recognition
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanbo Wen; Jing Qin; Ting Chen; Tao Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112862"&gt;10.1016/j.patcog.2025.112862&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;The removal of rain streaks and raindrops is essential for improving image visibility. However, most existing methods rely on paired rainy and clean images, which are difficult to acquire in real-world scenarios. To this end, we propose prior-oriented and frequency-regularized Schrödinger bridge (PFSB) for rain streaks and raindrops removal with unpaired training. Specifically, we initially formulate unpaired image deraining as a Schrödinger bridge problem. Furthermore, we demonstrate the locally quasi-convexity of structural similarity, and employ the multi-scale structural similarity constraint (MSSC) to minimize the duality gap between the primal and dual problems, ensuring linear convergence of gradient flow while preserving textural details. Meanwhile, we develop a context-preserving consistency modulator (CCM) guide the derained output toward clean content, thereby retaining rain-irrelevant features. Moreover, we propose a domain-representative prompt protocol (DPP), which enforces the generated sample to eliminate rain-relevant information and maintain alignment with the clean domain. Additionally, we utilize Bayesian frequency-domain regularization (BFR) to balance spectral consistency with clean references and repulsion from rainy patterns. Extensive experiments demonstrate that our method surpasses the existing well-performing unpaired learning approaches in both fidelity and photo-realism.&lt;/p&gt;</content:encoded></item><item><title>Rethinking the Use of Vision Transformers for AI-Generated Image Detection</title><link>https://arxiv.org/abs/2512.04969v1</link><guid>http://arxiv.org/abs/2512.04969v1</guid><pubDate>Thu, 04 Dec 2025 16:37:47 +0000</pubDate><dc:creator>NaHyeon Park</dc:creator><dc:creator>Kunhee Kim</dc:creator><dc:creator>Junsuk Choe</dc:creator><dc:creator>Hyunjung Shim</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.
Published: 2025-12-04T16:37:47+00:00
Venue: arXiv
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; NaHyeon Park; Kunhee Kim; Junsuk Choe; Hyunjung Shim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.&lt;/p&gt;</content:encoded></item><item><title>DANIM: Domain Adaptation Network with Intermediate Domain Masking for Night-time Scene Parsing</title><link>https://doi.org/10.1016/j.patcog.2025.112796</link><guid>10.1016/j.patcog.2025.112796</guid><pubDate>Sun, 07 Dec 2025 15:13:38 +0000</pubDate><dc:creator>Qijian Tian</dc:creator><dc:creator>Sen Wang</dc:creator><dc:creator>Ran Yi</dc:creator><dc:creator>Zufeng Zhang</dc:creator><dc:creator>Bin Sheng</dc:creator><dc:creator>Xin Tan</dc:creator><dc:creator>Lizhuang Ma</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112796</prism:doi><description>Night-time scene parsing is important for practical applications such as autonomous driving and robot vision. Since annotating is time-consuming, Unsupervised Domain Adaptation (UDA) is an effective solution for night-time scene parsing. Due to the low illumination, over/under-exposure, and motion blur in night-time scenes, existing methods can not connect daytime scenes and night-time scenes well, limiting their performance. Some methods rely on day-night paired images, which are costly to collect and therefore impractical. In this paper, we propose DANIM, a self-training UDA network for night-time scene parsing. We introduce an intermediate domain that explicitly models the connection between daytime scenes and night-time scenes from lighting and structure. The intermediate domain shares similar structure information with the night-time target domain and similar lighting information with the daytime source domain. By harnessing the rich prior knowledge of a pre-trained text-driven generative model, the intermediate domain can be generated and we propose a scoring mechanism for selecting the high-quality one for training. Besides, we propose intermediate domain masking to address the inconsistency between the intermediate domain and the target domain. We further design a coupled mask strategy to make the mask more effective. Extensive experiments show that DANIM has achieved first place on the DarkZurich leaderboard and outperforms state-of-the-art methods on other widely used night-time scene parsing benchmarks, i.e. ACDC-night, NightCity, and NighttimeDriving.
Published: 2025-12-07T15:13:38+00:00
Venue: Pattern Recognition
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qijian Tian; Sen Wang; Ran Yi; Zufeng Zhang; Bin Sheng; Xin Tan; Lizhuang Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112796"&gt;10.1016/j.patcog.2025.112796&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;Night-time scene parsing is important for practical applications such as autonomous driving and robot vision. Since annotating is time-consuming, Unsupervised Domain Adaptation (UDA) is an effective solution for night-time scene parsing. Due to the low illumination, over/under-exposure, and motion blur in night-time scenes, existing methods can not connect daytime scenes and night-time scenes well, limiting their performance. Some methods rely on day-night paired images, which are costly to collect and therefore impractical. In this paper, we propose DANIM, a self-training UDA network for night-time scene parsing. We introduce an intermediate domain that explicitly models the connection between daytime scenes and night-time scenes from lighting and structure. The intermediate domain shares similar structure information with the night-time target domain and similar lighting information with the daytime source domain. By harnessing the rich prior knowledge of a pre-trained text-driven generative model, the intermediate domain can be generated and we propose a scoring mechanism for selecting the high-quality one for training. Besides, we propose intermediate domain masking to address the inconsistency between the intermediate domain and the target domain. We further design a coupled mask strategy to make the mask more effective. Extensive experiments show that DANIM has achieved first place on the DarkZurich leaderboard and outperforms state-of-the-art methods on other widely used night-time scene parsing benchmarks, i.e. ACDC-night, NightCity, and NighttimeDriving.&lt;/p&gt;</content:encoded></item><item><title>GeoPE:A Unified Geometric Positional Embedding for Structured Tensors</title><link>https://arxiv.org/abs/2512.04963v1</link><guid>http://arxiv.org/abs/2512.04963v1</guid><pubDate>Thu, 04 Dec 2025 16:31:12 +0000</pubDate><dc:creator>Yupu Yao</dc:creator><dc:creator>Bowen Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.
Published: 2025-12-04T16:31:12+00:00
Venue: arXiv
Score: 0.758 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yupu Yao; Bowen Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.758 (consider)&lt;/p&gt;
&lt;p&gt;Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.&lt;/p&gt;</content:encoded></item><item><title>HSIseg: Progressively enhanced extensible multi-modality framework for large patch-wise hyperspectral image segmentation</title><link>https://doi.org/10.1016/j.neucom.2025.132261</link><guid>10.1016/j.neucom.2025.132261</guid><pubDate>Sat, 06 Dec 2025 16:08:42 +0000</pubDate><dc:creator>Weilian Zhou</dc:creator><dc:creator>Weixuan Xie</dc:creator><dc:creator>Sei-ichiro Kamata</dc:creator><dc:creator>Huiying (Cynthia) Hou</dc:creator><dc:creator>Man Sing Wong</dc:creator><dc:creator>Haipeng Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132261</prism:doi><description>Hyperspectral image (HSI) classification plays a critical role in remote sensing by enabling precise land-cover identification through rich spectral information. While deep learning has led to significant progress, over 90 % of existing methods rely on small patch-based networks, which suffer from two key limitations: (1) restricted receptive fields (e.g., , ) that hinder structural awareness and result in noisy misclassifications within homogeneous regions; and (2) undefined optimal patch sizes that lead to coarse label predictions and degraded accuracy. Inspired by large-scale image segmentation techniques such as U-Net architectures—known for their strong boundary delineation and spatial coherence—this study explores their adaptation to HSI classification. However, such adaptations remain underutilized due to challenges including performance concerns with large patches, abundant unlabeled regions, and input-shape mismatches. To address these gaps, we propose HSIseg, a segmentation-adapted framework for HSI classification. HSIseg incorporates three novel components—Dynamic Shifted Regional Transformer (DSRT), Discriminative Feature Selection (DFS), and Cross Feature Interaction (CFI)—to enhance feature representation and fusion. A progressive learning strategy with adaptive pseudo-labeling is employed to leverage unlabeled data, while multi-source data collaboration further strengthens the model’s capability. Extensive experiments on five public datasets demonstrate the effectiveness of HSIseg in overcoming the limitations of traditional patch-based approaches. Code is available at https://github.com/zhouweilian1904/HSI_Segmentation .
Published: 2025-12-06T16:08:42+00:00
Venue: Neurocomputing
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weilian Zhou; Weixuan Xie; Sei-ichiro Kamata; Huiying (Cynthia) Hou; Man Sing Wong; Haipeng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132261"&gt;10.1016/j.neucom.2025.132261&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;Hyperspectral image (HSI) classification plays a critical role in remote sensing by enabling precise land-cover identification through rich spectral information. While deep learning has led to significant progress, over 90 % of existing methods rely on small patch-based networks, which suffer from two key limitations: (1) restricted receptive fields (e.g., , ) that hinder structural awareness and result in noisy misclassifications within homogeneous regions; and (2) undefined optimal patch sizes that lead to coarse label predictions and degraded accuracy. Inspired by large-scale image segmentation techniques such as U-Net architectures—known for their strong boundary delineation and spatial coherence—this study explores their adaptation to HSI classification. However, such adaptations remain underutilized due to challenges including performance concerns with large patches, abundant unlabeled regions, and input-shape mismatches. To address these gaps, we propose HSIseg, a segmentation-adapted framework for HSI classification. HSIseg incorporates three novel components—Dynamic Shifted Regional Transformer (DSRT), Discriminative Feature Selection (DFS), and Cross Feature Interaction (CFI)—to enhance feature representation and fusion. A progressive learning strategy with adaptive pseudo-labeling is employed to leverage unlabeled data, while multi-source data collaboration further strengthens the model’s capability. Extensive experiments on five public datasets demonstrate the effectiveness of HSIseg in overcoming the limitations of traditional patch-based approaches. Code is available at https://github.com/zhouweilian1904/HSI_Segmentation .&lt;/p&gt;</content:encoded></item><item><title>Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks</title><link>https://arxiv.org/abs/2512.04970v1</link><guid>http://arxiv.org/abs/2512.04970v1</guid><pubDate>Thu, 04 Dec 2025 16:38:26 +0000</pubDate><dc:creator>Leonid Pogorelyuk</dc:creator><dc:creator>Niels Bracher</dc:creator><dc:creator>Aaron Verkleeren</dc:creator><dc:creator>Lars Kühmichel</dc:creator><dc:creator>Stefan T. Radev</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.
Published: 2025-12-04T16:38:26+00:00
Venue: arXiv
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Leonid Pogorelyuk; Niels Bracher; Aaron Verkleeren; Lars Kühmichel; Stefan T. Radev&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.&lt;/p&gt;</content:encoded></item><item><title>You Only Train Once (YOTO): A Retraining-Free Object Detection Framework</title><link>https://arxiv.org/abs/2512.04888v1</link><guid>http://arxiv.org/abs/2512.04888v1</guid><pubDate>Thu, 04 Dec 2025 15:15:43 +0000</pubDate><dc:creator>Priyanto Hidayatullah</dc:creator><dc:creator>Nurjannah Syakrani</dc:creator><dc:creator>Yudi Widhiyasana</dc:creator><dc:creator>Muhammad Rizqi Sholahuddin</dc:creator><dc:creator>Refdinal Tubagus</dc:creator><dc:creator>Zahri Al Adzani Hidayat</dc:creator><dc:creator>Hanri Fajar Ramadhan</dc:creator><dc:creator>Dafa Alfarizki Pratama</dc:creator><dc:creator>Farhan Muhammad Yasin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.
Published: 2025-12-04T15:15:43+00:00
Venue: arXiv
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Priyanto Hidayatullah; Nurjannah Syakrani; Yudi Widhiyasana; Muhammad Rizqi Sholahuddin; Refdinal Tubagus; Zahri Al Adzani Hidayat; Hanri Fajar Ramadhan; Dafa Alfarizki Pratama; Farhan Muhammad Yasin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework&amp;#x27;s feasibility for practical use.&lt;/p&gt;</content:encoded></item><item><title>Which Domain Fits Best? Domain Similarity Measures for Two-Step Heterogeneous Transfer Learning for Early Laryngeal Cancer Diagnosis</title><link>https://doi.org/10.1016/j.knosys.2025.115056</link><guid>10.1016/j.knosys.2025.115056</guid><pubDate>Sun, 07 Dec 2025 15:17:49 +0000</pubDate><dc:creator>Xinyi Fang</dc:creator><dc:creator>Yuqi Luo</dc:creator><dc:creator>Kei Long Wong</dc:creator><dc:creator>Benjamin K. Ng</dc:creator><dc:creator>Chan-Tong Lam</dc:creator><dc:creator>Marco Simões</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115056</prism:doi><description>Heterogeneous transfer learning is an effective approach for medical imaging problems with limited data and scarce public homogeneous resources, yet selecting the optimal domain for feature extraction remains an open, often intuition-driven challenge. This study proposes and validates a set of quantitative domain similarity measurements to a priori identify the most suitable intermediate domain for early laryngeal cancer detection within a two-step heterogeneous transfer learning (THTL) framework, thereby avoiding computationally expensive trial-and-error training. We introduce eight domain similarity measurements to access the similarity between intermediate domains and the target domain. Multiple common medical imaging modalities, including angiography, chest radiographs, lung computed tomography (CT), brain magnetic resonance imaging (MRI), pathological section images, diabetic retinopathy fundus images, skin lesion images, and gastroenteroscopy, are served as candidate intermediate domains. The resulting similarity scores are ranked and compared with actual THTL performance rankings. Finally, we employ normalized discounted cumulative gain (NDCG) to determine the most predictive measurement. Our findings reveal that Earth Mover’s Distance (EMD) is the most effective domain similarity measurement for grayscale images, while cosine similarity based on global features extracted from a convolutional neural network (CNN) is optimal for RGB images. Using these measurements, angiography and skin lesion images are identified as the most beneficial intermediate domains. This work establishes a validated, data-driven methodology that enables future researchers to replace subjective intuition in domain selection, thereby saving substantial computational resources while improving model performance.
Published: 2025-12-07T15:17:49+00:00
Venue: Knowledge-Based Systems
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyi Fang; Yuqi Luo; Kei Long Wong; Benjamin K. Ng; Chan-Tong Lam; Marco Simões&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115056"&gt;10.1016/j.knosys.2025.115056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;Heterogeneous transfer learning is an effective approach for medical imaging problems with limited data and scarce public homogeneous resources, yet selecting the optimal domain for feature extraction remains an open, often intuition-driven challenge. This study proposes and validates a set of quantitative domain similarity measurements to a priori identify the most suitable intermediate domain for early laryngeal cancer detection within a two-step heterogeneous transfer learning (THTL) framework, thereby avoiding computationally expensive trial-and-error training. We introduce eight domain similarity measurements to access the similarity between intermediate domains and the target domain. Multiple common medical imaging modalities, including angiography, chest radiographs, lung computed tomography (CT), brain magnetic resonance imaging (MRI), pathological section images, diabetic retinopathy fundus images, skin lesion images, and gastroenteroscopy, are served as candidate intermediate domains. The resulting similarity scores are ranked and compared with actual THTL performance rankings. Finally, we employ normalized discounted cumulative gain (NDCG) to determine the most predictive measurement. Our findings reveal that Earth Mover’s Distance (EMD) is the most effective domain similarity measurement for grayscale images, while cosine similarity based on global features extracted from a convolutional neural network (CNN) is optimal for RGB images. Using these measurements, angiography and skin lesion images are identified as the most beneficial intermediate domains. This work establishes a validated, data-driven methodology that enables future researchers to replace subjective intuition in domain selection, thereby saving substantial computational resources while improving model performance.&lt;/p&gt;</content:encoded></item><item><title>EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</title><link>https://arxiv.org/abs/2512.04810v1</link><guid>http://arxiv.org/abs/2512.04810v1</guid><pubDate>Thu, 04 Dec 2025 14:01:53 +0000</pubDate><dc:creator>Xin He</dc:creator><dc:creator>Longhui Wei</dc:creator><dc:creator>Jianbo Ouyang</dc:creator><dc:creator>Lingxi Xie</dc:creator><dc:creator>Qi Tian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.
Published: 2025-12-04T14:01:53+00:00
Venue: arXiv
Score: 0.756 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin He; Longhui Wei; Jianbo Ouyang; Lingxi Xie; Qi Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.756 (consider)&lt;/p&gt;
&lt;p&gt;We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.&lt;/p&gt;</content:encoded></item></channel></rss>