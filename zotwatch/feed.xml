<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 29 Jan 2026 03:19:14 +0000</lastBuildDate><item><title>DFormer++: Improving RGBD Representation Learning for Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2026.3658114</link><guid>10.1109/tpami.2026.3658114</guid><pubDate>Tue, 27 Jan 2026 20:29:16 +0000</pubDate><dc:creator>Bo-Wen Yin</dc:creator><dc:creator>Jiao-Long Cao</dc:creator><dc:creator>Dan Xu</dc:creator><dc:creator>Ming-Ming Cheng</dc:creator><dc:creator>Qibin Hou</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658114</prism:doi><description>We explore the potential of pretrain-and-finetune manner on the RGB-D semantic segmentation to solve the common mismatch problem in this field. Specifically, we present DFormer++, a novel RGB-D pretrain-and-finetune framework to learn transferable representations for RGB-D semantic segmentation. This paper has two vital innovations. 1) Framework perspective: Different from the existing methods that finetune RGB pretrained backbone to the RGB-D scenes, we pretrain the backbone using image-depth pairs from ImageNet-1K, and hence the model is endowed with the capacity to encode RGB-D representations; 2) Architecture perspective: Our model comprises a sequence of RGB-D attention blocks, which are tailored for encoding both RGB and depth information through a novel attention mechanism. Our DFormer++ avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pretrained backbones, which widely lies in previous works but has not been resolved. Meanwhile, the tailored architecture greatly reduces redundant parameters for encoding RGB-D data and achieves efficient and accurate perception. Experimental results show that our DFormer++ achieves new cutting-edge performance on three popular RGB-D semantic segmentation benchmarks. Our code is available at: https://github.com/VCIP-RGBD/DFormer.
Published: 2026-01-27T20:29:16+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo-Wen Yin; Jiao-Long Cao; Dan Xu; Ming-Ming Cheng; Qibin Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658114"&gt;10.1109/tpami.2026.3658114&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;We explore the potential of pretrain-and-finetune manner on the RGB-D semantic segmentation to solve the common mismatch problem in this field. Specifically, we present DFormer++, a novel RGB-D pretrain-and-finetune framework to learn transferable representations for RGB-D semantic segmentation. This paper has two vital innovations. 1) Framework perspective: Different from the existing methods that finetune RGB pretrained backbone to the RGB-D scenes, we pretrain the backbone using image-depth pairs from ImageNet-1K, and hence the model is endowed with the capacity to encode RGB-D representations; 2) Architecture perspective: Our model comprises a sequence of RGB-D attention blocks, which are tailored for encoding both RGB and depth information through a novel attention mechanism. Our DFormer++ avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pretrained backbones, which widely lies in previous works but has not been resolved. Meanwhile, the tailored architecture greatly reduces redundant parameters for encoding RGB-D data and achieves efficient and accurate perception. Experimental results show that our DFormer++ achieves new cutting-edge performance on three popular RGB-D semantic segmentation benchmarks. Our code is available at: https://github.com/VCIP-RGBD/DFormer.&lt;/p&gt;</content:encoded></item><item><title>Spatial-Frequency Domain Joint Learning With Shape Constraints for Fine-Grained Aircraft Detection in SAR Imagery</title><link>https://doi.org/10.1109/jstars.2026.3657853</link><guid>10.1109/jstars.2026.3657853</guid><pubDate>Tue, 27 Jan 2026 05:52:07 +0000</pubDate><dc:creator>Ru Luo</dc:creator><dc:creator>Qishan He</dc:creator><dc:creator>Jiajin Li</dc:creator><dc:creator>Siqian Zhang</dc:creator><dc:creator>Lingjun Zhao</dc:creator><dc:creator>Kefeng Ji</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3657853</prism:doi><description>Fine-grained aircraft detection aims to detect aircraft and identify its subcategory, which is important for military reconnaissance and airport management. Compared with optical imagery, aircraft in Synthetic Aperture Radar (SAR) images exhibit high azimuth sensitivity and discrete scattering characteristics, leading to significant intra-class variance and topological fragmentation, which make fine-grained aircraft detection very challenging. Existing methods mainly rely on spatial domain feature processing and scattering keypoint supervision, which do not fully utilize frequency domain features that are particularly important for fine-grained detection. This paper proposes a novel dual domain feature learning architecture with shape constraints, SAR-SFNet, to enhance the fine-grained aircraft detection performance in SAR imagery. First, a Spatial-Frequency Domain Joint Learning (SFDJL) is proposed via integrating Fractional Gabor Transform (FrGT)'s localized, orientation-tuned responses with the Fourier's global contextual cues to enhance the saliency of aircraft under varied aspect angles. Second, a Class-Aware Shape Constraint (CASC) is designed by leveraging class-specific shape priors to mitigate intra-class variance and topological fragmentation. Extensive experiments on SAR-RADD and FAIR-CSAR datasets demonstrate that SAR-SFNet achieves a mean Average Precision (mAP) of 79.3% and 50.6%, outperforming state-of-the-art methods by 3.7% and 5.2%, respectively, while maintaining a competitive inference speed of 39.5 Frames Per Second (FPS). Furthermore, with a lightweight architecture of 7.8 M parameters and 15.3 G Floating Point Operations (FLOPs), the proposed method exhibits its potential for resource-constrained, real-time applications.
Published: 2026-01-27T05:52:07+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ru Luo; Qishan He; Jiajin Li; Siqian Zhang; Lingjun Zhao; Kefeng Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3657853"&gt;10.1109/jstars.2026.3657853&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Fine-grained aircraft detection aims to detect aircraft and identify its subcategory, which is important for military reconnaissance and airport management. Compared with optical imagery, aircraft in Synthetic Aperture Radar (SAR) images exhibit high azimuth sensitivity and discrete scattering characteristics, leading to significant intra-class variance and topological fragmentation, which make fine-grained aircraft detection very challenging. Existing methods mainly rely on spatial domain feature processing and scattering keypoint supervision, which do not fully utilize frequency domain features that are particularly important for fine-grained detection. This paper proposes a novel dual domain feature learning architecture with shape constraints, SAR-SFNet, to enhance the fine-grained aircraft detection performance in SAR imagery. First, a Spatial-Frequency Domain Joint Learning (SFDJL) is proposed via integrating Fractional Gabor Transform (FrGT)&amp;#x27;s localized, orientation-tuned responses with the Fourier&amp;#x27;s global contextual cues to enhance the saliency of aircraft under varied aspect angles. Second, a Class-Aware Shape Constraint (CASC) is designed by leveraging class-specific shape priors to mitigate intra-class variance and topological fragmentation. Extensive experiments on SAR-RADD and FAIR-CSAR datasets demonstrate that SAR-SFNet achieves a mean Average Precision (mAP) of 79.3% and 50.6%, outperforming state-of-the-art methods by 3.7% and 5.2%, respectively, while maintaining a competitive inference speed of 39.5 Frames Per Second (FPS). Furthermore, with a lightweight architecture of 7.8 M parameters and 15.3 G Floating Point Operations (FLOPs), the proposed method exhibits its potential for resource-constrained, real-time applications.&lt;/p&gt;</content:encoded></item><item><title>Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment</title><link>https://doi.org/10.1109/tpami.2026.3657354</link><guid>10.1109/tpami.2026.3657354</guid><pubDate>Tue, 27 Jan 2026 05:48:07 +0000</pubDate><dc:creator>Lingling Xu</dc:creator><dc:creator>Haoran Xie</dc:creator><dc:creator>S. Joe Qin</dc:creator><dc:creator>Xiaohui Tao</dc:creator><dc:creator>Fu Lee Wang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3657354</prism:doi><description>With the continuous growth in the number of parameters of the Transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter-Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, extensive experiments are conducted using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.
Published: 2026-01-27T05:48:07+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lingling Xu; Haoran Xie; S. Joe Qin; Xiaohui Tao; Fu Lee Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3657354"&gt;10.1109/tpami.2026.3657354&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;With the continuous growth in the number of parameters of the Transformer-based pretrained language models (PLMs), particularly the emergence of large language models (LLMs) with billions of parameters, many natural language processing (NLP) tasks have demonstrated remarkable success. However, the enormous size and computational demands of these models pose significant challenges for adapting them to specific downstream tasks, especially in environments with limited computational resources. Parameter-Efficient Fine-Tuning (PEFT) offers an effective solution by reducing the number of fine-tuning parameters and memory usage while achieving comparable performance to full fine-tuning. The demands for fine-tuning PLMs, especially LLMs, have led to a surge in the development of PEFT methods, as depicted in Fig. 1. In this paper, we present a comprehensive and systematic review of PEFT methods for PLMs. We summarize these PEFT methods, discuss their applications, and outline future directions. Furthermore, extensive experiments are conducted using several representative PEFT methods to better understand their effectiveness in parameter efficiency and memory efficiency. By offering insights into the latest advancements and practical applications, this survey serves as an invaluable resource for researchers and practitioners seeking to navigate the challenges and opportunities presented by PEFT in the context of PLMs.&lt;/p&gt;</content:encoded></item><item><title>PISTTN: Profile-aware Infrared Small Target Tracking Network using Spatiotemporal Context Information</title><link>https://doi.org/10.1109/tgrs.2026.3657763</link><guid>10.1109/tgrs.2026.3657763</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Xingyu Zhou</dc:creator><dc:creator>Yue Hu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657763</prism:doi><description>Infrared small target detection and tracking play an increasingly important role in both military and civilian applications. However, challenges persist due to the small target size and low signal-to-noise ratio. For single-target detection and tracking, most existing methods require annotation in the initial frame. For multi-target detection and tracking, detectors often need to perform detection on each frame before tracking, which loses temporal features and struggles to handle occlusion effectively. Moreover, in some scenarios, the target often degenerates into a single point, posing significant challenges for detection and tracking. To address the challenges, we reformulate the infrared small target tracking task as a spatiotemporal profile detection problem, and proposes a novel infrared small target tracking network that unifies tracking and detection into a single end-to-end trainable architecture, termed the Profile-aware Infrared Small Target Tracking Network (PISTTN). Specifically, to address the loss of spatiotemporal information caused by single-frame detection in traditional tracking algorithms, we introduce a spatiotemporal tensor encoding module. This module automatically constructs sparse tensors based on target characteristics and employs 3D sparse convolution to extract profile-aware To address the challenges in detecting point-like targets, we propose a small target query module that integrates multi-scale features to enhance adaptability and generalization across varying target appearances, while generating distinct queries for different targets. In addition, we incorporate a profile detector to predict the spatiotemporal profile of targets, enabling accurate trajectory estimation through an efficient tracking strategy. Experimental results on multiple datasets demonstrate that the proposed network outperforms existing state-of-the-art methods in terms of visual and quantitative assessment.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingyu Zhou; Yue Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657763"&gt;10.1109/tgrs.2026.3657763&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection and tracking play an increasingly important role in both military and civilian applications. However, challenges persist due to the small target size and low signal-to-noise ratio. For single-target detection and tracking, most existing methods require annotation in the initial frame. For multi-target detection and tracking, detectors often need to perform detection on each frame before tracking, which loses temporal features and struggles to handle occlusion effectively. Moreover, in some scenarios, the target often degenerates into a single point, posing significant challenges for detection and tracking. To address the challenges, we reformulate the infrared small target tracking task as a spatiotemporal profile detection problem, and proposes a novel infrared small target tracking network that unifies tracking and detection into a single end-to-end trainable architecture, termed the Profile-aware Infrared Small Target Tracking Network (PISTTN). Specifically, to address the loss of spatiotemporal information caused by single-frame detection in traditional tracking algorithms, we introduce a spatiotemporal tensor encoding module. This module automatically constructs sparse tensors based on target characteristics and employs 3D sparse convolution to extract profile-aware To address the challenges in detecting point-like targets, we propose a small target query module that integrates multi-scale features to enhance adaptability and generalization across varying target appearances, while generating distinct queries for different targets. In addition, we incorporate a profile detector to predict the spatiotemporal profile of targets, enabling accurate trajectory estimation through an efficient tracking strategy. Experimental results on multiple datasets demonstrate that the proposed network outperforms existing state-of-the-art methods in terms of visual and quantitative assessment.&lt;/p&gt;</content:encoded></item><item><title>TextMonkey: an OCR-Free Large Multimodal Model for Understanding Document</title><link>https://doi.org/10.1109/tpami.2026.3653415</link><guid>10.1109/tpami.2026.3653415</guid><pubDate>Tue, 27 Jan 2026 05:48:07 +0000</pubDate><dc:creator>Yuliang Liu</dc:creator><dc:creator>Biao Yang</dc:creator><dc:creator>Qiang Liu</dc:creator><dc:creator>Zhang Li</dc:creator><dc:creator>Zhiyin Ma</dc:creator><dc:creator>Shuo Zhang</dc:creator><dc:creator>Xiang Bai</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653415</prism:doi><description>We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model's performance. Moreover, by expanding our model's capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.
Published: 2026-01-27T05:48:07+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuliang Liu; Biao Yang; Qiang Liu; Zhang Li; Zhiyin Ma; Shuo Zhang; Xiang Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653415"&gt;10.1109/tpami.2026.3653415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;We present TextMonkey, a large multimodal model (LMM) tailored for text-centric tasks. Our approach introduces enhancement across several dimensions: By adopting Shifted Window Attention layer, we achieve cross-window connectivity at higher input resolutions and stabilize early training; We hypothesize that images may contain redundant tokens, and by using similarity to filter out significant tokens, we can not only streamline the token length but also enhance the model&amp;#x27;s performance. Moreover, by expanding our model&amp;#x27;s capabilities to encompass text spotting and grounding, and incorporating positional information into responses, we enhance interpretability. Evaluation on 12 benchmarks shows notable improvements: 5.2% in Scene Text-Centric tasks (including STVQA, TextVQA, and OCRVQA), 6.9% in Document-Oriented tasks (such as DocVQA, InfoVQA, ChartVQA, DeepForm, Kleister Charity, and WikiTableQuestions), and 2.8% in Key Information Extraction tasks (comprising FUNSD, SROIE, and POIE). It outperforms in scene text spotting with a 10.9% increase and sets a new standard on OCRBench, a comprehensive benchmark consisting of 29 OCR-related assessments, with a score of 561, surpassing previous open-sourced large multimodal models for document understanding. Code is released at https://github.com/Yuliang-Liu/Monkey.&lt;/p&gt;</content:encoded></item><item><title>Consistency-Regularized GAN for Few-Shot SAR Target Recognition</title><link>https://doi.org/10.1109/tgrs.2026.3657831</link><guid>10.1109/tgrs.2026.3657831</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Yikui Zhai</dc:creator><dc:creator>Shikuang Liu</dc:creator><dc:creator>Wenlve Zhou</dc:creator><dc:creator>Hongsheng Zhang</dc:creator><dc:creator>Zhiheng Zhou</dc:creator><dc:creator>Xiaolin Tian</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657831</prism:doi><description>Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5% of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yikui Zhai; Shikuang Liu; Wenlve Zhou; Hongsheng Zhang; Zhiheng Zhou; Xiaolin Tian; C. L. Philip Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657831"&gt;10.1109/tgrs.2026.3657831&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5% of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.&lt;/p&gt;</content:encoded></item><item><title>Unified Local and Global Transformer for Infrared Small UAV Tracking</title><link>https://doi.org/10.1109/tgrs.2026.3657906</link><guid>10.1109/tgrs.2026.3657906</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Yaohong Chen</dc:creator><dc:creator>Tianlei Ma</dc:creator><dc:creator>Donglin Xue</dc:creator><dc:creator>Xinhao Liu</dc:creator><dc:creator>Weining Chen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657906</prism:doi><description>Long-term tracking of infrared small unmanned aerial vehicle (UAV) poses substantial challenges, including dynamic backgrounds, clutter interference, and target occlusion. This study introduces an innovative method that integrates a local tracking network with a global search strategy to effectively address these issues. Firstly, a background motion estimation module is proposed to mitigate dynamic background interference by aligning consecutive frames through motion state assessment. Secondly, a full-transformer local tracking network is developed to suppress background clutter. It enhances feature representation using Spectformer as the backbone and leverages cross-attention mechanisms to robustly handle clutter. Finally, a global search strategy featuring a large-scale search module is designed to address target occlusion. This module provides reliable local search regions for the tracking network when occlusion occurs. Extensive experiments on infrared drone datasets validate that the proposed method outperforms state-of-the-art approaches, achieving high success rates, high precision, and high real-time processing at 45 FPS under specific configurations.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaohong Chen; Tianlei Ma; Donglin Xue; Xinhao Liu; Weining Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657906"&gt;10.1109/tgrs.2026.3657906&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Long-term tracking of infrared small unmanned aerial vehicle (UAV) poses substantial challenges, including dynamic backgrounds, clutter interference, and target occlusion. This study introduces an innovative method that integrates a local tracking network with a global search strategy to effectively address these issues. Firstly, a background motion estimation module is proposed to mitigate dynamic background interference by aligning consecutive frames through motion state assessment. Secondly, a full-transformer local tracking network is developed to suppress background clutter. It enhances feature representation using Spectformer as the backbone and leverages cross-attention mechanisms to robustly handle clutter. Finally, a global search strategy featuring a large-scale search module is designed to address target occlusion. This module provides reliable local search regions for the tracking network when occlusion occurs. Extensive experiments on infrared drone datasets validate that the proposed method outperforms state-of-the-art approaches, achieving high success rates, high precision, and high real-time processing at 45 FPS under specific configurations.&lt;/p&gt;</content:encoded></item><item><title>Multi-agent AI systems need transparency</title><link>https://doi.org/10.1038/s42256-026-01183-2</link><guid>10.1038/s42256-026-01183-2</guid><pubDate>Tue, 27 Jan 2026 16:02:50 +0000</pubDate><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-026-01183-2</prism:doi><description>Agentic artificial intelligence (AI) frameworks are in vogue. However, implementing such systems in scientific research workflows requires clear motivations and explanations, given the risk of wasting computational as well as human resources.
Published: 2026-01-27T16:02:50+00:00
Venue: Nature Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-026-01183-2"&gt;10.1038/s42256-026-01183-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Agentic artificial intelligence (AI) frameworks are in vogue. However, implementing such systems in scientific research workflows requires clear motivations and explanations, given the risk of wasting computational as well as human resources.&lt;/p&gt;</content:encoded></item><item><title>Large Multimodal Models for Low-Resource Languages: A Survey</title><link>https://doi.org/10.1016/j.inffus.2026.104189</link><guid>10.1016/j.inffus.2026.104189</guid><pubDate>Wed, 28 Jan 2026 00:39:35 +0000</pubDate><dc:creator>Marian Lupaşcu</dc:creator><dc:creator>Ana-Cristina Rogoz</dc:creator><dc:creator>Mihai Sorin Stupariu</dc:creator><dc:creator>Radu Tudor Ionescu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104189</prism:doi><description>In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey .
Published: 2026-01-28T00:39:35+00:00
Venue: Information Fusion
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Marian Lupaşcu; Ana-Cristina Rogoz; Mihai Sorin Stupariu; Radu Tudor Ionescu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104189"&gt;10.1016/j.inffus.2026.104189&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey .&lt;/p&gt;</content:encoded></item><item><title>GEGAN: gradient-guided evolutionary framework for GAN optimization</title><link>https://doi.org/10.1016/j.eswa.2026.131257</link><guid>10.1016/j.eswa.2026.131257</guid><pubDate>Wed, 28 Jan 2026 12:52:57 +0000</pubDate><dc:creator>Wenwen Jia</dc:creator><dc:creator>Qi Yu</dc:creator><dc:creator>Xijun Liang</dc:creator><dc:creator>Mengzhen Li</dc:creator><dc:creator>Ling Jian</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131257</prism:doi><description>Generative adversarial networks (GANs) often suffer from unstable training, and degraded performance under data-scarce or multi-category conditions. To address these challenges, we propose GEGAN, a gradient-guided evolutionary framework that maintains a population of generators and updates them collaboratively using explicit gradient directions. A gradient-guided mutation operator assigns complementary learning behaviors to individuals, balancing global exploration and local convergence, while an accept-reject mechanism preserves improvements across generations. We establish convergence to an approximate local equilibrium under mild smoothness assumptions, providing theoretical foundations for the hybrid design. Extensive experiments demonstrate that GEGAN consistently enhances image quality and diversity, achieving the highest ranks on F q , F d , and MMD with statistically significant gains over canonical and evolutionary GANs.
Published: 2026-01-28T12:52:57+00:00
Venue: Expert Systems with Applications
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenwen Jia; Qi Yu; Xijun Liang; Mengzhen Li; Ling Jian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131257"&gt;10.1016/j.eswa.2026.131257&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Generative adversarial networks (GANs) often suffer from unstable training, and degraded performance under data-scarce or multi-category conditions. To address these challenges, we propose GEGAN, a gradient-guided evolutionary framework that maintains a population of generators and updates them collaboratively using explicit gradient directions. A gradient-guided mutation operator assigns complementary learning behaviors to individuals, balancing global exploration and local convergence, while an accept-reject mechanism preserves improvements across generations. We establish convergence to an approximate local equilibrium under mild smoothness assumptions, providing theoretical foundations for the hybrid design. Extensive experiments demonstrate that GEGAN consistently enhances image quality and diversity, achieving the highest ranks on F q , F d , and MMD with statistically significant gains over canonical and evolutionary GANs.&lt;/p&gt;</content:encoded></item><item><title>Learning Global Dynamic Query for Large–Motion Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3657842</link><guid>10.1109/tgrs.2026.3657842</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Chuiyi Deng</dc:creator><dc:creator>Yanyin Guo</dc:creator><dc:creator>Xiang Xu</dc:creator><dc:creator>Zhuoyi Zhao</dc:creator><dc:creator>Yixin Xia</dc:creator><dc:creator>Runxuan An</dc:creator><dc:creator>Junwei Li</dc:creator><dc:creator>Antonio Plaza</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657842</prism:doi><description>Motion Infrared Small Target Detection (MIRSTD) leverages multi-frame temporal dependencies to improve detection robustness. However, existing methods have difficulty modeling global consistency and achieving precise alignment in complex motion and large displacement scenarios, leading to dispersed target representations and higher error rates. To address these challenges, we propose Dynamic Query Aligner (DQAligner), which introduces global random large-displacement augmentation and a cross-scale bidirectional shared attention mechanism to enhance inter-frame consistency. A dynamic receptive field pyramid deformable convolution decomposes complex multi-scale motions, enabling precise target alignment. Furthermore, class query memory serves as the generalized residual form of deformable convolution, which iteratively learns dynamic query representations to facilitate global target localization within each frame and maintain semantic consistency across frames. DQAligner achieves a paradigm shift from rigid alignment to flexible matching, and significantly boosts detection performance in large displacement and dynamic scenarios. Experiments on extensive stationary and moving platform datasets show that DQAligner outperforms existing methods, especially under complex motion and low signal-noise-rate conditions. Code will be available at https://github.com/dengfa02/DQAligner_MIRSTD.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuiyi Deng; Yanyin Guo; Xiang Xu; Zhuoyi Zhao; Yixin Xia; Runxuan An; Junwei Li; Antonio Plaza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657842"&gt;10.1109/tgrs.2026.3657842&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Motion Infrared Small Target Detection (MIRSTD) leverages multi-frame temporal dependencies to improve detection robustness. However, existing methods have difficulty modeling global consistency and achieving precise alignment in complex motion and large displacement scenarios, leading to dispersed target representations and higher error rates. To address these challenges, we propose Dynamic Query Aligner (DQAligner), which introduces global random large-displacement augmentation and a cross-scale bidirectional shared attention mechanism to enhance inter-frame consistency. A dynamic receptive field pyramid deformable convolution decomposes complex multi-scale motions, enabling precise target alignment. Furthermore, class query memory serves as the generalized residual form of deformable convolution, which iteratively learns dynamic query representations to facilitate global target localization within each frame and maintain semantic consistency across frames. DQAligner achieves a paradigm shift from rigid alignment to flexible matching, and significantly boosts detection performance in large displacement and dynamic scenarios. Experiments on extensive stationary and moving platform datasets show that DQAligner outperforms existing methods, especially under complex motion and low signal-noise-rate conditions. Code will be available at https://github.com/dengfa02/DQAligner_MIRSTD.&lt;/p&gt;</content:encoded></item><item><title>Small Ship Detection Based on a Learning Model That Incorporates Spatial Attention Mechanism as a Loss Function in SU-ESRGAN</title><link>https://doi.org/10.3390/rs18030417</link><guid>10.3390/rs18030417</guid><pubDate>Tue, 27 Jan 2026 15:41:56 +0000</pubDate><dc:creator>Kohei Arai</dc:creator><dc:creator>Yu Morita</dc:creator><dc:creator>Hiroshi Okumura</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030417</prism:doi><description>Ship monitoring using Synthetic Aperture Radar (SAR) data faces significant challenges in detecting small vessels due to low spatial resolution and speckle noise. While ESRGAN (Enhanced Super-Resolution Generative Adversarial Network) has shown promise for image super-resolution, it struggles with SAR imagery characteristics. This study proposes SA/SU-ESRGAN, which extends the SU-ESRGAN framework by incorporating a spatial attention mechanism loss function. SU-ESRGAN introduced semantic structural loss to accurately preserve ship shapes and contours; our enhancement adds spatial attention to focus reconstruction efforts on ship regions while suppressing background noise. Experimental results demonstrate that SA/SU-ESRGAN successfully detects small vessels that remain undetectable by SU-ESRGAN, achieving improved detection capabilities with a PSNR of approximately 26 dB (SSIM is around 0.5) and enhanced visual clarity in ship boundaries. The spatial attention mechanism effectively reduces noise influence, producing clearer super-resolution results suitable for maritime surveillance applications. Based on the HRSID dataset, a representative dataset for evaluating ship detection performance using SAR data, we evaluated ship detection performance using images in which the spatial resolution of the SAR data was artificially degraded using a smoothing filter. We found that with a 4 × 4 filter, all eight ships were detected without any problems, but with an 8 × 8 filter, only three of the eight ships were detected. When super-resolution was applied to this, six ships were detected.
Published: 2026-01-27T15:41:56+00:00
Venue: Remote Sensing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kohei Arai; Yu Morita; Hiroshi Okumura&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030417"&gt;10.3390/rs18030417&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Ship monitoring using Synthetic Aperture Radar (SAR) data faces significant challenges in detecting small vessels due to low spatial resolution and speckle noise. While ESRGAN (Enhanced Super-Resolution Generative Adversarial Network) has shown promise for image super-resolution, it struggles with SAR imagery characteristics. This study proposes SA/SU-ESRGAN, which extends the SU-ESRGAN framework by incorporating a spatial attention mechanism loss function. SU-ESRGAN introduced semantic structural loss to accurately preserve ship shapes and contours; our enhancement adds spatial attention to focus reconstruction efforts on ship regions while suppressing background noise. Experimental results demonstrate that SA/SU-ESRGAN successfully detects small vessels that remain undetectable by SU-ESRGAN, achieving improved detection capabilities with a PSNR of approximately 26 dB (SSIM is around 0.5) and enhanced visual clarity in ship boundaries. The spatial attention mechanism effectively reduces noise influence, producing clearer super-resolution results suitable for maritime surveillance applications. Based on the HRSID dataset, a representative dataset for evaluating ship detection performance using SAR data, we evaluated ship detection performance using images in which the spatial resolution of the SAR data was artificially degraded using a smoothing filter. We found that with a 4 × 4 filter, all eight ships were detected without any problems, but with an 8 × 8 filter, only three of the eight ships were detected. When super-resolution was applied to this, six ships were detected.&lt;/p&gt;</content:encoded></item><item><title>MSMC: Multi-Scale Embedding and Meta-Contrastive Learning for Few-Shot Fine-Grained SAR Target Classification</title><link>https://doi.org/10.3390/rs18030415</link><guid>10.3390/rs18030415</guid><pubDate>Tue, 27 Jan 2026 09:05:27 +0000</pubDate><dc:creator>Bowen Chen</dc:creator><dc:creator>Minjia Yang</dc:creator><dc:creator>Yue Wang</dc:creator><dc:creator>Xueru Bai</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030415</prism:doi><description>Constrained by observation conditions and high inter-class similarity, effective feature extraction and classification of synthetic aperture radar (SAR) targets in few-shot scenarios remains a persistent challenge. To address this issue, this article proposes a few-shot fine-grained SAR target classification method based on multi-scale embedding network and meta-contrastive learning (MSMC). Specifically, the MSMC integrates two complementary training pipelines; the first employs metric-based meta-learning to facilitate few-shot classification, while the second adopts an auxiliary training strategy to enhance feature diversity through contrastive learning. Furthermore, a shared multi-scale embedding network (MSEN) is designed to extract discriminative multi-scale features via adaptive candidate region generation and joint multi-scale embedding. The experimental results on the MSTAR dataset demonstrate that the proposed method achieves superior few-shot fine-grained classification performance compared to existing methods.
Published: 2026-01-27T09:05:27+00:00
Venue: Remote Sensing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bowen Chen; Minjia Yang; Yue Wang; Xueru Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030415"&gt;10.3390/rs18030415&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Constrained by observation conditions and high inter-class similarity, effective feature extraction and classification of synthetic aperture radar (SAR) targets in few-shot scenarios remains a persistent challenge. To address this issue, this article proposes a few-shot fine-grained SAR target classification method based on multi-scale embedding network and meta-contrastive learning (MSMC). Specifically, the MSMC integrates two complementary training pipelines; the first employs metric-based meta-learning to facilitate few-shot classification, while the second adopts an auxiliary training strategy to enhance feature diversity through contrastive learning. Furthermore, a shared multi-scale embedding network (MSEN) is designed to extract discriminative multi-scale features via adaptive candidate region generation and joint multi-scale embedding. The experimental results on the MSTAR dataset demonstrate that the proposed method achieves superior few-shot fine-grained classification performance compared to existing methods.&lt;/p&gt;</content:encoded></item><item><title>EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery</title><link>https://arxiv.org/abs/2601.18597v1</link><guid>http://arxiv.org/abs/2601.18597v1</guid><pubDate>Mon, 26 Jan 2026 15:41:37 +0000</pubDate><dc:creator>Yu Xia</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Tianqi Xiang</dc:creator><dc:creator>Zhigang Tu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.
Published: 2026-01-26T15:41:37+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Xia; Chang Liu; Tianqi Xiang; Zhigang Tu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.&lt;/p&gt;</content:encoded></item><item><title>VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training</title><link>https://arxiv.org/abs/2601.17830v1</link><guid>http://arxiv.org/abs/2601.17830v1</guid><pubDate>Sun, 25 Jan 2026 13:22:38 +0000</pubDate><dc:creator>Mengmeng Wang</dc:creator><dc:creator>Dengyang Jiang</dc:creator><dc:creator>Liuzhuozheng Li</dc:creator><dc:creator>Yucheng Lin</dc:creator><dc:creator>Guojiang Shen</dc:creator><dc:creator>Xiangjie Kong</dc:creator><dc:creator>Yong Liu</dc:creator><dc:creator>Guang Dai</dc:creator><dc:creator>Jingdong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.
Published: 2026-01-25T13:22:38+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengmeng Wang; Dengyang Jiang; Liuzhuozheng Li; Yucheng Lin; Guojiang Shen; Xiangjie Kong; Yong Liu; Guang Dai; Jingdong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.&lt;/p&gt;</content:encoded></item><item><title>DHPT: Dual-Modality Heterogeneous Prompt Tuning for Online Test-time Adaption in Vision-language Models</title><link>https://doi.org/10.1109/tcsvt.2026.3657756</link><guid>10.1109/tcsvt.2026.3657756</guid><pubDate>Tue, 27 Jan 2026 06:05:44 +0000</pubDate><dc:creator>Guiqin Wang</dc:creator><dc:creator>Peng Zhao</dc:creator><dc:creator>Xiang Wang</dc:creator><dc:creator>Haoran Guo</dc:creator><dc:creator>Nan Qi</dc:creator><dc:creator>Shusen Yang</dc:creator><dc:creator>Qinghai Guo</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657756</prism:doi><description>Test-Time Adaptation (TTA) has recently emerged as a promising research direction, enabling vision-language models (VLMs) to adapt to unlabeled test data in zero-shot settings. Among TTA approaches, test-time prompt tuning has shown great potential for enhancing the practical applicability of VLMs. However, existing methods typically either focus on adapting a single modality or apply uniform optimization to both modalities, without explicitly defining modality-specific optimization objectives. Such a one-size-fits-all strategy often results in suboptimal performance under test-time conditions. To address this limitation, we propose Dual-modality Heterogeneous Prompt Tuning (DHPT), a novel framework designed to simultaneously capture fine-grained textual semantics and alleviate domain shift noise in the visual modality. Specifically, we leverage a large language model to provide textual cognition guidance for the text encoder, while on the vision side, we develop a lightweight calibration module that adaptively mitigates domain shift noise across different scales. Furthermore, we introduce a cluster-tight optimization objective that enhances the stability and generalizability of prompt tuning under distribution shifts. Extensive experiments conducted on 11 benchmark datasets demonstrate that DHPT consistently and significantly outperforms existing TTA methods for VLMs.
Published: 2026-01-27T06:05:44+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guiqin Wang; Peng Zhao; Xiang Wang; Haoran Guo; Nan Qi; Shusen Yang; Qinghai Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657756"&gt;10.1109/tcsvt.2026.3657756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Test-Time Adaptation (TTA) has recently emerged as a promising research direction, enabling vision-language models (VLMs) to adapt to unlabeled test data in zero-shot settings. Among TTA approaches, test-time prompt tuning has shown great potential for enhancing the practical applicability of VLMs. However, existing methods typically either focus on adapting a single modality or apply uniform optimization to both modalities, without explicitly defining modality-specific optimization objectives. Such a one-size-fits-all strategy often results in suboptimal performance under test-time conditions. To address this limitation, we propose Dual-modality Heterogeneous Prompt Tuning (DHPT), a novel framework designed to simultaneously capture fine-grained textual semantics and alleviate domain shift noise in the visual modality. Specifically, we leverage a large language model to provide textual cognition guidance for the text encoder, while on the vision side, we develop a lightweight calibration module that adaptively mitigates domain shift noise across different scales. Furthermore, we introduce a cluster-tight optimization objective that enhances the stability and generalizability of prompt tuning under distribution shifts. Extensive experiments conducted on 11 benchmark datasets demonstrate that DHPT consistently and significantly outperforms existing TTA methods for VLMs.&lt;/p&gt;</content:encoded></item><item><title>S²TA-Fuse: Semantic-Superpixel Tokenized Attention for Spatial Spectral Fusion</title><link>https://doi.org/10.1109/tgrs.2026.3657766</link><guid>10.1109/tgrs.2026.3657766</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Jiawei Jiang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Jieyuan Pei</dc:creator><dc:creator>Junwei Zhu</dc:creator><dc:creator>Honghui Xu</dc:creator><dc:creator>Yuchao Feng</dc:creator><dc:creator>Jianwei Zheng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657766</prism:doi><description>Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Jiang; Wei Li; Jieyuan Pei; Junwei Zhu; Honghui Xu; Yuchao Feng; Jianwei Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657766"&gt;10.1109/tgrs.2026.3657766&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Superpixel paradigms have long been regarded as a well-established approach of mitigating spatial redundancy in hyperspectral images, yet their reliance on non-differentiable and irreversible segmentation renders them unsuitable for end-to-end spatial–spectral fusion (SSF). To tackle this limitation, this study introduces S²TA-Fuse, a transformer-based solver named Semantic-Superpixel Tokenized Attention for Fusion, which preserves the efficiency of superpixels while removing the need for explicit segmentation. The central design lies in a semantic attention mechanism that adaptively organizes pixels into deformable and content-aware semantic groups. Pixels sharing similar latent states are softly aggregated and encoded as compact tokens, upon which attention is computed to capture intricate long-range dependencies. This formulation endows the model with an inherent ability to accommodate scale variations while maintaining linear computational complexity with respect to the number of pixels. On top of the semantic backbone, two complementary components are devised. The Local Spectral Pyramid enhances the representation of multi-scale spectral cues in the spatial domain, whereas FreqNet supplements global information by modeling frequency-dependent variations through amplitude and phase decomposition. Comprehensive experiments on widely used benchmarks for spatial–spectral fusion demonstrate that S²TA-Fuse consistently surpasses the state of the art both in quantitative accuracy and visual fidelity.&lt;/p&gt;</content:encoded></item><item><title>Instance-Guided Radar Depth Estimation for 3D Object Detection</title><link>https://arxiv.org/abs/2601.19314v1</link><guid>http://arxiv.org/abs/2601.19314v1</guid><pubDate>Tue, 27 Jan 2026 07:53:24 +0000</pubDate><dc:creator>Chen-Chou Lo</dc:creator><dc:creator>Patrick Vandewalle</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.
Published: 2026-01-27T07:53:24+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen-Chou Lo; Patrick Vandewalle&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.&lt;/p&gt;</content:encoded></item><item><title>Goal-oriented Dynamic Weight Optimization for Multi-Object Navigation</title><link>https://doi.org/10.1109/tpami.2026.3657778</link><guid>10.1109/tpami.2026.3657778</guid><pubDate>Tue, 27 Jan 2026 05:48:07 +0000</pubDate><dc:creator>Haitao Zeng</dc:creator><dc:creator>Xinhang Song</dc:creator><dc:creator>Shuqiang Jiang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3657778</prism:doi><description>Multi-object navigation (MON) tasks involve sequentially locating multiple targets in an unknown environment, requiring global long-term planning under incomplete information. This necessitates that the agent dynamically balance immediate actions and long-term rewards while considering both local adaptability and global foresight. However, current methods overly focus on local path optimization, which leads to slower convergence in sparse reward settings and increases the risk of deadlocks or trap states. The core challenge of MON lies in the deformation of the shared decision space, where independent optimization leads to redundant and overlapping paths. Thus, path planning requires dynamic, cross-task optimization rather than simple subtask aggregation. To minimize overall effort, the optimization process should adaptively balance task contributions through weight adjustment. Thus, we propose the Goal-oriented Dynamic Weight Optimization (GDWO) algorithm. GDWO integrates target-specific value loss functions into a unified optimization framework and dynamically adjusts weights through gradient-based updates. To prevent over-optimization, weights are normalized during training according to navigation success rates, prioritizing more challenging targets. This adaptive mechanism effectively addresses the challenge of sparse rewards and improves convergence efficiency. By leveraging this mechanism, GDWO unifies multiple objectives within a unified decision space, achieving efficient optimization and balancing short-term gains with long-term goals. Additionally, we introduce two auxiliary modules: prior knowledge-based navigation and frontier-aware exploration to further enhance GDWO's performance. Experimental results on the Gibson and Matterport3D datasets demonstrate that GDWO achieves improvements in key metrics for MON tasks. It optimizes path planning, reduces exploration costs, and enhances navigation efficiency, enabling the agent to perform tasks more effective...
Published: 2026-01-27T05:48:07+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haitao Zeng; Xinhang Song; Shuqiang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3657778"&gt;10.1109/tpami.2026.3657778&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-object navigation (MON) tasks involve sequentially locating multiple targets in an unknown environment, requiring global long-term planning under incomplete information. This necessitates that the agent dynamically balance immediate actions and long-term rewards while considering both local adaptability and global foresight. However, current methods overly focus on local path optimization, which leads to slower convergence in sparse reward settings and increases the risk of deadlocks or trap states. The core challenge of MON lies in the deformation of the shared decision space, where independent optimization leads to redundant and overlapping paths. Thus, path planning requires dynamic, cross-task optimization rather than simple subtask aggregation. To minimize overall effort, the optimization process should adaptively balance task contributions through weight adjustment. Thus, we propose the Goal-oriented Dynamic Weight Optimization (GDWO) algorithm. GDWO integrates target-specific value loss functions into a unified optimization framework and dynamically adjusts weights through gradient-based updates. To prevent over-optimization, weights are normalized during training according to navigation success rates, prioritizing more challenging targets. This adaptive mechanism effectively addresses the challenge of sparse rewards and improves convergence efficiency. By leveraging this mechanism, GDWO unifies multiple objectives within a unified decision space, achieving efficient optimization and balancing short-term gains with long-term goals. Additionally, we introduce two auxiliary modules: prior knowledge-based navigation and frontier-aware exploration to further enhance GDWO&amp;#x27;s performance. Experimental results on the Gibson and Matterport3D datasets demonstrate that GDWO achieves improvements in key metrics for MON tasks. It optimizes path planning, reduces exploration costs, and enhances navigation efficiency, enabling the agent to perform tasks more effective...&lt;/p&gt;</content:encoded></item><item><title>SONIC: Spectral Oriented Neural Invariant Convolutions</title><link>https://arxiv.org/abs/2601.19884v1</link><guid>http://arxiv.org/abs/2601.19884v1</guid><pubDate>Tue, 27 Jan 2026 18:51:11 +0000</pubDate><dc:creator>Gijs Joppe Moens</dc:creator><dc:creator>Regina Beets-Tan</dc:creator><dc:creator>Eduardo H. P. Pooch</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.
Published: 2026-01-27T18:51:11+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gijs Joppe Moens; Regina Beets-Tan; Eduardo H. P. Pooch&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.&lt;/p&gt;</content:encoded></item><item><title>Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification</title><link>https://arxiv.org/abs/2601.18088v1</link><guid>http://arxiv.org/abs/2601.18088v1</guid><pubDate>Mon, 26 Jan 2026 02:52:35 +0000</pubDate><dc:creator>Jianshu Chao</dc:creator><dc:creator>Tianhua Lv</dc:creator><dc:creator>Qiqiong Ma</dc:creator><dc:creator>Yunfei Qiu</dc:creator><dc:creator>Li Fang</dc:creator><dc:creator>Huifang Shen</dc:creator><dc:creator>Wei Yao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.
Published: 2026-01-26T02:52:35+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianshu Chao; Tianhua Lv; Qiqiong Ma; Yunfei Qiu; Li Fang; Huifang Shen; Wei Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model&amp;#x27;s capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method&amp;#x27;s effectiveness under resource-constrained conditions.&lt;/p&gt;</content:encoded></item><item><title>LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts</title><link>https://arxiv.org/abs/2601.18089v1</link><guid>http://arxiv.org/abs/2601.18089v1</guid><pubDate>Mon, 26 Jan 2026 02:59:23 +0000</pubDate><dc:creator>Venmugil Elango</dc:creator><dc:creator>Nidhi Bhatia</dc:creator><dc:creator>Roger Waleffe</dc:creator><dc:creator>Rasoul Shafipour</dc:creator><dc:creator>Tomer Asida</dc:creator><dc:creator>Abhinav Khattar</dc:creator><dc:creator>Nave Assaf</dc:creator><dc:creator>Maximilian Golub</dc:creator><dc:creator>Joey Guman</dc:creator><dc:creator>Tiyasa Mitra</dc:creator><dc:creator>Ritchie Zhao</dc:creator><dc:creator>Ritika Borkar</dc:creator><dc:creator>Ran Zilberstein</dc:creator><dc:creator>Mostofa Patwary</dc:creator><dc:creator>Mohammad Shoeybi</dc:creator><dc:creator>Bita Rouhani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).
Published: 2026-01-26T02:59:23+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Venmugil Elango; Nidhi Bhatia; Roger Waleffe; Rasoul Shafipour; Tomer Asida; Abhinav Khattar; Nave Assaf; Maximilian Golub; Joey Guman; Tiyasa Mitra; Ritchie Zhao; Ritika Borkar; Ran Zilberstein; Mostofa Patwary; Mohammad Shoeybi; Bita Rouhani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).&lt;/p&gt;</content:encoded></item><item><title>$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts</title><link>https://arxiv.org/abs/2601.17680v1</link><guid>http://arxiv.org/abs/2601.17680v1</guid><pubDate>Sun, 25 Jan 2026 03:55:51 +0000</pubDate><dc:creator>Shota Takashiro</dc:creator><dc:creator>Takeshi Kojima</dc:creator><dc:creator>Shohei Taniguchi</dc:creator><dc:creator>Yusuke Iwasawa</dc:creator><dc:creator>Yutaka Matsuo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.
Published: 2026-01-25T03:55:51+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shota Takashiro; Takeshi Kojima; Shohei Taniguchi; Yusuke Iwasawa; Yutaka Matsuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.&lt;/p&gt;</content:encoded></item><item><title>Distilling Structural Knowledge from CNNs to Vision Transformers for Data-Efficient Visual Recognition</title><link>https://doi.org/10.1016/j.neunet.2026.108601</link><guid>10.1016/j.neunet.2026.108601</guid><pubDate>Tue, 27 Jan 2026 17:06:21 +0000</pubDate><dc:creator>Dingyao Chen</dc:creator><dc:creator>Xiao Teng</dc:creator><dc:creator>Xingyu Shen</dc:creator><dc:creator>Xun Yang</dc:creator><dc:creator>Long Lan</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108601</prism:doi><description>Knowledge distillation (KD) is an effective strategy to transfer learned representations from a pre-trained teacher model to a smaller student model. Current methods for knowledge transfer from convolutional neural networks (CNNs) to vision transformers (ViTs) mainly align output logits. However, such approaches often overlook the rich semantic structures encoded in CNN features, thereby restricting ViTs from effectively inheriting the inductive biases inherent in convolutional architectures. To this end, this paper proposes a F eature-based CNN-to-ViT S tructural K nowledge D istillation framework, dubbed FSKD , which combines the semantic structural knowledge embedded in CNN ( teacher ) features with the strength of ViT ( student ) in capturing long-range dependencies. Specifically, this framework includes a feature alignment module to bridge the representational gap between CNN and ViT features, and it incorporates a global feature alignment loss. Additionally, we develop patch-wise and attention-wise distillation losses to transfer inter-patch similarity and attention distribution, facilitating semantic structural knowledge transfer from CNNs to ViTs. Experimental results demonstrate that the proposed method considerably enhances ViT performance in visual recognition tasks, particularly under scenarios with limited data. Code is available at Github .
Published: 2026-01-27T17:06:21+00:00
Venue: Neural Networks
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dingyao Chen; Xiao Teng; Xingyu Shen; Xun Yang; Long Lan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108601"&gt;10.1016/j.neunet.2026.108601&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation (KD) is an effective strategy to transfer learned representations from a pre-trained teacher model to a smaller student model. Current methods for knowledge transfer from convolutional neural networks (CNNs) to vision transformers (ViTs) mainly align output logits. However, such approaches often overlook the rich semantic structures encoded in CNN features, thereby restricting ViTs from effectively inheriting the inductive biases inherent in convolutional architectures. To this end, this paper proposes a F eature-based CNN-to-ViT S tructural K nowledge D istillation framework, dubbed FSKD , which combines the semantic structural knowledge embedded in CNN ( teacher ) features with the strength of ViT ( student ) in capturing long-range dependencies. Specifically, this framework includes a feature alignment module to bridge the representational gap between CNN and ViT features, and it incorporates a global feature alignment loss. Additionally, we develop patch-wise and attention-wise distillation losses to transfer inter-patch similarity and attention distribution, facilitating semantic structural knowledge transfer from CNNs to ViTs. Experimental results demonstrate that the proposed method considerably enhances ViT performance in visual recognition tasks, particularly under scenarios with limited data. Code is available at Github .&lt;/p&gt;</content:encoded></item><item><title>Addendum: Resolving data bias improves generalization in binding affinity prediction</title><link>https://doi.org/10.1038/s42256-025-01174-9</link><guid>10.1038/s42256-025-01174-9</guid><pubDate>Tue, 27 Jan 2026 19:22:36 +0000</pubDate><dc:creator>David Graber</dc:creator><dc:creator>Peter Stockinger</dc:creator><dc:creator>Fabian Meyer</dc:creator><dc:creator>Siddhartha Mishra</dc:creator><dc:creator>Claus Horn</dc:creator><dc:creator>Rebecca Buller</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01174-9</prism:doi><description>Addendum to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01124-5 , published online 21 October 2025.
Published: 2026-01-27T19:22:36+00:00
Venue: Nature Machine Intelligence
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; David Graber; Peter Stockinger; Fabian Meyer; Siddhartha Mishra; Claus Horn; Rebecca Buller&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01174-9"&gt;10.1038/s42256-025-01174-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Addendum to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01124-5 , published online 21 October 2025.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation</title><link>https://arxiv.org/abs/2601.18623v1</link><guid>http://arxiv.org/abs/2601.18623v1</guid><pubDate>Mon, 26 Jan 2026 16:00:36 +0000</pubDate><dc:creator>Zihao Wang</dc:creator><dc:creator>Yuzhou Chen</dc:creator><dc:creator>Shaogang Ren</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.
Published: 2026-01-26T16:00:36+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihao Wang; Yuzhou Chen; Shaogang Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model&amp;#x27;s role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.&lt;/p&gt;</content:encoded></item><item><title>PGSC: A Gradient Sparsification Communication Optimization Criterion for Nonequilibrium Thermodynamics</title><link>https://doi.org/10.1016/j.inffus.2026.104188</link><guid>10.1016/j.inffus.2026.104188</guid><pubDate>Tue, 27 Jan 2026 07:48:48 +0000</pubDate><dc:creator>Wenlong Zhang</dc:creator><dc:creator>Ying Li</dc:creator><dc:creator>Hanhan Du</dc:creator><dc:creator>Yan Wei</dc:creator><dc:creator>Aiqing Fang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104188</prism:doi><description>Gradient compression can reduce communication overhead. However, current static sparsity techniques may disturb gradient dynamics, resulting in unstable model convergence and reduced feature discriminative ability, whereas transmitting the complete gradient leads to high costs. To address this issue, inspired by nonequilibrium thermodynamics, this paper proposes a Physics-guided Gradient Sparsification Criterion (PGSC). Specifically, we formulate a continuous field equation based on the gradient magnitude distribution, deriving an adaptive decay rule for the sparsification threshold during the training phase. We then dynamically adjust the sparsification threshold according to this rule, effectively addressing the complexity of multimodal features and ensuring consistent information transmission. Our method achieves adaptive co-optimization of gradient compression and model accuracy by establishing a dynamic equilibrium mechanism between gradient dissipation and information entropy. This approach ensures stable convergence rates while preserving the gradient structure of multi-scale features. Extensive experiments on public datasets, including CIFAR-10, MNIST, and FLIR_ADAS_v2, demonstrate significant advantages over competitors such as TopK and quantization compression, while also reducing communication costs.
Published: 2026-01-27T07:48:48+00:00
Venue: Information Fusion
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenlong Zhang; Ying Li; Hanhan Du; Yan Wei; Aiqing Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104188"&gt;10.1016/j.inffus.2026.104188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Gradient compression can reduce communication overhead. However, current static sparsity techniques may disturb gradient dynamics, resulting in unstable model convergence and reduced feature discriminative ability, whereas transmitting the complete gradient leads to high costs. To address this issue, inspired by nonequilibrium thermodynamics, this paper proposes a Physics-guided Gradient Sparsification Criterion (PGSC). Specifically, we formulate a continuous field equation based on the gradient magnitude distribution, deriving an adaptive decay rule for the sparsification threshold during the training phase. We then dynamically adjust the sparsification threshold according to this rule, effectively addressing the complexity of multimodal features and ensuring consistent information transmission. Our method achieves adaptive co-optimization of gradient compression and model accuracy by establishing a dynamic equilibrium mechanism between gradient dissipation and information entropy. This approach ensures stable convergence rates while preserving the gradient structure of multi-scale features. Extensive experiments on public datasets, including CIFAR-10, MNIST, and FLIR_ADAS_v2, demonstrate significant advantages over competitors such as TopK and quantization compression, while also reducing communication costs.&lt;/p&gt;</content:encoded></item><item><title>A novel framework for marine oil spill detection in SAR imagery fusing edge supervision enhancement and group attention mechanism</title><link>https://doi.org/10.1016/j.rsase.2026.101901</link><guid>10.1016/j.rsase.2026.101901</guid><pubDate>Tue, 27 Jan 2026 07:53:53 +0000</pubDate><dc:creator>Xinrong Lyu</dc:creator><dc:creator>Haosha Su</dc:creator><dc:creator>Christos Grecos</dc:creator><dc:creator>Peng Ren</dc:creator><prism:publicationName>Remote Sensing Applications: Society and Environment</prism:publicationName><prism:doi>10.1016/j.rsase.2026.101901</prism:doi><description>Rapid and accurate detection of marine oil spills is crucial for environmental protection and emergency response. Synthetic Aperture Radar (SAR), a primary tool for sea surface oil spill monitoring, faces persistent challenges such as varying spill scales, blurred boundaries, and confusion with look-alike phenomena. To address these issues, this study proposes OilSeg-SARNet, a novel architecture tailored for SAR oil spill detection. The model incorporates a Group Convolutional Block Attention Module Enhancer to emphasize salient features and suppress background noise, an Atrous Spatial Pyramid Pooling module to capture multi-scale contextual information, and an improved Edge Supervision Enhancement Module to refine boundary representation and facilitate gradient propagation. These components work synergistically to enhance detection precision under complex marine conditions. Experimental results on the public SAR Oil Spill Detection Dataset demonstrate that OilSeg-SARNet achieves class-specific Intersection-over-Unions (IoUs) of 61.33%, 64.86%, and 45.10% for oil spill, look-alike, and ship categories, respectively, outperforming the best prior method by +0.85%, +3.73%, and +9.89%, respectively. The model attains an overall mean IoU (mIoU) of 72.22% and an F 1 " role="presentation"&gt; 1 1 -score of 79.33%. The proposed model surpasses existing methods with reduced complexity, offering a reliable and efficient framework for marine oil spill monitoring, thereby enhancing early detection and supporting timely environmental response.
Published: 2026-01-27T07:53:53+00:00
Venue: Remote Sensing Applications: Society and Environment
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinrong Lyu; Haosha Su; Christos Grecos; Peng Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing Applications: Society and Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rsase.2026.101901"&gt;10.1016/j.rsase.2026.101901&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Rapid and accurate detection of marine oil spills is crucial for environmental protection and emergency response. Synthetic Aperture Radar (SAR), a primary tool for sea surface oil spill monitoring, faces persistent challenges such as varying spill scales, blurred boundaries, and confusion with look-alike phenomena. To address these issues, this study proposes OilSeg-SARNet, a novel architecture tailored for SAR oil spill detection. The model incorporates a Group Convolutional Block Attention Module Enhancer to emphasize salient features and suppress background noise, an Atrous Spatial Pyramid Pooling module to capture multi-scale contextual information, and an improved Edge Supervision Enhancement Module to refine boundary representation and facilitate gradient propagation. These components work synergistically to enhance detection precision under complex marine conditions. Experimental results on the public SAR Oil Spill Detection Dataset demonstrate that OilSeg-SARNet achieves class-specific Intersection-over-Unions (IoUs) of 61.33%, 64.86%, and 45.10% for oil spill, look-alike, and ship categories, respectively, outperforming the best prior method by +0.85%, +3.73%, and +9.89%, respectively. The model attains an overall mean IoU (mIoU) of 72.22% and an F 1 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; 1 1 -score of 79.33%. The proposed model surpasses existing methods with reduced complexity, offering a reliable and efficient framework for marine oil spill monitoring, thereby enhancing early detection and supporting timely environmental response.&lt;/p&gt;</content:encoded></item><item><title>Persistent Scatterers Detection supported by Deep Learning: a Solution Based on U-Net</title><link>https://doi.org/10.1109/tgrs.2026.3657993</link><guid>10.1109/tgrs.2026.3657993</guid><pubDate>Tue, 27 Jan 2026 05:48:56 +0000</pubDate><dc:creator>Weili Tang</dc:creator><dc:creator>Sergio Vitale</dc:creator><dc:creator>Simona Verde</dc:creator><dc:creator>Gianfranco Fornaro</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3657993</prism:doi><description>Multitemporal Differential Interferometric Synthetic Aperture Radar (MT-DInSAR) allows accurate and longterm monitoring of displacements of ground Persistent Scatterers (PS). PS are typically detected using suitable statistical tests, built to strictly control the probability of false alarm. At full resolution, this detection strategy can lead to the rejection of PS characterized by spatial consistency of the estimated parameters. Reducing the density of PS measurements can impact the interpretation of the results. In this work we investigate the integration of a Deep-Learning (DL) solution, specifically U-Net, at the stage of PS detection. A three stream U-Net is proposed to replace the typical thresholding of the classical statistical indicators. Results on simulated data and on data acquired by the sensors of the COSMO-SkyMed (CSK) and COSMO-SkyMed Second Generation (CSG) constellation, demonstrate the superior performances of the proposed DL- PS detection scheme over the classical one.
Published: 2026-01-27T05:48:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weili Tang; Sergio Vitale; Simona Verde; Gianfranco Fornaro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3657993"&gt;10.1109/tgrs.2026.3657993&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Multitemporal Differential Interferometric Synthetic Aperture Radar (MT-DInSAR) allows accurate and longterm monitoring of displacements of ground Persistent Scatterers (PS). PS are typically detected using suitable statistical tests, built to strictly control the probability of false alarm. At full resolution, this detection strategy can lead to the rejection of PS characterized by spatial consistency of the estimated parameters. Reducing the density of PS measurements can impact the interpretation of the results. In this work we investigate the integration of a Deep-Learning (DL) solution, specifically U-Net, at the stage of PS detection. A three stream U-Net is proposed to replace the typical thresholding of the classical statistical indicators. Results on simulated data and on data acquired by the sensors of the COSMO-SkyMed (CSK) and COSMO-SkyMed Second Generation (CSG) constellation, demonstrate the superior performances of the proposed DL- PS detection scheme over the classical one.&lt;/p&gt;</content:encoded></item><item><title>YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection</title><link>https://arxiv.org/abs/2601.18172v1</link><guid>http://arxiv.org/abs/2601.18172v1</guid><pubDate>Mon, 26 Jan 2026 05:50:32 +0000</pubDate><dc:creator>Lin Huang</dc:creator><dc:creator>Yujuan Tan</dc:creator><dc:creator>Weisheng Li</dc:creator><dc:creator>Shitai Shan</dc:creator><dc:creator>Liu Liu</dc:creator><dc:creator>Bo Liu</dc:creator><dc:creator>Linlin Shen</dc:creator><dc:creator>Jing Yu</dc:creator><dc:creator>Yue Niu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.
Published: 2026-01-26T05:50:32+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lin Huang; Yujuan Tan; Weisheng Li; Shitai Shan; Liu Liu; Bo Liu; Linlin Shen; Jing Yu; Yue Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.&lt;/p&gt;</content:encoded></item></channel></rss>