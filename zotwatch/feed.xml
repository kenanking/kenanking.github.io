<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 09 Dec 2025 02:44:00 +0000</lastBuildDate><item><title>EnBoT-SORT: Hierarchical fusion-association tracking with pseudo-sample generation for dense thermal infrared UAVs</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.001</link><guid>10.1016/j.isprsjprs.2025.12.001</guid><pubDate>Mon, 08 Dec 2025 11:36:39 +0000</pubDate><dc:creator>Jinxin Guo</dc:creator><dc:creator>Weida Zhan</dc:creator><dc:creator>Yu Chen</dc:creator><dc:creator>Depeng Zhu</dc:creator><dc:creator>Yichun Jiang</dc:creator><dc:creator>Xiaoyu Xu</dc:creator><dc:creator>Deng Han</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.001</prism:doi><description>Thermal infrared dense UAV target detection and tracking present significant challenges at both data and algorithmic levels. At the data level, there exists a scarcity of accurately annotated real-world samples coupled with high acquisition costs. At the algorithmic level, the key difficulty lies in addressing frequent identity switches caused by highly dense target clustering, frequent occlusions, and reappearances. To overcome these challenges, this paper proposes an innovative infrared pseudo-sample generation paradigm by designing a physically-driven Heterogeneous Interactive Degradation Model (HIDM). This model simulates real infrared imaging through background-target cooperative degradation mechanisms that account for multiple coupled degradation factors, combined with a random trajectory generation strategy to produce large-scale physically realistic pseudo-sample data, significantly enhancing the domain adaptability of the generated data. Building upon this foundation, we propose a hierarchical fusion-association tracking framework—EnBoT-SORT. This framework employs YOLOv12 as a powerful detector and innovatively incorporates a dynamic target density regulator, a hybrid feature association engine, and a trajectory continuity enhancement module into BoT-SORT, effectively maintaining the continuity and stability of target IDs. Experimental results demonstrate that EnBoT-SORT significantly outperforms existing trackers in intensive UAV motion scenarios, achieving state-of-the-art performance on the IRT-B and IRC-B datasets with HOTA scores of 68.7% and 67.3%, and MOTA scores of 76.2% and 74.6%, respectively. Furthermore, cross-modal experiments on real infrared and visible-light datasets indicate that EnBoT-SORT possesses strong generalization capabilities. This work provides a comprehensive solution for infrared-intensive UAV tracking, spanning from data generation to algorithmic optimization. Our code and datasets are available at GitHub .
Published: 2025-12-08T11:36:39+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinxin Guo; Weida Zhan; Yu Chen; Depeng Zhu; Yichun Jiang; Xiaoyu Xu; Deng Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.001"&gt;10.1016/j.isprsjprs.2025.12.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Thermal infrared dense UAV target detection and tracking present significant challenges at both data and algorithmic levels. At the data level, there exists a scarcity of accurately annotated real-world samples coupled with high acquisition costs. At the algorithmic level, the key difficulty lies in addressing frequent identity switches caused by highly dense target clustering, frequent occlusions, and reappearances. To overcome these challenges, this paper proposes an innovative infrared pseudo-sample generation paradigm by designing a physically-driven Heterogeneous Interactive Degradation Model (HIDM). This model simulates real infrared imaging through background-target cooperative degradation mechanisms that account for multiple coupled degradation factors, combined with a random trajectory generation strategy to produce large-scale physically realistic pseudo-sample data, significantly enhancing the domain adaptability of the generated data. Building upon this foundation, we propose a hierarchical fusion-association tracking framework—EnBoT-SORT. This framework employs YOLOv12 as a powerful detector and innovatively incorporates a dynamic target density regulator, a hybrid feature association engine, and a trajectory continuity enhancement module into BoT-SORT, effectively maintaining the continuity and stability of target IDs. Experimental results demonstrate that EnBoT-SORT significantly outperforms existing trackers in intensive UAV motion scenarios, achieving state-of-the-art performance on the IRT-B and IRC-B datasets with HOTA scores of 68.7% and 67.3%, and MOTA scores of 76.2% and 74.6%, respectively. Furthermore, cross-modal experiments on real infrared and visible-light datasets indicate that EnBoT-SORT possesses strong generalization capabilities. This work provides a comprehensive solution for infrared-intensive UAV tracking, spanning from data generation to algorithmic optimization. Our code and datasets are available at GitHub .&lt;/p&gt;</content:encoded></item><item><title>Self-Adaptive Vision-Language Tracking with Context Prompting</title><link>https://doi.org/10.1109/tip.2025.3635016</link><guid>10.1109/tip.2025.3635016</guid><pubDate>Mon, 08 Dec 2025 18:42:57 +0000</pubDate><dc:creator>Jie Zhao</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Shengming Li</dc:creator><dc:creator>Chunjuan Bo</dc:creator><dc:creator>Dong Wang</dc:creator><dc:creator>Huchuan Lu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3635016</prism:doi><description>Due to the substantial gap between vision and language modalities, along with the mismatch problem between fixed language descriptions and dynamic visual information, existing vision-language tracking methods exhibit performance on par with or slightly worse than vision-only tracking. Effectively exploiting the rich semantics of language to enhance tracking robustness remains an open challenge. To address these issues, we propose a self-adaptive vision-language tracking framework that leverages the pre-trained multi-modal CLIP model to obtain well-aligned visual-language representations. A novel context-aware prompting mechanism is introduced to dynamically adapt linguistic cues based on the evolving visual context during tracking. Specifically, our context prompter extracts dynamic visual features from the current search image and integrates them into the text encoding process, enabling self-updating language embeddings. Furthermore, our framework employs a unified one-stream Transformer architecture, supporting joint training for both vision-only and vision-language tracking scenarios. Our method not only bridges the modality gap but also enhances robustness by allowing language features to evolve with visual context. Extensive experiments on four vision-language tracking benchmarks demonstrate that our method effectively leverages the advantages of language to enhance visual tracking. Our large model can obtain 55.0% AUC on LaSOTEXT and 69.0% AUC on TNL2K. Additionally, our language-only tracking model achieves performance comparable to that of state-of-the-art vision-only tracking methods on TNL2K. Code is available at https://github.com/zj5559/SAVLT.
Published: 2025-12-08T18:42:57+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Zhao; Xin Chen; Shengming Li; Chunjuan Bo; Dong Wang; Huchuan Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3635016"&gt;10.1109/tip.2025.3635016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the substantial gap between vision and language modalities, along with the mismatch problem between fixed language descriptions and dynamic visual information, existing vision-language tracking methods exhibit performance on par with or slightly worse than vision-only tracking. Effectively exploiting the rich semantics of language to enhance tracking robustness remains an open challenge. To address these issues, we propose a self-adaptive vision-language tracking framework that leverages the pre-trained multi-modal CLIP model to obtain well-aligned visual-language representations. A novel context-aware prompting mechanism is introduced to dynamically adapt linguistic cues based on the evolving visual context during tracking. Specifically, our context prompter extracts dynamic visual features from the current search image and integrates them into the text encoding process, enabling self-updating language embeddings. Furthermore, our framework employs a unified one-stream Transformer architecture, supporting joint training for both vision-only and vision-language tracking scenarios. Our method not only bridges the modality gap but also enhances robustness by allowing language features to evolve with visual context. Extensive experiments on four vision-language tracking benchmarks demonstrate that our method effectively leverages the advantages of language to enhance visual tracking. Our large model can obtain 55.0% AUC on LaSOTEXT and 69.0% AUC on TNL2K. Additionally, our language-only tracking model achieves performance comparable to that of state-of-the-art vision-only tracking methods on TNL2K. Code is available at https://github.com/zj5559/SAVLT.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Language Models in Agriculture: A Tutorial and Survey</title><link>https://doi.org/10.1016/j.inffus.2025.104042</link><guid>10.1016/j.inffus.2025.104042</guid><pubDate>Sun, 07 Dec 2025 15:19:43 +0000</pubDate><dc:creator>Mohammadreza Haghighat</dc:creator><dc:creator>Alzayat Saleh</dc:creator><dc:creator>Mostafa Rahimi Azghadi</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104042</prism:doi><description>The integration of artificial intelligence (AI) in agriculture is rapidly evolving, marked by increasing adoption of machine learning (ML), deep learning (DL), and the recent emergence of large language models (LLMs) and multimodal language models (MLMs). These technologies are transforming traditional agricultural practices through advanced data analysis and offering innovative solutions for smart agriculture. Although earlier methods often relied on unimodal data, primarily images, current research is shifting toward information fusion with multimodal AI systems that fuse text, imagery, and other types of data, such as agricultural knowledge graphs and videos of animal activities, for enhanced decision making. General-purpose MLMs struggle with agriculture-specific nuances due to domain gaps and limited multimodal datasets. The high computational demands of foundation models (FM) and domain-specific needs also restrict broader adoption. This study presents a tutorial on applying MLMs in agriculture, covering their main concepts. Building on this foundational knowledge, the tutorial surveys key developments in the literature and provides a comprehensive step-by-step guide for implementing and tailoring MLMs to agricultural applications. This will address the domain gaps by knowledge integration, synthetic multimodal data generation and efficient learning methods, thereby expanding their practical applications.
Published: 2025-12-07T15:19:43+00:00
Venue: Information Fusion
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohammadreza Haghighat; Alzayat Saleh; Mostafa Rahimi Azghadi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104042"&gt;10.1016/j.inffus.2025.104042&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;The integration of artificial intelligence (AI) in agriculture is rapidly evolving, marked by increasing adoption of machine learning (ML), deep learning (DL), and the recent emergence of large language models (LLMs) and multimodal language models (MLMs). These technologies are transforming traditional agricultural practices through advanced data analysis and offering innovative solutions for smart agriculture. Although earlier methods often relied on unimodal data, primarily images, current research is shifting toward information fusion with multimodal AI systems that fuse text, imagery, and other types of data, such as agricultural knowledge graphs and videos of animal activities, for enhanced decision making. General-purpose MLMs struggle with agriculture-specific nuances due to domain gaps and limited multimodal datasets. The high computational demands of foundation models (FM) and domain-specific needs also restrict broader adoption. This study presents a tutorial on applying MLMs in agriculture, covering their main concepts. Building on this foundational knowledge, the tutorial surveys key developments in the literature and provides a comprehensive step-by-step guide for implementing and tailoring MLMs to agricultural applications. This will address the domain gaps by knowledge integration, synthetic multimodal data generation and efficient learning methods, thereby expanding their practical applications.&lt;/p&gt;</content:encoded></item><item><title>Classifier Guidance and Domain Cooperation for Multisource Unsupervised Domain Adaptation</title><link>https://doi.org/10.1016/j.knosys.2025.115057</link><guid>10.1016/j.knosys.2025.115057</guid><pubDate>Sun, 07 Dec 2025 23:04:51 +0000</pubDate><dc:creator>Ming Zhao</dc:creator><dc:creator>Yifan Lan</dc:creator><dc:creator>Yuwu Lu</dc:creator><dc:creator>Leyao Yuan</dc:creator><dc:creator>Wenmeng Zhang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115057</prism:doi><description>Traditional deep learning models usually rely on identical data distributions for their training and testing sets. Unfortunately, this assumption rarely holds in real-world applications. This challenge has led to the proposal of unsupervised domain adaptation (UDA). While centred on transfers between single source and target domains, most UDA methods do not capitalize on the wealth of labelled resources that are present across multiple source domains. This gap is addressed herein by focusing on multisource unsupervised domain adaptation (MUDA), where knowledge is transferred from multiple source domains to an unlabelled target domain. The existing MUDA methods focus primarily on reducing discrepancies between the source and target domains and often overlook the goal of improving the performance of the utilized classifier across different domains, which is equally crucial for effectively implementing domain adaptation. To overcome this limitation, we propose a novel approach called classifier guidance and domain cooperation (CGDC). Our method calculates the accuracy of a classifier in the target domain using a probability-inspired approach that considers data transformation costs and classifier precision in the high-level feature space of the target domain. This enables high-performance classifiers to guide classifiers with lower performance. Additionally, we introduce a mixup-based multi-information fusion strategy to create enriched domains by combining information derived from the source and target domains. A comparison with many methods on public datasets shows the effectiveness of our CGDC approach in image classification tasks. Specifically, CGDC achieves improvements of 4.3% on Office-Home and 1.9% on Office-31 over the best baseline methods.
Published: 2025-12-07T23:04:51+00:00
Venue: Knowledge-Based Systems
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Zhao; Yifan Lan; Yuwu Lu; Leyao Yuan; Wenmeng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115057"&gt;10.1016/j.knosys.2025.115057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Traditional deep learning models usually rely on identical data distributions for their training and testing sets. Unfortunately, this assumption rarely holds in real-world applications. This challenge has led to the proposal of unsupervised domain adaptation (UDA). While centred on transfers between single source and target domains, most UDA methods do not capitalize on the wealth of labelled resources that are present across multiple source domains. This gap is addressed herein by focusing on multisource unsupervised domain adaptation (MUDA), where knowledge is transferred from multiple source domains to an unlabelled target domain. The existing MUDA methods focus primarily on reducing discrepancies between the source and target domains and often overlook the goal of improving the performance of the utilized classifier across different domains, which is equally crucial for effectively implementing domain adaptation. To overcome this limitation, we propose a novel approach called classifier guidance and domain cooperation (CGDC). Our method calculates the accuracy of a classifier in the target domain using a probability-inspired approach that considers data transformation costs and classifier precision in the high-level feature space of the target domain. This enables high-performance classifiers to guide classifiers with lower performance. Additionally, we introduce a mixup-based multi-information fusion strategy to create enriched domains by combining information derived from the source and target domains. A comparison with many methods on public datasets shows the effectiveness of our CGDC approach in image classification tasks. Specifically, CGDC achieves improvements of 4.3% on Office-Home and 1.9% on Office-31 over the best baseline methods.&lt;/p&gt;</content:encoded></item><item><title>K2-V2: A 360-Open, Reasoning-Enhanced LLM</title><link>https://arxiv.org/abs/2512.06201v1</link><guid>http://arxiv.org/abs/2512.06201v1</guid><pubDate>Fri, 05 Dec 2025 22:53:45 +0000</pubDate><dc:creator>K2 Team</dc:creator><dc:creator>Zhengzhong Liu</dc:creator><dc:creator>Liping Tang</dc:creator><dc:creator>Linghao Jin</dc:creator><dc:creator>Haonan Li</dc:creator><dc:creator>Nikhil Ranjan</dc:creator><dc:creator>Desai Fan</dc:creator><dc:creator>Shaurya Rohatgi</dc:creator><dc:creator>Richard Fan</dc:creator><dc:creator>Omkar Pangarkar</dc:creator><dc:creator>Huijuan Wang</dc:creator><dc:creator>Zhoujun Cheng</dc:creator><dc:creator>Suqi Sun</dc:creator><dc:creator>Seungwook Han</dc:creator><dc:creator>Bowen Tan</dc:creator><dc:creator>Gurpreet Gosal</dc:creator><dc:creator>Xudong Han</dc:creator><dc:creator>Varad Pimpalkhute</dc:creator><dc:creator>Shibo Hao</dc:creator><dc:creator>Ming Shan Hee</dc:creator><dc:creator>Joel Hestness</dc:creator><dc:creator>Haolong Jia</dc:creator><dc:creator>Liqun Ma</dc:creator><dc:creator>Aaryamonvikram Singh</dc:creator><dc:creator>Daria Soboleva</dc:creator><dc:creator>Natalia Vassilieva</dc:creator><dc:creator>Renxi Wang</dc:creator><dc:creator>Yingquan Wu</dc:creator><dc:creator>Yuekai Sun</dc:creator><dc:creator>Taylor Killian</dc:creator><dc:creator>Alexander Moreno</dc:creator><dc:creator>John Maggs</dc:creator><dc:creator>Hector Ren</dc:creator><dc:creator>Guowei He</dc:creator><dc:creator>Hongyi Wang</dc:creator><dc:creator>Xuezhe Ma</dc:creator><dc:creator>Yuqi Wang</dc:creator><dc:creator>Mikhail Yurochkin</dc:creator><dc:creator>Eric P. Xing</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.
Published: 2025-12-05T22:53:45+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; K2 Team; Zhengzhong Liu; Liping Tang; Linghao Jin; Haonan Li; Nikhil Ranjan; Desai Fan; Shaurya Rohatgi; Richard Fan; Omkar Pangarkar; Huijuan Wang; Zhoujun Cheng; Suqi Sun; Seungwook Han; Bowen Tan; Gurpreet Gosal; Xudong Han; Varad Pimpalkhute; Shibo Hao; Ming Shan Hee; Joel Hestness; Haolong Jia; Liqun Ma; Aaryamonvikram Singh; Daria Soboleva; Natalia Vassilieva; Renxi Wang; Yingquan Wu; Yuekai Sun; Taylor Killian; Alexander Moreno; John Maggs; Hector Ren; Guowei He; Hongyi Wang; Xuezhe Ma; Yuqi Wang; Mikhail Yurochkin; Eric P. Xing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.&lt;/p&gt;</content:encoded></item><item><title>PMFM-kdTransformer: An enhanced multi-modal fusion architecture leveraging knowledge distillation for intra-hour solar irradiance prediction</title><link>https://doi.org/10.1016/j.inffus.2025.104043</link><guid>10.1016/j.inffus.2025.104043</guid><pubDate>Mon, 08 Dec 2025 23:58:22 +0000</pubDate><dc:creator>Menggang Kou</dc:creator><dc:creator>Runze Li</dc:creator><dc:creator>Tong Niu</dc:creator><dc:creator>Quansheng Qian</dc:creator><dc:creator>Zhiwu Li</dc:creator><dc:creator>Jianzhou Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104043</prism:doi><description>With the increasing proportion of photovoltaic power in the energy structure, the volatility of solar irradiance poses a significant challenge to grid security, making solar irradiance forecasting a critical approach for enhancing renewable energy integration. Existing methods face several limitations: purely temporal modeling struggles to capture dynamic correlations under abrupt weather conditions, while sky-image-based approaches, though promising, suffer from insufficient inter-modal interaction exploration in current multi-modal fusion models (MFMs). Additionally, the MFMs are hindered by the high cost of all-sky imagers, limiting their widespread adoption. To address these issues, this study proposes a parallel multimodal fusion model (PMFM) and a knowledge-distilled Transformer (kdTransformer). The PMFM adopts a parallel processing architecture with modality-specific feature extraction strategies for three data: cloud images processed through deformable convolution and pyramid structure, meteorological variables modeled via convolutional operations, and irradiation sequences processed through multilayer perceptrons. The extracted features are then fused through enhanced dynamic gating mechanisms with cross-modal attention. For regions lacking cloud image, the kdTransformer transfers the multimodal knowledge learned by the PMFM to a temporal model via knowledge distillation, where the distillation loss extends the traditional distillation framework by introducing a feature alignment loss and a relational distillation loss, combined dynamically with weights. Experiments demonstrate that the PMFM achieves a 7.92% average improvement across seven metrics and two year’s datasets over the renowned Sunset baseline, while the kdTransformer achieves a 24.58% average enhancement compared to the non-distilled Transformer. Furthermore, the PMFM-kdTransformer achieves a trade-off between computational efficiency (65.1 FLOPs/G) and prediction accuracy.
Published: 2025-12-08T23:58:22+00:00
Venue: Information Fusion
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Menggang Kou; Runze Li; Tong Niu; Quansheng Qian; Zhiwu Li; Jianzhou Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104043"&gt;10.1016/j.inffus.2025.104043&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;With the increasing proportion of photovoltaic power in the energy structure, the volatility of solar irradiance poses a significant challenge to grid security, making solar irradiance forecasting a critical approach for enhancing renewable energy integration. Existing methods face several limitations: purely temporal modeling struggles to capture dynamic correlations under abrupt weather conditions, while sky-image-based approaches, though promising, suffer from insufficient inter-modal interaction exploration in current multi-modal fusion models (MFMs). Additionally, the MFMs are hindered by the high cost of all-sky imagers, limiting their widespread adoption. To address these issues, this study proposes a parallel multimodal fusion model (PMFM) and a knowledge-distilled Transformer (kdTransformer). The PMFM adopts a parallel processing architecture with modality-specific feature extraction strategies for three data: cloud images processed through deformable convolution and pyramid structure, meteorological variables modeled via convolutional operations, and irradiation sequences processed through multilayer perceptrons. The extracted features are then fused through enhanced dynamic gating mechanisms with cross-modal attention. For regions lacking cloud image, the kdTransformer transfers the multimodal knowledge learned by the PMFM to a temporal model via knowledge distillation, where the distillation loss extends the traditional distillation framework by introducing a feature alignment loss and a relational distillation loss, combined dynamically with weights. Experiments demonstrate that the PMFM achieves a 7.92% average improvement across seven metrics and two year’s datasets over the renowned Sunset baseline, while the kdTransformer achieves a 24.58% average enhancement compared to the non-distilled Transformer. Furthermore, the PMFM-kdTransformer achieves a trade-off between computational efficiency (65.1 FLOPs/G) and prediction accuracy.&lt;/p&gt;</content:encoded></item><item><title>Rethinking Infrared Small Target Detection: A Foundation-Driven Efficient Paradigm</title><link>https://arxiv.org/abs/2512.05511v1</link><guid>http://arxiv.org/abs/2512.05511v1</guid><pubDate>Fri, 05 Dec 2025 08:12:35 +0000</pubDate><dc:creator>Chuang Yu</dc:creator><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Yaokun Li</dc:creator><dc:creator>Xiujun Shu</dc:creator><dc:creator>Yuanhao Feng</dc:creator><dc:creator>Bo Wang</dc:creator><dc:creator>Yimian Dai</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework
Published: 2025-12-05T08:12:35+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuang Yu; Jinmiao Zhao; Yunpeng Liu; Yaokun Li; Xiujun Shu; Yuanhao Feng; Bo Wang; Yimian Dai; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;While large-scale visual foundation models (VFMs) exhibit strong generalization across diverse visual domains, their potential for single-frame infrared small target (SIRST) detection remains largely unexplored. To fill this gap, we systematically introduce the frozen representations from VFMs into the SIRST task for the first time and propose a Foundation-Driven Efficient Paradigm (FDEP), which can seamlessly adapt to existing encoder-decoder-based methods and significantly improve accuracy without additional inference overhead. Specifically, a Semantic Alignment Modulation Fusion (SAMF) module is designed to achieve dynamic alignment and deep fusion of the global semantic priors from VFMs with task-specific features. Meanwhile, to avoid the inference time burden introduced by VFMs, we propose a Collaborative Optimization-based Implicit Self-Distillation (CO-ISD) strategy, which enables implicit semantic transfer between the main and lightweight branches through parameter sharing and synchronized backpropagation. In addition, to unify the fragmented evaluation system, we construct a Holistic SIRST Evaluation (HSE) metric that performs multi-threshold integral evaluation at both pixel-level confidence and target-level robustness, providing a stable and comprehensive basis for fair model comparison. Extensive experiments demonstrate that the SIRST detection networks equipped with our FDEP framework achieve state-of-the-art (SOTA) performance on multiple public datasets. Our code is available at https://github.com/YuChuang1205/FDEP-Framework&lt;/p&gt;</content:encoded></item><item><title>M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration</title><link>https://doi.org/10.1109/tip.2025.3638662</link><guid>10.1109/tip.2025.3638662</guid><pubDate>Mon, 08 Dec 2025 18:42:57 +0000</pubDate><dc:creator>Yongzhen Wang</dc:creator><dc:creator>Yongjun Li</dc:creator><dc:creator>Zhuoran Zheng</dc:creator><dc:creator>Xiao-Ping Zhang</dc:creator><dc:creator>Mingqiang Wei</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3638662</prism:doi><description>Natural images are often degraded by complex, composite degradations such as rain, snow, and haze, which adversely impact downstream vision applications. While existing image restoration efforts have achieved notable success, they are still hindered by two critical challenges: limited generalization across dynamically varying degradation scenarios and a suboptimal balance between preserving local details and modeling global dependencies. To overcome these challenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based Mamba-CNN fusion framework for efficient and robust all-in-one image restoration. M2Restore introduces three key contributions: First, to boost the model’s generalization across diverse degradation conditions, we exploit a CLIP-guided MoE gating mechanism that fuses task-conditioned prompts with CLIP-derived semantic priors. This mechanism is further refined via cross-modal feature calibration, which enables precise expert selection for various degradation types. Second, to jointly capture global contextual dependencies and fine-grained local details, we design a dual-stream architecture that integrates the localized representational strength of CNNs with the long-range modeling efficiency of Mamba. This integration enables collaborative optimization of global semantic relationships and local structural fidelity, preserving global coherence while enhancing detail restoration. Third, we introduce an edge-aware dynamic gating mechanism that adaptively balances global modeling and local enhancement by reallocating computational attention to degradation-sensitive regions. This targeted focus leads to more efficient and precise restoration. Extensive experiments across multiple image restoration benchmarks validate the superiority of M2Restore in both visual quality and quantitative performance.
Published: 2025-12-08T18:42:57+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongzhen Wang; Yongjun Li; Zhuoran Zheng; Xiao-Ping Zhang; Mingqiang Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3638662"&gt;10.1109/tip.2025.3638662&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Natural images are often degraded by complex, composite degradations such as rain, snow, and haze, which adversely impact downstream vision applications. While existing image restoration efforts have achieved notable success, they are still hindered by two critical challenges: limited generalization across dynamically varying degradation scenarios and a suboptimal balance between preserving local details and modeling global dependencies. To overcome these challenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based Mamba-CNN fusion framework for efficient and robust all-in-one image restoration. M2Restore introduces three key contributions: First, to boost the model’s generalization across diverse degradation conditions, we exploit a CLIP-guided MoE gating mechanism that fuses task-conditioned prompts with CLIP-derived semantic priors. This mechanism is further refined via cross-modal feature calibration, which enables precise expert selection for various degradation types. Second, to jointly capture global contextual dependencies and fine-grained local details, we design a dual-stream architecture that integrates the localized representational strength of CNNs with the long-range modeling efficiency of Mamba. This integration enables collaborative optimization of global semantic relationships and local structural fidelity, preserving global coherence while enhancing detail restoration. Third, we introduce an edge-aware dynamic gating mechanism that adaptively balances global modeling and local enhancement by reallocating computational attention to degradation-sensitive regions. This targeted focus leads to more efficient and precise restoration. Extensive experiments across multiple image restoration benchmarks validate the superiority of M2Restore in both visual quality and quantitative performance.&lt;/p&gt;</content:encoded></item><item><title>Bridging the Gap Between Computer Vision and Bioelectrical Signal Analysis</title><link>https://doi.org/10.1016/j.inffus.2025.104047</link><guid>10.1016/j.inffus.2025.104047</guid><pubDate>Mon, 08 Dec 2025 16:44:34 +0000</pubDate><dc:creator>Yanan Wang</dc:creator><dc:creator>Shuaicong Hu</dc:creator><dc:creator>Jian Liu</dc:creator><dc:creator>Aiguo Wang</dc:creator><dc:creator>Guohui Zhou</dc:creator><dc:creator>Cuiwei Yang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104047</prism:doi><description>Bioelectrical signals are vital for non-invasive physiological monitoring and disease diagnosis. The application of artificial intelligence (AI) in automatic bioelectrical signal analysis has garnered attention. However, training supervised AI models requires abundant labeled data, which is particularly challenging in medical scenarios. Existing transfer learning (TL) approaches attempt to transfer knowledge from images in the computer vision (CV) domain, but face significant challenges due to dimensional difference and distribution gaps between CV and bioelectrical signal domains. Traditional domain adaptation (DA) assumes domain task invariance and focuses mainly on feature distribution difference, without considering label inconsistency and the complex cross-dimensional scenarios. Our motivation is to address the fundamental challenges in cross-domain knowledge transfer by jointly solving the dimensional gap and task discrepancy between CV and bioelectrical signal domains, thereby facilitating robust learning with limited labeled medical data. In this paper, we propose a cross-dimensional information transfer (CDIT) framework that enables effective information fusion through parallel encoding-decoding modules, which preserve the discriminative characteristics of both two-dimensional (2D) CV images and one-dimensional (1D) bioelectrical signals while mapping them into a shared feature space. Furthermore, we develop a cross-task DA method that synergistically integrates feature and label information to address the label inconsistency challenge in the DA phase of CDITF. We conduct extensive experiments on two representative scenarios using six databases. The experimental results demonstrate that CDITF with cross-task DA (CDITF-CTDA) achieves successful knowledge transfer from CV into bioelectrical signal domains despite their inherent difference, consistently outperforming baseline methods with improvements of 0.02–0.07 in AUC for bioelectrical signal analysis. These results demonstrate the effectiveness of CDITF-CTDA in leveraging CV knowledge for bioelectrical signal analysis under limited-labeled data medical scenarios.
Published: 2025-12-08T16:44:34+00:00
Venue: Information Fusion
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanan Wang; Shuaicong Hu; Jian Liu; Aiguo Wang; Guohui Zhou; Cuiwei Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104047"&gt;10.1016/j.inffus.2025.104047&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Bioelectrical signals are vital for non-invasive physiological monitoring and disease diagnosis. The application of artificial intelligence (AI) in automatic bioelectrical signal analysis has garnered attention. However, training supervised AI models requires abundant labeled data, which is particularly challenging in medical scenarios. Existing transfer learning (TL) approaches attempt to transfer knowledge from images in the computer vision (CV) domain, but face significant challenges due to dimensional difference and distribution gaps between CV and bioelectrical signal domains. Traditional domain adaptation (DA) assumes domain task invariance and focuses mainly on feature distribution difference, without considering label inconsistency and the complex cross-dimensional scenarios. Our motivation is to address the fundamental challenges in cross-domain knowledge transfer by jointly solving the dimensional gap and task discrepancy between CV and bioelectrical signal domains, thereby facilitating robust learning with limited labeled medical data. In this paper, we propose a cross-dimensional information transfer (CDIT) framework that enables effective information fusion through parallel encoding-decoding modules, which preserve the discriminative characteristics of both two-dimensional (2D) CV images and one-dimensional (1D) bioelectrical signals while mapping them into a shared feature space. Furthermore, we develop a cross-task DA method that synergistically integrates feature and label information to address the label inconsistency challenge in the DA phase of CDITF. We conduct extensive experiments on two representative scenarios using six databases. The experimental results demonstrate that CDITF with cross-task DA (CDITF-CTDA) achieves successful knowledge transfer from CV into bioelectrical signal domains despite their inherent difference, consistently outperforming baseline methods with improvements of 0.02–0.07 in AUC for bioelectrical signal analysis. These results demonstrate the effectiveness of CDITF-CTDA in leveraging CV knowledge for bioelectrical signal analysis under limited-labeled data medical scenarios.&lt;/p&gt;</content:encoded></item><item><title>Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models</title><link>https://arxiv.org/abs/2512.06281v1</link><guid>http://arxiv.org/abs/2512.06281v1</guid><pubDate>Sat, 06 Dec 2025 04:20:13 +0000</pubDate><dc:creator>Hengzhuang Li</dc:creator><dc:creator>Xinsong Zhang</dc:creator><dc:creator>Qiming Peng</dc:creator><dc:creator>Bin Luo</dc:creator><dc:creator>Han Hu</dc:creator><dc:creator>Dengyang Jiang</dc:creator><dc:creator>Han-Jia Ye</dc:creator><dc:creator>Teng Zhang</dc:creator><dc:creator>Hai Jin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.
Published: 2025-12-06T04:20:13+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengzhuang Li; Xinsong Zhang; Qiming Peng; Bin Luo; Han Hu; Dengyang Jiang; Han-Jia Ye; Teng Zhang; Hai Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.&lt;/p&gt;</content:encoded></item><item><title>LeAD-M3D: Leveraging Asymmetric Distillation for Real-time Monocular 3D Detection</title><link>https://arxiv.org/abs/2512.05663v1</link><guid>http://arxiv.org/abs/2512.05663v1</guid><pubDate>Fri, 05 Dec 2025 12:08:18 +0000</pubDate><dc:creator>Johannes Meier</dc:creator><dc:creator>Jonathan Michel</dc:creator><dc:creator>Oussema Dhaouadi</dc:creator><dc:creator>Yung-Hsu Yang</dc:creator><dc:creator>Christoph Reich</dc:creator><dc:creator>Zuria Bauer</dc:creator><dc:creator>Stefan Roth</dc:creator><dc:creator>Marc Pollefeys</dc:creator><dc:creator>Jacques Kaiser</dc:creator><dc:creator>Daniel Cremers</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.
Published: 2025-12-05T12:08:18+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Johannes Meier; Jonathan Michel; Oussema Dhaouadi; Yung-Hsu Yang; Christoph Reich; Zuria Bauer; Stefan Roth; Marc Pollefeys; Jacques Kaiser; Daniel Cremers&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time monocular 3D object detection remains challenging due to severe depth ambiguity, viewpoint shifts, and the high computational cost of 3D reasoning. Existing approaches either rely on LiDAR or geometric priors to compensate for missing depth, or sacrifice efficiency to achieve competitive accuracy. We introduce LeAD-M3D, a monocular 3D detector that achieves state-of-the-art accuracy and real-time inference without extra modalities. Our method is powered by three key components. Asymmetric Augmentation Denoising Distillation (A2D2) transfers geometric knowledge from a clean-image teacher to a mixup-noised student via a quality- and importance-weighted depth-feature loss, enabling stronger depth reasoning without LiDAR supervision. 3D-aware Consistent Matching (CM3D) improves prediction-to-ground truth assignment by integrating 3D MGIoU into the matching score, yielding more stable and precise supervision. Finally, Confidence-Gated 3D Inference (CGI3D) accelerates detection by restricting expensive 3D regression to top-confidence regions. Together, these components set a new Pareto frontier for monocular 3D detection: LeAD-M3D achieves state-of-the-art accuracy on KITTI and Waymo, and the best reported car AP on Rope3D, while running up to 3.6x faster than prior high-accuracy methods. Our results demonstrate that high fidelity and real-time efficiency in monocular 3D detection are simultaneously attainable - without LiDAR, stereo, or geometric assumptions.&lt;/p&gt;</content:encoded></item><item><title>RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs</title><link>https://arxiv.org/abs/2512.06392v1</link><guid>http://arxiv.org/abs/2512.06392v1</guid><pubDate>Sat, 06 Dec 2025 10:48:51 +0000</pubDate><dc:creator>Runlong Zhou</dc:creator><dc:creator>Lefan Zhang</dc:creator><dc:creator>Shang-Chen Wu</dc:creator><dc:creator>Kelvin Zou</dc:creator><dc:creator>Hanzhi Zhou</dc:creator><dc:creator>Ke Ye</dc:creator><dc:creator>Yihao Feng</dc:creator><dc:creator>Dong Yin</dc:creator><dc:creator>Alex Guillen Garcia</dc:creator><dc:creator>Dmytro Babych</dc:creator><dc:creator>Rohit Chatterjee</dc:creator><dc:creator>Matthew Hopkins</dc:creator><dc:creator>Xiang Kong</dc:creator><dc:creator>Chang Lan</dc:creator><dc:creator>Lezhi Li</dc:creator><dc:creator>Yiping Ma</dc:creator><dc:creator>Daniele Molinari</dc:creator><dc:creator>Senyu Tong</dc:creator><dc:creator>Yanchao Sun</dc:creator><dc:creator>Thomas Voice</dc:creator><dc:creator>Jianyu Wang</dc:creator><dc:creator>Chong Wang</dc:creator><dc:creator>Simon Wang</dc:creator><dc:creator>Floris Weers</dc:creator><dc:creator>Yechen Xu</dc:creator><dc:creator>Guolin Yin</dc:creator><dc:creator>Muyang Yu</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Zheng Zhou</dc:creator><dc:creator>Danyang Zhuo</dc:creator><dc:creator>Ruoming Pang</dc:creator><dc:creator>Cheng Leong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.
Published: 2025-12-06T10:48:51+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runlong Zhou; Lefan Zhang; Shang-Chen Wu; Kelvin Zou; Hanzhi Zhou; Ke Ye; Yihao Feng; Dong Yin; Alex Guillen Garcia; Dmytro Babych; Rohit Chatterjee; Matthew Hopkins; Xiang Kong; Chang Lan; Lezhi Li; Yiping Ma; Daniele Molinari; Senyu Tong; Yanchao Sun; Thomas Voice; Jianyu Wang; Chong Wang; Simon Wang; Floris Weers; Yechen Xu; Guolin Yin; Muyang Yu; Yi Zhang; Zheng Zhou; Danyang Zhuo; Ruoming Pang; Cheng Leong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B&amp;#x27;s pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.&lt;/p&gt;</content:encoded></item><item><title>Enhancing monocular height estimation via sparse LiDAR-guided correction</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.004</link><guid>10.1016/j.isprsjprs.2025.12.004</guid><pubDate>Mon, 08 Dec 2025 15:28:16 +0000</pubDate><dc:creator>Jian Song</dc:creator><dc:creator>Hongruixuan Chen</dc:creator><dc:creator>Naoto Yokoya</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.004</prism:doi><description>Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. While state-of-the-art monocular height estimation (MHE) and depth estimation (MDE) models show great promise, their robustness under varied illumination conditions remains a significant challenge. To address this, we introduce a novel and fully automated correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep learning outputs to enhance local accuracy and robustness. Importantly, the entire workflow is fully automated and built solely on publicly available models and datasets, requiring only a single georeferenced optical image to generate corrected height maps, thereby ensuring unprecedented accessibility and global scalability. Furthermore, we establish the first comprehensive benchmark for this task, evaluating a suite of correction methods that includes two random forest-based approaches, four parameter-efficient fine-tuning techniques, and full fine-tuning. We conduct extensive experiments across six large-scale, diverse regions at 0.5 m resolution, totaling approximately 297 km 2 " role="presentation"&gt; 2 2 , encompassing the urban cores of Tokyo, Paris, and São Paulo, as well as mixed suburban and forest landscapes. Experimental results demonstrate that the best-performing correction method reduces the MHE model’s mean absolute error (MAE) by an average of 30.9% and improves its F 1 HE " role="presentation"&gt; F 1 HE F 1 HE score by 44.2%. For the MDE model, the MAE is improved by 24.1% and the F 1 HE " role="presentation"&gt; F 1 HE F 1 HE score by 25.1%. These findings validate the effectiveness of our correction pipeline, demonstrating how sparse real-world LiDAR data can systematically bolster the robustness of both MHE and MDE models and paving the way for scalable, low-cost, and globally applicable 3D mapping solutions.
Published: 2025-12-08T15:28:16+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Song; Hongruixuan Chen; Naoto Yokoya&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.004"&gt;10.1016/j.isprsjprs.2025.12.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. While state-of-the-art monocular height estimation (MHE) and depth estimation (MDE) models show great promise, their robustness under varied illumination conditions remains a significant challenge. To address this, we introduce a novel and fully automated correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep learning outputs to enhance local accuracy and robustness. Importantly, the entire workflow is fully automated and built solely on publicly available models and datasets, requiring only a single georeferenced optical image to generate corrected height maps, thereby ensuring unprecedented accessibility and global scalability. Furthermore, we establish the first comprehensive benchmark for this task, evaluating a suite of correction methods that includes two random forest-based approaches, four parameter-efficient fine-tuning techniques, and full fine-tuning. We conduct extensive experiments across six large-scale, diverse regions at 0.5 m resolution, totaling approximately 297 km 2 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; 2 2 , encompassing the urban cores of Tokyo, Paris, and São Paulo, as well as mixed suburban and forest landscapes. Experimental results demonstrate that the best-performing correction method reduces the MHE model’s mean absolute error (MAE) by an average of 30.9% and improves its F 1 HE &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; F 1 HE F 1 HE score by 44.2%. For the MDE model, the MAE is improved by 24.1% and the F 1 HE &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; F 1 HE F 1 HE score by 25.1%. These findings validate the effectiveness of our correction pipeline, demonstrating how sparse real-world LiDAR data can systematically bolster the robustness of both MHE and MDE models and paving the way for scalable, low-cost, and globally applicable 3D mapping solutions.&lt;/p&gt;</content:encoded></item><item><title>LightSearcher: Efficient DeepSearch via Experiential Memory</title><link>https://arxiv.org/abs/2512.06653v1</link><guid>http://arxiv.org/abs/2512.06653v1</guid><pubDate>Sun, 07 Dec 2025 04:29:52 +0000</pubDate><dc:creator>Hengzhi Lan</dc:creator><dc:creator>Yue Yu</dc:creator><dc:creator>Li Qian</dc:creator><dc:creator>Li Peng</dc:creator><dc:creator>Jie Wu</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Jian Luan</dc:creator><dc:creator>Ting Bai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.
Published: 2025-12-07T04:29:52+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengzhi Lan; Yue Yu; Li Qian; Li Peng; Jie Wu; Wei Liu; Jian Luan; Ting Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;DeepSearch paradigms have become a core enabler for deep reasoning models, allowing them to invoke external search tools to access up-to-date, domain-specific knowledge beyond parametric boundaries, thereby enhancing the depth and factual reliability of reasoning. Building upon this foundation, recent advances in reinforcement learning (RL) have further empowered models to autonomously and strategically control search tool usage, optimizing when and how to query external knowledge sources. Yet, these RL-driven DeepSearch systems often reveal a see-saw trade-off between accuracy and efficiency-frequent tool invocations can improve factual correctness but lead to unnecessary computational overhead and diminished efficiency. To address this challenge, we propose LightSearcher, an efficient RL framework that incorporates textual experiential memory by learning contrastive reasoning trajectories to generate interpretable summaries of successful reasoning patterns. In addition, it employs an adaptive reward shaping mechanism that penalizes redundant tool calls only in correct-answer scenarios. This design effectively balances the inherent accuracy-efficiency trade-off in DeepSearch paradigms. Experiments on four multi-hop QA benchmarks show that LightSearcher maintains accuracy comparable to SOTA baseline ReSearch, while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%, demonstrating its superior efficiency.&lt;/p&gt;</content:encoded></item><item><title>SegAssess: Panoramic quality mapping for robust and transferable unsupervised segmentation assessment</title><link>https://doi.org/10.1109/tgrs.2025.3641753</link><guid>10.1109/tgrs.2025.3641753</guid><pubDate>Mon, 08 Dec 2025 18:40:17 +0000</pubDate><dc:creator>Bingnan Yang</dc:creator><dc:creator>Mi Zhang</dc:creator><dc:creator>Zhili Zhang</dc:creator><dc:creator>Zhan Zhang</dc:creator><dc:creator>Yuanxin Zhao</dc:creator><dc:creator>Xiangyun Hu</dc:creator><dc:creator>Jianya Gong</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3641753</prism:doi><description>High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at https://github.com/Yangbn97/SegAssess.
Published: 2025-12-08T18:40:17+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bingnan Yang; Mi Zhang; Zhili Zhang; Zhan Zhang; Yuanxin Zhao; Xiangyun Hu; Jianya Gong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3641753"&gt;10.1109/tgrs.2025.3641753&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;High-quality image segmentation is fundamental to pixel-level geospatial analysis in remote sensing, necessitating robust segmentation quality assessment (SQA), particularly in unsupervised settings lacking ground truth. Although recent deep learning (DL) based unsupervised SQA methods show potential, they often suffer from coarse evaluation granularity, incomplete assessments, and poor transferability. To overcome these limitations, this paper introduces Panoramic Quality Mapping (PQM) as a new paradigm for comprehensive, pixel-wise SQA, and presents SegAssess, a novel deep learning framework realizing this approach. SegAssess distinctively formulates SQA as a fine-grained, four-class panoramic segmentation task, classifying pixels within a segmentation mask under evaluation into true positive (TP), false positive (FP), true negative (TN), and false negative (FN) categories, thereby generating a complete quality map. Leveraging an enhanced Segment Anything Model (SAM) architecture, SegAssess uniquely employs the input mask as a prompt for effective feature integration via cross-attention. Key innovations include an Edge Guided Compaction (EGC) branch with an Aggregated Semantic Filter (ASF) module to refine predictions near challenging object edges, and an Augmented Mixup Sampling (AMS) training strategy integrating multi-source masks to significantly boost cross-domain robustness and zero-shot transferability. Comprehensive experiments demonstrate that SegAssess achieves state-of-the-art (SOTA) performance and exhibits remarkable zero-shot transferability to unseen masks. The code is available at https://github.com/Yangbn97/SegAssess.&lt;/p&gt;</content:encoded></item><item><title>Hyperspectral Anomaly Detection via Hybrid Convolutional and Transformer-Based U-Net With Error Attention Mechanism</title><link>https://doi.org/10.1109/tnnls.2025.3634765</link><guid>10.1109/tnnls.2025.3634765</guid><pubDate>Mon, 08 Dec 2025 18:41:27 +0000</pubDate><dc:creator>Xiaoyi Wang</dc:creator><dc:creator>Peng Wang</dc:creator><dc:creator>Juan Cheng</dc:creator><dc:creator>Daiyin Zhu</dc:creator><dc:creator>Henry Leung</dc:creator><dc:creator>Paolo Gamba</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3634765</prism:doi><description>Hyperspectral anomaly detection is a crucial technique for recognizing abnormal pixels in hyperspectral images (HSIs), that is, those with distinct spectral characteristics from those of the surrounding background. Traditional methods always fall short in effectively leveraging the information regarding the spectral and spatial aspects of the dataset simultaneously, limiting their detection performances. This article proposes a novel framework using U-Net, termed hybrid convolution and transformer-based U-Net (HCT-Unet), which integrates convolution with a multihead attention mechanism in Transformer for enhanced hyperspectral anomaly detection. To ensure a more comprehensive understanding of spatial and spectral interactions, the HCT-Unet architecture capitalizes on the strengths of local feature extraction of convolutional layers and the capabilities of the long-range dependency modeling of Transformers. A key innovation of this framework is an error attention mechanism, which facilitates adaptive multiscale feature fusion and enhances the feature representation capacity. Furthermore, a new anomaly score calculation method is proposed, which combines reconstruction error with the pixelwise structural similarity index (SSIM) to determine pixel anomaly from both local structural preservation and global spectral consistency perspectives. Experiments carried out on seven different hyperspectral datasets reveal that the proposed method consistently outperforms the widely accepted state-of-the-art methods in hyperspectral anomaly detection.
Published: 2025-12-08T18:41:27+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyi Wang; Peng Wang; Juan Cheng; Daiyin Zhu; Henry Leung; Paolo Gamba&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3634765"&gt;10.1109/tnnls.2025.3634765&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral anomaly detection is a crucial technique for recognizing abnormal pixels in hyperspectral images (HSIs), that is, those with distinct spectral characteristics from those of the surrounding background. Traditional methods always fall short in effectively leveraging the information regarding the spectral and spatial aspects of the dataset simultaneously, limiting their detection performances. This article proposes a novel framework using U-Net, termed hybrid convolution and transformer-based U-Net (HCT-Unet), which integrates convolution with a multihead attention mechanism in Transformer for enhanced hyperspectral anomaly detection. To ensure a more comprehensive understanding of spatial and spectral interactions, the HCT-Unet architecture capitalizes on the strengths of local feature extraction of convolutional layers and the capabilities of the long-range dependency modeling of Transformers. A key innovation of this framework is an error attention mechanism, which facilitates adaptive multiscale feature fusion and enhances the feature representation capacity. Furthermore, a new anomaly score calculation method is proposed, which combines reconstruction error with the pixelwise structural similarity index (SSIM) to determine pixel anomaly from both local structural preservation and global spectral consistency perspectives. Experiments carried out on seven different hyperspectral datasets reveal that the proposed method consistently outperforms the widely accepted state-of-the-art methods in hyperspectral anomaly detection.&lt;/p&gt;</content:encoded></item><item><title>DaGRPO: Rectifying Gradient Conflict in Reasoning via Distinctiveness-Aware Group Relative Policy Optimization</title><link>https://arxiv.org/abs/2512.06337v1</link><guid>http://arxiv.org/abs/2512.06337v1</guid><pubDate>Sat, 06 Dec 2025 07:51:36 +0000</pubDate><dc:creator>Xuan Xie</dc:creator><dc:creator>Xuan Wang</dc:creator><dc:creator>Wenjie Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.
Published: 2025-12-06T07:51:36+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuan Xie; Xuan Wang; Wenjie Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;The evolution of Large Language Models (LLMs) has catalyzed a paradigm shift from superficial instruction following to rigorous long-horizon reasoning. While Group Relative Policy Optimization (GRPO) has emerged as a pivotal mechanism for eliciting such post-training reasoning capabilities due to its exceptional performance, it remains plagued by significant training instability and poor sample efficiency. We theoretically identify the root cause of these issues as the lack of distinctiveness within on-policy rollouts: for routine queries, highly homogeneous samples induce destructive gradient conflicts; whereas for hard queries, the scarcity of valid positive samples results in ineffective optimization. To bridge this gap, we propose Distinctiveness-aware Group Relative Policy Optimization (DaGRPO). DaGRPO incorporates two core mechanisms: (1) Sequence-level Gradient Rectification, which utilizes fine-grained scoring to dynamically mask sample pairs with low distinctiveness, thereby eradicating gradient conflicts at the source; and (2) Off-policy Data Augmentation, which introduces high-quality anchors to recover training signals for challenging tasks. Extensive experiments across 9 mathematical reasoning and out-of-distribution (OOD) generalization benchmarks demonstrate that DaGRPO significantly surpasses existing SFT, GRPO, and hybrid baselines, achieving new state-of-the-art performance (e.g., a +4.7% average accuracy gain on math benchmarks). Furthermore, in-depth analysis confirms that DaGRPO effectively mitigates gradient explosion and accelerates the emergence of long-chain reasoning capabilities.&lt;/p&gt;</content:encoded></item><item><title>Source-Free Domain Adaptation via Multimodal Space-Guided Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112827</link><guid>10.1016/j.patcog.2025.112827</guid><pubDate>Sun, 07 Dec 2025 15:13:39 +0000</pubDate><dc:creator>Lijuan Chen</dc:creator><dc:creator>Yunxiang Bai</dc:creator><dc:creator>Ying Hu</dc:creator><dc:creator>Qiong Wang</dc:creator><dc:creator>Xiaozhi Qi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112827</prism:doi><description>Conventional UDA requires access to the source domain, invalidating it in the information security and privacy protection scenarios. In contrast, Source-free Domain Adaptation (SFDA) involves transferring a pre-trained source model to an unlabeled target domain while the source data is absent. However, prior methods based on self-supervised learning have struggled to find a quality domain invariant representation space due to the lack of source data. To address this challenge, in this work, we propose leveraging the success of vision-language pre-trained (ViL) models (e.g., CLIP). To integrate the domain generality of the ViL model and the task specificity of source model more effectively, we introduce a novel M ulti M odal Space- G uided A lignment ( MMGA ) approach. Specifically, we start with a multimodal feature calibration for achieving coarse alignment between the target visual domain and the multimodal space. However, this ViL space is still not the domain invariant space, being trained on a large number of samples. To achieve further fine-grained alignment towards the domain invariant space, we have designed two methods: Potential category consistency and prediction consistency alignment. These methods push the potential categories distribution and the prediction distribution closer to the fused pseudo-supervision by the ViL model and the adapted source model, respectively. This strategy corrects the errors of feature alignment to the ViL space. Extensive experiments show that our MMGA approach significantly outperforms current state-of-the-art alternatives. The code and data are available at https://github.com/YunxiangBai0/MMGA/
Published: 2025-12-07T15:13:39+00:00
Venue: Pattern Recognition
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lijuan Chen; Yunxiang Bai; Ying Hu; Qiong Wang; Xiaozhi Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112827"&gt;10.1016/j.patcog.2025.112827&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Conventional UDA requires access to the source domain, invalidating it in the information security and privacy protection scenarios. In contrast, Source-free Domain Adaptation (SFDA) involves transferring a pre-trained source model to an unlabeled target domain while the source data is absent. However, prior methods based on self-supervised learning have struggled to find a quality domain invariant representation space due to the lack of source data. To address this challenge, in this work, we propose leveraging the success of vision-language pre-trained (ViL) models (e.g., CLIP). To integrate the domain generality of the ViL model and the task specificity of source model more effectively, we introduce a novel M ulti M odal Space- G uided A lignment ( MMGA ) approach. Specifically, we start with a multimodal feature calibration for achieving coarse alignment between the target visual domain and the multimodal space. However, this ViL space is still not the domain invariant space, being trained on a large number of samples. To achieve further fine-grained alignment towards the domain invariant space, we have designed two methods: Potential category consistency and prediction consistency alignment. These methods push the potential categories distribution and the prediction distribution closer to the fused pseudo-supervision by the ViL model and the adapted source model, respectively. This strategy corrects the errors of feature alignment to the ViL space. Extensive experiments show that our MMGA approach significantly outperforms current state-of-the-art alternatives. The code and data are available at https://github.com/YunxiangBai0/MMGA/&lt;/p&gt;</content:encoded></item><item><title>Classification and Feature Extraction Algorithm for On-Orbit Spacecraft Based on Optical-SAR Multimodal Information</title><link>https://doi.org/10.1109/taes.2025.3641558</link><guid>10.1109/taes.2025.3641558</guid><pubDate>Mon, 08 Dec 2025 18:42:43 +0000</pubDate><dc:creator>Bijiang Jie</dc:creator><dc:creator>Baojiang Li</dc:creator><dc:creator>Manliang Cao</dc:creator><dc:creator>Jiahao Chen</dc:creator><dc:creator>Xing Gui</dc:creator><dc:creator>Chenhan Zhang</dc:creator><dc:creator>Yucan Zhang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3641558</prism:doi><description>On-orbit capture, servicing, and assembly require multimodal perception modules that remain robust under illumination changes, speckle noise, and large attitude variations. We propose a Multimodal Synergistic Transformer Framework (MSTF) that jointly processes optical and Synthetic Aperture Radar (SAR) imagery for spacecraft classification and component-level segmentation. A two-dimensional discrete wavelet transform decomposes three-channel optical images into 12 sub-bands and single-channel SAR images into 4 sub-bands, explicitly separating low-frequency structure from high-frequency detail. Each sub-band is encoded by an enhanced ResNet-50, and a cross-modal multi-head self-attention module fuses heterogeneous semantics into a shared multimodal backbone. Parallel heads then perform classification and segmentation simultaneously. Experiments on public benchmarks and our MS-CD dataset demonstrate that multimodal fusion with MSTF outperforms state-of-the-art unimodal and multimodal methods by ∼1.5% in classification accuracy and 1.15% in mean Intersection-over-Union (mIoU). The results indicate that MSTF is a reliable multimodal approach for spacecraft classification and component-level segmentation under varying illumination and speckle conditions.
Published: 2025-12-08T18:42:43+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bijiang Jie; Baojiang Li; Manliang Cao; Jiahao Chen; Xing Gui; Chenhan Zhang; Yucan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3641558"&gt;10.1109/taes.2025.3641558&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;On-orbit capture, servicing, and assembly require multimodal perception modules that remain robust under illumination changes, speckle noise, and large attitude variations. We propose a Multimodal Synergistic Transformer Framework (MSTF) that jointly processes optical and Synthetic Aperture Radar (SAR) imagery for spacecraft classification and component-level segmentation. A two-dimensional discrete wavelet transform decomposes three-channel optical images into 12 sub-bands and single-channel SAR images into 4 sub-bands, explicitly separating low-frequency structure from high-frequency detail. Each sub-band is encoded by an enhanced ResNet-50, and a cross-modal multi-head self-attention module fuses heterogeneous semantics into a shared multimodal backbone. Parallel heads then perform classification and segmentation simultaneously. Experiments on public benchmarks and our MS-CD dataset demonstrate that multimodal fusion with MSTF outperforms state-of-the-art unimodal and multimodal methods by ∼1.5% in classification accuracy and 1.15% in mean Intersection-over-Union (mIoU). The results indicate that MSTF is a reliable multimodal approach for spacecraft classification and component-level segmentation under varying illumination and speckle conditions.&lt;/p&gt;</content:encoded></item><item><title>SPOD-YOLO: A Modular Approach for Small and Oriented Aircraft Detection in Satellite Remote Sensing Imagery</title><link>https://doi.org/10.3390/rs17243963</link><guid>10.3390/rs17243963</guid><pubDate>Mon, 08 Dec 2025 14:52:24 +0000</pubDate><dc:creator>Jiajian Chen</dc:creator><dc:creator>Pengyu Guo</dc:creator><dc:creator>Yong Liu</dc:creator><dc:creator>Lu Cao</dc:creator><dc:creator>Dechao Ran</dc:creator><dc:creator>Kai Wang</dc:creator><dc:creator>Wei Hu</dc:creator><dc:creator>Liyang Wan</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs17243963</prism:doi><description>The accurate detection of small, densely packed and arbitrarily oriented aircraft in high-resolution remote sensing imagery remains highly challenging due to significant variations in object scale, orientation and background complexity. Existing detection frameworks often struggle with insufficient representation of small objects, instability of rotated bounding box regression and inability to adapt to complex background. To address these limitations, we propose SPOD-YOLO, a novel detection framework specifically designed for small aircraft in remote sensing images. This method is based on YOLOv11, combined with the feature attention mechanism of swintransformer, through targeted improvements on cross-scale feature modelling, dynamic convolutional adaptation, and rotational geometry optimization to achieve effective detection. Additionally, we have constructed a new dataset based on satellite remote sensing images, which has high density of small aircraft with rotated bounding box annotations to provide more realistic and challenging evaluation settings. Extensive experiments on MAR20, UCAS-AOD and the constructed dataset demonstrate that our method achieves consistent performance gains over state-of-the-art approaches. SPOD-YOLO achieves an 4.54% increase in mAP50 and a 11.78% gain in mAP50:95 with only 3.77 million parameters on the constructed dataset. These results validate the effectiveness and robustness of our approach in complex remote sensing scenarios, offering a practical advancement for the detection of small objects in aerospace imagery.
Published: 2025-12-08T14:52:24+00:00
Venue: Remote Sensing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiajian Chen; Pengyu Guo; Yong Liu; Lu Cao; Dechao Ran; Kai Wang; Wei Hu; Liyang Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs17243963"&gt;10.3390/rs17243963&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;The accurate detection of small, densely packed and arbitrarily oriented aircraft in high-resolution remote sensing imagery remains highly challenging due to significant variations in object scale, orientation and background complexity. Existing detection frameworks often struggle with insufficient representation of small objects, instability of rotated bounding box regression and inability to adapt to complex background. To address these limitations, we propose SPOD-YOLO, a novel detection framework specifically designed for small aircraft in remote sensing images. This method is based on YOLOv11, combined with the feature attention mechanism of swintransformer, through targeted improvements on cross-scale feature modelling, dynamic convolutional adaptation, and rotational geometry optimization to achieve effective detection. Additionally, we have constructed a new dataset based on satellite remote sensing images, which has high density of small aircraft with rotated bounding box annotations to provide more realistic and challenging evaluation settings. Extensive experiments on MAR20, UCAS-AOD and the constructed dataset demonstrate that our method achieves consistent performance gains over state-of-the-art approaches. SPOD-YOLO achieves an 4.54% increase in mAP50 and a 11.78% gain in mAP50:95 with only 3.77 million parameters on the constructed dataset. These results validate the effectiveness and robustness of our approach in complex remote sensing scenarios, offering a practical advancement for the detection of small objects in aerospace imagery.&lt;/p&gt;</content:encoded></item><item><title>KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity</title><link>https://arxiv.org/abs/2512.05916v1</link><guid>http://arxiv.org/abs/2512.05916v1</guid><pubDate>Fri, 05 Dec 2025 17:51:10 +0000</pubDate><dc:creator>Damien Lesens</dc:creator><dc:creator>Beheshteh T. Rakhshan</dc:creator><dc:creator>Guillaume Rabusseau</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.
Published: 2025-12-05T17:51:10+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Damien Lesens; Beheshteh T. Rakhshan; Guillaume Rabusseau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.&lt;/p&gt;</content:encoded></item><item><title>Representation Learning for Point Cloud Understanding</title><link>https://arxiv.org/abs/2512.06058v1</link><guid>http://arxiv.org/abs/2512.06058v1</guid><pubDate>Fri, 05 Dec 2025 17:09:49 +0000</pubDate><dc:creator>Siming Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.
Published: 2025-12-05T17:09:49+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siming Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.&lt;/p&gt;</content:encoded></item><item><title>Optimizing Optimizers for Fast Gradient-Based Learning</title><link>https://arxiv.org/abs/2512.06370v1</link><guid>http://arxiv.org/abs/2512.06370v1</guid><pubDate>Sat, 06 Dec 2025 09:50:41 +0000</pubDate><dc:creator>Jaerin Lee</dc:creator><dc:creator>Kyoung Mu Lee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.
Published: 2025-12-06T09:50:41+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jaerin Lee; Kyoung Mu Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.&lt;/p&gt;</content:encoded></item><item><title>A Systematic Review of Skeleton-Based Action Recognition: Methods, Challenges, and Future Directions</title><link>https://doi.org/10.1109/tnnls.2025.3632689</link><guid>10.1109/tnnls.2025.3632689</guid><pubDate>Mon, 08 Dec 2025 18:41:27 +0000</pubDate><dc:creator>Yi Liu</dc:creator><dc:creator>Ruyi Liu</dc:creator><dc:creator>Yuzhi Hu</dc:creator><dc:creator>Mengyao Wu</dc:creator><dc:creator>Wentian Xin</dc:creator><dc:creator>Qiguang Miao</dc:creator><dc:creator>Shuai Wu</dc:creator><dc:creator>Long Li</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3632689</prism:doi><description>Human action recognition (HAR), which aims to recognize and understand individual actions and intentions, has rapidly become a research hotspot in computer vision. Compared with other data modalities, skeleton data offers more efficient node semantics and more coherent spatio-temporal motion patterns, effectively reducing the impact of lighting and background changes. In recent years, many researchers have focused on skeleton-based action recognition methods and have made significant progress. However, we believe that the current skeleton-based action recognition methods still face three major challenges: 1) reducing reliance on expensive labeled data while maintaining model performance; 2) enabling the model to understand and recognize new behavior classes with a limited number of samples; and 3) addressing the challenges posed by the lack of skeleton information in single-modality spatio-temporal motion representation learning. Based on these challenges, we conduct a comprehensive review of the existing skeleton-based action recognition methods. Additionally, we provide an extensive review and analysis of publicly available action recognition datasets. This review aims to offer researchers a comprehensive perspective, stimulate more innovative ideas, and promote the application and breakthrough of skeleton action recognition in a wider range of computer vision tasks.
Published: 2025-12-08T18:41:27+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Liu; Ruyi Liu; Yuzhi Hu; Mengyao Wu; Wentian Xin; Qiguang Miao; Shuai Wu; Long Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3632689"&gt;10.1109/tnnls.2025.3632689&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Human action recognition (HAR), which aims to recognize and understand individual actions and intentions, has rapidly become a research hotspot in computer vision. Compared with other data modalities, skeleton data offers more efficient node semantics and more coherent spatio-temporal motion patterns, effectively reducing the impact of lighting and background changes. In recent years, many researchers have focused on skeleton-based action recognition methods and have made significant progress. However, we believe that the current skeleton-based action recognition methods still face three major challenges: 1) reducing reliance on expensive labeled data while maintaining model performance; 2) enabling the model to understand and recognize new behavior classes with a limited number of samples; and 3) addressing the challenges posed by the lack of skeleton information in single-modality spatio-temporal motion representation learning. Based on these challenges, we conduct a comprehensive review of the existing skeleton-based action recognition methods. Additionally, we provide an extensive review and analysis of publicly available action recognition datasets. This review aims to offer researchers a comprehensive perspective, stimulate more innovative ideas, and promote the application and breakthrough of skeleton action recognition in a wider range of computer vision tasks.&lt;/p&gt;</content:encoded></item><item><title>MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models</title><link>https://arxiv.org/abs/2512.05530v1</link><guid>http://arxiv.org/abs/2512.05530v1</guid><pubDate>Fri, 05 Dec 2025 08:41:44 +0000</pubDate><dc:creator>Chuang Yu</dc:creator><dc:creator>Jinmiao Zhao</dc:creator><dc:creator>Mingxuan Zhao</dc:creator><dc:creator>Yunpeng Liu</dc:creator><dc:creator>Xiujun Shu</dc:creator><dc:creator>Yuanhao Feng</dc:creator><dc:creator>Bo Wang</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of "Understand -&gt; Rethink -&gt; Correct", and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND
Published: 2025-12-05T08:41:44+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuang Yu; Jinmiao Zhao; Mingxuan Zhao; Yunpeng Liu; Xiujun Shu; Yuanhao Feng; Bo Wang; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, multimodal large language models (MLLMs) have been widely applied to reasoning tasks. However, they suffer from limited multi-rationale semantic modeling, insufficient logical robustness, and are susceptible to misleading interpretations in complex scenarios. Therefore, we propose a Multi-rationale INtegrated Discriminative (MIND) reasoning framework, which is designed to endow MLLMs with human-like cognitive abilities of &amp;quot;Understand -&amp;gt; Rethink -&amp;gt; Correct&amp;quot;, and achieves a paradigm evolution from passive imitation-based reasoning to active discriminative reasoning. Specifically, we introduce a Rationale Augmentation and Discrimination (RAD) paradigm, which automatically and efficiently expands existing datasets by generating diverse rationales, providing a unified and extensible data foundation. Meanwhile, we design a Progressive Two-stage Correction Learning (P2CL) strategy. The first phase enhances multi-rationale positive learning, while the second phase enables active logic discrimination and correction. In addition, to mitigate representation entanglement in the multi-rationale semantic space, we propose a Multi-rationale Contrastive Alignment (MCA) optimization strategy, which achieves semantic aggregation of correct reasoning and boundary separation of incorrect reasoning. Extensive experiments demonstrate that the proposed MIND reasoning framework achieves state-of-the-art (SOTA) performance on multiple public datasets covering scientific, commonsense, and mathematical scenarios. It provides a new perspective for advancing MLLMs towards higher levels of cognitive intelligence. Our code is available at https://github.com/YuChuang1205/MIND&lt;/p&gt;</content:encoded></item><item><title>VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors</title><link>https://arxiv.org/abs/2512.06759v1</link><guid>http://arxiv.org/abs/2512.06759v1</guid><pubDate>Sun, 07 Dec 2025 09:48:10 +0000</pubDate><dc:creator>Wenbo Lyu</dc:creator><dc:creator>Yingjun Du</dc:creator><dc:creator>Jinglin Zhao</dc:creator><dc:creator>Xianton Zhen</dc:creator><dc:creator>Ling Shao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench
Published: 2025-12-07T09:48:10+00:00
Venue: arXiv
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenbo Lyu; Yingjun Du; Jinglin Zhao; Xianton Zhen; Ling Shao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs&amp;#x27; ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench&lt;/p&gt;</content:encoded></item><item><title>ULDGN: Uncertainty-Aware Language-Guided Domain Generalization Network for Cross-Scene Hyperspectral Image Classification</title><link>https://doi.org/10.1016/j.patcog.2025.112839</link><guid>10.1016/j.patcog.2025.112839</guid><pubDate>Mon, 08 Dec 2025 16:00:55 +0000</pubDate><dc:creator>Xiaomin Liu</dc:creator><dc:creator>Tianyang Duan</dc:creator><dc:creator>Haoyu Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112839</prism:doi><description>The strong demand for target domain samples and the difficulty in obtaining labeled hyperspectral images (HSIs) make cross-scene HSI classification based on domain generalization a hot topic. Currently, domain generalization methods usually focus on generating subdomains to expand the source domain distribution. Most of these methods lack effective constraints on source domain expansion, which leads to dual challenges in the diversity and authenticity of the generated subdomains. To address this challenge, this paper proposes an uncertainty-aware language-guided domain generalization network, which takes into account the diversity and authenticity of subdomains through divergent sample mining and language prior guidance. Specifically, first, based on uncertainty aware, divergent samples are selected and the potential effective diversity information in the source domain is fully mined to ensure that the generated subdomains can expand the distribution of the source domain and achieve full coverage of the potential target domain. Second, the class language description prior is introduced as guiding information to standardize the feature distribution of the subdomain in the feature space to ensure its authenticity. Finally, by establishing a contribution decision balance mechanism, the diversity features and authenticity features are dynamically balanced during the subdomain generation process to prevent the model from: On the one hand, overly focusing on authenticity features and falling into “learning laziness”. On the other hand, over-focusing on diversity features and falling into “learning anxiety”. Experimental results on multiple cross-scene HSI datasets show that the proposed method achieves state-of-the-art performance.
Published: 2025-12-08T16:00:55+00:00
Venue: Pattern Recognition
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaomin Liu; Tianyang Duan; Haoyu Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112839"&gt;10.1016/j.patcog.2025.112839&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;The strong demand for target domain samples and the difficulty in obtaining labeled hyperspectral images (HSIs) make cross-scene HSI classification based on domain generalization a hot topic. Currently, domain generalization methods usually focus on generating subdomains to expand the source domain distribution. Most of these methods lack effective constraints on source domain expansion, which leads to dual challenges in the diversity and authenticity of the generated subdomains. To address this challenge, this paper proposes an uncertainty-aware language-guided domain generalization network, which takes into account the diversity and authenticity of subdomains through divergent sample mining and language prior guidance. Specifically, first, based on uncertainty aware, divergent samples are selected and the potential effective diversity information in the source domain is fully mined to ensure that the generated subdomains can expand the distribution of the source domain and achieve full coverage of the potential target domain. Second, the class language description prior is introduced as guiding information to standardize the feature distribution of the subdomain in the feature space to ensure its authenticity. Finally, by establishing a contribution decision balance mechanism, the diversity features and authenticity features are dynamically balanced during the subdomain generation process to prevent the model from: On the one hand, overly focusing on authenticity features and falling into “learning laziness”. On the other hand, over-focusing on diversity features and falling into “learning anxiety”. Experimental results on multiple cross-scene HSI datasets show that the proposed method achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Finder, Evaluator, Explainer, Generator (FEEG): A Bloom’s Taxonomy-Based Query Classification Framework for LLMs and Generative AI</title><link>https://doi.org/10.1016/j.eswa.2025.130755</link><guid>10.1016/j.eswa.2025.130755</guid><pubDate>Sun, 07 Dec 2025 23:05:07 +0000</pubDate><dc:creator>Jim Samuel</dc:creator><dc:creator>Meng Ye</dc:creator><dc:creator>Tanya Khanna</dc:creator><dc:creator>Yi Yao</dc:creator><dc:creator>Xiao Lin</dc:creator><dc:creator>Rick Anderson</dc:creator><dc:creator>Anish Gupta</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130755</prism:doi><description>Generative artificial intelligence (AI) and natural language processing (NLP) applications, particularly those utilizing large language models (LLMs) with Retrieval-Augmented Generation (RAG), fine-tuning and their variations, are receiving widespread attention. However, despite their promise, the dependability of LLM applications remains uncertain. This study introduces a new dimension to improving LLM performance through systematic query classification. A taxonomy-based approach was developed to categorize queries by their likelihood of eliciting accurate responses, from those likely to succeed to those prone to failure. Bloom’s Taxonomy (BT) provided the conceptual direction for assessing and classifying queries using a curated dataset of question-answer pairs mapped to BT levels to evaluate RAG-LLM performance across categories. Building on insights from BT-based analysis, an innovative ‘Taxonomical Query Classifier’ (TQC) framework for LLMs is introduced: Finder (F), Evaluator (Ev), Explainer (Ex), and Generator (G) - collectively termed FEEG. FEEG-TQC classifies queries to improve predictability, accuracy, and enhance human-AI interaction. It differentiates queries based on intent, whether they (F) seek factual information, (Ev) evaluate content, (Ex) explain concepts, or (G) generate new content. By developing the concept of query category variations, this research provides a systematic approach to managing query quality and predictive estimations of LLM response accuracy. The findings contribute to the development of more effective human-AI interaction opportunities for LLM-based Gen AI systems, emphasizing query classification as an important factor in improving Gen AI systems.
Published: 2025-12-07T23:05:07+00:00
Venue: Expert Systems with Applications
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jim Samuel; Meng Ye; Tanya Khanna; Yi Yao; Xiao Lin; Rick Anderson; Anish Gupta&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130755"&gt;10.1016/j.eswa.2025.130755&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Generative artificial intelligence (AI) and natural language processing (NLP) applications, particularly those utilizing large language models (LLMs) with Retrieval-Augmented Generation (RAG), fine-tuning and their variations, are receiving widespread attention. However, despite their promise, the dependability of LLM applications remains uncertain. This study introduces a new dimension to improving LLM performance through systematic query classification. A taxonomy-based approach was developed to categorize queries by their likelihood of eliciting accurate responses, from those likely to succeed to those prone to failure. Bloom’s Taxonomy (BT) provided the conceptual direction for assessing and classifying queries using a curated dataset of question-answer pairs mapped to BT levels to evaluate RAG-LLM performance across categories. Building on insights from BT-based analysis, an innovative ‘Taxonomical Query Classifier’ (TQC) framework for LLMs is introduced: Finder (F), Evaluator (Ev), Explainer (Ex), and Generator (G) - collectively termed FEEG. FEEG-TQC classifies queries to improve predictability, accuracy, and enhance human-AI interaction. It differentiates queries based on intent, whether they (F) seek factual information, (Ev) evaluate content, (Ex) explain concepts, or (G) generate new content. By developing the concept of query category variations, this research provides a systematic approach to managing query quality and predictive estimations of LLM response accuracy. The findings contribute to the development of more effective human-AI interaction opportunities for LLM-based Gen AI systems, emphasizing query classification as an important factor in improving Gen AI systems.&lt;/p&gt;</content:encoded></item><item><title>How Is Language Intelligence Evolving? A Multi-Dimensional Survey of Large Language Models</title><link>https://doi.org/10.1016/j.eswa.2025.130637</link><guid>10.1016/j.eswa.2025.130637</guid><pubDate>Mon, 08 Dec 2025 16:06:34 +0000</pubDate><dc:creator>Xiaojin Chen</dc:creator><dc:creator>Li Zhou</dc:creator><dc:creator>Jing Chen</dc:creator><dc:creator>Guoyi Wang</dc:creator><dc:creator>Xinxing Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130637</prism:doi><description>Large language models (LLMs) have moved from experimental systems to widely deployed components in numerous applications. Their behavior depends not only on model architecture but also on data curation, training and alignment, inference-time optimization, deployment patterns, and governance. Adopting a lifecycle-oriented perspective, this survey integrates these aspects into a unified framework. We first review the evolution from rule-based and statistical methods to pretrained and aligned Transformer models, covering representative English- and Chinese-language LLM families. We then analyze core architectural choices, including attention, positional encodings, normalization, feedforward design, and sparse experts, to clarify their impact on scaling behavior, expressivity, and efficiency. We also compare encoder and decoder paradigms in terms of capacity, efficiency, and the handling of long contexts. Next, we systematize the training and inference stack, from data engineering and multi-stage optimization to scaling mechanisms and serving techniques such as retrieval augmentation, tool use, KV caching, quantization, and speculative decoding. Building on this foundation, we summarize applications in knowledge-centric domains such as finance, law, and education, as well as in safety-critical and physical-world settings including healthcare, agriculture, robotics, UAV operations, and cybersecurity. For each class of application, we outline typical system architectures and discuss task-specific evaluation signals. Finally, we present a risk taxonomy that spans hallucinations, reasoning failures, biases, privacy leakage, compute and environmental costs, explainability, knowledge staleness, and evaluation gaps, and we relate these risks to technical mitigations and emerging governance frameworks. Collectively, the survey connects model design, training and inference pipelines, deployment practices, and risk management, offering a structured reference for research and engineering with LLMs.
Published: 2025-12-08T16:06:34+00:00
Venue: Expert Systems with Applications
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaojin Chen; Li Zhou; Jing Chen; Guoyi Wang; Xinxing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130637"&gt;10.1016/j.eswa.2025.130637&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) have moved from experimental systems to widely deployed components in numerous applications. Their behavior depends not only on model architecture but also on data curation, training and alignment, inference-time optimization, deployment patterns, and governance. Adopting a lifecycle-oriented perspective, this survey integrates these aspects into a unified framework. We first review the evolution from rule-based and statistical methods to pretrained and aligned Transformer models, covering representative English- and Chinese-language LLM families. We then analyze core architectural choices, including attention, positional encodings, normalization, feedforward design, and sparse experts, to clarify their impact on scaling behavior, expressivity, and efficiency. We also compare encoder and decoder paradigms in terms of capacity, efficiency, and the handling of long contexts. Next, we systematize the training and inference stack, from data engineering and multi-stage optimization to scaling mechanisms and serving techniques such as retrieval augmentation, tool use, KV caching, quantization, and speculative decoding. Building on this foundation, we summarize applications in knowledge-centric domains such as finance, law, and education, as well as in safety-critical and physical-world settings including healthcare, agriculture, robotics, UAV operations, and cybersecurity. For each class of application, we outline typical system architectures and discuss task-specific evaluation signals. Finally, we present a risk taxonomy that spans hallucinations, reasoning failures, biases, privacy leakage, compute and environmental costs, explainability, knowledge staleness, and evaluation gaps, and we relate these risks to technical mitigations and emerging governance frameworks. Collectively, the survey connects model design, training and inference pipelines, deployment practices, and risk management, offering a structured reference for research and engineering with LLMs.&lt;/p&gt;</content:encoded></item><item><title>ObjectAdd: Adding Objects into Image via a Training-Free Diffusion Modification Fashion</title><link>https://doi.org/10.1016/j.patcog.2025.112807</link><guid>10.1016/j.patcog.2025.112807</guid><pubDate>Mon, 08 Dec 2025 07:43:59 +0000</pubDate><dc:creator>Ziyue Zhang</dc:creator><dc:creator>Mingbao Lin</dc:creator><dc:creator>Quanjian Song</dc:creator><dc:creator>Yuxin Zhang</dc:creator><dc:creator>Rongrong Ji</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112807</prism:doi><description>We introduce ObjectAdd, a training-free diffusion modification method to add user-specified objects into designated regions. The motivation of ObjectAdd stems from: first, describing everything in one prompt can be difficult, and second, users often need to add objects into the generated image. To better align with real-world usage, our ObjectAdd maintains high image consistency after adding objects with technical innovations in: (1) embedding-level concatenation to ensure correct text embedding coalescence; (2) object-driven layout control with latent and attention injection to ensure objects appear in the user-specified area; (3) prompted image inpainting through attention refocusing &amp; object expansion to ensure that the rest of the image remains unchanged. With a text-prompted image, our ObjectAdd allows users to specify a bounding box and an object, and achieves: (1) adding the object inside the box area; (2) preserving the exact content outside the box area; (3) seamless fusion between the two areas. The project is at https://github.com/potato-kitty/ObjectAdd .
Published: 2025-12-08T07:43:59+00:00
Venue: Pattern Recognition
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyue Zhang; Mingbao Lin; Quanjian Song; Yuxin Zhang; Rongrong Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112807"&gt;10.1016/j.patcog.2025.112807&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;We introduce ObjectAdd, a training-free diffusion modification method to add user-specified objects into designated regions. The motivation of ObjectAdd stems from: first, describing everything in one prompt can be difficult, and second, users often need to add objects into the generated image. To better align with real-world usage, our ObjectAdd maintains high image consistency after adding objects with technical innovations in: (1) embedding-level concatenation to ensure correct text embedding coalescence; (2) object-driven layout control with latent and attention injection to ensure objects appear in the user-specified area; (3) prompted image inpainting through attention refocusing &amp;amp; object expansion to ensure that the rest of the image remains unchanged. With a text-prompted image, our ObjectAdd allows users to specify a bounding box and an object, and achieves: (1) adding the object inside the box area; (2) preserving the exact content outside the box area; (3) seamless fusion between the two areas. The project is at https://github.com/potato-kitty/ObjectAdd .&lt;/p&gt;</content:encoded></item></channel></rss>