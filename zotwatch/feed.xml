<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 31 Dec 2025 02:44:19 +0000</lastBuildDate><item><title>CrossEarth: Geospatial Vision Foundation Model for Domain Generalizable Remote Sensing Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3649001</link><guid>10.1109/tpami.2025.3649001</guid><pubDate>Mon, 29 Dec 2025 18:38:19 +0000</pubDate><dc:creator>Ziyang Gong</dc:creator><dc:creator>Zhixiang Wei</dc:creator><dc:creator>Di Wang</dc:creator><dc:creator>Xiaoxing Hu</dc:creator><dc:creator>Xianzheng Ma</dc:creator><dc:creator>Hongruixuan Chen</dc:creator><dc:creator>Yuru Jia</dc:creator><dc:creator>Yupeng Deng</dc:creator><dc:creator>Zhenming Ji</dc:creator><dc:creator>Xiangwei Zhu</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Naoto Yokoya</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Bo Du</dc:creator><dc:creator>Junchi Yan</dc:creator><dc:creator>Liangpei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649001</prism:doi><description>Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.
Published: 2025-12-29T18:38:19+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.842 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyang Gong; Zhixiang Wei; Di Wang; Xiaoxing Hu; Xianzheng Ma; Hongruixuan Chen; Yuru Jia; Yupeng Deng; Zhenming Ji; Xiangwei Zhu; Xue Yang; Naoto Yokoya; Jing Zhang; Bo Du; Junchi Yan; Liangpei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649001"&gt;10.1109/tpami.2025.3649001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.842 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the substantial domain gaps in Remote Sensing (RS) images that are characterized by variabilities such as location, wavelength, and sensor type, Remote Sensing Domain Generalization (RSDG) has emerged as a critical and valuable research frontier, focusing on developing models that generalize effectively across diverse scenarios. However, research in this area remains underexplored: (1) Current cross-domain methods primarily focus on Domain Adaptation (DA), which adapts models to predefined domains rather than to unseen ones; (2) Few studies target the RSDG issue, especially for semantic segmentation tasks. Existing related models are developed for specific unknown domains, struggling with issues of underfitting on other unseen scenarios; (3) Existing RS foundation models tend to prioritize in-domain performance over cross-domain generalization. To this end, we introduce the first vision foundation model for RSDG semantic segmentation, CrossEarth. CrossEarth demonstrates strong cross-domain generalization through a specially designed data-level Earth-Style Injection pipeline and a model-level Multi-Task Training pipeline. In addition, for the semantic segmentation task, we have curated an RSDG benchmark comprising 32 semantic segmentation scenarios across various regions, spectral bands, platforms, and climates, providing comprehensive evaluations of the generalizability of future RSDG models. Extensive experiments on this collection demonstrate the superiority of CrossEarth over existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>On the Transferability and Discriminability of Representation Learning in Unsupervised Domain Adaptation</title><link>https://doi.org/10.1109/tpami.2025.3649294</link><guid>10.1109/tpami.2025.3649294</guid><pubDate>Tue, 30 Dec 2025 18:37:35 +0000</pubDate><dc:creator>Wenwen Qiang</dc:creator><dc:creator>Ziyin Gu</dc:creator><dc:creator>Lingyu Si</dc:creator><dc:creator>Jiangmeng Li</dc:creator><dc:creator>Changwen Zheng</dc:creator><dc:creator>Fuchun Sun</dc:creator><dc:creator>Hui Xiong</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649294</prism:doi><description>In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical–practical gap, we defined “good representation learning” as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.
Published: 2025-12-30T18:37:35+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenwen Qiang; Ziyin Gu; Lingyu Si; Jiangmeng Li; Changwen Zheng; Fuchun Sun; Hui Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649294"&gt;10.1109/tpami.2025.3649294&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical–practical gap, we defined “good representation learning” as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.&lt;/p&gt;</content:encoded></item><item><title>FocusPatch AD: Few-Shot Multi-Class Anomaly Detection with Unified Keywords Patch Prompts</title><link>https://doi.org/10.1109/tip.2025.3646861</link><guid>10.1109/tip.2025.3646861</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Xicheng Ding</dc:creator><dc:creator>Xiaofan Li</dc:creator><dc:creator>Mingang Chen</dc:creator><dc:creator>Jingyu Gong</dc:creator><dc:creator>Yuan Xie</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646861</prism:doi><description>Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xicheng Ding; Xiaofan Li; Mingang Chen; Jingyu Gong; Yuan Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646861"&gt;10.1109/tip.2025.3646861&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Industrial few-shot anomaly detection (FSAD) requires identifying various abnormal states by leveraging as few normal samples as possible (abnormal samples are unavailable during training). However, current methods often require training a separate model for each category, leading to increased computation and storage overhead. Thus, designing a unified anomaly detection model that supports multiple categories remains a challenging task, as such a model must recognize anomalous patterns across diverse objects and domains. To tackle these challenges, this paper introduces FocusPatch AD, a unified anomaly detection framework based on vision-language models, achieving anomaly detection under few-shot multi-class settings. FocusPatch AD links anomaly state keywords to highly relevant discrete local regions within the image, guiding the model to focus on cross-category anomalies while filtering out background interference. This approach mitigates the false detection issues caused by global semantic alignment in vision-language models. We evaluate the proposed method on the MVTec, VisA, and Real-IAD datasets, comparing them against several prevailing anomaly detection methods. In both image-level and pixel-level anomaly detection tasks, FocusPatch AD achieves significant gains in classification and localization performance, demonstrating excellent generalization and adaptability.&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Fine-Grained Classification with Foreground-Aware Kernelized Feature Reconstruction Network</title><link>https://doi.org/10.1109/tip.2025.3646940</link><guid>10.1109/tip.2025.3646940</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Yangfan Li</dc:creator><dc:creator>Wei Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646940</prism:doi><description>Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yangfan Li; Wei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646940"&gt;10.1109/tip.2025.3646940&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Feature reconstruction networks have achieved remarkable performance in few-shot fine-grained classification tasks. Nonetheless, traditional feature reconstruction networks rely on linear regression. This linearity may cause the loss of subtle discriminative cues, ultimately resulting in less precise reconstructed features. Moreover, in situations where the background predominantly occupies the image, the background reconstruction errors tend to overshadow foreground reconstruction errors, resulting in inaccurate reconstruction errors. In order to address the two key issues, a novel approach called the Foreground-Aware Kernelized Feature Reconstruction Network (FKFRN) is proposed. Specifically, to address the problem of imprecise reconstructed features, we introduce kernel methods into linear feature reconstruction, extending it to nonlinear feature reconstruction, thus enabling the reconstruction of richer, finer-grained discriminative features. To tackle the issue of inaccurate reconstruction errors, the foreground-aware reconstruction error is proposed. Specifically, the model assigns higher weights to features containing more foreground information and lower weights to those dominated by background content, which reduces the impact of background errors on the overall reconstruction. To estimate these weights accurately, we design two complementary strategies: an explicit probabilistic graphical model and an implicit neural network–based approach. Extensive experimental results on eight datasets validate the effectiveness of the proposed approach for few-shot fine-grained classification.&lt;/p&gt;</content:encoded></item><item><title>Exploring Syn-to-Real Domain Adaptation for Military Target Detection</title><link>https://arxiv.org/abs/2512.23208v1</link><guid>http://arxiv.org/abs/2512.23208v1</guid><pubDate>Mon, 29 Dec 2025 05:05:41 +0000</pubDate><dc:creator>Jongoh Jeong</dc:creator><dc:creator>Youngjin Oh</dc:creator><dc:creator>Gyeongrae Nam</dc:creator><dc:creator>Jeongeun Lee</dc:creator><dc:creator>Kuk-Jin Yoon</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.
Published: 2025-12-29T05:05:41+00:00
Venue: arXiv
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jongoh Jeong; Youngjin Oh; Gyeongrae Nam; Jeongeun Lee; Kuk-Jin Yoon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.&lt;/p&gt;</content:encoded></item><item><title>YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection</title><link>https://arxiv.org/abs/2512.23273v1</link><guid>http://arxiv.org/abs/2512.23273v1</guid><pubDate>Mon, 29 Dec 2025 07:54:49 +0000</pubDate><dc:creator>Xu Lin</dc:creator><dc:creator>Jinlong Peng</dc:creator><dc:creator>Zhenye Gan</dc:creator><dc:creator>Jiawen Zhu</dc:creator><dc:creator>Jun Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.
Published: 2025-12-29T07:54:49+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xu Lin; Jinlong Peng; Zhenye Gan; Jiawen Zhu; Jun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.&lt;/p&gt;</content:encoded></item><item><title>LNet: Lightweight Network for Driver Attention Estimation via Scene and Gaze Consistency</title><link>https://doi.org/10.1109/tip.2025.3646893</link><guid>10.1109/tip.2025.3646893</guid><pubDate>Tue, 30 Dec 2025 18:39:42 +0000</pubDate><dc:creator>Daosong Hu</dc:creator><dc:creator>Xi Li</dc:creator><dc:creator>Mingyue Cui</dc:creator><dc:creator>Kai Huang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646893</prism:doi><description>In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.
Published: 2025-12-30T18:39:42+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daosong Hu; Xi Li; Mingyue Cui; Kai Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646893"&gt;10.1109/tip.2025.3646893&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;In resource-constrained vehicle systems, establishing consistency between multi-view scenes and driver gaze remains challenging. Prior methods mainly focus on cross-source data fusion, estimating gaze or attention maps through unidirectional implicit links between scene and facial features. Although bidirectional projection can correct misalignment between predictions and ground truth, the high resolution of scene images and complex semantic extraction incur heavy computational loads. To address these issues, we propose a lightweight driver-attention estimation framework that leverages geometric consistency between scene and gaze to guide feature extraction bidirectionally, thereby strengthening representation. Specifically, we first introduce a lightweight feature extraction module that captures global and local information in parallel through dual asymmetric branches to efficiently extract facial and scene features. An information cross fusion module is then designed to promote interaction between the scene and gaze streams. The multi-branch architecture extracts gaze and geometric cues at multiple scales, reducing the computational redundancy caused by mixed features when modeling geometric consistency across both views. Experiments on a large public dataset show that incorporating scene information introduces no significant computational overhead and yields a better trade-off between accuracy and efficiency. Moreover, leveraging bidirectional projection and the temporal continuity of gaze, we preliminarily explore the framework’s potential for predicting attention trends.&lt;/p&gt;</content:encoded></item><item><title>Foundation Model-based Auxiliary Framework for Object Detection in Aerial Remote Sensing Images</title><link>https://doi.org/10.1109/taes.2025.3649185</link><guid>10.1109/taes.2025.3649185</guid><pubDate>Mon, 29 Dec 2025 18:40:09 +0000</pubDate><dc:creator>Wanjie Lu</dc:creator><dc:creator>Chaoyang Niu</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Chaozhen Lan</dc:creator><dc:creator>Shiju Wang</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3649185</prism:doi><description>When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.
Published: 2025-12-29T18:40:09+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wanjie Lu; Chaoyang Niu; Wei Liu; Chaozhen Lan; Shiju Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3649185"&gt;10.1109/taes.2025.3649185&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;When lightweight backbones pretrained on natural scene datasets are applied to object detection in aerial remote sensing images (ARSIs), the detection performance varies significantly. This variation stems from factors including the data domain gap, dataset scale, training configuration, and model architecture. Since remote sensing foundation models (RSFMs) are pretrained on large-scale remote sensing datasets and exhibit strong feature extraction capabilities, we propose an RSFM-based auxiliary framework to enable existing lightweight backbones to achieve enhanced performance on ARSI datasets of varying scales. Specifically, the RSFM is leveraged to efficiently extract features with robust and rich representational capabilities from input ARSIs. A foundation feature fusion module is designed to fuse the features extracted by the RSFM with those from the lightweight backbone, addressing the inadequacy in representational capacity of various lightweight backbones when extracting ARSI features. Furthermore, a feature aggregation and expansion module is introduced to enhance the representational power of the fused features. Experimental results on four ARSI datasets of different scales demonstrate that the performance of various lightweight backbones is improved when integrated with the proposed RSFM-based auxiliary framework. In most cases, this performance is superior to that of larger-scale networks. Specifically, on the DOTA 1.5 and DIOR datasets, the performance of these lightweight backbones (integrated with the framework) is significantly enhanced compared to that of state-of-the-art (SOTA) models. Collectively, these results validate the effectiveness of the proposed RSFM-based auxiliary framework, confirming its ability to effectively improve the performance of existing backbones and thereby facilitate the popularization and application of existing technical advancements.&lt;/p&gt;</content:encoded></item><item><title>Versatile cardiovascular signal generation with a unified diffusion transformer</title><link>https://doi.org/10.1038/s42256-025-01147-y</link><guid>10.1038/s42256-025-01147-y</guid><pubDate>Mon, 29 Dec 2025 10:01:34 +0000</pubDate><dc:creator>Zehua Chen</dc:creator><dc:creator>Yuyang Miao</dc:creator><dc:creator>Liyuan Wang</dc:creator><dc:creator>Luyun Fan</dc:creator><dc:creator>Danilo P. Mandic</dc:creator><dc:creator>Jun Zhu</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01147-y</prism:doi><description>Cardiovascular signals such as photoplethysmography, electrocardiography and blood pressure are inherently correlated and complementary, together reflecting the health of the cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multimodal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, as well as ensuring interpretability for human experts. These advantages establish UniCardio as a practical and robust framework for advancing artificial-intelligence-assisted healthcare. UniCardio is a unified framework for versatile multimodal cardiovascular signal generation, enabling robust signal restoration and cross-modal translation to detect abnormal conditions and estimate vital signs in real-time health monitoring.
Published: 2025-12-29T10:01:34+00:00
Venue: Nature Machine Intelligence
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zehua Chen; Yuyang Miao; Liyuan Wang; Luyun Fan; Danilo P. Mandic; Jun Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01147-y"&gt;10.1038/s42256-025-01147-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Cardiovascular signals such as photoplethysmography, electrocardiography and blood pressure are inherently correlated and complementary, together reflecting the health of the cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multimodal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, as well as ensuring interpretability for human experts. These advantages establish UniCardio as a practical and robust framework for advancing artificial-intelligence-assisted healthcare. UniCardio is a unified framework for versatile multimodal cardiovascular signal generation, enabling robust signal restoration and cross-modal translation to detect abnormal conditions and estimate vital signs in real-time health monitoring.&lt;/p&gt;</content:encoded></item><item><title>FDPFNet: A Frequency-Domain Progressive Fusion Network for Optical-SAR Multi-Label Remote Sensing Scene Classification</title><link>https://doi.org/10.1109/jstars.2025.3649036</link><guid>10.1109/jstars.2025.3649036</guid><pubDate>Mon, 29 Dec 2025 18:38:55 +0000</pubDate><dc:creator>Yiming Zhao</dc:creator><dc:creator>Kunlun Qi</dc:creator><dc:creator>Yaxian Qing</dc:creator><dc:creator>Kelong Tu</dc:creator><dc:creator>Jiajun Tao</dc:creator><dc:creator>Hongge Li</dc:creator><dc:creator>Chao Yang</dc:creator><dc:creator>Hongyan Zhang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649036</prism:doi><description>The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.
Published: 2025-12-29T18:38:55+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Zhao; Kunlun Qi; Yaxian Qing; Kelong Tu; Jiajun Tao; Hongge Li; Chao Yang; Hongyan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649036"&gt;10.1109/jstars.2025.3649036&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;The fusion of optical and SAR remote sensing imagery has become increasingly crucial for accurate multi-label remote sensing scene classification (MRSSC), which plays an essential role in producing reliable land use and land cover (LULC) products. However, visual heterogeneity between optical and SAR data, together with the speckle noise inherent in SAR imagery, greatly limits the performance of existing multimodal fusion approaches. To overcome these challenges, this paper proposes a Frequency-Domain Progressive Fusion Network (FDPFNet) that adopts a hybrid CNN–Transformer architecture to serve as an effective and unified multimodal backbone for MRSSC. First, a Low-Frequency Convolution (LFConv) block is introduced, utilizing wavelet transform to highlight low-frequency components shared across modalities while suppressing high-frequency noise in SAR data. Second, a Two-Frequency Decomposition (TFD) block is designed to decompose features into high- and low-frequency components, allowing comprehensive fusion of modality-shared low-frequency semantics while mitigating the adverse effects of inconsistent high-frequency details. Finally, an Adaptive Feature Fusion (AFF) block is developed to dynamically balance intra-modal feature consistency and inter-modal complementarity across multiple hierarchical levels, thereby achieving more effective optical–SAR fusion. Extensive experiments conducted on the BigEarthNet-MM and SEN12-MLRS datasets demonstrate that FDPFNet consistently outperforms state-of-the-art methods, and the ablation studies further verify the effectiveness of each proposed module and the overall architecture.&lt;/p&gt;</content:encoded></item><item><title>Continuous Review and Timely Correction: Enhancing the Resistance to Noisy Labels Via Self-Not-True and Class-Wise Distillation</title><link>https://doi.org/10.1109/tpami.2025.3649111</link><guid>10.1109/tpami.2025.3649111</guid><pubDate>Mon, 29 Dec 2025 18:38:19 +0000</pubDate><dc:creator>Long Lan</dc:creator><dc:creator>Jingyi Wang</dc:creator><dc:creator>Xinghao Wu</dc:creator><dc:creator>Bo Han</dc:creator><dc:creator>Xinwang Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649111</prism:doi><description>Deep neural networks possess remarkable learning capabilities and expressive power, but this makes them vulnerable to overfitting, especially when they encounter mislabeled data. A notable phenomenon called the memorization effect occurs when networks first learn the correctly labeled data and later memorize the mislabeled instances. While early stopping can mitigate overfitting, it doesn't entirely prevent networks from adapting to incorrect labels during the initial training phases, which can result in losing valuable insights from accurate data. Moreover, early stopping cannot rectify the mistakes caused by mislabeled inputs, underscoring the need for improved strategies. In this paper, we introduce an innovative mechanism for continuous review and timely correction of learned knowledge. Our approach allows the network to repeatedly revisit and reinforce correct information while promptly addressing any inaccuracies stemming from mislabeled data. We present a novel method called self-not-true-distillation (SNTD). This technique employs self-distillation, where the network from previous training iterations acts as a teacher, guiding the current network to review and solidify its understanding of accurate labels. Crucially, SNTD masks the true class label in the logits during this process, concentrating on the non-true classes to correct any erroneous knowledge that may have been acquired. We also recognize that different data classes follow distinct learning trajectories. A single teacher network might struggle to effectively guide the learning of all classes at once, which necessitates selecting different teacher networks for each specific class. Additionally, the influence of the teacher network's guidance varies throughout the training process. To address these challenges, we propose SNTD+, which integrates a class-wise distillation strategy along with a dynamic weight adjustment mechanism. Together, these enhancements significantly bolster SNTD's robustness in...
Published: 2025-12-29T18:38:19+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Long Lan; Jingyi Wang; Xinghao Wu; Bo Han; Xinwang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649111"&gt;10.1109/tpami.2025.3649111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Deep neural networks possess remarkable learning capabilities and expressive power, but this makes them vulnerable to overfitting, especially when they encounter mislabeled data. A notable phenomenon called the memorization effect occurs when networks first learn the correctly labeled data and later memorize the mislabeled instances. While early stopping can mitigate overfitting, it doesn&amp;#x27;t entirely prevent networks from adapting to incorrect labels during the initial training phases, which can result in losing valuable insights from accurate data. Moreover, early stopping cannot rectify the mistakes caused by mislabeled inputs, underscoring the need for improved strategies. In this paper, we introduce an innovative mechanism for continuous review and timely correction of learned knowledge. Our approach allows the network to repeatedly revisit and reinforce correct information while promptly addressing any inaccuracies stemming from mislabeled data. We present a novel method called self-not-true-distillation (SNTD). This technique employs self-distillation, where the network from previous training iterations acts as a teacher, guiding the current network to review and solidify its understanding of accurate labels. Crucially, SNTD masks the true class label in the logits during this process, concentrating on the non-true classes to correct any erroneous knowledge that may have been acquired. We also recognize that different data classes follow distinct learning trajectories. A single teacher network might struggle to effectively guide the learning of all classes at once, which necessitates selecting different teacher networks for each specific class. Additionally, the influence of the teacher network&amp;#x27;s guidance varies throughout the training process. To address these challenges, we propose SNTD+, which integrates a class-wise distillation strategy along with a dynamic weight adjustment mechanism. Together, these enhancements significantly bolster SNTD&amp;#x27;s robustness in...&lt;/p&gt;</content:encoded></item><item><title>Text-Assisted Multi-Modal Adaptive Registration and Fusion Classification Network</title><link>https://doi.org/10.1109/tgrs.2025.3649754</link><guid>10.1109/tgrs.2025.3649754</guid><pubDate>Tue, 30 Dec 2025 18:37:56 +0000</pubDate><dc:creator>Yufei He</dc:creator><dc:creator>Bobo Xi</dc:creator><dc:creator>Guocheng Li</dc:creator><dc:creator>Tie Zheng</dc:creator><dc:creator>Yunsong Li</dc:creator><dc:creator>Changbin Xue</dc:creator><dc:creator>Ming Shen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649754</prism:doi><description>Due to inherent sensor discrepancies, hyperspectral image (HSI) and LiDAR data in fusion classification tasks often suffer from spatial misalignment and limited high-quality annotations, which severely constrain classification performance. While some recent methods attempt to address these issues by introducing semantic alignment strategies or contrastive learning (CL), challenges such as inconsistent cross-modal representations and limited semantic generalization still persist. To address these issues, we propose a novel text-assisted adaptive registration and fusion classification network (TARCNet). First, we develop a feature adaptive alignment (FAA) module, which adaptively adjusts LiDAR features to alleviate semantic inconsistency under misregistration. Second, we introduce a text-assisted contrastive learning (TCL) module, which leverages linguistic priors to strengthen cross-modal consistency and improve the discriminability of the learned representations. Third, we incorporate a multi-loss joint optimization (MLO) module to ensure consistent and stable optimization across heterogeneous modalities. Extensive experiments conducted on three HSI-LiDAR datasets with misregistration and limited annotations demonstrate that our method outperforms several state-of-the-art approaches, validating its effectiveness and generalization capability.
Published: 2025-12-30T18:37:56+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yufei He; Bobo Xi; Guocheng Li; Tie Zheng; Yunsong Li; Changbin Xue; Ming Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649754"&gt;10.1109/tgrs.2025.3649754&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Due to inherent sensor discrepancies, hyperspectral image (HSI) and LiDAR data in fusion classification tasks often suffer from spatial misalignment and limited high-quality annotations, which severely constrain classification performance. While some recent methods attempt to address these issues by introducing semantic alignment strategies or contrastive learning (CL), challenges such as inconsistent cross-modal representations and limited semantic generalization still persist. To address these issues, we propose a novel text-assisted adaptive registration and fusion classification network (TARCNet). First, we develop a feature adaptive alignment (FAA) module, which adaptively adjusts LiDAR features to alleviate semantic inconsistency under misregistration. Second, we introduce a text-assisted contrastive learning (TCL) module, which leverages linguistic priors to strengthen cross-modal consistency and improve the discriminability of the learned representations. Third, we incorporate a multi-loss joint optimization (MLO) module to ensure consistent and stable optimization across heterogeneous modalities. Extensive experiments conducted on three HSI-LiDAR datasets with misregistration and limited annotations demonstrate that our method outperforms several state-of-the-art approaches, validating its effectiveness and generalization capability.&lt;/p&gt;</content:encoded></item><item><title>UDG-Prom: A Unified Dense-Guided Semantic Prompting for Cross-Domain Few-Shot Image Segmentation</title><link>https://doi.org/10.1016/j.knosys.2025.115207</link><guid>10.1016/j.knosys.2025.115207</guid><pubDate>Mon, 29 Dec 2025 17:00:32 +0000</pubDate><dc:creator>Jiaqi Yang</dc:creator><dc:creator>Xiangjian He</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Yaning Zhang</dc:creator><dc:creator>Jingxi Hu</dc:creator><dc:creator>Linlin Shen</dc:creator><dc:creator>Guoping Qiu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115207</prism:doi><description>Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.
Published: 2025-12-29T17:00:32+00:00
Venue: Knowledge-Based Systems
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Yang; Xiangjian He; Xin Chen; Yaning Zhang; Jingxi Hu; Linlin Shen; Guoping Qiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115207"&gt;10.1016/j.knosys.2025.115207&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Large Vision Models (LVMs), exemplified by SAM, contain powerful general knowledge from extensive pre-training, yet they often underperform in highly specialized domains. Building large models tailored for each domain is usually impractical due to the substantial cost of data collection and training. Therefore, a key challenge is how to tap into SAM’s strong knowledge base and transfer it effectively to new, domain-specific tasks, especially under Cross-Domain or Few-Shot constraints. Previous efforts have leveraged prior knowledge from foundation models for transfer learning; however, they typically target specific tasks and exhibit limited robustness in broader applications. To tackle this issue, we propose a Unified Dense-Guided Semantic Prompting framework (UDG-Prom), a new paradigm for Cross-Domain Few-Shot Segmentation (CD-FSS). First, a Multi-level Adaptation Framework (MAF) is used for integrated feature extraction as prior knowledge. Then, we incorporate a Task-Adaptive Auto Meta Prompt (TA 2 MP) module to enable the extraction of class-domain-agnostic features and generate high-quality, learnable visual prompts. By combining learnable prompts with a structured model and prototype disentanglement, this method retains SAM’s prior knowledge and effectively adapts to CD-FSS through category and domain cues. Extensive experiments on four benchmarks show that our model not only surpasses state-of-the-art CD-FSS approaches but also achieves a remarkable improvement in average accuracy.&lt;/p&gt;</content:encoded></item><item><title>Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects</title><link>https://arxiv.org/abs/2512.22949v1</link><guid>http://arxiv.org/abs/2512.22949v1</guid><pubDate>Sun, 28 Dec 2025 14:27:55 +0000</pubDate><dc:creator>Zhicheng Zhao</dc:creator><dc:creator>Xuanang Fan</dc:creator><dc:creator>Lingma Sun</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.
Published: 2025-12-28T14:27:55+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhicheng Zhao; Xuanang Fan; Lingma Sun; Chenglong Li; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.&lt;/p&gt;</content:encoded></item><item><title>YOLOSAM: A YOLO-Guided SAM for Accurate Building Segmentation in Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2025.3649081</link><guid>10.1109/jstars.2025.3649081</guid><pubDate>Mon, 29 Dec 2025 18:38:55 +0000</pubDate><dc:creator>Musarat Hussain</dc:creator><dc:creator>Ji Huang</dc:creator><dc:creator>Xiankui Liu</dc:creator><dc:creator>Yulin Duan</dc:creator><dc:creator>Hongyan Wu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649081</prism:doi><description>Accurate building segmentation in remote sensing images is crucial for applications like disaster assessment, 3D urban modeling, and monitoring urban transformations. However, this task presents significant challenges due to the vast geographical coverage, dense building clusters, and the complexity of building contours, roof geometries, and surrounding environments. While the Segment Anything Model (SAM) offers a promising solution for extracting building masks in remote sensing images, its reliance on interactive input cues, difficulty in capturing fine edge details, and inability to integrate global semantic context with local fine-grained visual features often result in poor boundary detection and fragmented masks, limiting its effectiveness in fully automated, end-to-end building segmentation. To address these limitations, we propose YOLOSAM, a YOLO-guided adaptation of SAM designed for precise and automated building segmentation. Our framework introduces three lightweight yet effective innovations: (i) an Automatic Prompt Generator, based on YOLOv8, that automatically produces bounding box prompts to eliminate manual input; (ii) a High-Quality Token (HQ-Token) that improves edge fidelity and mask coherence by refining SAM's decoder representations; and (iii) a Global-Local Feature Fusion module, which enhances segmentation quality by fusing semantic context from deeper layers with fine edge details from earlier stages of SAM's frozen architecture. Importantly, our method preserves SAM's pre-trained generalization ability by freezing the original encoder and decoder while training only the lightweight modules. Experimental results demonstrate a significant improvement in segmentation accuracy, with mIoU increasing to 76.7% on the WHU building segmentation dataset, 69.1% on the Vaihingen building dataset, and 73.2% on the Inria Aerial Image Labeling dataset, compared to SAM's “segment everything” mode. Our model also significantly outperforms both classical deep...
Published: 2025-12-29T18:38:55+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Musarat Hussain; Ji Huang; Xiankui Liu; Yulin Duan; Hongyan Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649081"&gt;10.1109/jstars.2025.3649081&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate building segmentation in remote sensing images is crucial for applications like disaster assessment, 3D urban modeling, and monitoring urban transformations. However, this task presents significant challenges due to the vast geographical coverage, dense building clusters, and the complexity of building contours, roof geometries, and surrounding environments. While the Segment Anything Model (SAM) offers a promising solution for extracting building masks in remote sensing images, its reliance on interactive input cues, difficulty in capturing fine edge details, and inability to integrate global semantic context with local fine-grained visual features often result in poor boundary detection and fragmented masks, limiting its effectiveness in fully automated, end-to-end building segmentation. To address these limitations, we propose YOLOSAM, a YOLO-guided adaptation of SAM designed for precise and automated building segmentation. Our framework introduces three lightweight yet effective innovations: (i) an Automatic Prompt Generator, based on YOLOv8, that automatically produces bounding box prompts to eliminate manual input; (ii) a High-Quality Token (HQ-Token) that improves edge fidelity and mask coherence by refining SAM&amp;#x27;s decoder representations; and (iii) a Global-Local Feature Fusion module, which enhances segmentation quality by fusing semantic context from deeper layers with fine edge details from earlier stages of SAM&amp;#x27;s frozen architecture. Importantly, our method preserves SAM&amp;#x27;s pre-trained generalization ability by freezing the original encoder and decoder while training only the lightweight modules. Experimental results demonstrate a significant improvement in segmentation accuracy, with mIoU increasing to 76.7% on the WHU building segmentation dataset, 69.1% on the Vaihingen building dataset, and 73.2% on the Inria Aerial Image Labeling dataset, compared to SAM&amp;#x27;s “segment everything” mode. Our model also significantly outperforms both classical deep...&lt;/p&gt;</content:encoded></item><item><title>Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations</title><link>https://doi.org/10.1016/j.inffus.2025.104107</link><guid>10.1016/j.inffus.2025.104107</guid><pubDate>Tue, 30 Dec 2025 17:02:53 +0000</pubDate><dc:creator>Yibo Cui</dc:creator><dc:creator>Liang Xie</dc:creator><dc:creator>Yu Zhao</dc:creator><dc:creator>Jiawei Sun</dc:creator><dc:creator>Erwei Yin</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104107</prism:doi><description>Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.
Published: 2025-12-30T17:02:53+00:00
Venue: Information Fusion
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yibo Cui; Liang Xie; Yu Zhao; Jiawei Sun; Erwei Yin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104107"&gt;10.1016/j.inffus.2025.104107&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Navigation (VLN) enables intelligent agents to navigate environments by integrating visual perception and natural language instructions, yet faces significant challenges due to the scarcity of fine-grained cross-modal alignment annotations. Existing datasets primarily focus on global instruction-trajectory matching, neglecting sub-instruction-level and entity-level alignments critical for accurate navigation action decision-making. To address this limitation, we propose FCA-NIG, a generative framework that automatically constructs navigation instructions with dual-level fine-grained cross-modal annotations. In this framework, an augmented trajectory is first divided into sub-trajectories, which are then processed through GLIP-based landmark detection, crafted instruction construction, OFA-Speaker based R2R-like instruction generation, and CLIP-powered entity selection, generating sub-instruction-trajectory pairs with entity-landmark annotations. Finally, these sub-pairs are aggregated to form a complete instruction-trajectory pair. The framework generates the FCA-R2R dataset, the first large-scale augmentation dataset featuring precise sub-instruction-sub-trajectory and entity-landmark alignments. Extensive experiments demonstrate that training with FCA-R2R significantly improves the performance of multiple state-of-the-art VLN agents, including SF, EnvDrop, RecBERT, HAMT, DUET, and BEVBERT. Incorporating sub-instruction-trajectory alignment enhances agents’ state awareness and decision accuracy, while entity-landmark alignment further boosts navigation performance and generalization. These results highlight the effectiveness of FCA-NIG in generating high-quality, scalable training data without manual annotation, advancing fine-grained cross-modal learning in complex navigation tasks.&lt;/p&gt;</content:encoded></item><item><title>Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection</title><link>https://arxiv.org/abs/2512.22972v1</link><guid>http://arxiv.org/abs/2512.22972v1</guid><pubDate>Sun, 28 Dec 2025 15:32:17 +0000</pubDate><dc:creator>Runwei Guan</dc:creator><dc:creator>Jianan Liu</dc:creator><dc:creator>Shaofeng Liang</dc:creator><dc:creator>Fangqiang Ding</dc:creator><dc:creator>Shanliang Yao</dc:creator><dc:creator>Xiaokai Bai</dc:creator><dc:creator>Daizong Liu</dc:creator><dc:creator>Tao Huang</dc:creator><dc:creator>Guoqiang Mao</dc:creator><dc:creator>Hui Xiong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.
Published: 2025-12-28T15:32:17+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Runwei Guan; Jianan Liu; Shaofeng Liang; Fangqiang Ding; Shanliang Yao; Xiaokai Bai; Daizong Liu; Tao Huang; Guoqiang Mao; Hui Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.&lt;/p&gt;</content:encoded></item><item><title>Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework</title><link>https://arxiv.org/abs/2512.22447v1</link><guid>http://arxiv.org/abs/2512.22447v1</guid><pubDate>Sat, 27 Dec 2025 03:16:48 +0000</pubDate><dc:creator>Zhicheng Zhao</dc:creator><dc:creator>Yuancheng Xu</dc:creator><dc:creator>Andong Lu</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.
Published: 2025-12-27T03:16:48+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhicheng Zhao; Yuancheng Xu; Andong Lu; Chenglong Li; Jin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.&lt;/p&gt;</content:encoded></item><item><title>SDE Diffusion Models for SAR Image Active Jamming Suppression with Pseudo-Paired SAR images</title><link>https://doi.org/10.1109/jstars.2025.3649264</link><guid>10.1109/jstars.2025.3649264</guid><pubDate>Tue, 30 Dec 2025 18:38:21 +0000</pubDate><dc:creator>Xunhao Lin</dc:creator><dc:creator>Dawei Ren</dc:creator><dc:creator>Ping Lang</dc:creator><dc:creator>Huizhang Yang</dc:creator><dc:creator>Junjun Yin</dc:creator><dc:creator>Jian Yang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3649264</prism:doi><description>Synthetic Aperture Radar (SAR) imaging is susceptible to various types of jamming, which can severely degrade image quality and hinder downstream tasks. To address this issue, this paper proposes a jamming suppression method through a stochastic differential equation (SDE) based diffusion model trained on pseudo-paired SAR images. Firstly, candidate jamming regions are identified in suppression jamming SAR images through energy concentration and low-rank characteristics. Then, pseudo-paired SAR images representing low and high jamming states are constructed by combining these candidate regions with the original SAR images (referred to as clean images in the following text). Lastly, a diffusion model, with images evolving from the low jamming state to the high jamming state during the forward process and allowing the reverse process to effectively reconstruct clean images from heavily corrupted inputs, is trained to learn the transition between states. This yields a network capable of progressively suppressing jamming and recovering the clean images. Experiments on simulated SAR images with multiple active suppression jamming types and practical Sentinel-1 datasets demonstrate that the proposed method adapts well to diverse jamming types and intensity levels, exhibiting notable effectiveness, robustness, and practical applicability. The training strategy eliminates the need for prior knowledge of suppression jamming patterns and the availability of real paired SAR images, making it especially suitable for complex real-world scenarios where jamming characteristics are difficult to characterize.
Published: 2025-12-30T18:38:21+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xunhao Lin; Dawei Ren; Ping Lang; Huizhang Yang; Junjun Yin; Jian Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3649264"&gt;10.1109/jstars.2025.3649264&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) imaging is susceptible to various types of jamming, which can severely degrade image quality and hinder downstream tasks. To address this issue, this paper proposes a jamming suppression method through a stochastic differential equation (SDE) based diffusion model trained on pseudo-paired SAR images. Firstly, candidate jamming regions are identified in suppression jamming SAR images through energy concentration and low-rank characteristics. Then, pseudo-paired SAR images representing low and high jamming states are constructed by combining these candidate regions with the original SAR images (referred to as clean images in the following text). Lastly, a diffusion model, with images evolving from the low jamming state to the high jamming state during the forward process and allowing the reverse process to effectively reconstruct clean images from heavily corrupted inputs, is trained to learn the transition between states. This yields a network capable of progressively suppressing jamming and recovering the clean images. Experiments on simulated SAR images with multiple active suppression jamming types and practical Sentinel-1 datasets demonstrate that the proposed method adapts well to diverse jamming types and intensity levels, exhibiting notable effectiveness, robustness, and practical applicability. The training strategy eliminates the need for prior knowledge of suppression jamming patterns and the availability of real paired SAR images, making it especially suitable for complex real-world scenarios where jamming characteristics are difficult to characterize.&lt;/p&gt;</content:encoded></item><item><title>Task-Guided Prompting for Unified Remote Sensing Image Restoration</title><link>https://doi.org/10.1109/tgrs.2025.3649021</link><guid>10.1109/tgrs.2025.3649021</guid><pubDate>Mon, 29 Dec 2025 18:38:33 +0000</pubDate><dc:creator>Wenli Huang</dc:creator><dc:creator>Yang Wu</dc:creator><dc:creator>Xiaomeng Xin</dc:creator><dc:creator>Zhihong Liu</dc:creator><dc:creator>Jinjun Wang</dc:creator><dc:creator>Ye Deng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3649021</prism:doi><description>Remote sensing image restoration (RSIR) is essential for recovering high-fidelity imagery from degraded observations, enabling accurate downstream analysis. However, most existing methods focus on single degradation types within homogeneous data, restricting their practicality in real-world scenarios where multiple degradations often across diverse spectral bands or sensor modalities, creating a significant operational bottleneck. To address this fundamental gap, we propose TGPNet, a unified framework capable of handling denoising, cloud removal, shadow removal, deblurring, and SAR despeckling within a single, unified architecture. The core of our framework is a novel Task-Guided Prompting (TGP) strategy. TGP leverages learnable, task-specific embeddings to generate degradation-aware cues, which then hierarchically modulate features throughout the decoder. This task-adaptive mechanism allows the network to precisely tailor its restoration process for distinct degradation patterns while maintaining a single set of shared weights. To validate our framework, we construct a unified RSIR benchmark covering RGB, multispectral, SAR, and thermal infrared modalities for five aforementioned restoration tasks. Experimental results demonstrate that TGPNet achieves state-of-the-art performance on both unified multi-task scenarios and unseen composite degradations, surpassing even specialized models in individual domains such as cloud removal. By successfully unifying heterogeneous degradation removal within a single adaptive framework, this work presents a significant advancement for multi-task RSIR, offering a practical and scalable solution for operational pipelines. The code and benchmark will be released at https://github.com/huangwenwenlili/TGPNet.
Published: 2025-12-29T18:38:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenli Huang; Yang Wu; Xiaomeng Xin; Zhihong Liu; Jinjun Wang; Ye Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3649021"&gt;10.1109/tgrs.2025.3649021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing image restoration (RSIR) is essential for recovering high-fidelity imagery from degraded observations, enabling accurate downstream analysis. However, most existing methods focus on single degradation types within homogeneous data, restricting their practicality in real-world scenarios where multiple degradations often across diverse spectral bands or sensor modalities, creating a significant operational bottleneck. To address this fundamental gap, we propose TGPNet, a unified framework capable of handling denoising, cloud removal, shadow removal, deblurring, and SAR despeckling within a single, unified architecture. The core of our framework is a novel Task-Guided Prompting (TGP) strategy. TGP leverages learnable, task-specific embeddings to generate degradation-aware cues, which then hierarchically modulate features throughout the decoder. This task-adaptive mechanism allows the network to precisely tailor its restoration process for distinct degradation patterns while maintaining a single set of shared weights. To validate our framework, we construct a unified RSIR benchmark covering RGB, multispectral, SAR, and thermal infrared modalities for five aforementioned restoration tasks. Experimental results demonstrate that TGPNet achieves state-of-the-art performance on both unified multi-task scenarios and unseen composite degradations, surpassing even specialized models in individual domains such as cloud removal. By successfully unifying heterogeneous degradation removal within a single adaptive framework, this work presents a significant advancement for multi-task RSIR, offering a practical and scalable solution for operational pipelines. The code and benchmark will be released at https://github.com/huangwenwenlili/TGPNet.&lt;/p&gt;</content:encoded></item><item><title>Author Correction: Scalable and robust DNA-based storage via coding theory and deep learning</title><link>https://doi.org/10.1038/s42256-025-01175-8</link><guid>10.1038/s42256-025-01175-8</guid><pubDate>Tue, 30 Dec 2025 07:18:03 +0000</pubDate><dc:creator>Daniella Bar-Lev</dc:creator><dc:creator>Itai Orr</dc:creator><dc:creator>Omer Sabary</dc:creator><dc:creator>Tuvi Etzion</dc:creator><dc:creator>Eitan Yaakobi</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01175-8</prism:doi><description>Correction to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01003-z , published online 21 February 2025.
Published: 2025-12-30T07:18:03+00:00
Venue: Nature Machine Intelligence
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daniella Bar-Lev; Itai Orr; Omer Sabary; Tuvi Etzion; Eitan Yaakobi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01175-8"&gt;10.1038/s42256-025-01175-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Correction to: Nature Machine Intelligence https://doi.org/10.1038/s42256-025-01003-z , published online 21 February 2025.&lt;/p&gt;</content:encoded></item><item><title>风力发电机叶片缺陷目标检测研究综述</title><link>https://doi.org/10.11834/jig.250458</link><guid>10.11834/jig.250458</guid><pubDate>Mon, 29 Dec 2025 02:45:35 +0000</pubDate><dc:creator>Zhang Jinghong</dc:creator><dc:creator>Su Pan</dc:creator><dc:creator>Zhu Qingyuan</dc:creator><dc:creator>Zhang Chaogang</dc:creator><dc:creator>Li Bing</dc:creator><dc:creator>Cao Wangbin</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250458</prism:doi><description>全球能源转型推动了风力发电机的大规模部署，基于无人机图像采集的风机叶片缺陷目标检测技术已成为研究热点。风力发电机叶片的无人机图像存在质量低、缺陷与纹理相似、缺陷特征多尺度等问题，且标注缺乏统一标准并高度依赖人工，这些因素显著增加了缺陷检测的难度。当前关于风力发电机叶片缺陷检测的综述主要聚焦于无损检测方法，针对图像检测技术进行系统性评述的综述仍然不足，且多集中于模型架构的优化，而忽略了图像预处理和学习策略对检测性能的重要影响。因此，本文将图像预处理和学习策略纳入视角，系统综述了风力发电机叶片缺陷目标检测的研究进展：（1） 详细列举叶片缺陷类型及特征。（2） 与电力系统其他检测任务的进行对比研究并总结主要问题。（3） 从图像预处理、网络改进和学习策略三个维度总结现有风力发电机叶片缺陷目标检测方法。在图像预处理方面，基于任务导向将现有方法归纳为三类：图像质量增强、几何校正以及结构特征提取；在网络优化方面，沿目标检测模型发展脉络，梳理了各类架构的改进策略；最后，总结了该领域常用的学习策略。（4） 针对近年来数据集与常用评估指标进行汇总。（5） 对风机叶片缺陷目标检测方法面临的挑战及未来研究方向进行总结和展望。
Published: 2025-12-29T02:45:35+00:00
Venue: Journal of Image and Graphics
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhang Jinghong; Su Pan; Zhu Qingyuan; Zhang Chaogang; Li Bing; Cao Wangbin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250458"&gt;10.11834/jig.250458&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;全球能源转型推动了风力发电机的大规模部署，基于无人机图像采集的风机叶片缺陷目标检测技术已成为研究热点。风力发电机叶片的无人机图像存在质量低、缺陷与纹理相似、缺陷特征多尺度等问题，且标注缺乏统一标准并高度依赖人工，这些因素显著增加了缺陷检测的难度。当前关于风力发电机叶片缺陷检测的综述主要聚焦于无损检测方法，针对图像检测技术进行系统性评述的综述仍然不足，且多集中于模型架构的优化，而忽略了图像预处理和学习策略对检测性能的重要影响。因此，本文将图像预处理和学习策略纳入视角，系统综述了风力发电机叶片缺陷目标检测的研究进展：（1） 详细列举叶片缺陷类型及特征。（2） 与电力系统其他检测任务的进行对比研究并总结主要问题。（3） 从图像预处理、网络改进和学习策略三个维度总结现有风力发电机叶片缺陷目标检测方法。在图像预处理方面，基于任务导向将现有方法归纳为三类：图像质量增强、几何校正以及结构特征提取；在网络优化方面，沿目标检测模型发展脉络，梳理了各类架构的改进策略；最后，总结了该领域常用的学习策略。（4） 针对近年来数据集与常用评估指标进行汇总。（5） 对风机叶片缺陷目标检测方法面临的挑战及未来研究方向进行总结和展望。&lt;/p&gt;</content:encoded></item><item><title>GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection</title><link>https://arxiv.org/abs/2512.23176v1</link><guid>http://arxiv.org/abs/2512.23176v1</guid><pubDate>Mon, 29 Dec 2025 03:34:39 +0000</pubDate><dc:creator>Yi Zhang</dc:creator><dc:creator>Yi Wang</dc:creator><dc:creator>Lei Yao</dc:creator><dc:creator>Lap-Pui Chau</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).
Published: 2025-12-29T03:34:39+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Zhang; Yi Wang; Lei Yao; Lap-Pui Chau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).&lt;/p&gt;</content:encoded></item><item><title>Bi-C
                    &lt;sup&gt;2&lt;/sup&gt;
                    R: Bidirectional Continual Compatible Representation for Re-Indexing Free Lifelong Person Re-Identification</title><link>https://doi.org/10.1109/tpami.2025.3649078</link><guid>10.1109/tpami.2025.3649078</guid><pubDate>Mon, 29 Dec 2025 18:38:19 +0000</pubDate><dc:creator>Zhenyu Cui</dc:creator><dc:creator>Jiahuan Zhou</dc:creator><dc:creator>Yuxin Peng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3649078</prism:doi><description>Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as “re-indexing”. However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. Specifically, a bidirectional compatible transfer network is first designed to bridge the relationship between new and old knowledge and continuously update the old gallery features to the new feature space after the updating. Secondly, a bidirectional compatible distillation module and a bidirectional anti-forgetting distillation model are designed to balance the compatibility between the new and old knowledge in dual feature spaces. Finally, a feature-level exponential moving average ...
Published: 2025-12-29T18:38:19+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyu Cui; Jiahuan Zhou; Yuxin Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3649078"&gt;10.1109/tpami.2025.3649078&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as “re-indexing”. However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. Specifically, a bidirectional compatible transfer network is first designed to bridge the relationship between new and old knowledge and continuously update the old gallery features to the new feature space after the updating. Secondly, a bidirectional compatible distillation module and a bidirectional anti-forgetting distillation model are designed to balance the compatibility between the new and old knowledge in dual feature spaces. Finally, a feature-level exponential moving average ...&lt;/p&gt;</content:encoded></item><item><title>CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision</title><link>https://arxiv.org/abs/2512.22969v1</link><guid>http://arxiv.org/abs/2512.22969v1</guid><pubDate>Sun, 28 Dec 2025 15:21:20 +0000</pubDate><dc:creator>Behnam Raoufi</dc:creator><dc:creator>Hossein Sharify</dc:creator><dc:creator>Mohamad Mahdee Ramezanee</dc:creator><dc:creator>Khosrow Hajsadeghi</dc:creator><dc:creator>Saeed Bagheri Shouraki</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.
Published: 2025-12-28T15:21:20+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Behnam Raoufi; Hossein Sharify; Mohamad Mahdee Ramezanee; Khosrow Hajsadeghi; Saeed Bagheri Shouraki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.&lt;/p&gt;</content:encoded></item><item><title>Geometrically-Guided Transformer with Volume-Pose Positional Encoding for Multi-View Stereo</title><link>https://doi.org/10.1016/j.knosys.2025.115240</link><guid>10.1016/j.knosys.2025.115240</guid><pubDate>Tue, 30 Dec 2025 15:51:15 +0000</pubDate><dc:creator>Tianyu Han</dc:creator><dc:creator>Jiangming Kan</dc:creator><dc:creator>Ruifang Dong</dc:creator><dc:creator>Xixuan Zhao</dc:creator><dc:creator>Shun Yao</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115240</prism:doi><description>This paper presents a novel learning-based framework for Multi-View Stereo (MVS) that effectively utilizes Transformer architectures and geometric relationships to enhance 3D reconstruction. We utilize the frozen backbone of DINOv2 to extract robust multi-view patch features. A key component of our approach is the Frustum-Intersection Guided Selection (FIGS) algorithm, which geometrically selects the most relevant image patches from different viewpoints to augment each cost volume cube generated by the FPN These matched patches and volume cubes serve as tokens in a Perceiver-Transformer framework for multi-modal fusion and cost volume regularization, incorporating a bias term in the attention computation derived from the FIGS to improve efficiency. To further enhance geometric awareness, the graph Laplacian spectrum, derived from the patch-cube adjacency matrix generated by the FIGS, is introduced to capture the global structure. This spectrum, combined with the locational 3D spatial information of both the cost sub-volumes and the camera poses, is processed through a Learnable Encoding Network (LEN) to produce an optimized Volume-Pose Positional Encoding (VPPE). Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive performance in 3D reconstruction, validating the effectiveness of the proposed techniques.
Published: 2025-12-30T15:51:15+00:00
Venue: Knowledge-Based Systems
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyu Han; Jiangming Kan; Ruifang Dong; Xixuan Zhao; Shun Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115240"&gt;10.1016/j.knosys.2025.115240&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;This paper presents a novel learning-based framework for Multi-View Stereo (MVS) that effectively utilizes Transformer architectures and geometric relationships to enhance 3D reconstruction. We utilize the frozen backbone of DINOv2 to extract robust multi-view patch features. A key component of our approach is the Frustum-Intersection Guided Selection (FIGS) algorithm, which geometrically selects the most relevant image patches from different viewpoints to augment each cost volume cube generated by the FPN These matched patches and volume cubes serve as tokens in a Perceiver-Transformer framework for multi-modal fusion and cost volume regularization, incorporating a bias term in the attention computation derived from the FIGS to improve efficiency. To further enhance geometric awareness, the graph Laplacian spectrum, derived from the patch-cube adjacency matrix generated by the FIGS, is introduced to capture the global structure. This spectrum, combined with the locational 3D spatial information of both the cost sub-volumes and the camera poses, is processed through a Learnable Encoding Network (LEN) to produce an optimized Volume-Pose Positional Encoding (VPPE). Extensive experiments on three benchmark datasets demonstrate that our method achieves competitive performance in 3D reconstruction, validating the effectiveness of the proposed techniques.&lt;/p&gt;</content:encoded></item><item><title>SAM 3D for 3D Object Reconstruction from Remote Sensing Images</title><link>https://arxiv.org/abs/2512.22452v1</link><guid>http://arxiv.org/abs/2512.22452v1</guid><pubDate>Sat, 27 Dec 2025 03:47:39 +0000</pubDate><dc:creator>Junsheng Yao</dc:creator><dc:creator>Lichao Mou</dc:creator><dc:creator>Qingyu Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.
Published: 2025-12-27T03:47:39+00:00
Venue: arXiv
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junsheng Yao; Lichao Mou; Qingyu Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.&lt;/p&gt;</content:encoded></item><item><title>融合自适应增强与动态筛选机制的多尺度InSAR相位滤波方法</title><link>https://doi.org/10.11834/jrs.20254438</link><guid>10.11834/jrs.20254438</guid><pubDate>Tue, 30 Dec 2025 04:10:12 +0000</pubDate><dc:creator>GAO Yandong</dc:creator><dc:creator>ZHANG Di</dc:creator><dc:creator>LU Zhong</dc:creator><dc:creator>LI Shijin</dc:creator><dc:creator>ZHAO Jinqi</dc:creator><dc:creator>TIAN Yu</dc:creator><dc:creator>ZHANG Shubi</dc:creator><dc:creator>LI Zhi</dc:creator><prism:publicationName>National Remote Sensing Bulletin</prism:publicationName><prism:doi>10.11834/jrs.20254438</prism:doi><description>2025年12月4日中国矿业大学环境与测绘学院的高延东、张帝团队在《遥感学报》发文，介绍了其在多尺度InSAR相位滤波领域的研究进展，提出了融合自适应增强与动态筛选机制的AASTM方法，为提高相位滤波精度和细节保留能力提供解决方案。
Published: 2025-12-30T04:10:12+00:00
Venue: National Remote Sensing Bulletin
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; GAO Yandong; ZHANG Di; LU Zhong; LI Shijin; ZHAO Jinqi; TIAN Yu; ZHANG Shubi; LI Zhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; National Remote Sensing Bulletin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jrs.20254438"&gt;10.11834/jrs.20254438&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;2025年12月4日中国矿业大学环境与测绘学院的高延东、张帝团队在《遥感学报》发文，介绍了其在多尺度InSAR相位滤波领域的研究进展，提出了融合自适应增强与动态筛选机制的AASTM方法，为提高相位滤波精度和细节保留能力提供解决方案。&lt;/p&gt;</content:encoded></item><item><title>RSMT: Robust stereo matching training with geometric correction, clean pixel selection and loss weighting</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.014</link><guid>10.1016/j.isprsjprs.2025.12.014</guid><pubDate>Tue, 30 Dec 2025 09:51:18 +0000</pubDate><dc:creator>Haoxuan Sun</dc:creator><dc:creator>Taoyang Wang</dc:creator><dc:creator>Qian Cheng</dc:creator><dc:creator>Jiaxuan Huang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.014</prism:doi><description>Inaccurate or noisy labels have a huge impact on the training of deep learning models. To date, few studies have focused on the label error problem in satellite image stereo matching. In this paper, we analyzed and found the two open datasets US3D and WHU-Stereo contain label errors that cannot be overlooked. A new task is extremely necessary: learning from inaccurate labels with neural networks. Our motivation is to deal with label errors at the training level. A robust stereo matching training framework (RSMT) with geometric correction, clean pixel selection, and loss weighting modules is proposed. In addition, we also propose a dataset correcting method and provide two inaccurate-label stereo matching datasets US3D(E) and WHU(E) based on raw datasets. The framework can be applied to common stereo methods like IGEV-Stereo and ACVNet to achieve SOTA performance on the corrected datasets. To the best of our knowledge, the study is the first systemic inaccurate-label learning framework dedicated to stereo matching. Datasets are available at https://github.com/endu111/robust-satellite-image-stereo-matching .
Published: 2025-12-30T09:51:18+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoxuan Sun; Taoyang Wang; Qian Cheng; Jiaxuan Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.014"&gt;10.1016/j.isprsjprs.2025.12.014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;Inaccurate or noisy labels have a huge impact on the training of deep learning models. To date, few studies have focused on the label error problem in satellite image stereo matching. In this paper, we analyzed and found the two open datasets US3D and WHU-Stereo contain label errors that cannot be overlooked. A new task is extremely necessary: learning from inaccurate labels with neural networks. Our motivation is to deal with label errors at the training level. A robust stereo matching training framework (RSMT) with geometric correction, clean pixel selection, and loss weighting modules is proposed. In addition, we also propose a dataset correcting method and provide two inaccurate-label stereo matching datasets US3D(E) and WHU(E) based on raw datasets. The framework can be applied to common stereo methods like IGEV-Stereo and ACVNet to achieve SOTA performance on the corrected datasets. To the best of our knowledge, the study is the first systemic inaccurate-label learning framework dedicated to stereo matching. Datasets are available at https://github.com/endu111/robust-satellite-image-stereo-matching .&lt;/p&gt;</content:encoded></item><item><title>SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration</title><link>https://arxiv.org/abs/2512.22503v1</link><guid>http://arxiv.org/abs/2512.22503v1</guid><pubDate>Sat, 27 Dec 2025 07:08:03 +0000</pubDate><dc:creator>Xin Chen</dc:creator><dc:creator>Kang Luo</dc:creator><dc:creator>Yangyi Xiao</dc:creator><dc:creator>Hesheng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.
Published: 2025-12-27T07:08:03+00:00
Venue: arXiv
Score: 0.778 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Chen; Kang Luo; Yangyi Xiao; Hesheng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (consider)&lt;/p&gt;
&lt;p&gt;Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.&lt;/p&gt;</content:encoded></item></channel></rss>