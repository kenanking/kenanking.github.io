<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 14 Jan 2026 03:17:33 +0000</lastBuildDate><item><title>CLIP-Powered Domain Generalization and Domain Adaptation: A Comprehensive Survey</title><link>https://doi.org/10.1109/tpami.2026.3651700</link><guid>10.1109/tpami.2026.3651700</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Jindong Li</dc:creator><dc:creator>Yongguang Li</dc:creator><dc:creator>Yali Fu</dc:creator><dc:creator>Jiahong Liu</dc:creator><dc:creator>Yixin Liu</dc:creator><dc:creator>Menglin Yang</dc:creator><dc:creator>Irwin King</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651700</prism:doi><description>As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for improving model robustness across diverse environments. Contrastive Language–Image Pretraining (CLIP) plays a central role in these tasks, offering strong zero-shot capabilities that allow models to operate effectively in unseen domains. Yet, despite CLIP's growing influence, no comprehensive survey has systematically examined its applications in DG and DA, underscoring the need for this review. This survey provides a unified and in-depth overview of CLIP-driven DG and DA. Before reviewing methods, we establish precise and complete scenario definitions covering source accessibility (SA vs. SF), source number (SS vs. MS), and label relations (CS, PS, OS, OPS), forming a coherent taxonomy that structures all subsequent analyses. For DG, we categorize methods into prompt optimization techniques that enhance task alignment and architectures that leverage CLIP as a backbone for transferable feature extraction. For DA, we examine both source-available approaches that rely on labeled source data and source-free approaches operating primarily on target-domain samples, emphasizing the knowledge transfer mechanisms that enable adaptation across heterogeneous settings. We further provide consolidated trend analyses for both DG and DA, revealing overarching patterns, methodological principles, and scenario-dependent behaviors. We then discuss key challenges such as realistic deployment scenarios, LLM knowledge integration, multimodal fusion, interpretability, and catastrophic forgetting, and outline future directions for developing scalable and trustworthy CLIP-based DG and DA systems. By synthesizing existing studies and highlighting critical gaps, this survey offers actionable insights for researchers and practitioners, motivating new strategies for leveraging CLIP to advance domain robustness in real-world scenarios. A continuously updated list of related works is maint...
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.842 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jindong Li; Yongguang Li; Yali Fu; Jiahong Liu; Yixin Liu; Menglin Yang; Irwin King&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651700"&gt;10.1109/tpami.2026.3651700&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.842 (must_read)&lt;/p&gt;
&lt;p&gt;As machine learning evolves, domain generalization (DG) and domain adaptation (DA) have become crucial for improving model robustness across diverse environments. Contrastive Language–Image Pretraining (CLIP) plays a central role in these tasks, offering strong zero-shot capabilities that allow models to operate effectively in unseen domains. Yet, despite CLIP&amp;#x27;s growing influence, no comprehensive survey has systematically examined its applications in DG and DA, underscoring the need for this review. This survey provides a unified and in-depth overview of CLIP-driven DG and DA. Before reviewing methods, we establish precise and complete scenario definitions covering source accessibility (SA vs. SF), source number (SS vs. MS), and label relations (CS, PS, OS, OPS), forming a coherent taxonomy that structures all subsequent analyses. For DG, we categorize methods into prompt optimization techniques that enhance task alignment and architectures that leverage CLIP as a backbone for transferable feature extraction. For DA, we examine both source-available approaches that rely on labeled source data and source-free approaches operating primarily on target-domain samples, emphasizing the knowledge transfer mechanisms that enable adaptation across heterogeneous settings. We further provide consolidated trend analyses for both DG and DA, revealing overarching patterns, methodological principles, and scenario-dependent behaviors. We then discuss key challenges such as realistic deployment scenarios, LLM knowledge integration, multimodal fusion, interpretability, and catastrophic forgetting, and outline future directions for developing scalable and trustworthy CLIP-based DG and DA systems. By synthesizing existing studies and highlighting critical gaps, this survey offers actionable insights for researchers and practitioners, motivating new strategies for leveraging CLIP to advance domain robustness in real-world scenarios. A continuously updated list of related works is maint...&lt;/p&gt;</content:encoded></item><item><title>Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models</title><link>https://doi.org/10.1109/tpami.2026.3651319</link><guid>10.1109/tpami.2026.3651319</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Hao Dong</dc:creator><dc:creator>Moru Liu</dc:creator><dc:creator>Kaiyang Zhou</dc:creator><dc:creator>Eleni Chatzi</dc:creator><dc:creator>Juho Kannala</dc:creator><dc:creator>Cyrill Stachniss</dc:creator><dc:creator>Olga Fink</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651319</prism:doi><description>Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at https://github.com/donghao51/Awesome-Multimodal-Adaptation.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.837 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Dong; Moru Liu; Kaiyang Zhou; Eleni Chatzi; Juho Kannala; Cyrill Stachniss; Olga Fink&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651319"&gt;10.1109/tpami.2026.3651319&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.837 (must_read)&lt;/p&gt;
&lt;p&gt;Domain adaptation and generalization are crucial for real-world applications, such as autonomous driving and medical imaging where the model must operate reliably across environments with distinct data distributions. However, these tasks are challenging because the model needs to overcome various domain gaps caused by variations in, for example, lighting, weather, sensor configurations, and so on. Addressing domain gaps simultaneously in different modalities, known as multimodal domain adaptation and generalization, is even more challenging due to unique challenges in different modalities. Over the past few years, significant progress has been made in these areas, with applications ranging from action recognition to semantic segmentation, and more. Recently, the emergence of large-scale pre-trained multimodal foundation models, such as CLIP, has inspired numerous research studies, which leverage these models to enhance downstream adaptation and generalization. This survey summarizes recent advances in multimodal adaptation and generalization, particularly how these areas evolve from traditional approaches to foundation models. Specifically, this survey covers (1) multimodal domain adaptation, (2) multimodal test-time adaptation, (3) multimodal domain generalization, (4) domain adaptation and generalization with the help of multimodal foundation models, and (5) adaptation of multimodal foundation models. For each topic, we formally define the problem and give a thorough review of existing methods. Additionally, we analyze relevant datasets and applications, highlighting open challenges and potential future research directions. We also maintain an active repository that contains up-to-date literature and supports research activities in these fields at https://github.com/donghao51/Awesome-Multimodal-Adaptation.&lt;/p&gt;</content:encoded></item><item><title>SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding</title><link>https://doi.org/10.1109/tgrs.2026.3652099</link><guid>10.1109/tgrs.2026.3652099</guid><pubDate>Mon, 12 Jan 2026 22:00:35 +0000</pubDate><dc:creator>Yimin Wei</dc:creator><dc:creator>Aoran Xiao</dc:creator><dc:creator>Yexian Ren</dc:creator><dc:creator>Yuting Zhu</dc:creator><dc:creator>Hongruixuan Chen</dc:creator><dc:creator>Junshi Xia</dc:creator><dc:creator>Naoto Yokoya</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3652099</prism:doi><description>Synthetic Aperture Radar (SAR) is a crucial remote sensing technology, enabling all-weather, day-and-night observation with strong surface penetration for precise and continuous environmental monitoring and analysis. However, SAR image interpretation remains challenging due to its complex physical imaging mechanisms and significant visual disparities from human perception. Recently, Vision-Language Models (VLMs) have demonstrated remarkable success in RGB image understanding, offering powerful open-vocabulary interpretation and flexible language interaction. However, their application to SAR images is severely constrained by the absence of SAR-specific knowledge in their training distributions, leading to suboptimal performance. To address this limitation, we introduce SARLANG-1M, a large-scale benchmark tailored for multimodal SAR image understanding, with a primary focus on integrating SAR with textual modality. SARLANG-1M comprises more than 1 million high-quality SAR image-text pairs collected from over 59 cities worldwide. It features hierarchical resolutions (ranging from 0.1 to 25 meters), fine-grained semantic descriptions (including both concise and detailed captions), diverse remote sensing categories (1,696 object types and 16 land cover classes), and multi-task question-answering pairs spanning seven applications and 1,012 question types. Extensive experiments on mainstream VLMs demonstrate that fine-tuning with SARLANG-1M significantly enhances their performance in SAR image interpretation, reaching performance comparable to human experts. The dataset and code will be made publicly available at https://github.com/Jimmyxichen/SARLANG-1M.
Published: 2026-01-12T22:00:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.836 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yimin Wei; Aoran Xiao; Yexian Ren; Yuting Zhu; Hongruixuan Chen; Junshi Xia; Naoto Yokoya&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3652099"&gt;10.1109/tgrs.2026.3652099&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.836 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is a crucial remote sensing technology, enabling all-weather, day-and-night observation with strong surface penetration for precise and continuous environmental monitoring and analysis. However, SAR image interpretation remains challenging due to its complex physical imaging mechanisms and significant visual disparities from human perception. Recently, Vision-Language Models (VLMs) have demonstrated remarkable success in RGB image understanding, offering powerful open-vocabulary interpretation and flexible language interaction. However, their application to SAR images is severely constrained by the absence of SAR-specific knowledge in their training distributions, leading to suboptimal performance. To address this limitation, we introduce SARLANG-1M, a large-scale benchmark tailored for multimodal SAR image understanding, with a primary focus on integrating SAR with textual modality. SARLANG-1M comprises more than 1 million high-quality SAR image-text pairs collected from over 59 cities worldwide. It features hierarchical resolutions (ranging from 0.1 to 25 meters), fine-grained semantic descriptions (including both concise and detailed captions), diverse remote sensing categories (1,696 object types and 16 land cover classes), and multi-task question-answering pairs spanning seven applications and 1,012 question types. Extensive experiments on mainstream VLMs demonstrate that fine-tuning with SARLANG-1M significantly enhances their performance in SAR image interpretation, reaching performance comparable to human experts. The dataset and code will be made publicly available at https://github.com/Jimmyxichen/SARLANG-1M.&lt;/p&gt;</content:encoded></item><item><title>MPCNet: Multi-scale Perception and Cross-attention Feature Fusion Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3653023</link><guid>10.1109/tgrs.2026.3653023</guid><pubDate>Mon, 12 Jan 2026 22:00:35 +0000</pubDate><dc:creator>Yingmei Zhang</dc:creator><dc:creator>Wangtao Bao</dc:creator><dc:creator>Yong Yang</dc:creator><dc:creator>Weiguo Wan</dc:creator><dc:creator>Qin Xiao</dc:creator><dc:creator>Xueting Zou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3653023</prism:doi><description>Infrared small target detection (IRSTD) is a challenging task due to the small size and low contrast of targets in infrared images. Traditional methods rely on extensive prior knowledge and exhibit poor robustness against complex backgrounds. Although CNN-based methods have made remarkable progress in IRSTD, effectively extracting and fully utilizing features at different levels remains challenging. To address this challenge, we propose two strategies for IRSTD: 1) multi-scale perception and 2) cross-attention feature fusion. Based on these strategies, a multi-scale perception and cross-attention feature fusion network is constructed, named MPCNet. Specifically, a multi-scale perception module in the encoder is designed to capture rich contextual information and enhance the localization perception of small targets. For cross-attention feature fusion, a global semantic-aware fusion module in the encoder and a semantic-guided cross-attention fusion module in the decoder is devised, respectively. The former achieves more refined feature fusion by narrowing the semantic gap between features at different levels, while the latter further captures target features accurately by enhancing the semantic associations between features in the encoder and decoder. Experimental results verify the effectiveness of the proposed MPCNet compared to other state-of-the-art (SOTA) IRSTD methods in both quantitative and qualitative evaluations. The code will be released on https://github.com/Wangtao-Bao/MPCNet.
Published: 2026-01-12T22:00:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.835 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yingmei Zhang; Wangtao Bao; Yong Yang; Weiguo Wan; Qin Xiao; Xueting Zou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3653023"&gt;10.1109/tgrs.2026.3653023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.835 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is a challenging task due to the small size and low contrast of targets in infrared images. Traditional methods rely on extensive prior knowledge and exhibit poor robustness against complex backgrounds. Although CNN-based methods have made remarkable progress in IRSTD, effectively extracting and fully utilizing features at different levels remains challenging. To address this challenge, we propose two strategies for IRSTD: 1) multi-scale perception and 2) cross-attention feature fusion. Based on these strategies, a multi-scale perception and cross-attention feature fusion network is constructed, named MPCNet. Specifically, a multi-scale perception module in the encoder is designed to capture rich contextual information and enhance the localization perception of small targets. For cross-attention feature fusion, a global semantic-aware fusion module in the encoder and a semantic-guided cross-attention fusion module in the decoder is devised, respectively. The former achieves more refined feature fusion by narrowing the semantic gap between features at different levels, while the latter further captures target features accurately by enhancing the semantic associations between features in the encoder and decoder. Experimental results verify the effectiveness of the proposed MPCNet compared to other state-of-the-art (SOTA) IRSTD methods in both quantitative and qualitative evaluations. The code will be released on https://github.com/Wangtao-Bao/MPCNet.&lt;/p&gt;</content:encoded></item><item><title>RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition</title><link>https://doi.org/10.1109/tip.2025.3644175</link><guid>10.1109/tip.2025.3644175</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Ziyu Liu</dc:creator><dc:creator>Zeyi Sun</dc:creator><dc:creator>Yuhang Zang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Pan Zhang</dc:creator><dc:creator>Xiaoyi Dong</dc:creator><dc:creator>Yuanjun Xiong</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Jiaqi Wang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644175</prism:doi><description>CLIP (Contrastive Language–Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the models comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyu Liu; Zeyi Sun; Yuhang Zang; Wei Li; Pan Zhang; Xiaoyi Dong; Yuanjun Xiong; Dahua Lin; Jiaqi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644175"&gt;10.1109/tip.2025.3644175&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;CLIP (Contrastive Language–Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size. To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the models comprehensive knowledge base, significantly boosting accuracy across a range of vision-language recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting.&lt;/p&gt;</content:encoded></item><item><title>Learnable Object Queries for Few-Shot Semantic Segmentation</title><link>https://doi.org/10.1109/tip.2025.3650372</link><guid>10.1109/tip.2025.3650372</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Yadang Chen</dc:creator><dc:creator>Wenbo Chen</dc:creator><dc:creator>Yuhui Zheng</dc:creator><dc:creator>Zhi-Xin Yang</dc:creator><dc:creator>Enhua Wu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3650372</prism:doi><description>Few-shot semantic segmentation (FSS) aims to segment unseen-category objects given only a few annotated samples. Although significant progress has been made in the field of FSS, selecting an appropriate feature matching method remains a challenge. Traditional prototype-based methods can preserve high-level semantic features, but they tend to lose detailed information. On the other hand, pixel-level comparison methods retain fine-grained details but are vulnerable to distractors and noise, leading to poor robustness. To address these issues, this paper proposes a target-agnostic object-based method. Specifically, we propose a set of learnable “object queries” to extract object features, which preserve both high-level semantic information and fine-grained details. Additionally, during the training phase, we exploit the prior knowledge of foreground and background embedded in the samples to enhance the model’s performance. In the inference phase, the model utilizes both the support set and the learned prior knowledge to perform segmentation tasks, mitigating the data distribution bias caused by limited samples. Extensive experiments on benchmark datasets demonstrate that our method outperforms state-of-the-art approaches in both accuracy and robustness. Code is available at https://github.com/wenbo456/OTBNet.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yadang Chen; Wenbo Chen; Yuhui Zheng; Zhi-Xin Yang; Enhua Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3650372"&gt;10.1109/tip.2025.3650372&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot semantic segmentation (FSS) aims to segment unseen-category objects given only a few annotated samples. Although significant progress has been made in the field of FSS, selecting an appropriate feature matching method remains a challenge. Traditional prototype-based methods can preserve high-level semantic features, but they tend to lose detailed information. On the other hand, pixel-level comparison methods retain fine-grained details but are vulnerable to distractors and noise, leading to poor robustness. To address these issues, this paper proposes a target-agnostic object-based method. Specifically, we propose a set of learnable “object queries” to extract object features, which preserve both high-level semantic information and fine-grained details. Additionally, during the training phase, we exploit the prior knowledge of foreground and background embedded in the samples to enhance the model’s performance. In the inference phase, the model utilizes both the support set and the learned prior knowledge to perform segmentation tasks, mitigating the data distribution bias caused by limited samples. Extensive experiments on benchmark datasets demonstrate that our method outperforms state-of-the-art approaches in both accuracy and robustness. Code is available at https://github.com/wenbo456/OTBNet.&lt;/p&gt;</content:encoded></item><item><title>Masked Self-Attention Fusion Network for Joint Classification of Hyperspectral and LiDAR Data</title><link>https://doi.org/10.1109/tip.2025.3648926</link><guid>10.1109/tip.2025.3648926</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Lulu Shi</dc:creator><dc:creator>Chunchao Li</dc:creator><dc:creator>Zhengchao Zeng</dc:creator><dc:creator>Puhong Duan</dc:creator><dc:creator>Behnood Rasti</dc:creator><dc:creator>Antonio Plaza</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648926</prism:doi><description>Hyperspectral imaging (HSI) captures abundant spectral information of land covers while light detection and ranging (LiDAR) provides elevation and structural characteristics. Joint classification of HSI and LiDAR data can effectively merge spectral and elevation information to enhance the outcome of land cover classification. Current HSI and LiDAR joint classification approaches mainly employ a three-layer deep network to extract high-order features, followed by a concatenation or weighted fusion scheme which cannot fully exploit the unique properties of different data modalities. Meanwhile, these methods usually require high computational resources. To alleviate these issues, this paper proposes a masked self-attention fusion network (MSAF) for joint HSI and LiDAR classification, where a cascaded cross-attention fusion framework is designed to fully merge different stages of features. First, a mobile convolution block is developed to extract multi-modal data features. Then, a multi-view sequence embedding method is proposed to effectively integrate elevation information and spectral-spatial information so as to obtain token sequences. Finally, an effective masked self-attention mechanism is designed to fuse token sequences. Experimental results on multiple datasets indicate that the proposed framework significantly outperforms other advanced multi-modal fusion methods in terms of classification performance and computing efficiency. The code of this manuscript is available on https://github.com/lulushh/MSAF.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.827 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lulu Shi; Chunchao Li; Zhengchao Zeng; Puhong Duan; Behnood Rasti; Antonio Plaza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648926"&gt;10.1109/tip.2025.3648926&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.827 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral imaging (HSI) captures abundant spectral information of land covers while light detection and ranging (LiDAR) provides elevation and structural characteristics. Joint classification of HSI and LiDAR data can effectively merge spectral and elevation information to enhance the outcome of land cover classification. Current HSI and LiDAR joint classification approaches mainly employ a three-layer deep network to extract high-order features, followed by a concatenation or weighted fusion scheme which cannot fully exploit the unique properties of different data modalities. Meanwhile, these methods usually require high computational resources. To alleviate these issues, this paper proposes a masked self-attention fusion network (MSAF) for joint HSI and LiDAR classification, where a cascaded cross-attention fusion framework is designed to fully merge different stages of features. First, a mobile convolution block is developed to extract multi-modal data features. Then, a multi-view sequence embedding method is proposed to effectively integrate elevation information and spectral-spatial information so as to obtain token sequences. Finally, an effective masked self-attention mechanism is designed to fuse token sequences. Experimental results on multiple datasets indicate that the proposed framework significantly outperforms other advanced multi-modal fusion methods in terms of classification performance and computing efficiency. The code of this manuscript is available on https://github.com/lulushh/MSAF.&lt;/p&gt;</content:encoded></item><item><title>SAR-W-MixMAE: Polarization-Aware Self-Supervised Pretraining for Masked Autoencoders on SAR Data</title><link>https://doi.org/10.1109/jstars.2026.3652404</link><guid>10.1109/jstars.2026.3652404</guid><pubDate>Mon, 12 Jan 2026 22:01:17 +0000</pubDate><dc:creator>Ali Caglayan</dc:creator><dc:creator>Nevrez Imamoglu</dc:creator><dc:creator>Toru Kouyama</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3652404</prism:doi><description>Self-supervised pretraining has emerged as a powerful approach for learning transferable representations from large-scale unlabeled data, significantly reducing reliance on taskspecific labeled datasets. Although masked autoencoders (MAEs) have shown considerable success in optical remote sensing, such as RGB and multispectral imagery, their application to synthetic aperture radar (SAR) data remains underexplored due to its unique imaging characteristics, including speckle content and intensity variability. In this work, we investigate the effectiveness of masked autoencoders for SAR pretraining, specifically applying MixMAE [1] to Sentinel-1 SAR imagery. We introduce SARW- MixMAE, a domain-aware self-supervised learning approach that incorporates a SAR-specific pixel-wise weighting strategy into the reconstruction loss, mitigating the effects of speckle content and high-intensity backscatter variations. Experimental results demonstrate that SAR-W-MixMAE consistently improves baseline models in multilabel SAR image classification and flood detection tasks, extending the state-of-the-art performance on the popular BigEarthNet dataset. Extensive ablation studies reveal that pretraining duration and fine-tuning dataset size significantly impact downstream performance. In particular, early stopping during pretraining can yield optimal downstream task accuracy, challenging the assumption that prolonged pretraining enhances results. These insights contribute to the development of foundation models tailored for SAR imagery and provide practical guidelines for optimizing pretraining strategies in remote sensing applications.
Published: 2026-01-12T22:01:17+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ali Caglayan; Nevrez Imamoglu; Toru Kouyama&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3652404"&gt;10.1109/jstars.2026.3652404&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised pretraining has emerged as a powerful approach for learning transferable representations from large-scale unlabeled data, significantly reducing reliance on taskspecific labeled datasets. Although masked autoencoders (MAEs) have shown considerable success in optical remote sensing, such as RGB and multispectral imagery, their application to synthetic aperture radar (SAR) data remains underexplored due to its unique imaging characteristics, including speckle content and intensity variability. In this work, we investigate the effectiveness of masked autoencoders for SAR pretraining, specifically applying MixMAE [1] to Sentinel-1 SAR imagery. We introduce SARW- MixMAE, a domain-aware self-supervised learning approach that incorporates a SAR-specific pixel-wise weighting strategy into the reconstruction loss, mitigating the effects of speckle content and high-intensity backscatter variations. Experimental results demonstrate that SAR-W-MixMAE consistently improves baseline models in multilabel SAR image classification and flood detection tasks, extending the state-of-the-art performance on the popular BigEarthNet dataset. Extensive ablation studies reveal that pretraining duration and fine-tuning dataset size significantly impact downstream performance. In particular, early stopping during pretraining can yield optimal downstream task accuracy, challenging the assumption that prolonged pretraining enhances results. These insights contribute to the development of foundation models tailored for SAR imagery and provide practical guidelines for optimizing pretraining strategies in remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>Information-Maximized Soft Variable Discretization for Self-Supervised Image Representation Learning</title><link>https://doi.org/10.1109/tip.2025.3648138</link><guid>10.1109/tip.2025.3648138</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Chuang Niu</dc:creator><dc:creator>Wenjun Xia</dc:creator><dc:creator>Hongming Shan</dc:creator><dc:creator>Ge Wang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648138</prism:doi><description>Self-supervised learning (SSL) has emerged as a crucial technique in image processing, encoding, and understanding, especially for developing today’s vision foundation models that utilize large-scale datasets without annotations to enhance various downstream tasks. This study introduces a novel SSL approach, Information-Maximized Soft Variable Discretization (IMSVD), for image representation learning. Specifically, IMSVD softly discretizes each variable in the latent space, enabling the estimation of their probability distributions over training batches and allowing the learning process to be directly guided by information measures. Motivated by the MultiView assumption, we propose an information-theoretic objective function to learn transform-invariant, non-trivial, and redundancy-minimized representation features. We then derive a joint-cross entropy loss function for self-supervised image representation learning, which theoretically enjoys superiority over the existing methods in reducing feature redundancy. Notably, our non-contrastive IMSVD method statistically performs contrastive learning. Extensive experimental results demonstrate the effectiveness of IMSVD on various downstream tasks in terms of both accuracy and efficiency. Thanks to our variable discretization, the embedding features optimized by IMSVD offer unique explainability at the variable level. IMSVD has the potential to be adapted to other learning paradigms. Our code is publicly available at https://github.com/niuchuangnn/IMSVD.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chuang Niu; Wenjun Xia; Hongming Shan; Ge Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648138"&gt;10.1109/tip.2025.3648138&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised learning (SSL) has emerged as a crucial technique in image processing, encoding, and understanding, especially for developing today’s vision foundation models that utilize large-scale datasets without annotations to enhance various downstream tasks. This study introduces a novel SSL approach, Information-Maximized Soft Variable Discretization (IMSVD), for image representation learning. Specifically, IMSVD softly discretizes each variable in the latent space, enabling the estimation of their probability distributions over training batches and allowing the learning process to be directly guided by information measures. Motivated by the MultiView assumption, we propose an information-theoretic objective function to learn transform-invariant, non-trivial, and redundancy-minimized representation features. We then derive a joint-cross entropy loss function for self-supervised image representation learning, which theoretically enjoys superiority over the existing methods in reducing feature redundancy. Notably, our non-contrastive IMSVD method statistically performs contrastive learning. Extensive experimental results demonstrate the effectiveness of IMSVD on various downstream tasks in terms of both accuracy and efficiency. Thanks to our variable discretization, the embedding features optimized by IMSVD offer unique explainability at the variable level. IMSVD has the potential to be adapted to other learning paradigms. Our code is publicly available at https://github.com/niuchuangnn/IMSVD.&lt;/p&gt;</content:encoded></item><item><title>Neuron Abandoning Attention Flow: Visual Explanation of Dynamics Inside CNN Models</title><link>https://doi.org/10.1109/tpami.2026.3651260</link><guid>10.1109/tpami.2026.3651260</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Yi Liao</dc:creator><dc:creator>Yongsheng Gao</dc:creator><dc:creator>Weichuan Zhang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651260</prism:doi><description>In this paper, we present a Neuron Abandoning Attention Flow (NAFlow) method to address the unsolved problem of visually explaining the attention evolution dynamics inside CNNs when making their classification decisions. A novel cascading neuron abandoning back- propagation algorithm is designed to precisely exclude the abandoned neurons on all intermediate layers inside a CNN model for the first time. Firstly, a Neuron Abandoning Back-Propagation module is proposed to generate Back-Propagation Feature Maps (BPFM) by using inverse function of the intermediate layers of CNN models, on which the neurons not used for decision-making are removed. Meanwhile, the cascading NA-BP modules calculate the tensors of importance coefficients which are linearly combined with the tensors of BPFMs to form the NAFlow. Secondly, to be able to visualize attention flow for similarity metric-based CNN models, a new channel contribution weights module is proposed to calculate the importance coefficients via Jacobian Matrix. Extensive evaluations demonstrate the effectiveness of the proposed NAFlow across eleven widely-used CNN models for various tasks of general image classification, contrastive learning classification, few-shot image classification, and image retrieval.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Liao; Yongsheng Gao; Weichuan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651260"&gt;10.1109/tpami.2026.3651260&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we present a Neuron Abandoning Attention Flow (NAFlow) method to address the unsolved problem of visually explaining the attention evolution dynamics inside CNNs when making their classification decisions. A novel cascading neuron abandoning back- propagation algorithm is designed to precisely exclude the abandoned neurons on all intermediate layers inside a CNN model for the first time. Firstly, a Neuron Abandoning Back-Propagation module is proposed to generate Back-Propagation Feature Maps (BPFM) by using inverse function of the intermediate layers of CNN models, on which the neurons not used for decision-making are removed. Meanwhile, the cascading NA-BP modules calculate the tensors of importance coefficients which are linearly combined with the tensors of BPFMs to form the NAFlow. Secondly, to be able to visualize attention flow for similarity metric-based CNN models, a new channel contribution weights module is proposed to calculate the importance coefficients via Jacobian Matrix. Extensive evaluations demonstrate the effectiveness of the proposed NAFlow across eleven widely-used CNN models for various tasks of general image classification, contrastive learning classification, few-shot image classification, and image retrieval.&lt;/p&gt;</content:encoded></item><item><title>OIF-PCR++: Point Cloud Registration via Progressive Distillation of Conditional Positional Encoding</title><link>https://doi.org/10.1109/tpami.2026.3652316</link><guid>10.1109/tpami.2026.3652316</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Fan Yang</dc:creator><dc:creator>Zhi Chen</dc:creator><dc:creator>Nanjun Yuan</dc:creator><dc:creator>Lin Guo</dc:creator><dc:creator>Wenbing Tao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3652316</prism:doi><description>Transformer architecture has shown significant potential in various visual tasks, including point cloud registration. Positional encoding, as an order-aware module, plays a crucial role in Transformer framework. In this paper, we propose OIF-PCR++, a conditional positional encoding (CPE) method for point cloud registration. The core CPE module utilizes length and vector encoding at different stages, conditioned on the relative pose states between the point clouds to be registered. As a result, it progressively alleviates the feature ambiguity through the incorporation of geometric cues. Building upon the proposed CPE, we introduce an iterative positional encoding optimization pipeline comprising two stages: 1) We find one correspondence via a differentiable optimal transport layer, and use it to encode length information into the point cloud features, which alleviates challenges arising from differing reference frames by enhancing spatial consistency. 2) We apply a progressive direction alignment strategy to achieve rough alignment between the paired point clouds, and then gradually incorporate direction information with the aid of this alignment, further enhancing feature distinctiveness and reducing feature ambiguity. Through this iterative optimization process, length and direction information are effectively integrated to achieve consistent and distinctive positional encoding, thus enabling the learning of discriminative point cloud features. Additionally, we present an inlier propagation mechanism that harmoniously integrates consistent geometric information for positional encoding. The proposed positional encoding is highly efficient, introducing only a marginal increase in computational overhead while significantly improving feature distinguishability. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art methods across indoor, outdoor, object-level, and multi-way benchmarks, while also generalizing well ...
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fan Yang; Zhi Chen; Nanjun Yuan; Lin Guo; Wenbing Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3652316"&gt;10.1109/tpami.2026.3652316&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer architecture has shown significant potential in various visual tasks, including point cloud registration. Positional encoding, as an order-aware module, plays a crucial role in Transformer framework. In this paper, we propose OIF-PCR++, a conditional positional encoding (CPE) method for point cloud registration. The core CPE module utilizes length and vector encoding at different stages, conditioned on the relative pose states between the point clouds to be registered. As a result, it progressively alleviates the feature ambiguity through the incorporation of geometric cues. Building upon the proposed CPE, we introduce an iterative positional encoding optimization pipeline comprising two stages: 1) We find one correspondence via a differentiable optimal transport layer, and use it to encode length information into the point cloud features, which alleviates challenges arising from differing reference frames by enhancing spatial consistency. 2) We apply a progressive direction alignment strategy to achieve rough alignment between the paired point clouds, and then gradually incorporate direction information with the aid of this alignment, further enhancing feature distinctiveness and reducing feature ambiguity. Through this iterative optimization process, length and direction information are effectively integrated to achieve consistent and distinctive positional encoding, thus enabling the learning of discriminative point cloud features. Additionally, we present an inlier propagation mechanism that harmoniously integrates consistent geometric information for positional encoding. The proposed positional encoding is highly efficient, introducing only a marginal increase in computational overhead while significantly improving feature distinguishability. Extensive experiments demonstrate that our method achieves superior performance compared to state-of-the-art methods across indoor, outdoor, object-level, and multi-way benchmarks, while also generalizing well ...&lt;/p&gt;</content:encoded></item><item><title>G
                    &lt;sup&gt;2&lt;/sup&gt;
                    HFNet: GeoGran-Aware Hierarchical Feature Fusion Network for Salient Object Detection in Optical Remote Sensing Images</title><link>https://doi.org/10.1109/tcsvt.2026.3653188</link><guid>10.1109/tcsvt.2026.3653188</guid><pubDate>Mon, 12 Jan 2026 22:04:02 +0000</pubDate><dc:creator>Bin Wan</dc:creator><dc:creator>Runmin Cong</dc:creator><dc:creator>Xiaofei Zhou</dc:creator><dc:creator>Hao Fang</dc:creator><dc:creator>Chengtao Lv</dc:creator><dc:creator>Sam Kwong</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3653188</prism:doi><description>Remote sensing images captured from aerial perspectives often exhibit significant scale variations and complex backgrounds, posing challenges for salient object detection (SOD). Existing methods typically extract multi-level features at a single scale using uniform attention mechanisms, leading to suboptimal representations and incomplete detection results. To address these issues, we propose a GeoGran-Aware Hierarchical Feature Fusion Network (G2HFNet) that fully exploits geometric and granular cues in optical remote sensing images. Specifically, G2HFNet adopts Swin Transformer as the backbone to extract multi-level features and integrates three key modules: the multi-scale detail enhancement (MDE) module to handle object scale variations and enrich fine details, the dual-branch geo-gran complementary (DGC) module to jointly capture fine-grained details and positional information in mid-level features, and the deep semantic perception (DSP) module to refine high-level positional cues via self-attention. Additionally, a local-global guidance fusion (LGF) module is introduced to replace traditional convolutions for effective multi-level feature integration. Extensive experiments demonstrate that G2HFNet achieves high-quality saliency maps and significantly improves detection performance in challenging remote sensing scenarios.
Published: 2026-01-12T22:04:02+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bin Wan; Runmin Cong; Xiaofei Zhou; Hao Fang; Chengtao Lv; Sam Kwong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3653188"&gt;10.1109/tcsvt.2026.3653188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing images captured from aerial perspectives often exhibit significant scale variations and complex backgrounds, posing challenges for salient object detection (SOD). Existing methods typically extract multi-level features at a single scale using uniform attention mechanisms, leading to suboptimal representations and incomplete detection results. To address these issues, we propose a GeoGran-Aware Hierarchical Feature Fusion Network (G2HFNet) that fully exploits geometric and granular cues in optical remote sensing images. Specifically, G2HFNet adopts Swin Transformer as the backbone to extract multi-level features and integrates three key modules: the multi-scale detail enhancement (MDE) module to handle object scale variations and enrich fine details, the dual-branch geo-gran complementary (DGC) module to jointly capture fine-grained details and positional information in mid-level features, and the deep semantic perception (DSP) module to refine high-level positional cues via self-attention. Additionally, a local-global guidance fusion (LGF) module is introduced to replace traditional convolutions for effective multi-level feature integration. Extensive experiments demonstrate that G2HFNet achieves high-quality saliency maps and significantly improves detection performance in challenging remote sensing scenarios.&lt;/p&gt;</content:encoded></item><item><title>Completing Missing Entities: Exploring Consistency Reasoning for Remote Sensing Object Detection</title><link>https://doi.org/10.1109/tip.2025.3648164</link><guid>10.1109/tip.2025.3648164</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Peng Sun</dc:creator><dc:creator>Yongbin Zheng</dc:creator><dc:creator>Wanying Xu</dc:creator><dc:creator>Jian Li</dc:creator><dc:creator>Jiansong Yang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648164</prism:doi><description>Recent studies in remote sensing object detection have made excellent progress and shown promising performance. However, most current detectors only explore rotation-invariant feature extraction but disregard the valuable spatial and semantic prior knowledge in remote sensing images (RSIs), which limits the detection performance when encountering blurred or heavy occluded objects. To address this issue, we propose a mask-reconstruction relation learning (MRRL) framework to learn such prior knowledge among objects and a consistency-reasoning transformer over relation proposals (CTRP) to recognize objects with limited visual features via consistency reasoning. Specifically, MRRL framework applies random mask to some objects in the training dataset and performs masked objects reconstruction to guide the network to learn the distribution consistency of objects. CTRP is the core component of the MRRL framework, which models the interaction between spatial and semantic priors, and uses easy detected objects to reason hard detected objects. The trained CTRP can be integrated into the existing detector to improve the ability of object detection with limited visual features in RSIs. Extensive experiments on widely-used datasets for two distinct tasks, namely remote sensing object detection task and occluded object detection task, demonstrate the effectiveness of the proposed method. Source code is available at https://github.com/sunpeng96/CTRP_mmrotate.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peng Sun; Yongbin Zheng; Wanying Xu; Jian Li; Jiansong Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648164"&gt;10.1109/tip.2025.3648164&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Recent studies in remote sensing object detection have made excellent progress and shown promising performance. However, most current detectors only explore rotation-invariant feature extraction but disregard the valuable spatial and semantic prior knowledge in remote sensing images (RSIs), which limits the detection performance when encountering blurred or heavy occluded objects. To address this issue, we propose a mask-reconstruction relation learning (MRRL) framework to learn such prior knowledge among objects and a consistency-reasoning transformer over relation proposals (CTRP) to recognize objects with limited visual features via consistency reasoning. Specifically, MRRL framework applies random mask to some objects in the training dataset and performs masked objects reconstruction to guide the network to learn the distribution consistency of objects. CTRP is the core component of the MRRL framework, which models the interaction between spatial and semantic priors, and uses easy detected objects to reason hard detected objects. The trained CTRP can be integrated into the existing detector to improve the ability of object detection with limited visual features in RSIs. Extensive experiments on widely-used datasets for two distinct tasks, namely remote sensing object detection task and occluded object detection task, demonstrate the effectiveness of the proposed method. Source code is available at https://github.com/sunpeng96/CTRP_mmrotate.&lt;/p&gt;</content:encoded></item><item><title>Balancing Performance and Efficiency: Towards Superior Image Segmentation with Adaptive Sparse Attention</title><link>https://doi.org/10.1109/tcsvt.2026.3651347</link><guid>10.1109/tcsvt.2026.3651347</guid><pubDate>Mon, 12 Jan 2026 22:04:02 +0000</pubDate><dc:creator>Xiaoqing Zhao</dc:creator><dc:creator>Chaojun Zhang</dc:creator><dc:creator>Yuan Gao</dc:creator><dc:creator>Jing Yang</dc:creator><dc:creator>Laurence T. Yang</dc:creator><dc:creator>Jieming Yang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651347</prism:doi><description>Recently, most image segmentation methods exhibit an extreme trade-off between performance and efficiency, resulting in approaches with high performance typically having low computational efficiency, while efficient methods compromise on segmentation accuracy. To address this dual challenge, this study introduces a simple yet efficient segmentation framework based on Multi-scale Prototype matching and visual Sparse Attention mechanisms (MPSA), which is a transformer-based architecture designed to optimize the balance between performance and efficiency. The proposed MPSA integrates a novel lightweight cross-attention mechanism and prototype selection and filtering strategy to accurately correlate category queries with corresponding visual objects with a multi-scale Feature Pyramid Network (FPN). Within the pixel decoder, our Axial Convolution Enhanced (ACE) module mitigates lost global context by combining depth-wise separable convolutions with deformable convolutions, thereby recovering global semantics while preserving fine-grained spatial details. Through this innovative design, MPSA demonstrates outstanding performance in both semantic and panoptic segmentation tasks across multiple datasets. Remarkably, MPSA achieves surprising 83.9% mIoU with only 114M parameters on the Cityscapes dataset while compared to some state-of-the-art architectures, highlighting its ability to deliver exceptional results with significantly reduced resource consumption. Our code is released at https://github.com/zxqing01/MPSA.
Published: 2026-01-12T22:04:02+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoqing Zhao; Chaojun Zhang; Yuan Gao; Jing Yang; Laurence T. Yang; Jieming Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651347"&gt;10.1109/tcsvt.2026.3651347&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, most image segmentation methods exhibit an extreme trade-off between performance and efficiency, resulting in approaches with high performance typically having low computational efficiency, while efficient methods compromise on segmentation accuracy. To address this dual challenge, this study introduces a simple yet efficient segmentation framework based on Multi-scale Prototype matching and visual Sparse Attention mechanisms (MPSA), which is a transformer-based architecture designed to optimize the balance between performance and efficiency. The proposed MPSA integrates a novel lightweight cross-attention mechanism and prototype selection and filtering strategy to accurately correlate category queries with corresponding visual objects with a multi-scale Feature Pyramid Network (FPN). Within the pixel decoder, our Axial Convolution Enhanced (ACE) module mitigates lost global context by combining depth-wise separable convolutions with deformable convolutions, thereby recovering global semantics while preserving fine-grained spatial details. Through this innovative design, MPSA demonstrates outstanding performance in both semantic and panoptic segmentation tasks across multiple datasets. Remarkably, MPSA achieves surprising 83.9% mIoU with only 114M parameters on the Cityscapes dataset while compared to some state-of-the-art architectures, highlighting its ability to deliver exceptional results with significantly reduced resource consumption. Our code is released at https://github.com/zxqing01/MPSA.&lt;/p&gt;</content:encoded></item><item><title>DR-Net: Dual-Representation Network with Motion-Aware Augmentation for 3D Object Detection based on 4D Radars</title><link>https://doi.org/10.1109/tcsvt.2026.3651715</link><guid>10.1109/tcsvt.2026.3651715</guid><pubDate>Mon, 12 Jan 2026 22:04:02 +0000</pubDate><dc:creator>Jinrong Cao</dc:creator><dc:creator>Qiang Ling</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651715</prism:doi><description>Nowadays 4D millimeter-wave radars can generate LiDAR-equivalent 3D point clouds with superior cost efficiency and enhanced stability in harsh environments, and have found wide applications in 3D object detection. However, existing 4D radar-based object detection methods may overlook the detrimental effects of information loss during feature processing and fail to effectively leverage the velocity information of 4D radars. These limitations hinder the full exploitation of 4D radar’s potential. To resolve this issue, we propose a dual-representation network with motion-aware augmentation, named as DR-Net. Specifically, to compensate the information loss, we propose a dual-representation encoder (DRE) and a sampling fusion backbone (SFB). The encoder extracts radar features from both pillars’ and points’ perspectives, well exploiting the complementarity between pillar-level structural context and point-level fine-grained details. The backbone fuses features of the pillar representation and the point representation, effectively mitigating the feature-level information loss. Instead of simply taking the velocity information as an additional input, we design a motion-aware augmentation (MAA) module to augment 4D radar point cloud data from the perspectives of both raw point cloud features and training instances. Finally, we extend the original 3D detection head by incorporating an additional velocity supervision branch to enhance the capability of perceiving both static and dynamic objects. We conduct comprehensive experiments on the View-of-Delft (VoD) and TJ4DRadSet datasets. Experimental results reveal that compared with some state-of-the-art 4D radar-based approaches, our DR-Net achieves significant performance advancement.
Published: 2026-01-12T22:04:02+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinrong Cao; Qiang Ling&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651715"&gt;10.1109/tcsvt.2026.3651715&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Nowadays 4D millimeter-wave radars can generate LiDAR-equivalent 3D point clouds with superior cost efficiency and enhanced stability in harsh environments, and have found wide applications in 3D object detection. However, existing 4D radar-based object detection methods may overlook the detrimental effects of information loss during feature processing and fail to effectively leverage the velocity information of 4D radars. These limitations hinder the full exploitation of 4D radar’s potential. To resolve this issue, we propose a dual-representation network with motion-aware augmentation, named as DR-Net. Specifically, to compensate the information loss, we propose a dual-representation encoder (DRE) and a sampling fusion backbone (SFB). The encoder extracts radar features from both pillars’ and points’ perspectives, well exploiting the complementarity between pillar-level structural context and point-level fine-grained details. The backbone fuses features of the pillar representation and the point representation, effectively mitigating the feature-level information loss. Instead of simply taking the velocity information as an additional input, we design a motion-aware augmentation (MAA) module to augment 4D radar point cloud data from the perspectives of both raw point cloud features and training instances. Finally, we extend the original 3D detection head by incorporating an additional velocity supervision branch to enhance the capability of perceiving both static and dynamic objects. We conduct comprehensive experiments on the View-of-Delft (VoD) and TJ4DRadSet datasets. Experimental results reveal that compared with some state-of-the-art 4D radar-based approaches, our DR-Net achieves significant performance advancement.&lt;/p&gt;</content:encoded></item><item><title>Causal Inference Via Style Bias Deconfounding for Domain Generalization</title><link>https://doi.org/10.1109/tpami.2026.3652609</link><guid>10.1109/tpami.2026.3652609</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Jiaxi Li</dc:creator><dc:creator>Di Lin</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Hongying Liu</dc:creator><dc:creator>Liang Wan</dc:creator><dc:creator>Wei Feng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3652609</prism:doi><description>Deep neural networks (DNNs) often struggle with out-of-distribution data, limiting their reliability in real-world visual applications. To address this issue, domain generalization methods have been developed to learn domain-invariant features from single or multiple training domains, enabling generalization to unseen testing domains. However, existing approaches usually overlook the impact of style frequency within the training set. This oversight predisposes models to capture spurious visual correlations caused by style confounding factors, rather than learning truly causal representations, thereby undermining inference reliability. In this work, we introduce Style Deconfounding Causal Learning (SDCL), a novel causal inference-based framework that explicitly addresses style as a confounding factor to enhance domain generalization in image modalities. Our approaches begins with constructing a structural causal model (SCM) tailored to the domain generalization problem and applies a backdoor adjustment strategy to account for style influence. Building on this foundation, we design a style-guided expert module (SGEM) to adaptively clusters style distributions during training, capturing the global confounding style. Additionally, a backdoor causal learning module (BDCL) performs causal interventions during feature extraction, ensuring fair integration of global confounding styles into sample predictions, effectively reducing style bias. The SDCL framework is highly versatile and can be seamlessly integrated with state-of-the-art data augmentation techniques. Extensive experiments across diverse natural and medical image recognition tasks validate its efficacy, demonstrating superior performance in both multi-domain and the more challenging single-domain generalization scenarios.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaxi Li; Di Lin; Hao Chen; Hongying Liu; Liang Wan; Wei Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3652609"&gt;10.1109/tpami.2026.3652609&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Deep neural networks (DNNs) often struggle with out-of-distribution data, limiting their reliability in real-world visual applications. To address this issue, domain generalization methods have been developed to learn domain-invariant features from single or multiple training domains, enabling generalization to unseen testing domains. However, existing approaches usually overlook the impact of style frequency within the training set. This oversight predisposes models to capture spurious visual correlations caused by style confounding factors, rather than learning truly causal representations, thereby undermining inference reliability. In this work, we introduce Style Deconfounding Causal Learning (SDCL), a novel causal inference-based framework that explicitly addresses style as a confounding factor to enhance domain generalization in image modalities. Our approaches begins with constructing a structural causal model (SCM) tailored to the domain generalization problem and applies a backdoor adjustment strategy to account for style influence. Building on this foundation, we design a style-guided expert module (SGEM) to adaptively clusters style distributions during training, capturing the global confounding style. Additionally, a backdoor causal learning module (BDCL) performs causal interventions during feature extraction, ensuring fair integration of global confounding styles into sample predictions, effectively reducing style bias. The SDCL framework is highly versatile and can be seamlessly integrated with state-of-the-art data augmentation techniques. Extensive experiments across diverse natural and medical image recognition tasks validate its efficacy, demonstrating superior performance in both multi-domain and the more challenging single-domain generalization scenarios.&lt;/p&gt;</content:encoded></item><item><title>WDHN-Det: Wavelet-Coordinated Dynamic Hyperbolic Normalization for Multi-Source Remote Sensing Object Detection</title><link>https://doi.org/10.1109/tgrs.2026.3651259</link><guid>10.1109/tgrs.2026.3651259</guid><pubDate>Mon, 12 Jan 2026 22:00:35 +0000</pubDate><dc:creator>Ruchan Dong</dc:creator><dc:creator>Wenjing Wu</dc:creator><dc:creator>Licheng Jiao</dc:creator><dc:creator>Xu Liu</dc:creator><dc:creator>Jin Zhao</dc:creator><dc:creator>Shunyao Yin</dc:creator><dc:creator>Zengxin Yun</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3651259</prism:doi><description>Multi-source remote sensing image object detection is essential for disaster monitoring and national defense. However, challenges remain due to heterogeneous imaging mechanisms, limited multi-scale feature representation, and complex background interference. To address these issues, we propose WDHN-Det(Wavelet-Coordinated Dynamic Hyperbolic Normalization for Multi-Source Remote Sensing Object Detection), a unified object detection framework designed for robust and accurate detection in multi-source remote sensing scenarios. The framework incorporates three core modules: (1) a Cross-Scale Wavelet Feature Fusion (CSWFF) module that utilizes Haar wavelet decomposition in place of conventional downsampling operations, enabling cross-scale feature integration while preserving edge details and reducing high-frequency information loss; (2) a Dynamic Hyperbolic Pyramid Squeeze Attention (DyHPSA) module that combines dynamic attention with hyperbolic normalization, enabling adaptive feature modulation through learnable curvature parameters to improve robustness against outliers; and (3) a Cross-Modal Adaptive Fine-Grained Pyramid Fusion (CM-AFGPF) module that embeds dual-dimension attention into the SPPF structure, refining feature granularity and boosting detection accuracy. Extensive experiments conducted on the RSOD (optical) and SSDD (SAR) datasets show that WDHN-Det achieves state-of-the-art performance, with mAP/Precision of 95.86%/95.08% on RSOD and 98.7%/98.7% on SSDD. Compared with baseline methods, WDHN-Det improves mAP by 2.68% and 0.41%, and Precision by 6.68% and 0.66% on RSOD and SSDD, respectively, while maintaining high detection efficiency. These results demonstrate the effectiveness and generalization capability of WDHN-Det for multi-source remote sensing object detection. The project is publicly available on https://github.com/cicadachan/WDHN-Det.
Published: 2026-01-12T22:00:35+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruchan Dong; Wenjing Wu; Licheng Jiao; Xu Liu; Jin Zhao; Shunyao Yin; Zengxin Yun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3651259"&gt;10.1109/tgrs.2026.3651259&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-source remote sensing image object detection is essential for disaster monitoring and national defense. However, challenges remain due to heterogeneous imaging mechanisms, limited multi-scale feature representation, and complex background interference. To address these issues, we propose WDHN-Det(Wavelet-Coordinated Dynamic Hyperbolic Normalization for Multi-Source Remote Sensing Object Detection), a unified object detection framework designed for robust and accurate detection in multi-source remote sensing scenarios. The framework incorporates three core modules: (1) a Cross-Scale Wavelet Feature Fusion (CSWFF) module that utilizes Haar wavelet decomposition in place of conventional downsampling operations, enabling cross-scale feature integration while preserving edge details and reducing high-frequency information loss; (2) a Dynamic Hyperbolic Pyramid Squeeze Attention (DyHPSA) module that combines dynamic attention with hyperbolic normalization, enabling adaptive feature modulation through learnable curvature parameters to improve robustness against outliers; and (3) a Cross-Modal Adaptive Fine-Grained Pyramid Fusion (CM-AFGPF) module that embeds dual-dimension attention into the SPPF structure, refining feature granularity and boosting detection accuracy. Extensive experiments conducted on the RSOD (optical) and SSDD (SAR) datasets show that WDHN-Det achieves state-of-the-art performance, with mAP/Precision of 95.86%/95.08% on RSOD and 98.7%/98.7% on SSDD. Compared with baseline methods, WDHN-Det improves mAP by 2.68% and 0.41%, and Precision by 6.68% and 0.66% on RSOD and SSDD, respectively, while maintaining high detection efficiency. These results demonstrate the effectiveness and generalization capability of WDHN-Det for multi-source remote sensing object detection. The project is publicly available on https://github.com/cicadachan/WDHN-Det.&lt;/p&gt;</content:encoded></item><item><title>TSCCD: Temporal Self-Construction Cross Domain Learning for Unsupervised Hyperspectral Change Detection</title><link>https://doi.org/10.1109/tip.2025.3650387</link><guid>10.1109/tip.2025.3650387</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Tianyuan Zhou</dc:creator><dc:creator>Fulin Luo</dc:creator><dc:creator>Chuan Fu</dc:creator><dc:creator>Tan Guo</dc:creator><dc:creator>Bo Du</dc:creator><dc:creator>Xinbo Gao</dc:creator><dc:creator>Liangpei Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3650387</prism:doi><description>Multi-temporal hyperspectral imagery (HSI) has become a powerful tool for change detection (CD) owing to its rich spectral signatures and detailed spatial information. Nevertheless, the application of paired HSIs is constrained by the scarcity of annotated training data. While unsupervised domain adaptation (UDA) offers a potential solution by transferring change detection knowledge from source to target domains, two critical limitations persist: (1) the labor-intensive process of acquiring and annotating source-domain paired samples, and (2) the suboptimal transfer performance caused by substantial cross-domain distribution discrepancies. To address these challenges, we present a Temporal Self-Construction Cross-Domain learning (TSCCD) framework for UDA-based HSI-CD. Our TSCCD framework introduces an innovative temporal self-construction mechanism that synthesizes bi-temporal source-domain data from existing HSI classification datasets while simultaneously performing initial data-level alignment. Furthermore, we develop a reweighted amplitude maximum mean discrepancy (MMD) metric to enhance feature-level domain adaptation. The proposed architecture incorporates an attention-based Kolmogorov-Arnold network (KAN) with high-frequency feature augmentation within an encoder-decoder structure to effectively capture change characteristics. Comprehensive experiments conducted on three benchmark HSI datasets demonstrate that TSCCD achieves superior performance compared to current state-of-the-art methods in HSI change detection tasks. Codes are available at https://github.com/Zhoutya/TSCCD.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyuan Zhou; Fulin Luo; Chuan Fu; Tan Guo; Bo Du; Xinbo Gao; Liangpei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3650387"&gt;10.1109/tip.2025.3650387&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-temporal hyperspectral imagery (HSI) has become a powerful tool for change detection (CD) owing to its rich spectral signatures and detailed spatial information. Nevertheless, the application of paired HSIs is constrained by the scarcity of annotated training data. While unsupervised domain adaptation (UDA) offers a potential solution by transferring change detection knowledge from source to target domains, two critical limitations persist: (1) the labor-intensive process of acquiring and annotating source-domain paired samples, and (2) the suboptimal transfer performance caused by substantial cross-domain distribution discrepancies. To address these challenges, we present a Temporal Self-Construction Cross-Domain learning (TSCCD) framework for UDA-based HSI-CD. Our TSCCD framework introduces an innovative temporal self-construction mechanism that synthesizes bi-temporal source-domain data from existing HSI classification datasets while simultaneously performing initial data-level alignment. Furthermore, we develop a reweighted amplitude maximum mean discrepancy (MMD) metric to enhance feature-level domain adaptation. The proposed architecture incorporates an attention-based Kolmogorov-Arnold network (KAN) with high-frequency feature augmentation within an encoder-decoder structure to effectively capture change characteristics. Comprehensive experiments conducted on three benchmark HSI datasets demonstrate that TSCCD achieves superior performance compared to current state-of-the-art methods in HSI change detection tasks. Codes are available at https://github.com/Zhoutya/TSCCD.&lt;/p&gt;</content:encoded></item><item><title>Hi-RWKV: Hierarchical RWKV Modeling for Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tip.2025.3648554</link><guid>10.1109/tip.2025.3648554</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Yunbiao Wang</dc:creator><dc:creator>Dongbo Yu</dc:creator><dc:creator>Ye Tao</dc:creator><dc:creator>Hengyu Niu</dc:creator><dc:creator>Daifeng Xiao</dc:creator><dc:creator>Lupeng Liu</dc:creator><dc:creator>Jun Xiao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648554</prism:doi><description>Hyperspectral image (HSI) classification demands models that can jointly capture long-range spatial relations and high-dimensional spectral structures while remaining scalable to large scenes and robust under limited supervision. Existing CNN-, Transformer-, and state-space-based approaches either suffer from restricted receptive fields, quadratic attention complexity, or directional biases that hinder dense pixel-wise prediction. To address these limitations, we propose Hi-RWKV, a hierarchical recurrent weighted key–value framework tailored for hyperspectral analysis. Hi-RWKV introduces three key innovations: (1) a spatial structure–guided bidirectional propagation mechanism that integrates global spatial context while preserving boundary fidelity via edge-aware gating; (2) a spectral identity–driven channel mixing module that incorporates learnable band embeddings and whitening transforms to enhance cross-band discriminability; and (3) a multi-stage hierarchical encoder that progressively refines spectral–spatial representations with strictly linear complexity. Together, these designs enable efficient, direction-free spectral–spatial reasoning essential for large-scale HSI interpretation. Extensive experiments on four benchmarks demonstrate that Hi-RWKV consistently achieves state-of-the-art accuracy under diverse training regimes. Ablation studies confirm that each proposed module offers complementary gains in boundary preservation, spectral discrimination, and data efficiency. By unifying scalable recurrence with hyperspectral-specific structural modeling, Hi-RWKV establishes a strong and efficient paradigm for high-resolution remote sensing. The logs and source data of this article are available at https://github.com/HSI-Lab/Hi-RWKV.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunbiao Wang; Dongbo Yu; Ye Tao; Hengyu Niu; Daifeng Xiao; Lupeng Liu; Jun Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648554"&gt;10.1109/tip.2025.3648554&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral image (HSI) classification demands models that can jointly capture long-range spatial relations and high-dimensional spectral structures while remaining scalable to large scenes and robust under limited supervision. Existing CNN-, Transformer-, and state-space-based approaches either suffer from restricted receptive fields, quadratic attention complexity, or directional biases that hinder dense pixel-wise prediction. To address these limitations, we propose Hi-RWKV, a hierarchical recurrent weighted key–value framework tailored for hyperspectral analysis. Hi-RWKV introduces three key innovations: (1) a spatial structure–guided bidirectional propagation mechanism that integrates global spatial context while preserving boundary fidelity via edge-aware gating; (2) a spectral identity–driven channel mixing module that incorporates learnable band embeddings and whitening transforms to enhance cross-band discriminability; and (3) a multi-stage hierarchical encoder that progressively refines spectral–spatial representations with strictly linear complexity. Together, these designs enable efficient, direction-free spectral–spatial reasoning essential for large-scale HSI interpretation. Extensive experiments on four benchmarks demonstrate that Hi-RWKV consistently achieves state-of-the-art accuracy under diverse training regimes. Ablation studies confirm that each proposed module offers complementary gains in boundary preservation, spectral discrimination, and data efficiency. By unifying scalable recurrence with hyperspectral-specific structural modeling, Hi-RWKV establishes a strong and efficient paradigm for high-resolution remote sensing. The logs and source data of this article are available at https://github.com/HSI-Lab/Hi-RWKV.&lt;/p&gt;</content:encoded></item><item><title>Modality Adaptive Network for Arbitrary Modality Salient Object Detection</title><link>https://doi.org/10.1109/tmm.2026.3651119</link><guid>10.1109/tmm.2026.3651119</guid><pubDate>Mon, 12 Jan 2026 22:01:52 +0000</pubDate><dc:creator>Yang Yang</dc:creator><dc:creator>Nianchang Huang</dc:creator><dc:creator>Qiang Zhang</dc:creator><dc:creator>Jungong Han</dc:creator><dc:creator>Jin Huang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651119</prism:doi><description>This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from the images with arbitrary modality types or arbitrary modality numbers by using a single model trained once. Specifically, we develop a novel model, termed modality adaptive network (MAN), for AM SOD, which addresses two fundamental challenges in AM SOD: the diverse modality discrepancies arising from varying modality types and the dynamic fusion dilemma resulting from an unfixed number of modalities in the input data. Technically, MAN first introduces a novel Modality-Adaptive Feature Extractor (MAFE) to adaptively extract features from different input modalities based on their characteristics by utilizing a set of learnable modality prompts. Concurrently, a new modality translation contractive (MTC) loss is devised to facilitate the training of MAFE as well as modality prompts, thereby effectively addressing the inherent modality discrepancies and extracting more discriminative features from each modality image. Subsequently, MAN presents a hybrid dynamic fusion (HDF) strategy to effectively resolve the challenge of dynamic inputs in multi-modal feature fusion as well as enhance the exploitation of complementary information across different modalities. This is specially achieved by a Channel- wise Dynamic Fusion Module (CDFM) and a Spatial- wise Dynamic Fusion Module (SDFM). Experimental results show that by virtue of MAFE, MTC loss and HDF strategy, our proposed method achieves significant increasements over existing models on benchmark datasets.
Published: 2026-01-12T22:01:52+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Yang; Nianchang Huang; Qiang Zhang; Jungong Han; Jin Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651119"&gt;10.1109/tmm.2026.3651119&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;This paper delves into the task of arbitrary modality salient object detection (AM SOD), aiming to detect salient objects from the images with arbitrary modality types or arbitrary modality numbers by using a single model trained once. Specifically, we develop a novel model, termed modality adaptive network (MAN), for AM SOD, which addresses two fundamental challenges in AM SOD: the diverse modality discrepancies arising from varying modality types and the dynamic fusion dilemma resulting from an unfixed number of modalities in the input data. Technically, MAN first introduces a novel Modality-Adaptive Feature Extractor (MAFE) to adaptively extract features from different input modalities based on their characteristics by utilizing a set of learnable modality prompts. Concurrently, a new modality translation contractive (MTC) loss is devised to facilitate the training of MAFE as well as modality prompts, thereby effectively addressing the inherent modality discrepancies and extracting more discriminative features from each modality image. Subsequently, MAN presents a hybrid dynamic fusion (HDF) strategy to effectively resolve the challenge of dynamic inputs in multi-modal feature fusion as well as enhance the exploitation of complementary information across different modalities. This is specially achieved by a Channel- wise Dynamic Fusion Module (CDFM) and a Spatial- wise Dynamic Fusion Module (SDFM). Experimental results show that by virtue of MAFE, MTC loss and HDF strategy, our proposed method achieves significant increasements over existing models on benchmark datasets.&lt;/p&gt;</content:encoded></item><item><title>SARVehicle Data Generation With Scattering Features For Target Recognition</title><link>https://doi.org/10.1109/jstars.2026.3652520</link><guid>10.1109/jstars.2026.3652520</guid><pubDate>Mon, 12 Jan 2026 22:01:17 +0000</pubDate><dc:creator>Dongdong Guan</dc:creator><dc:creator>Rui Feng</dc:creator><dc:creator>Yuzhen Xie</dc:creator><dc:creator>Huaiyue Ding</dc:creator><dc:creator>Yang Cui</dc:creator><dc:creator>Deliang Xiang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3652520</prism:doi><description>As is well known, obtaining high-quality measured SAR vehicle data is difficult. As a result, deep learning-based data generation is frequently utilized for SAR target augmentation because of its affordability and simplicity of use. However, existing methods do not adequately consider the target scattering information during data generation, resulting in generated target SAR data that does not conform to the physical scattering laws of SAR imaging. In this paper, we propose a SAR target data generation method based on target scattering features and Cycle-Consistent Generative Adversarial Networks (CycleGAN). First, a physical model-based method called Orthogonal Matching Pursuit (OMP) is adopted to extract the Attribute Scattering Centers (ASC) of SAR vehicle targets. Then, a multidimensional SAR target feature representation is constructed. Based on the scattering difference between the generated and real SAR target images, we introduce a loss function and further develop a generative model based on the CycleGAN. Therefore, the scattering mechanisms of SAR targets can be well learned, making the generated SAR data conform to the target scattering features. We conduct SAR target generation experiments under standard operating conditions (SOC) and extended operating conditions (EOC) on our self-acquired dataset as well as SAMPLE and MSTAR datasets. The SAR vehicle target data generated under SOC shows a more accurate scattering feature distribution to the real target data than other state-of-the-art methods. In addition, we generate SAR target data under EOC that conforms to SAR imaging patterns by modulating ASC feature parameters. Finally, the target recognition performance based on our proposed generated SAR vehicle data under SOC is validated, where the recognition rate increased by 4% after the addition of our generated target data. The code for the proposed method is publicly available at https://github.com/freel3/ Transformation _Mstar2SAMPLE.
Published: 2026-01-12T22:01:17+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dongdong Guan; Rui Feng; Yuzhen Xie; Huaiyue Ding; Yang Cui; Deliang Xiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3652520"&gt;10.1109/jstars.2026.3652520&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;As is well known, obtaining high-quality measured SAR vehicle data is difficult. As a result, deep learning-based data generation is frequently utilized for SAR target augmentation because of its affordability and simplicity of use. However, existing methods do not adequately consider the target scattering information during data generation, resulting in generated target SAR data that does not conform to the physical scattering laws of SAR imaging. In this paper, we propose a SAR target data generation method based on target scattering features and Cycle-Consistent Generative Adversarial Networks (CycleGAN). First, a physical model-based method called Orthogonal Matching Pursuit (OMP) is adopted to extract the Attribute Scattering Centers (ASC) of SAR vehicle targets. Then, a multidimensional SAR target feature representation is constructed. Based on the scattering difference between the generated and real SAR target images, we introduce a loss function and further develop a generative model based on the CycleGAN. Therefore, the scattering mechanisms of SAR targets can be well learned, making the generated SAR data conform to the target scattering features. We conduct SAR target generation experiments under standard operating conditions (SOC) and extended operating conditions (EOC) on our self-acquired dataset as well as SAMPLE and MSTAR datasets. The SAR vehicle target data generated under SOC shows a more accurate scattering feature distribution to the real target data than other state-of-the-art methods. In addition, we generate SAR target data under EOC that conforms to SAR imaging patterns by modulating ASC feature parameters. Finally, the target recognition performance based on our proposed generated SAR vehicle data under SOC is validated, where the recognition rate increased by 4% after the addition of our generated target data. The code for the proposed method is publicly available at https://github.com/freel3/ Transformation _Mstar2SAMPLE.&lt;/p&gt;</content:encoded></item><item><title>Physics-Driven Feature Decoupling for Infrared Small Targets: A Dual Geometry-Guided Experts Network</title><link>https://doi.org/10.1016/j.knosys.2026.115268</link><guid>10.1016/j.knosys.2026.115268</guid><pubDate>Tue, 13 Jan 2026 00:40:52 +0000</pubDate><dc:creator>Yubing Lu</dc:creator><dc:creator>Pingping Liu</dc:creator><dc:creator>Tongshun Zhang</dc:creator><dc:creator>Aohua Li</dc:creator><dc:creator>Qiuzhan Zhou</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115268</prism:doi><description>Infrared small target detection (ISTD) presents a critical state estimation challenge for autonomous systems, where the core challenge lies in the reliable identification of weak, low-contrast targets obscured by complex background clutter. Conventional deep learning methods suffer from feature entanglement between targets and backgrounds, limiting detection efficacy and stability. To address this, we propose the Dual Geometry-Guided Experts Network (DGGENet), a novel architecture built upon the Mixture-of-Experts (MoE) paradigm. DGGENet’s innovation centers on its Target-Background Decoupling Module (TBDM). TBDM employs a geometry-guided dynamic routing mechanism functioning as an adaptive gating network. This gate continuously analyzes input features to dynamically establish two specialized expert groups: one dedicated to target channels and another to background channels. Each group functions as a sub-network specialized for its assigned feature subspace. Guided by Robust Principal Component Analysis (RPCA) principles, the inter-group expert iteration promotes low-rank background and sparse target representations during collaborative feature refinement. Subsequently, a cross fusion module acts as state feedback, enabling semantically consistent interaction between the optimized representations. This closed-loop interaction explicitly models target-background correlations, ensuring global consistency and stability in the final output and yielding effectively disentangled feature representations. Comprehensive evaluation demonstrates that integrating TBDM into a U-Net architecture (as DGGENet) achieves superior performance on established benchmarks, attaining remarkable mIoU scores of 96.10% on the NUDT-SIRST dataset and 69.39% on the IRSTD-1K dataset. Furthermore, TBDM proves to be a versatile plug-and-play component, consistently enhancing diverse ISTD frameworks and underscoring its broad applicability across detection paradigms.
Published: 2026-01-13T00:40:52+00:00
Venue: Knowledge-Based Systems
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yubing Lu; Pingping Liu; Tongshun Zhang; Aohua Li; Qiuzhan Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115268"&gt;10.1016/j.knosys.2026.115268&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) presents a critical state estimation challenge for autonomous systems, where the core challenge lies in the reliable identification of weak, low-contrast targets obscured by complex background clutter. Conventional deep learning methods suffer from feature entanglement between targets and backgrounds, limiting detection efficacy and stability. To address this, we propose the Dual Geometry-Guided Experts Network (DGGENet), a novel architecture built upon the Mixture-of-Experts (MoE) paradigm. DGGENet’s innovation centers on its Target-Background Decoupling Module (TBDM). TBDM employs a geometry-guided dynamic routing mechanism functioning as an adaptive gating network. This gate continuously analyzes input features to dynamically establish two specialized expert groups: one dedicated to target channels and another to background channels. Each group functions as a sub-network specialized for its assigned feature subspace. Guided by Robust Principal Component Analysis (RPCA) principles, the inter-group expert iteration promotes low-rank background and sparse target representations during collaborative feature refinement. Subsequently, a cross fusion module acts as state feedback, enabling semantically consistent interaction between the optimized representations. This closed-loop interaction explicitly models target-background correlations, ensuring global consistency and stability in the final output and yielding effectively disentangled feature representations. Comprehensive evaluation demonstrates that integrating TBDM into a U-Net architecture (as DGGENet) achieves superior performance on established benchmarks, attaining remarkable mIoU scores of 96.10% on the NUDT-SIRST dataset and 69.39% on the IRSTD-1K dataset. Furthermore, TBDM proves to be a versatile plug-and-play component, consistently enhancing diverse ISTD frameworks and underscoring its broad applicability across detection paradigms.&lt;/p&gt;</content:encoded></item><item><title>Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification</title><link>https://doi.org/10.1109/tcsvt.2026.3651774</link><guid>10.1109/tcsvt.2026.3651774</guid><pubDate>Mon, 12 Jan 2026 22:04:02 +0000</pubDate><dc:creator>Jintao Rong</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Linlin Ou</dc:creator><dc:creator>Tianxiao Chen</dc:creator><dc:creator>Xinyi Yu</dc:creator><dc:creator>Yifan Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651774</prism:doi><description>The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. RePrompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.
Published: 2026-01-12T22:04:02+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jintao Rong; Hao Chen; Linlin Ou; Tianxiao Chen; Xinyi Yu; Yifan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651774"&gt;10.1109/tcsvt.2026.3651774&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;The Contrastive Language-Image Pretraining (CLIP) model has been widely used in various downstream vision tasks. The few-shot learning paradigm has been widely adopted to augment its capacity for these tasks. However, current paradigms may struggle with fine-grained classification, such as satellite image recognition, due to widening domain gaps. To address this limitation, we propose retrieval-enhanced visual prompt learning (RePrompt), which introduces retrieval mechanisms to cache and reuse the knowledge of downstream tasks. RePrompt constructs a retrieval database from either training examples or external data if available, and uses a retrieval mechanism to enhance multiple stages of a simple prompt learning baseline, thus narrowing the domain gap. During inference, our enhanced model can reference similar samples brought by retrieval to make more accurate predictions. A detailed analysis reveals that retrieval helps to improve the distribution of late features, thus, improving generalization for downstream tasks. RePrompt attains state-of-the-art performance on a wide range of vision datasets, including 11 image datasets, 3 video datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.&lt;/p&gt;</content:encoded></item><item><title>OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation</title><link>https://arxiv.org/abs/2601.06835v1</link><guid>http://arxiv.org/abs/2601.06835v1</guid><pubDate>Sun, 11 Jan 2026 09:57:04 +0000</pubDate><dc:creator>Hyunseo Lee</dc:creator><dc:creator>Sang Min Kim</dc:creator><dc:creator>Ho Kyung Shin</dc:creator><dc:creator>Taeheon Kim</dc:creator><dc:creator>Woo-Jeoung Nam</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.
Published: 2026-01-11T09:57:04+00:00
Venue: arXiv
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyunseo Lee; Sang Min Kim; Ho Kyung Shin; Taeheon Kim; Woo-Jeoung Nam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>Seeing through Satellite Images at Street Views</title><link>https://doi.org/10.1109/tpami.2026.3652860</link><guid>10.1109/tpami.2026.3652860</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Ming Qian</dc:creator><dc:creator>Bin Tan</dc:creator><dc:creator>Qiuyu Wang</dc:creator><dc:creator>Xianwei Zheng</dc:creator><dc:creator>Hanjiang Xiong</dc:creator><dc:creator>Gui-Song Xia</dc:creator><dc:creator>Yujun Shen</dc:creator><dc:creator>Nan Xue</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3652860</prism:doi><description>This paper studies the task of SatStreet-view synthesis, which aims to render photorealistic street-view panorama images and videos given a satellite image and specified camera positions or trajectories. Our approach involves learning a satellite image conditioned neural radiance field from paired images captured from both satellite and street viewpoints, which comes to be a challenging learning problem due to the sparse-view nature and the extremely large viewpoint changes between satellite and street-view images. We tackle the challenges based on a task-specific observation that street-view specific elements, including the sky and illumination effects, are only visible in street-view panoramas, and present a novel approach, Sat2Density++, to accomplish the goal of photo-realistic street-view panorama rendering by modeling these street-view specific elements in neural networks. In the experiments, our method is evaluated on both urban and suburban scene datasets, demonstrating that Sat2Density++ is capable of rendering photorealistic street-view panoramas that are consistent across multiple views and faithful to the satellite image. Project page is available at https://qianmingduowan.github.io/sat2density-pp/.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Qian; Bin Tan; Qiuyu Wang; Xianwei Zheng; Hanjiang Xiong; Gui-Song Xia; Yujun Shen; Nan Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3652860"&gt;10.1109/tpami.2026.3652860&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;This paper studies the task of SatStreet-view synthesis, which aims to render photorealistic street-view panorama images and videos given a satellite image and specified camera positions or trajectories. Our approach involves learning a satellite image conditioned neural radiance field from paired images captured from both satellite and street viewpoints, which comes to be a challenging learning problem due to the sparse-view nature and the extremely large viewpoint changes between satellite and street-view images. We tackle the challenges based on a task-specific observation that street-view specific elements, including the sky and illumination effects, are only visible in street-view panoramas, and present a novel approach, Sat2Density++, to accomplish the goal of photo-realistic street-view panorama rendering by modeling these street-view specific elements in neural networks. In the experiments, our method is evaluated on both urban and suburban scene datasets, demonstrating that Sat2Density++ is capable of rendering photorealistic street-view panoramas that are consistent across multiple views and faithful to the satellite image. Project page is available at https://qianmingduowan.github.io/sat2density-pp/.&lt;/p&gt;</content:encoded></item><item><title>Unleashing the Power of Text-to-Image Diffusion Models for Category-Agnostic Pose Estimation</title><link>https://doi.org/10.1109/tpami.2026.3651728</link><guid>10.1109/tpami.2026.3651728</guid><pubDate>Mon, 12 Jan 2026 22:00:30 +0000</pubDate><dc:creator>Duo Peng</dc:creator><dc:creator>Zhengbo Zhang</dc:creator><dc:creator>Ping Hu</dc:creator><dc:creator>Qiuhong Ke</dc:creator><dc:creator>De Wen Soh</dc:creator><dc:creator>Mohammed Bennamoun</dc:creator><dc:creator>Jun Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651728</prism:doi><description>Category-Agnostic Pose Estimation (CAPE) aims to detect keypoints of unseen object categories in a few-shot setting, where the scarcity of labeled data poses significant challenges to generalization. In this work, we propose Prompt Pose Matching (PPM), a novel framework that unleashes the power of off-the-shelf text-to-image diffusion models for CAPE. PPM learns pseudo prompts from few-shot examples via the text-to-image diffusion model. These learned pseudo prompts capture semantic information of keypoints, which can then be used to locate the same type of keypoints from images. To provide prompts with representative initialization, we introduce a category-agnostic pre-training strategy to capture the foreground prior shared across categories and keypoints. To support the reliable prompt pre-training, we propose a Foreground-Aware Region Aggregation (FARA) module to provide robust and consistent supervision signal. Based on the foreground prior, a Foreground-Guided Attention Refinement (FGAR) module is further proposed to reinforce cross-attention responses for accurate keypoint localization. For efficiency, a Prompt Ensemble Inference (PEI) scheme enables joint keypoint prediction. Unlike previous methods that highly rely on base-category annotated data, our PPM framework can operate in a base-category-free setting while retaining strong performance.
Published: 2026-01-12T22:00:30+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Duo Peng; Zhengbo Zhang; Ping Hu; Qiuhong Ke; De Wen Soh; Mohammed Bennamoun; Jun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651728"&gt;10.1109/tpami.2026.3651728&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Category-Agnostic Pose Estimation (CAPE) aims to detect keypoints of unseen object categories in a few-shot setting, where the scarcity of labeled data poses significant challenges to generalization. In this work, we propose Prompt Pose Matching (PPM), a novel framework that unleashes the power of off-the-shelf text-to-image diffusion models for CAPE. PPM learns pseudo prompts from few-shot examples via the text-to-image diffusion model. These learned pseudo prompts capture semantic information of keypoints, which can then be used to locate the same type of keypoints from images. To provide prompts with representative initialization, we introduce a category-agnostic pre-training strategy to capture the foreground prior shared across categories and keypoints. To support the reliable prompt pre-training, we propose a Foreground-Aware Region Aggregation (FARA) module to provide robust and consistent supervision signal. Based on the foreground prior, a Foreground-Guided Attention Refinement (FGAR) module is further proposed to reinforce cross-attention responses for accurate keypoint localization. For efficiency, a Prompt Ensemble Inference (PEI) scheme enables joint keypoint prediction. Unlike previous methods that highly rely on base-category annotated data, our PPM framework can operate in a base-category-free setting while retaining strong performance.&lt;/p&gt;</content:encoded></item><item><title>COME: A Collaborative Optimization Framework with Low-rank MoE for Indoor 3D Object Detection</title><link>https://doi.org/10.1109/tip.2025.3648200</link><guid>10.1109/tip.2025.3648200</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Hongbo Gao</dc:creator><dc:creator>Zimeng Tong</dc:creator><dc:creator>Fuyuan Qiu</dc:creator><dc:creator>Tao Xie</dc:creator><dc:creator>Ruifeng Li</dc:creator><dc:creator>Lijun Zhao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648200</prism:doi><description>Indoor 3D object detection serves as a fundamental task in computer vision and robotics. Existing research predominantly focuses on training domain-specific optimal models for individual datasets, yet it overlooks the potential value of capturing universal geometric attributes that can substantially enhance object detection performance across diverse domains. To resolve this gap, we propose COME, a novel and effective collaborative optimization framework designed to seamlessly integrate these universal attributes while preserving the domain-specific characteristics of each dataset domain. COME is built on VoteNet and incorporates a Cross-Domain Expert Parameter Sharing Strategy (CEPSS) that draws inspiration from the Mixture of Experts (MoE) framework. Its core innovation resides in the dual-expert design of CEPSS: domain-shared experts capture universal geometric relationships across datasets, whereas domain-specific experts encode unique features for individual datasets. This separation enables the model to focus on learning both generic and domain-specialized visual cues, without mutual interference. In addition, to dynamically adapt to different domains, we design a lightweight gating network that automatically selects relevant experts, eliminating irrelevant feature interference and enhancing model specialization. Compared to standard parameter-sharing architectures, this design significantly reduces gradient conflicts during multi-domain training. We further optimize computational efficiency by implementing low-rank structures for domain-shared and domain-specific experts, thus striking a better balance between memory overhead and detection performance. Experiments show that COME achieves state-of-the-art results across benchmarks, with acceptable parameter growth, and outperforms existing multi-domain detection methods.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongbo Gao; Zimeng Tong; Fuyuan Qiu; Tao Xie; Ruifeng Li; Lijun Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648200"&gt;10.1109/tip.2025.3648200&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Indoor 3D object detection serves as a fundamental task in computer vision and robotics. Existing research predominantly focuses on training domain-specific optimal models for individual datasets, yet it overlooks the potential value of capturing universal geometric attributes that can substantially enhance object detection performance across diverse domains. To resolve this gap, we propose COME, a novel and effective collaborative optimization framework designed to seamlessly integrate these universal attributes while preserving the domain-specific characteristics of each dataset domain. COME is built on VoteNet and incorporates a Cross-Domain Expert Parameter Sharing Strategy (CEPSS) that draws inspiration from the Mixture of Experts (MoE) framework. Its core innovation resides in the dual-expert design of CEPSS: domain-shared experts capture universal geometric relationships across datasets, whereas domain-specific experts encode unique features for individual datasets. This separation enables the model to focus on learning both generic and domain-specialized visual cues, without mutual interference. In addition, to dynamically adapt to different domains, we design a lightweight gating network that automatically selects relevant experts, eliminating irrelevant feature interference and enhancing model specialization. Compared to standard parameter-sharing architectures, this design significantly reduces gradient conflicts during multi-domain training. We further optimize computational efficiency by implementing low-rank structures for domain-shared and domain-specific experts, thus striking a better balance between memory overhead and detection performance. Experiments show that COME achieves state-of-the-art results across benchmarks, with acceptable parameter growth, and outperforms existing multi-domain detection methods.&lt;/p&gt;</content:encoded></item><item><title>Turbidity–Similarity Decoupling: Feature-Consistent Mutual Learning for Underwater Salient Object Detection</title><link>https://doi.org/10.1109/tip.2025.3648880</link><guid>10.1109/tip.2025.3648880</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Wujie Zhou</dc:creator><dc:creator>Beibei Tang</dc:creator><dc:creator>Runmin Cong</dc:creator><dc:creator>Qiuping Jiang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3648880</prism:doi><description>Underwater salient object detection (USOD) faces two major challenges that hinder accurate detection: substantial image noise owing to water turbidity and low foreground-background contrast caused by high visual similarity. In this study, a dual-model architecture based on mutual learning is proposed to address these issues. First, DenoisedNet, which focuses on addressing water turbidity issues, is developed. Using a separation–denoising–enhancement processing framework, it suppresses noise while maintaining target feature integrity through domain separation and cleaning enhancement modules. Second, SearchNet is designed to address the foreground–background similarity issue. It achieves precise localization through pseudo-label generation and layer-by-layer search mechanisms. To enable both networks to address these challenges collaboratively, a feature-consistent mutual-learning strategy is proposed, which aligns encoded features and prediction results, via evaluation and cross modes, respectively. This strategy enables their respective strengths to be complemented and the challenges of USOD to be solved more comprehensively. Our DenoisedNet and SearchNet outperform the best existing methods on the USOD10K and USOD benchmarks, achieving MAE improvements of 4.52%/5.52% and 1.61%/8.94%, respectively. The source code is available at https://github.com/BeibeiIsFreshman/DSNet_CL.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wujie Zhou; Beibei Tang; Runmin Cong; Qiuping Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3648880"&gt;10.1109/tip.2025.3648880&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Underwater salient object detection (USOD) faces two major challenges that hinder accurate detection: substantial image noise owing to water turbidity and low foreground-background contrast caused by high visual similarity. In this study, a dual-model architecture based on mutual learning is proposed to address these issues. First, DenoisedNet, which focuses on addressing water turbidity issues, is developed. Using a separation–denoising–enhancement processing framework, it suppresses noise while maintaining target feature integrity through domain separation and cleaning enhancement modules. Second, SearchNet is designed to address the foreground–background similarity issue. It achieves precise localization through pseudo-label generation and layer-by-layer search mechanisms. To enable both networks to address these challenges collaboratively, a feature-consistent mutual-learning strategy is proposed, which aligns encoded features and prediction results, via evaluation and cross modes, respectively. This strategy enables their respective strengths to be complemented and the challenges of USOD to be solved more comprehensively. Our DenoisedNet and SearchNet outperform the best existing methods on the USOD10K and USOD benchmarks, achieving MAE improvements of 4.52%/5.52% and 1.61%/8.94%, respectively. The source code is available at https://github.com/BeibeiIsFreshman/DSNet_CL.&lt;/p&gt;</content:encoded></item><item><title>Prototype-based Meta-Prompt Tuning: Toward Rehearsal-free Few-Shot Class-Incremental Learning for Multimodal Remote Sensing Image</title><link>https://doi.org/10.1109/tip.2025.3650395</link><guid>10.1109/tip.2025.3650395</guid><pubDate>Mon, 12 Jan 2026 22:04:36 +0000</pubDate><dc:creator>Yuanbo Yang</dc:creator><dc:creator>Jiahui Qu</dc:creator><dc:creator>Wenqian Dong</dc:creator><dc:creator>Ling Huang</dc:creator><dc:creator>Yunsong Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3650395</prism:doi><description>Recent research on the joint classification of multi-modal remote sensing data has achieved outstanding performance in tasks within predefined label spaces. However, surface conditions are dynamic and change over time, resulting in variations in land cover classes collected from the same region at different time points. As a result, when new classes are discovered, the previous works must use a combination of old and new class data to retrain the model, which incurs high computational costs and raises concerns about data privacy. In this work, we propose the prototype-based meta-prompt tuning (PMPT) framework, which fine-tunes only a few session-relevant visual prompts to adapt to incremental classes, while simultaneously learning prototype embeddings for each class to preserve historical knowledge. Specifically, the PMPT consists of a meta-learning-based feature representation backbone and an incrementally updated nearest-class-mean (NCM) classifier. The backbone is trained on base class data to learn shared and stable global knowledge, then frozen, with only the prompts fine-tuned to extract sessions-specific local knowledge from incremental sessions. The NCM classifier is a globally shared classifier that measures the similarity between test samples and prototypes, effectively alleviating the issues of knowledge forgetting and overfitting. Additionally, we propose an incremental prototype contrastive loss to reduce semantic drift and prototype overlap in the embedding space. During the testing phase, the PMPT reproduces the complete embedding function by matching samples, class prototypes, and visual prompts, thereby enabling accurate classification of unknown samples. The method has been tested on widely used multimodal remote sensing datasets, demonstrating the effectiveness of the proposed PMPT in addressing the dilemma of stability-plasticity with limited incremental samples.
Published: 2026-01-12T22:04:36+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanbo Yang; Jiahui Qu; Wenqian Dong; Ling Huang; Yunsong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3650395"&gt;10.1109/tip.2025.3650395&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Recent research on the joint classification of multi-modal remote sensing data has achieved outstanding performance in tasks within predefined label spaces. However, surface conditions are dynamic and change over time, resulting in variations in land cover classes collected from the same region at different time points. As a result, when new classes are discovered, the previous works must use a combination of old and new class data to retrain the model, which incurs high computational costs and raises concerns about data privacy. In this work, we propose the prototype-based meta-prompt tuning (PMPT) framework, which fine-tunes only a few session-relevant visual prompts to adapt to incremental classes, while simultaneously learning prototype embeddings for each class to preserve historical knowledge. Specifically, the PMPT consists of a meta-learning-based feature representation backbone and an incrementally updated nearest-class-mean (NCM) classifier. The backbone is trained on base class data to learn shared and stable global knowledge, then frozen, with only the prompts fine-tuned to extract sessions-specific local knowledge from incremental sessions. The NCM classifier is a globally shared classifier that measures the similarity between test samples and prototypes, effectively alleviating the issues of knowledge forgetting and overfitting. Additionally, we propose an incremental prototype contrastive loss to reduce semantic drift and prototype overlap in the embedding space. During the testing phase, the PMPT reproduces the complete embedding function by matching samples, class prototypes, and visual prompts, thereby enabling accurate classification of unknown samples. The method has been tested on widely used multimodal remote sensing datasets, demonstrating the effectiveness of the proposed PMPT in addressing the dilemma of stability-plasticity with limited incremental samples.&lt;/p&gt;</content:encoded></item><item><title>SCRTN: Enhancing Multi-modal 3D Object Detection in Complex Environments</title><link>https://doi.org/10.1016/j.patcog.2026.113068</link><guid>10.1016/j.patcog.2026.113068</guid><pubDate>Tue, 13 Jan 2026 17:04:41 +0000</pubDate><dc:creator>Xiufeng Zhu</dc:creator><dc:creator>Qing Shen</dc:creator><dc:creator>Zhenfang Liu</dc:creator><dc:creator>Kang Zhao</dc:creator><dc:creator>Jungang Lou</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113068</prism:doi><description>In complex application scenarios, noise and environmental interference significantly challenge the accurate association of multi-modal features for 3D object detection. To tackle this issue, this study introduces an advanced multi-modal framework, the Sparse Convolutional Residual Network. The framework integrates two key innovations: first, a region-of-interest feature fusion module called ResTransfusion, which enhances global feature associations between voxel point clouds and augmented color-based point clouds; second, a distant voxel retention sampling strategy that strategically reduces voxel count while maintaining key spatial information, thereby improving computational efficiency. Extensive experiments on the KITTI, NuScenes, and Waymo Open datasets demonstrate the effectiveness of the proposed approach. Notably, it achieves a state-of-the-art mean average precision (mAP) of 89.67% on the KITTI Hard benchmark and delivers competitive performance on NuScenes and Waymo, particularly in noisy and occluded real-world settings where it surpasses existing methods. Our project page is available at https://github.com/zhuxzhuif/SCRTN .
Published: 2026-01-13T17:04:41+00:00
Venue: Pattern Recognition
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiufeng Zhu; Qing Shen; Zhenfang Liu; Kang Zhao; Jungang Lou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113068"&gt;10.1016/j.patcog.2026.113068&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;In complex application scenarios, noise and environmental interference significantly challenge the accurate association of multi-modal features for 3D object detection. To tackle this issue, this study introduces an advanced multi-modal framework, the Sparse Convolutional Residual Network. The framework integrates two key innovations: first, a region-of-interest feature fusion module called ResTransfusion, which enhances global feature associations between voxel point clouds and augmented color-based point clouds; second, a distant voxel retention sampling strategy that strategically reduces voxel count while maintaining key spatial information, thereby improving computational efficiency. Extensive experiments on the KITTI, NuScenes, and Waymo Open datasets demonstrate the effectiveness of the proposed approach. Notably, it achieves a state-of-the-art mean average precision (mAP) of 89.67% on the KITTI Hard benchmark and delivers competitive performance on NuScenes and Waymo, particularly in noisy and occluded real-world settings where it surpasses existing methods. Our project page is available at https://github.com/zhuxzhuif/SCRTN .&lt;/p&gt;</content:encoded></item></channel></rss>