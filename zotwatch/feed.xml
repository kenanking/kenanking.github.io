<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 07 Dec 2025 15:35:06 +0000</lastBuildDate><item><title>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</title><link>https://doi.org/10.1109/tpami.2025.3640589</link><guid>10.1109/tpami.2025.3640589</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Xiang Xu</dc:creator><dc:creator>Lingdong Kong</dc:creator><dc:creator>Hui Shuai</dc:creator><dc:creator>Wenwei Zhang</dc:creator><dc:creator>Liang Pan</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Ziwei Liu</dc:creator><dc:creator>Qingshan Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640589</prism:doi><description>LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.844 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Xu; Lingdong Kong; Hui Shuai; Wenwei Zhang; Liang Pan; Kai Chen; Ziwei Liu; Qingshan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640589"&gt;10.1109/tpami.2025.3640589&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.844 (must_read)&lt;/p&gt;
&lt;p&gt;LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.&lt;/p&gt;</content:encoded></item><item><title>WMRNet: Wavelet Mamba with Reversible Structure for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tip.2025.3637729</link><guid>10.1109/tip.2025.3637729</guid><pubDate>Fri, 05 Dec 2025 18:41:09 +0000</pubDate><dc:creator>Mingjin Zhang</dc:creator><dc:creator>Xiaolong Li</dc:creator><dc:creator>Jie Guo</dc:creator><dc:creator>Yunsong Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3637729</prism:doi><description>Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods.
Published: 2025-12-05T18:41:09+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingjin Zhang; Xiaolong Li; Jie Guo; Yunsong Li; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3637729"&gt;10.1109/tip.2025.3637729&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Heatmap Pooling Network for Action Recognition From RGB Videos</title><link>https://doi.org/10.1109/tpami.2025.3640697</link><guid>10.1109/tpami.2025.3640697</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Mengyuan Liu</dc:creator><dc:creator>Jinfu Liu</dc:creator><dc:creator>Yongkang Jiang</dc:creator><dc:creator>Bin He</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640697</prism:doi><description>Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengyuan Liu; Jinfu Liu; Yongkang Jiang; Bin He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640697"&gt;10.1109/tpami.2025.3640697&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.&lt;/p&gt;</content:encoded></item><item><title>SAR-D-FINE: A Context-Aware Detector for Small and Densely Packed Ship Detection in SAR Imagery</title><link>https://doi.org/10.1109/lgrs.2025.3640683</link><guid>10.1109/lgrs.2025.3640683</guid><pubDate>Fri, 05 Dec 2025 18:41:23 +0000</pubDate><dc:creator>Xiaobing Fan</dc:creator><dc:creator>Bowen Xing</dc:creator><dc:creator>Xingchen Wang</dc:creator><dc:creator>Hongdan Liu</dc:creator><dc:creator>Chuanxu Yan</dc:creator><dc:creator>Pengfei Zhi</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3640683</prism:doi><description>Synthetic Aperture Radar (SAR) provides all-weather imaging, yet small-scale, densely clustered ships remain difficult to detect because coherent speckle noise and coastal clutter often mask target echoes. Current detectors, derived mainly from optical imaging methods, fail to extract weak signatures from minute vessels and to separate closely spaced targets from background clutter, leading to frequent missed detections and elevated false-alarm rates. This paper presents SAR-D-FINE, a context-aware detection framework tailored for SAR imagery. Extending the D-FINE architecture, we design a hybrid backbone with a StarStage module that strengthens nonlinear feature extraction under heavy noise. A Focusing Diffusion Encoder, integrating a Multi-Kernel Aggregation Module (MKAM) and a parameter-free Shuffle-and-Shift Upsampling (SSU) unit, is adopted to aggregate multi-scale features without sacrificing fine spatial details. Experimental results on SSDD and HRSID indicate that SAR-D-FINE surpasses existing methods, including dedicated SAR detectors such as YOLO-SARSI and SW-Net, achieving AP improvements of 2.0% and 1.8% over the baseline, respectively. The results confirm the advantages of the proposed model, particularly for detecting small, densely distributed vessels.
Published: 2025-12-05T18:41:23+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaobing Fan; Bowen Xing; Xingchen Wang; Hongdan Liu; Chuanxu Yan; Pengfei Zhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3640683"&gt;10.1109/lgrs.2025.3640683&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) provides all-weather imaging, yet small-scale, densely clustered ships remain difficult to detect because coherent speckle noise and coastal clutter often mask target echoes. Current detectors, derived mainly from optical imaging methods, fail to extract weak signatures from minute vessels and to separate closely spaced targets from background clutter, leading to frequent missed detections and elevated false-alarm rates. This paper presents SAR-D-FINE, a context-aware detection framework tailored for SAR imagery. Extending the D-FINE architecture, we design a hybrid backbone with a StarStage module that strengthens nonlinear feature extraction under heavy noise. A Focusing Diffusion Encoder, integrating a Multi-Kernel Aggregation Module (MKAM) and a parameter-free Shuffle-and-Shift Upsampling (SSU) unit, is adopted to aggregate multi-scale features without sacrificing fine spatial details. Experimental results on SSDD and HRSID indicate that SAR-D-FINE surpasses existing methods, including dedicated SAR detectors such as YOLO-SARSI and SW-Net, achieving AP improvements of 2.0% and 1.8% over the baseline, respectively. The results confirm the advantages of the proposed model, particularly for detecting small, densely distributed vessels.&lt;/p&gt;</content:encoded></item><item><title>EquivFisheye: A Spherical Fusion Framework for Panoramic 3D Perception with Surround-View Fisheye Cameras</title><link>https://doi.org/10.1016/j.inffus.2025.104024</link><guid>10.1016/j.inffus.2025.104024</guid><pubDate>Sat, 06 Dec 2025 07:56:17 +0000</pubDate><dc:creator>Zhao Yang</dc:creator><dc:creator>Xinglin Pu</dc:creator><dc:creator>Weixiang Xu</dc:creator><dc:creator>Zezhong Qian</dc:creator><dc:creator>Kang Ke</dc:creator><dc:creator>Haonan Zhang</dc:creator><dc:creator>Longjun Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104024</prism:doi><description>Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.
Published: 2025-12-06T07:56:17+00:00
Venue: Information Fusion
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Yang; Xinglin Pu; Weixiang Xu; Zezhong Qian; Kang Ke; Haonan Zhang; Longjun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104024"&gt;10.1016/j.inffus.2025.104024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Surround-view fisheye cameras are widely deployed in autonomous driving for near-field perception tasks, such as automated parking and urban navigation. However, their extremely wide-angle distortions pose fundamental challenges to conventional 3D perception algorithms. Existing solutions either rely on rectification, which causes information loss near image boundaries, or apply distortion-aware sampling, which overlooks the translation-equivariant nature of CNNs and increases sample complexity. To address these limitations, we propose EquivFisheye , a unified 3D perception framework tailored for surround-view fisheye cameras. Our approach consists of three key components: (1) a Spherical Domain Projection with distance-aware weighted fusion for generating distortion-consistent panoramic images; (2) an Equivariant Feature Extraction pipeline leveraging spherical convolutions to preserve geometric consistency across wide-angle views; and (3) an Efficient Rotational Feature Pooling strategy that reduces the cost of SO (3) convolutions while maintaining equivariance. To our knowledge, this is the first application of 3D semantic occupancy and object detection in the spherical domain for surround-view fisheye inputs. Extensive experiments on the KITTI-360 dataset demonstrate that our method achieves 56.5% mRayIoU for 3D semantic occupancy and 42.4% NDS for 3D detection, surpassing the previous state-of-the-art by 1.1% and 1.8%, respectively. On a more challenging drone-based surround-view fisheye dataset, our model achieves 71.0% RayIoU, outperforming the best prior approach by 2.1%. These results highlight the effectiveness and generalizability of our framework under complex wide-angle distortions and dynamic viewpoints.&lt;/p&gt;</content:encoded></item><item><title>融合小波卷积与频域注意力的小目标检测改进</title><link>https://doi.org/10.11834/jig.250293</link><guid>10.11834/jig.250293</guid><pubDate>Fri, 05 Dec 2025 08:29:30 +0000</pubDate><dc:creator>Liu Xu</dc:creator><dc:creator>Song Peibo</dc:creator><dc:creator>Bao Fangxun</dc:creator><dc:creator>Du Hongwei</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250293</prism:doi><description>目的无人机拍摄图像存在小目标数量多，易受恶劣天气等噪声污染的特点，针对无人机拍摄图像的小目标检测技术在军用领域和商用领域都发挥着重要作用。然而，现有的目标检测方法在定位小目标方面仍然存在检测精度低的问题。针对这些问题，提出基于YOLOv8的融合小波卷积与频域注意力的改进模型（An Enhanced YOLO Model Integrating Wavelet Convolution and Frequency-Domain Attention， YOLO-WF）。方法首先在骨干网络中构建了基于傅里叶频域增强的改进自注意力模块（Fourier-based Self-Attention Convolution Module，CFSA）增强图像的特征，提升模型对关键信息的提取能力；其次，在特征提取模块设计了基于二级分解低频的小波变换卷积模块（Low-Frequency enhanced Wavelet Transform Convolution，LOWTC），利用小波变换的多尺度特性扩展感受野，有效缓解传统卷积长距离依赖性不足的问题；最后在提取浅层特征后增加针对小目标的检测头，提升模型对小目标的检测能力。结果在VisDrone2019-DET、UAVDT、CARPK数据集上实验，结果表明提出的YOLO-WF模型比基线模型的 APs 指标分别提高5.5个、3.08个、6.8个百分点，达到19.9%、38.54%和33.3%。 AP50 和 APm 指标也均有提升，以VisDrone2019-DET为例， AP50 、 APm 分别达到47.1%、40.3%，相比基线模型分别提高3.5、3.0个百分点，且参数量下降0.4%。结论YOLO-WF通过频域-小波融合策略，显著提升了中小目标的检测精度，且未引入额外存储负担，可直接迁移至其他航拍检测任务。
Published: 2025-12-05T08:29:30+00:00
Venue: Journal of Image and Graphics
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liu Xu; Song Peibo; Bao Fangxun; Du Hongwei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250293"&gt;10.11834/jig.250293&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;目的无人机拍摄图像存在小目标数量多，易受恶劣天气等噪声污染的特点，针对无人机拍摄图像的小目标检测技术在军用领域和商用领域都发挥着重要作用。然而，现有的目标检测方法在定位小目标方面仍然存在检测精度低的问题。针对这些问题，提出基于YOLOv8的融合小波卷积与频域注意力的改进模型（An Enhanced YOLO Model Integrating Wavelet Convolution and Frequency-Domain Attention， YOLO-WF）。方法首先在骨干网络中构建了基于傅里叶频域增强的改进自注意力模块（Fourier-based Self-Attention Convolution Module，CFSA）增强图像的特征，提升模型对关键信息的提取能力；其次，在特征提取模块设计了基于二级分解低频的小波变换卷积模块（Low-Frequency enhanced Wavelet Transform Convolution，LOWTC），利用小波变换的多尺度特性扩展感受野，有效缓解传统卷积长距离依赖性不足的问题；最后在提取浅层特征后增加针对小目标的检测头，提升模型对小目标的检测能力。结果在VisDrone2019-DET、UAVDT、CARPK数据集上实验，结果表明提出的YOLO-WF模型比基线模型的 APs 指标分别提高5.5个、3.08个、6.8个百分点，达到19.9%、38.54%和33.3%。 AP50 和 APm 指标也均有提升，以VisDrone2019-DET为例， AP50 、 APm 分别达到47.1%、40.3%，相比基线模型分别提高3.5、3.0个百分点，且参数量下降0.4%。结论YOLO-WF通过频域-小波融合策略，显著提升了中小目标的检测精度，且未引入额外存储负担，可直接迁移至其他航拍检测任务。&lt;/p&gt;</content:encoded></item><item><title>Multi-modal Collaborative Learning with Vision Foundation Model Prompt Boosts 3D Semi-supervised Semantic Segmentation</title><link>https://doi.org/10.1016/j.inffus.2025.104019</link><guid>10.1016/j.inffus.2025.104019</guid><pubDate>Fri, 05 Dec 2025 00:44:45 +0000</pubDate><dc:creator>Xiang He</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Baidan Li</dc:creator><dc:creator>Zhiyuan Xu</dc:creator><dc:creator>Qimin Xu</dc:creator><dc:creator>Hongwei Lu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104019</prism:doi><description>3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.
Published: 2025-12-05T00:44:45+00:00
Venue: Information Fusion
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang He; Xu Li; Baidan Li; Zhiyuan Xu; Qimin Xu; Hongwei Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104019"&gt;10.1016/j.inffus.2025.104019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.&lt;/p&gt;</content:encoded></item><item><title>Harnessing Lightweight Transformer With Contextual Synergic Enhancement for Efficient 3D Medical Image Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3640233</link><guid>10.1109/tpami.2025.3640233</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Xinyu Liu</dc:creator><dc:creator>Zhen Chen</dc:creator><dc:creator>Wuyang Li</dc:creator><dc:creator>Chenxin Li</dc:creator><dc:creator>Yixuan Yuan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640233</prism:doi><description>Transformers have shown remarkable performance in 3D medical image segmentation, but their high computational requirements and need for large amounts of labeled data limit their applicability. To address these challenges, we consider two crucial aspects: model efficiency and data efficiency. Specifically, we propose Light-UNETR, a lightweight transformer designed to achieve model efficiency. Light-UNETR features a Lightweight Dimension Reductive Attention (LIDR) module, which reduces spatial and channel dimensions while capturing both global and local features via multi-branch attention. Additionally, we introduce a Compact Gated Linear Unit (CGLU) to selectively control channel interaction with minimal parameters. Furthermore, we introduce a Contextual Synergic Enhancement (CSE) learning strategy, which aims to boost the data efficiency of Transformers. It first leverages the extrinsic contextual information to support the learning of unlabeled data with Attention-Guided Replacement, then applies Spatial Masking Consistency that utilizes intrinsic contextual information to enhance the spatial context reasoning for unlabeled data. Extensive experiments on various benchmarks demonstrate the superiority of our approach in both performance and efficiency. For example, with only 10% labeled data on the Left Atrial Segmentation dataset, our method surpasses BCP by 1.43% Jaccard while drastically reducing the FLOPs by 90.8% and parameters by 85.8%. Code is released at https://github.com/CUHK-AIM-Group/Light-UNETR.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Liu; Zhen Chen; Wuyang Li; Chenxin Li; Yixuan Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640233"&gt;10.1109/tpami.2025.3640233&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have shown remarkable performance in 3D medical image segmentation, but their high computational requirements and need for large amounts of labeled data limit their applicability. To address these challenges, we consider two crucial aspects: model efficiency and data efficiency. Specifically, we propose Light-UNETR, a lightweight transformer designed to achieve model efficiency. Light-UNETR features a Lightweight Dimension Reductive Attention (LIDR) module, which reduces spatial and channel dimensions while capturing both global and local features via multi-branch attention. Additionally, we introduce a Compact Gated Linear Unit (CGLU) to selectively control channel interaction with minimal parameters. Furthermore, we introduce a Contextual Synergic Enhancement (CSE) learning strategy, which aims to boost the data efficiency of Transformers. It first leverages the extrinsic contextual information to support the learning of unlabeled data with Attention-Guided Replacement, then applies Spatial Masking Consistency that utilizes intrinsic contextual information to enhance the spatial context reasoning for unlabeled data. Extensive experiments on various benchmarks demonstrate the superiority of our approach in both performance and efficiency. For example, with only 10% labeled data on the Left Atrial Segmentation dataset, our method surpasses BCP by 1.43% Jaccard while drastically reducing the FLOPs by 90.8% and parameters by 85.8%. Code is released at https://github.com/CUHK-AIM-Group/Light-UNETR.&lt;/p&gt;</content:encoded></item><item><title>LPATR-Net: Learnable Piecewise Affine Transformation Regression Assisted Data-Driven Dehazing Framework</title><link>https://doi.org/10.1109/tip.2025.3637687</link><guid>10.1109/tip.2025.3637687</guid><pubDate>Fri, 05 Dec 2025 18:41:09 +0000</pubDate><dc:creator>Yuelong Li</dc:creator><dc:creator>Fei Chen</dc:creator><dc:creator>Zhenwei Liu</dc:creator><dc:creator>Tianyu Zang</dc:creator><dc:creator>Jianming Wang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3637687</prism:doi><description>Nowadays, data-driven learning based deep neural network (DNN) is the most dominant SOTA image dehazing framework. Here, learning to perfectly simulate the underlying mapping rules (from hazy to clear) told by massive paired training data is its core driving force. However, under genuine scenarios, it is extremely hard to guarantee the 100% qualification of all collected ground truth (GT) haze-free data. That’s because natural weather is hardly controlled, and many weathers are actually in a chaotic status existing between foggy and fog-free. Thus, unlike most supervised learning issues, the image dehazing society is born with the torture of part of faulty ground truth no-haze samples. Therefore, totally trusting training data and solely pursuing more fitting powerful data-driven model may not be a wise solution. To cope with this thorny challenge, in this paper, instead of faithfully pursuing for fitting capacity promotion, we on the contrary choose to intentionally cut down the fitting flexibility to achieve higher-level robustness. That is the LPATR-Net, a novel dehazing framework specially armed with fitting power suppression mechanism to resist intrinsic annoying faulty GT. This solution does not involve any extra manually labeling. Specifically, the LPATR-Net architecture is created completely around elaborately designed fitting-restrained learnable piecewise affine transformation regression. Since such low-order linear regression structure genetically can only fit for majority of data, the interference of minority of unqualified GT samples is expected to be effectively suppressed. Through further coupled with a highly customized multi-concerns high-accuracy dehazing fitting companion component, All-Mattering, proposed LPATR-Net elegantly achieves the seamless integration of traditional majority determining fixed-form regression and modern all freedom data-driven deep learning. Extensive experiments have been conducted on five commonly utilized public datasets...
Published: 2025-12-05T18:41:09+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuelong Li; Fei Chen; Zhenwei Liu; Tianyu Zang; Jianming Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3637687"&gt;10.1109/tip.2025.3637687&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Nowadays, data-driven learning based deep neural network (DNN) is the most dominant SOTA image dehazing framework. Here, learning to perfectly simulate the underlying mapping rules (from hazy to clear) told by massive paired training data is its core driving force. However, under genuine scenarios, it is extremely hard to guarantee the 100% qualification of all collected ground truth (GT) haze-free data. That’s because natural weather is hardly controlled, and many weathers are actually in a chaotic status existing between foggy and fog-free. Thus, unlike most supervised learning issues, the image dehazing society is born with the torture of part of faulty ground truth no-haze samples. Therefore, totally trusting training data and solely pursuing more fitting powerful data-driven model may not be a wise solution. To cope with this thorny challenge, in this paper, instead of faithfully pursuing for fitting capacity promotion, we on the contrary choose to intentionally cut down the fitting flexibility to achieve higher-level robustness. That is the LPATR-Net, a novel dehazing framework specially armed with fitting power suppression mechanism to resist intrinsic annoying faulty GT. This solution does not involve any extra manually labeling. Specifically, the LPATR-Net architecture is created completely around elaborately designed fitting-restrained learnable piecewise affine transformation regression. Since such low-order linear regression structure genetically can only fit for majority of data, the interference of minority of unqualified GT samples is expected to be effectively suppressed. Through further coupled with a highly customized multi-concerns high-accuracy dehazing fitting companion component, All-Mattering, proposed LPATR-Net elegantly achieves the seamless integration of traditional majority determining fixed-form regression and modern all freedom data-driven deep learning. Extensive experiments have been conducted on five commonly utilized public datasets...&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Domain Adaptive Object Detection via Discriminative Instance Teacher</title><link>https://doi.org/10.1016/j.eswa.2025.130656</link><guid>10.1016/j.eswa.2025.130656</guid><pubDate>Fri, 05 Dec 2025 00:36:16 +0000</pubDate><dc:creator>Yiming Ge</dc:creator><dc:creator>Hui Liu</dc:creator><dc:creator>Yanjie Hu</dc:creator><dc:creator>Jie Zhao</dc:creator><dc:creator>Junzhao Du</dc:creator><dc:creator>Ertong Shang</dc:creator><dc:creator>Zhaocheng Niu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130656</prism:doi><description>Domain adaptive object detection (DAOD) poses significant challenges due to pronounced domain shifts. Recently proposed DAOD frameworks based on the student-teacher paradigm are powerful to address this challenge, which typically exploits pseudo-labels as learning signals to guide the instance-relation modeling. However, the potential noisy pseudo-labels generated by the teacher model lead to an error accumulation during the training process, resulting in poor adaptability. Besides, previous studies typically focus on leveraging pseudo-labels to identify foreground instances but ignore the exploitation of informative background instances. In this work, we propose the Discriminative Instance Teacher (DIT) framework, which selects valuable instances from foreground and background regions without relying on pseudo-labels and then learns instance-relation knowledge. Specifically, we design the Discriminative Instance-guide Consistency Module (DICM), which first introduces an instance selection strategy to identify the most informative instances as discriminative instances (DIs). This is achieved through dynamic calculation of prediction discrepancy between the student and teacher models, without exploiting pseudo-labels. Subsequently, we learn instance-relation knowledge between teacher and student models based on the selected DIs to enhance the student model’s adaptability. Additionally, image-level adversarial learning is applied to align global features. Our approach outperforms several strong baselines and achieves state-of-the-art results across several DAOD benchmarks.
Published: 2025-12-05T00:36:16+00:00
Venue: Expert Systems with Applications
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Ge; Hui Liu; Yanjie Hu; Jie Zhao; Junzhao Du; Ertong Shang; Zhaocheng Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130656"&gt;10.1016/j.eswa.2025.130656&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Domain adaptive object detection (DAOD) poses significant challenges due to pronounced domain shifts. Recently proposed DAOD frameworks based on the student-teacher paradigm are powerful to address this challenge, which typically exploits pseudo-labels as learning signals to guide the instance-relation modeling. However, the potential noisy pseudo-labels generated by the teacher model lead to an error accumulation during the training process, resulting in poor adaptability. Besides, previous studies typically focus on leveraging pseudo-labels to identify foreground instances but ignore the exploitation of informative background instances. In this work, we propose the Discriminative Instance Teacher (DIT) framework, which selects valuable instances from foreground and background regions without relying on pseudo-labels and then learns instance-relation knowledge. Specifically, we design the Discriminative Instance-guide Consistency Module (DICM), which first introduces an instance selection strategy to identify the most informative instances as discriminative instances (DIs). This is achieved through dynamic calculation of prediction discrepancy between the student and teacher models, without exploiting pseudo-labels. Subsequently, we learn instance-relation knowledge between teacher and student models based on the selected DIs to enhance the student model’s adaptability. Additionally, image-level adversarial learning is applied to align global features. Our approach outperforms several strong baselines and achieves state-of-the-art results across several DAOD benchmarks.&lt;/p&gt;</content:encoded></item><item><title>MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms</title><link>https://arxiv.org/abs/2512.03640v1</link><guid>http://arxiv.org/abs/2512.03640v1</guid><pubDate>Wed, 03 Dec 2025 10:22:27 +0000</pubDate><dc:creator>Jiahao Zhang</dc:creator><dc:creator>Xiao Zhao</dc:creator><dc:creator>Guangyu Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1007/978-981-96-2061-6_29</prism:doi><description>Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.
Published: 2025-12-03T10:22:27+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Zhang; Xiao Zhao; Guangyu Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/978-981-96-2061-6_29"&gt;10.1007/978-981-96-2061-6_29&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network&amp;#x27;s ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet&amp;#x27;s superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.&lt;/p&gt;</content:encoded></item><item><title>Super-Resolution Imaging of Sparse Sea Surface Targets by Multi-Feature Divide-and-Conquer Framework</title><link>https://doi.org/10.1109/tgrs.2025.3640647</link><guid>10.1109/tgrs.2025.3640647</guid><pubDate>Fri, 05 Dec 2025 18:38:00 +0000</pubDate><dc:creator>Jiahao Shen</dc:creator><dc:creator>Deqing Mao</dc:creator><dc:creator>Mingjie Yang</dc:creator><dc:creator>Yin Zhang</dc:creator><dc:creator>Jianyu Yang</dc:creator><dc:creator>Yulin Huang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3640647</prism:doi><description>Real aperture radar superresolution imaging of sea surface targets has significant applications in sea surface surveillance and maritime rescue. However, its performance rapidly deteriorates when the echo data of the valid targets are merged into the strong sea clutter. To address this challenge, this paper proposes a multi-feature divide-and-conquer (MF-D&amp;C) framework by forming a complex multi-feature enhancement network (CMFE-NET) and data-divide-and-conquer-based (DD&amp;C-based) sparse Bayesian learning (SBL) algorithm. First, to separate sea clutter echo from valid targets’ echo, a CMFE-NET is proposed to transform the complex echoes into four distinct feature spaces: amplitude, phase, frequency, and dwell time. Second, based on the separated sea clutter echo and the valid targets’ echo, a DD&amp;C-based SBL algorithm is proposed to perform Bayesian parameter estimation on both the clutter and target components, which improves the model parameter estimation performance within the Bayesian framework. Finally, a parameter pruning solver is introduced in EM estimation to eliminate inactive parameters during the iterative super-resolution process, significantly reducing computational overhead. The proposed framework demonstrates superior capabilities resolution enhancement in sea surface target superresolution imaging.
Published: 2025-12-05T18:38:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Shen; Deqing Mao; Mingjie Yang; Yin Zhang; Jianyu Yang; Yulin Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3640647"&gt;10.1109/tgrs.2025.3640647&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Real aperture radar superresolution imaging of sea surface targets has significant applications in sea surface surveillance and maritime rescue. However, its performance rapidly deteriorates when the echo data of the valid targets are merged into the strong sea clutter. To address this challenge, this paper proposes a multi-feature divide-and-conquer (MF-D&amp;amp;C) framework by forming a complex multi-feature enhancement network (CMFE-NET) and data-divide-and-conquer-based (DD&amp;amp;C-based) sparse Bayesian learning (SBL) algorithm. First, to separate sea clutter echo from valid targets’ echo, a CMFE-NET is proposed to transform the complex echoes into four distinct feature spaces: amplitude, phase, frequency, and dwell time. Second, based on the separated sea clutter echo and the valid targets’ echo, a DD&amp;amp;C-based SBL algorithm is proposed to perform Bayesian parameter estimation on both the clutter and target components, which improves the model parameter estimation performance within the Bayesian framework. Finally, a parameter pruning solver is introduced in EM estimation to eliminate inactive parameters during the iterative super-resolution process, significantly reducing computational overhead. The proposed framework demonstrates superior capabilities resolution enhancement in sea surface target superresolution imaging.&lt;/p&gt;</content:encoded></item><item><title>声呐图像质量评价与增强研究综述</title><link>https://doi.org/10.11834/jig.250421</link><guid>10.11834/jig.250421</guid><pubDate>Fri, 05 Dec 2025 08:29:44 +0000</pubDate><dc:creator>Lin Jie</dc:creator><dc:creator>Chen Weiling</dc:creator><dc:creator>Xu Xiaoyi</dc:creator><dc:creator>Zhao Tiesong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250421</prism:doi><description>声呐图像（sonar image，SI）是海洋信息的重要载体，然而，受水体散射、多径效应及声波波长限制等因素影响，声呐图像普遍存在分辨率低、斑点噪声强、对比度弱等现象。这些问题不仅直接影响声呐图像的视觉效果，更会显著降低目标识别等下游任务的精度与可靠性。因此，发展有效的声呐图像质量评价（image quality assessment，IQA）与增强技术，对保障海洋探测任务至关重要。尽管声呐图像处理研究近年来取得显著进展，但现有综述工作主要集中于水下光学图像，或仅聚焦于特定应用任务，对声呐图像质量评价体系及增强算法的系统性梳理、技术演进脉络的深入剖析尚显不足。为此，本综述首次构建了声呐图像质量的“评价-增强”研究框架，旨在提供一份更系统、全面的介绍。具体而言，本文系统梳理了声呐图像质量评价与质量增强领域的关键技术演进、代表性模型及最新研究进展；实验方面，针对质量评价领域，在公开数据集上对梳理出的主流声呐图像质量评价算法进行了性能对比实验，揭示了其在多种失真场景下的性能差异与适用性；针对质量增强领域，鉴于开源的声呐图像增强算法较少，重点评估了代表性通用超分辨率（super-resolution， SR）重建与去噪算法在声呐图像上的性能表现。综上，本文旨在为声呐图像处理领域的发展提供系统的理论参照与技术路线，通过实验揭示算法性能瓶颈与适用边界，最终为研究人员把握现状、理解挑战、规划未来研究提供重要支撑。
Published: 2025-12-05T08:29:44+00:00
Venue: Journal of Image and Graphics
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lin Jie; Chen Weiling; Xu Xiaoyi; Zhao Tiesong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250421"&gt;10.11834/jig.250421&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;声呐图像（sonar image，SI）是海洋信息的重要载体，然而，受水体散射、多径效应及声波波长限制等因素影响，声呐图像普遍存在分辨率低、斑点噪声强、对比度弱等现象。这些问题不仅直接影响声呐图像的视觉效果，更会显著降低目标识别等下游任务的精度与可靠性。因此，发展有效的声呐图像质量评价（image quality assessment，IQA）与增强技术，对保障海洋探测任务至关重要。尽管声呐图像处理研究近年来取得显著进展，但现有综述工作主要集中于水下光学图像，或仅聚焦于特定应用任务，对声呐图像质量评价体系及增强算法的系统性梳理、技术演进脉络的深入剖析尚显不足。为此，本综述首次构建了声呐图像质量的“评价-增强”研究框架，旨在提供一份更系统、全面的介绍。具体而言，本文系统梳理了声呐图像质量评价与质量增强领域的关键技术演进、代表性模型及最新研究进展；实验方面，针对质量评价领域，在公开数据集上对梳理出的主流声呐图像质量评价算法进行了性能对比实验，揭示了其在多种失真场景下的性能差异与适用性；针对质量增强领域，鉴于开源的声呐图像增强算法较少，重点评估了代表性通用超分辨率（super-resolution， SR）重建与去噪算法在声呐图像上的性能表现。综上，本文旨在为声呐图像处理领域的发展提供系统的理论参照与技术路线，通过实验揭示算法性能瓶颈与适用边界，最终为研究人员把握现状、理解挑战、规划未来研究提供重要支撑。&lt;/p&gt;</content:encoded></item><item><title>AttBEV: Enhancing Multi-Modal 3D Object Detection with CBAM Attention in BEVFusion for Autonomous Driving</title><link>https://doi.org/10.1109/lra.2025.3641130</link><guid>10.1109/lra.2025.3641130</guid><pubDate>Fri, 05 Dec 2025 18:40:34 +0000</pubDate><dc:creator>Na Zhang</dc:creator><dc:creator>Edmundo Guerra</dc:creator><dc:creator>Antoni Grau</dc:creator><prism:publicationName>IEEE Robotics and Automation Letters</prism:publicationName><prism:doi>10.1109/lra.2025.3641130</prism:doi><description>Multimodal fusion has an important research value in environmental perception for autonomous driving. Among them, BEVFusion has become one of the mainstream framework for LiDAR camera fusion by unifying multimodal features in the bird's-eye view (BEV) space. However, its performance is limited by inefficient cross-modal interaction and information loss during BEV projection, especially for dynamic objects and edge cases. To address these limitations, we propose AttBEV, an advanced fusion architecture that introduces a CBAM at the feature fusion layer: a lightweight attention mechanism that improves the model's ability to capture key information through dynamic feature calibration of channel and spatial dimensions. Extensive experiments on the nuScenes dataset demonstrate that AttBEV achieves superior performance compared to BEVFusion on most evaluation metrics. NDS reaches 0.6795, which is 2.63% higher than BEVFusion's 0.6532, and mAP reaches 0.6426, which is 1.79% higher than BEVFusion's 0.6247. In general, AttBEV outperforms existing methods in both model accuracy and generalization ability and significantly improves the performance of 3D object detection in autonomous driving scenarios.
Published: 2025-12-05T18:40:34+00:00
Venue: IEEE Robotics and Automation Letters
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Na Zhang; Edmundo Guerra; Antoni Grau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Robotics and Automation Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lra.2025.3641130"&gt;10.1109/lra.2025.3641130&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal fusion has an important research value in environmental perception for autonomous driving. Among them, BEVFusion has become one of the mainstream framework for LiDAR camera fusion by unifying multimodal features in the bird&amp;#x27;s-eye view (BEV) space. However, its performance is limited by inefficient cross-modal interaction and information loss during BEV projection, especially for dynamic objects and edge cases. To address these limitations, we propose AttBEV, an advanced fusion architecture that introduces a CBAM at the feature fusion layer: a lightweight attention mechanism that improves the model&amp;#x27;s ability to capture key information through dynamic feature calibration of channel and spatial dimensions. Extensive experiments on the nuScenes dataset demonstrate that AttBEV achieves superior performance compared to BEVFusion on most evaluation metrics. NDS reaches 0.6795, which is 2.63% higher than BEVFusion&amp;#x27;s 0.6532, and mAP reaches 0.6426, which is 1.79% higher than BEVFusion&amp;#x27;s 0.6247. In general, AttBEV outperforms existing methods in both model accuracy and generalization ability and significantly improves the performance of 3D object detection in autonomous driving scenarios.&lt;/p&gt;</content:encoded></item><item><title>Towards Cognition-Driven 3D Object Detection: A LiDAR-Based Framework</title><link>https://doi.org/10.1016/j.knosys.2025.115051</link><guid>10.1016/j.knosys.2025.115051</guid><pubDate>Fri, 05 Dec 2025 17:34:55 +0000</pubDate><dc:creator>Yewei Shi</dc:creator><dc:creator>Baicang Guo</dc:creator><dc:creator>Lisheng Jin</dc:creator><dc:creator>Xiao Yang</dc:creator><dc:creator>Hongyu Zhang</dc:creator><dc:creator>Xingchen Liu</dc:creator><dc:creator>Menglin Li</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115051</prism:doi><description>With the continuous advancement of autonomous driving technology, increasingly stringent requirements are placed on the accuracy and robustness of 3D object detection. Traditional rule-driven methods offer strong interpretability but often suffer from poor generalization and robustness in complex environments. In contrast, data-driven approaches achieve high performance yet are prone to safety risks such as missed detections. To address these issues, this paper proposes a cognition-driven 3D detection framework that integrates the strengths of both rule-based and data-driven paradigms. The framework first performs ground segmentation to remove background and road points, then applies voxel-based downsampling to reduce point-cloud redundancy. Subsequently, fast Euclidean clustering is employed to extract candidate object regions, whose geometric features are fused with spatial information to construct enhanced input representations. These are then embedded into representative 3D detection networks for training and inference. Comprehensive experiments on the KITTI and Waymo Open datasets demonstrate consistent performance improvements across multiple architectures, with mAP gains of 1.18-4.80% on KITTI and 1.20-4.04% on Waymo for PillarNet, PV-RCNN, and IA-SSD. Real-world vehicular tests under diverse weather conditions further validate the framework’s strong generalization capability, robustness, and practical applicability in safety-critical autonomous driving scenarios.
Published: 2025-12-05T17:34:55+00:00
Venue: Knowledge-Based Systems
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yewei Shi; Baicang Guo; Lisheng Jin; Xiao Yang; Hongyu Zhang; Xingchen Liu; Menglin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115051"&gt;10.1016/j.knosys.2025.115051&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;With the continuous advancement of autonomous driving technology, increasingly stringent requirements are placed on the accuracy and robustness of 3D object detection. Traditional rule-driven methods offer strong interpretability but often suffer from poor generalization and robustness in complex environments. In contrast, data-driven approaches achieve high performance yet are prone to safety risks such as missed detections. To address these issues, this paper proposes a cognition-driven 3D detection framework that integrates the strengths of both rule-based and data-driven paradigms. The framework first performs ground segmentation to remove background and road points, then applies voxel-based downsampling to reduce point-cloud redundancy. Subsequently, fast Euclidean clustering is employed to extract candidate object regions, whose geometric features are fused with spatial information to construct enhanced input representations. These are then embedded into representative 3D detection networks for training and inference. Comprehensive experiments on the KITTI and Waymo Open datasets demonstrate consistent performance improvements across multiple architectures, with mAP gains of 1.18-4.80% on KITTI and 1.20-4.04% on Waymo for PillarNet, PV-RCNN, and IA-SSD. Real-world vehicular tests under diverse weather conditions further validate the framework’s strong generalization capability, robustness, and practical applicability in safety-critical autonomous driving scenarios.&lt;/p&gt;</content:encoded></item><item><title>Underwater complex environment domain adaptation for few-shot object detection based on transfer learning</title><link>https://doi.org/10.1016/j.neucom.2025.132341</link><guid>10.1016/j.neucom.2025.132341</guid><pubDate>Fri, 05 Dec 2025 00:34:39 +0000</pubDate><dc:creator>Shouyu Ren</dc:creator><dc:creator>Hongchi Hao</dc:creator><dc:creator>Yuxiang Zhang</dc:creator><dc:creator>Zhibin Yu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132341</prism:doi><description>Although cross-domain few-shot object detection (CD-FSOD) has advanced in recent years, its application to underwater scenarios remains limited. Underwater environments pose unique challenges—light attenuation, color distortion, and scattering cause background interference and large distribution shifts across scenes, reducing the effectiveness of traditional transfer methods. The scarcity of high-quality annotated underwater data further hampers training, intensifying few-shot difficulties. Moreover, marine organisms often exhibit high visual similarity between foreground and background (camouflage effect), making it harder for models to capture critical features. These factors form a bottleneck: existing models struggle to adapt to new underwater domains with few samples, while current few-shot methods fail to handle domain shifts specific to underwater settings. We propose a transfer learning framework for CD-FSOD in complex underwater environments, using a two-stage strategy: pretraining on large-scale source-domain data, followed by fine-tuning selected parameters with limited target-domain samples to reduce overfitting and improve cross-domain performance. Two core modules enhance transferability and robustness: (1) Dynamic Background Suppression Module (DBSM), which adaptively weakens background noise and highlights salient regions; and (2) Foreground Focus Module (FFM), combining edge-aware enhancement and local window attention to strengthen fine-grained discriminative features. These modules jointly address background interference and camouflage challenges. Extensive experiments on five public underwater datasets—S-UODAC2020, Brackish, Aquarium, URPC2022, and CSIRO Crown-of-Thorns Starfish—demonstrate the superior performance of our method in cross-domain few-shot detection under challenging underwater conditions.
Published: 2025-12-05T00:34:39+00:00
Venue: Neurocomputing
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shouyu Ren; Hongchi Hao; Yuxiang Zhang; Zhibin Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132341"&gt;10.1016/j.neucom.2025.132341&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Although cross-domain few-shot object detection (CD-FSOD) has advanced in recent years, its application to underwater scenarios remains limited. Underwater environments pose unique challenges—light attenuation, color distortion, and scattering cause background interference and large distribution shifts across scenes, reducing the effectiveness of traditional transfer methods. The scarcity of high-quality annotated underwater data further hampers training, intensifying few-shot difficulties. Moreover, marine organisms often exhibit high visual similarity between foreground and background (camouflage effect), making it harder for models to capture critical features. These factors form a bottleneck: existing models struggle to adapt to new underwater domains with few samples, while current few-shot methods fail to handle domain shifts specific to underwater settings. We propose a transfer learning framework for CD-FSOD in complex underwater environments, using a two-stage strategy: pretraining on large-scale source-domain data, followed by fine-tuning selected parameters with limited target-domain samples to reduce overfitting and improve cross-domain performance. Two core modules enhance transferability and robustness: (1) Dynamic Background Suppression Module (DBSM), which adaptively weakens background noise and highlights salient regions; and (2) Foreground Focus Module (FFM), combining edge-aware enhancement and local window attention to strengthen fine-grained discriminative features. These modules jointly address background interference and camouflage challenges. Extensive experiments on five public underwater datasets—S-UODAC2020, Brackish, Aquarium, URPC2022, and CSIRO Crown-of-Thorns Starfish—demonstrate the superior performance of our method in cross-domain few-shot detection under challenging underwater conditions.&lt;/p&gt;</content:encoded></item><item><title>3D building reconstruction from monocular remote sensing imagery via diffusion models and geometric priors</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.018</link><guid>10.1016/j.isprsjprs.2025.11.018</guid><pubDate>Fri, 05 Dec 2025 22:05:01 +0000</pubDate><dc:creator>Zhenghao Hu</dc:creator><dc:creator>Weijia Li</dc:creator><dc:creator>Jinhua Yu</dc:creator><dc:creator>Minfa Liu</dc:creator><dc:creator>Junyan Ye</dc:creator><dc:creator>Peimin Chen</dc:creator><dc:creator>Huabing Huang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.018</prism:doi><description>3D building reconstruction from monocular remote sensing imagery has emerged as a critical research topic due to its cost-effective data acquisition and scalability for large-area applications. However, the reconstruction accuracy of existing methods remains limited due to suboptimal performance in both contour extraction and height estimation. The limited feature extraction capabilities of the models and the difficulty in differentiating building roofs from facades collectively restrict the accuracy of reconstructed contours in existing methods. Meanwhile, building height estimation remains particularly challenging in monocular images due to the limited information from a single perspective. To address these challenges, we propose DG-BRF, a Diffusion-Geometric based Building Reconstruction Framework that integrates a diffusion-model-based roof and facade segmentation network with a geometric prior-driven offset calculation method for precise 3D reconstruction from monocular remote sensing images. To improve the accuracy of building contour extraction, we design an effective diffusion-model-based roof and facade segmentation network and improve roof-facade differentiation by novelly incorporating a depth-aware encoder. Moreover, unlike conventional methods that rely on challenging height regression, we propose a geometric prior-driven offset calculation method, strategically converting the challenging height regression problem into a simple roof-facade matching task. Experimental results on two newly proposed 3D reconstruction datasets demonstrate the effectiveness of our framework. DG-BRF achieves superior performance, outperforming the current state-of-the-art by 3% and 13% in height estimation accuracy and 3% and 6% in footprint segmentation F1-score, demonstrating its capability to overcome the limitations of existing methods and offer a novel solution for 3D building reconstruction from monocular remote sensing images. The dataset and source code of this work will be available at https://github.com/zhenghaohu/DG-BRF .
Published: 2025-12-05T22:05:01+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenghao Hu; Weijia Li; Jinhua Yu; Minfa Liu; Junyan Ye; Peimin Chen; Huabing Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.018"&gt;10.1016/j.isprsjprs.2025.11.018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;3D building reconstruction from monocular remote sensing imagery has emerged as a critical research topic due to its cost-effective data acquisition and scalability for large-area applications. However, the reconstruction accuracy of existing methods remains limited due to suboptimal performance in both contour extraction and height estimation. The limited feature extraction capabilities of the models and the difficulty in differentiating building roofs from facades collectively restrict the accuracy of reconstructed contours in existing methods. Meanwhile, building height estimation remains particularly challenging in monocular images due to the limited information from a single perspective. To address these challenges, we propose DG-BRF, a Diffusion-Geometric based Building Reconstruction Framework that integrates a diffusion-model-based roof and facade segmentation network with a geometric prior-driven offset calculation method for precise 3D reconstruction from monocular remote sensing images. To improve the accuracy of building contour extraction, we design an effective diffusion-model-based roof and facade segmentation network and improve roof-facade differentiation by novelly incorporating a depth-aware encoder. Moreover, unlike conventional methods that rely on challenging height regression, we propose a geometric prior-driven offset calculation method, strategically converting the challenging height regression problem into a simple roof-facade matching task. Experimental results on two newly proposed 3D reconstruction datasets demonstrate the effectiveness of our framework. DG-BRF achieves superior performance, outperforming the current state-of-the-art by 3% and 13% in height estimation accuracy and 3% and 6% in footprint segmentation F1-score, demonstrating its capability to overcome the limitations of existing methods and offer a novel solution for 3D building reconstruction from monocular remote sensing images. The dataset and source code of this work will be available at https://github.com/zhenghaohu/DG-BRF .&lt;/p&gt;</content:encoded></item><item><title>Triple-Way Visual Modulation for Zero-Shot Sketch-Based Image Retrieval</title><link>https://doi.org/10.1109/tcsvt.2025.3640868</link><guid>10.1109/tcsvt.2025.3640868</guid><pubDate>Fri, 05 Dec 2025 18:40:41 +0000</pubDate><dc:creator>Haoxiang Zhang</dc:creator><dc:creator>Qiqi Kou</dc:creator><dc:creator>He Jiang</dc:creator><dc:creator>Tianshu Song</dc:creator><dc:creator>Liangliang Chen</dc:creator><dc:creator>Deqiang Cheng</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3640868</prism:doi><description>Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) seeks to correlate unseen hand-drawn sketches with unseen real images by leveraging trained models on visible categories. Recent CLIP-based models, which primarily focus on visual-textual interaction, have demonstrated strong competitiveness in ZS-SBIR. However, they still fall short in the exploration of cross-modal visual representations, especially in terms of cross-modal visual shared and specific information. Differing from the aforementioned researches, we start with class-level, prompt-level, and patch-level visual information, committed to unlocking the potential of visual feature representation. On this foundation, we introduce the vision-centric Triple-way visual modulATion (TAT) framework to enhance the model’s perception of visual shared and specific information. Specifically, we establish unified multi-modal perception by integrating visual-level modality prompter into the CLIP architecture. We then conduct triple-way modulation modeling on prompt, token, and patch levels to effectively mine shared and specific features. Lastly, we develop an enhanced calibration strategy incorporating prompt-aware, token-aware, and logit-aware alignment modules to amplify the model’s proficiency in probing shared-specific features. We thoroughly test our approach to confirm its excellence and the efficacy of individual components. The comparison results on the popular datasets Sketchy, Sketchyv2, Tuberlin, and QuickDraw show that the developed algorithm significantly surpasses the current state-of-the-art technologies.
Published: 2025-12-05T18:40:41+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoxiang Zhang; Qiqi Kou; He Jiang; Tianshu Song; Liangliang Chen; Deqiang Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3640868"&gt;10.1109/tcsvt.2025.3640868&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-Shot Sketch-Based Image Retrieval (ZS-SBIR) seeks to correlate unseen hand-drawn sketches with unseen real images by leveraging trained models on visible categories. Recent CLIP-based models, which primarily focus on visual-textual interaction, have demonstrated strong competitiveness in ZS-SBIR. However, they still fall short in the exploration of cross-modal visual representations, especially in terms of cross-modal visual shared and specific information. Differing from the aforementioned researches, we start with class-level, prompt-level, and patch-level visual information, committed to unlocking the potential of visual feature representation. On this foundation, we introduce the vision-centric Triple-way visual modulATion (TAT) framework to enhance the model’s perception of visual shared and specific information. Specifically, we establish unified multi-modal perception by integrating visual-level modality prompter into the CLIP architecture. We then conduct triple-way modulation modeling on prompt, token, and patch levels to effectively mine shared and specific features. Lastly, we develop an enhanced calibration strategy incorporating prompt-aware, token-aware, and logit-aware alignment modules to amplify the model’s proficiency in probing shared-specific features. We thoroughly test our approach to confirm its excellence and the efficacy of individual components. The comparison results on the popular datasets Sketchy, Sketchyv2, Tuberlin, and QuickDraw show that the developed algorithm significantly surpasses the current state-of-the-art technologies.&lt;/p&gt;</content:encoded></item><item><title>Difference Decomposition Networks for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2512.03470v1</link><guid>http://arxiv.org/abs/2512.03470v1</guid><pubDate>Wed, 03 Dec 2025 05:52:06 +0000</pubDate><dc:creator>Chen Hu</dc:creator><dc:creator>Mingyu Zhou</dc:creator><dc:creator>Shuai Yuan</dc:creator><dc:creator>Hongbo Hu</dc:creator><dc:creator>Xiangyu Qiu</dc:creator><dc:creator>Junhai Luo</dc:creator><dc:creator>Tian Pu</dc:creator><dc:creator>Xiyin Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.
Published: 2025-12-03T05:52:06+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Hu; Mingyu Zhou; Shuai Yuan; Hongbo Hu; Xiangyu Qiu; Junhai Luo; Tian Pu; Xiyin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.&lt;/p&gt;</content:encoded></item><item><title>MSG-CLIP: Enhancing CLIP’s Ability to Learn Fine-grained Structural Associations through Multi-modal Scene Graph Alignment</title><link>https://doi.org/10.1016/j.patcog.2025.112794</link><guid>10.1016/j.patcog.2025.112794</guid><pubDate>Sat, 06 Dec 2025 23:14:20 +0000</pubDate><dc:creator>Xiaotian Lv</dc:creator><dc:creator>Yue Zhao</dc:creator><dc:creator>Hanlong Yin</dc:creator><dc:creator>Yifei Chen</dc:creator><dc:creator>Jianxing Liu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112794</prism:doi><description>As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.
Published: 2025-12-06T23:14:20+00:00
Venue: Pattern Recognition
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaotian Lv; Yue Zhao; Hanlong Yin; Yifei Chen; Jianxing Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112794"&gt;10.1016/j.patcog.2025.112794&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;As a typical representative of Vision-Language foundation models, the Contrastive Language-Image Pre-training (CLIP) framework has garnered extensive attention due to its cross-modal understanding capabilities. Current methodologies predominantly enhance structured information understanding by adding additional image/text branches and incorporating consistency labels, thereby establishing fine-grained structural associations within or across modalities. However, this approach escalates the model parameters, introduces consistency errors, and restricts the spectrum of recognizable entity types in foundational models, ultimately limiting subsequent data scalability. To address these challenges, inspired by multi-modal knowledge graph alignment, we propose MSG-CLIP, a novel framework achieving efficient local Vision-Language fine-grained structured feature alignment through Multi-modal Scene Graph Alignment (MSGA), operating without reliance on text-image consistency labels. Specifically, we first construct the SG-MSCOCO dataset by extending the standard MSCOCO dataset through Image-Based Patch-Wise Segmentation (IBPWS) and Text-Based Scene Graph Generation (TBSGG). Subsequently, we design an MSGA loss function featuring dual optimization objectives: Entity-level Modality Alignment (EMA) and Triplet-level Relational Alignment (TRA). Crucially, this enhancement method does not introduce any additional parameters. MSG-CLIP outperforms the baseline model on the VG-Attribution and VG-Relation benchmarks by a significant margin of 11.2% and 2.5%, respectively. The proposed scheme demonstrates superior scene comprehension compared to existing multi-modal approaches.&lt;/p&gt;</content:encoded></item><item><title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title><link>https://arxiv.org/abs/2512.04581v1</link><guid>http://arxiv.org/abs/2512.04581v1</guid><pubDate>Thu, 04 Dec 2025 08:49:23 +0000</pubDate><dc:creator>Houzhang Fang</dc:creator><dc:creator>Chenxing Wu</dc:creator><dc:creator>Kun Bai</dc:creator><dc:creator>Tianqi Chen</dc:creator><dc:creator>Xiaolin Wang</dc:creator><dc:creator>Xiyang Liu</dc:creator><dc:creator>Yi Chang</dc:creator><dc:creator>Luxin Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
Published: 2025-12-04T08:49:23+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Houzhang Fang; Chenxing Wu; Kun Bai; Tianqi Chen; Xiaolin Wang; Xiyang Liu; Yi Chang; Luxin Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network&amp;#x27;s focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.&lt;/p&gt;</content:encoded></item><item><title>Mutually Guided Fusion Learning for Collaborative Camouflaged Object Segmentation</title><link>https://doi.org/10.1109/tnnls.2025.3636523</link><guid>10.1109/tnnls.2025.3636523</guid><pubDate>Fri, 05 Dec 2025 18:39:49 +0000</pubDate><dc:creator>Chen Li</dc:creator><dc:creator>Xiao Luan</dc:creator><dc:creator>Linghui Liu</dc:creator><dc:creator>Yanzhao Su</dc:creator><dc:creator>Yule Fu</dc:creator><dc:creator>Weisheng Li</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3636523</prism:doi><description>Collaborative camouflaged object segmentation (CoCOS) is a challenging task, focusing on identifying objects that blend closely with their backgrounds by jointly processing intraclass images. Existing methods fail to fully leverage the shared features (e.g., shape, texture, and contour) from these intraclass images, which leads to poor segmentation performance in relatively complex scenarios. To address this issue, we propose a novel mutually guided fusion refinement network (MFRNet), which improves the model performance by more effectively collaborating and optimizing the shared information. Specifically, it includes feature encoding, single-image branch feature enhancement, multiimage branch feature enhancement, and mutual guidance. After the feature encoding step, we design the graph convolution self-attention (GCS) and spatial context exploration (SCE) modules to enhance multilevel features of the single-image and multiimage branches, respectively. Moreover, we propose a mutual guidance fusion (MGF) module to utilize cross-scene image information for mutual guidance and progressive refinement, enhancing intraclass collaboration for improving target feature distinction. Extensive experimental results demonstrate that our MFRNet significantly outperforms existing CoCOS methods, achieving a mean E-measure score of 0.846 on the CoCOD8K dataset. Our code will be published at https://github.com/another-u/MFRNet
Published: 2025-12-05T18:39:49+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Li; Xiao Luan; Linghui Liu; Yanzhao Su; Yule Fu; Weisheng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3636523"&gt;10.1109/tnnls.2025.3636523&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Collaborative camouflaged object segmentation (CoCOS) is a challenging task, focusing on identifying objects that blend closely with their backgrounds by jointly processing intraclass images. Existing methods fail to fully leverage the shared features (e.g., shape, texture, and contour) from these intraclass images, which leads to poor segmentation performance in relatively complex scenarios. To address this issue, we propose a novel mutually guided fusion refinement network (MFRNet), which improves the model performance by more effectively collaborating and optimizing the shared information. Specifically, it includes feature encoding, single-image branch feature enhancement, multiimage branch feature enhancement, and mutual guidance. After the feature encoding step, we design the graph convolution self-attention (GCS) and spatial context exploration (SCE) modules to enhance multilevel features of the single-image and multiimage branches, respectively. Moreover, we propose a mutual guidance fusion (MGF) module to utilize cross-scene image information for mutual guidance and progressive refinement, enhancing intraclass collaboration for improving target feature distinction. Extensive experimental results demonstrate that our MFRNet significantly outperforms existing CoCOS methods, achieving a mean E-measure score of 0.846 on the CoCOD8K dataset. Our code will be published at https://github.com/another-u/MFRNet&lt;/p&gt;</content:encoded></item><item><title>URDM: Hyperspectral Unmixing Regularized by Diffusion Models</title><link>https://doi.org/10.1109/tip.2025.3638151</link><guid>10.1109/tip.2025.3638151</guid><pubDate>Fri, 05 Dec 2025 18:41:09 +0000</pubDate><dc:creator>Min Zhao</dc:creator><dc:creator>Linruize Tang</dc:creator><dc:creator>Jie Chen</dc:creator><dc:creator>Bo Huang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3638151</prism:doi><description>Hyperspectral unmixing aims to decompose the mixed pixels into pure spectra and calculate their corresponding fractional abundances. It holds a critical position in hyperspectral image processing. Traditional model-based unmixing methods use convex optimization to iteratively solve the unmixing problem with hand-crafted regularizers. While their performance is limited by these manually designed constraints, which may not fully capture the structural information of the data. Recently, deep learning-based unmixing methods have shown remarkable capability for this task. However, they have limited generalizability and lack interpretability. In this paper, we propose a novel hyperspectral unmixing method regularized by a diffusion model (URDM) to overcome these shortcomings. Our method leverages the advantages of both conventional optimization algorithms and deep generative models. Specifically, we formulate the unmixing objective function from a variational perspective and integrate it into a diffusion sampling process to introduce generative priors from a denoising diffusion probabilistic model (DDPM). Since the original objective function is challenging to optimize, we introduce a splitting-based strategy to decouple it into simpler subproblems. Extensive experiment results conducted on both synthetic and real datasets demonstrate the efficiency and superior performance of our proposed method.
Published: 2025-12-05T18:41:09+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Min Zhao; Linruize Tang; Jie Chen; Bo Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3638151"&gt;10.1109/tip.2025.3638151&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral unmixing aims to decompose the mixed pixels into pure spectra and calculate their corresponding fractional abundances. It holds a critical position in hyperspectral image processing. Traditional model-based unmixing methods use convex optimization to iteratively solve the unmixing problem with hand-crafted regularizers. While their performance is limited by these manually designed constraints, which may not fully capture the structural information of the data. Recently, deep learning-based unmixing methods have shown remarkable capability for this task. However, they have limited generalizability and lack interpretability. In this paper, we propose a novel hyperspectral unmixing method regularized by a diffusion model (URDM) to overcome these shortcomings. Our method leverages the advantages of both conventional optimization algorithms and deep generative models. Specifically, we formulate the unmixing objective function from a variational perspective and integrate it into a diffusion sampling process to introduce generative priors from a denoising diffusion probabilistic model (DDPM). Since the original objective function is challenging to optimize, we introduce a splitting-based strategy to decouple it into simpler subproblems. Extensive experiment results conducted on both synthetic and real datasets demonstrate the efficiency and superior performance of our proposed method.&lt;/p&gt;</content:encoded></item><item><title>Lightweight Image Super-Resolution Network with Adaptive Token Selection and Feature Enhancement</title><link>https://doi.org/10.1016/j.knosys.2025.115055</link><guid>10.1016/j.knosys.2025.115055</guid><pubDate>Sat, 06 Dec 2025 16:09:36 +0000</pubDate><dc:creator>Detian Huang</dc:creator><dc:creator>Mingxin Lin</dc:creator><dc:creator>Xinwei Gan</dc:creator><dc:creator>Luanyuan Dai</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115055</prism:doi><description>Transformer has demonstrated remarkable performance in image super-resolution due to its powerful long-range dependency modeling. However, existing transformer-based methods face two challenges. First, their self-attention mechanisms often treat all tokens equally, introducing redundant correlations and noise. Second, the use of window-based attention restricts the receptive field, hindering effective long-range dependency modeling. To address these issues, we introduce TSFE, a lightweight super-resolution network with adaptive Token Selection and Feature Enhancement. Specifically, we design a Token Selection Channel self-Attention (TSCA) that adaptively selects the top- K most relevant tokens along the channel dimension, preserving critical features while reducing redundant computations. Then, we present a Sparse Spatial Self-Attention (SSSA) that enhances critical features by suppressing low-attention weights and amplifying high-attention ones in the spatial dimension. To expand the receptive field, SSSA integrates a prior dictionary that guides the attention distribution with external prior, improving its dependency modeling capability. The collaboration of TSCA and SSSA components enables TSFE to achieve precise self-attention computation with low complexity. Extensive experiments validate that the proposed TSFE outperforms state-of-the-art methods across benchmarks.
Published: 2025-12-06T16:09:36+00:00
Venue: Knowledge-Based Systems
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Detian Huang; Mingxin Lin; Xinwei Gan; Luanyuan Dai; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115055"&gt;10.1016/j.knosys.2025.115055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer has demonstrated remarkable performance in image super-resolution due to its powerful long-range dependency modeling. However, existing transformer-based methods face two challenges. First, their self-attention mechanisms often treat all tokens equally, introducing redundant correlations and noise. Second, the use of window-based attention restricts the receptive field, hindering effective long-range dependency modeling. To address these issues, we introduce TSFE, a lightweight super-resolution network with adaptive Token Selection and Feature Enhancement. Specifically, we design a Token Selection Channel self-Attention (TSCA) that adaptively selects the top- K most relevant tokens along the channel dimension, preserving critical features while reducing redundant computations. Then, we present a Sparse Spatial Self-Attention (SSSA) that enhances critical features by suppressing low-attention weights and amplifying high-attention ones in the spatial dimension. To expand the receptive field, SSSA integrates a prior dictionary that guides the attention distribution with external prior, improving its dependency modeling capability. The collaboration of TSCA and SSSA components enables TSFE to achieve precise self-attention computation with low complexity. Extensive experiments validate that the proposed TSFE outperforms state-of-the-art methods across benchmarks.&lt;/p&gt;</content:encoded></item><item><title>频域增强与边界感知的自动驾驶实时行人检测</title><link>https://doi.org/10.11834/jig.250437</link><guid>10.11834/jig.250437</guid><pubDate>Fri, 05 Dec 2025 08:29:44 +0000</pubDate><dc:creator>Liu Tao</dc:creator><dc:creator>OuYang Hui</dc:creator><dc:creator>Gao Yimeng</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250437</prism:doi><description>目的在自动驾驶场景中，行人尺度变化剧烈、遮挡现象频繁和复杂环境干扰等问题会导致检测性能显著下降。针对这些挑战，本文提出了一种结合多重频域增强策略与边界感知机制的实时行人检测算法FEBA-DETR（Frequency-Enhanced and Bound-Aware Detection Transformer）。方法FEBA-DETR基于RT-DETR架构进行改进，通过使用频域增强、边界感知机制以及优化损失函数，提升对小目标的检测能力和遮挡情况下的检测能力。最后再结合频域子带数据增强方法，进一步提升算法在雨、雾、雪和低光照等复杂环境下的检测性能。结果与原RT-DETR算法相比，FEBA-DETR在CityPerosns数据集上，AP50与AP50：95分别提升2.3%和2%，引入频域子带增强后分别提升3%和2.4%；在代表部分遮挡和严重遮挡的场景中，MR -2 分别下降4.92%和2.82%。结论FEBA-DETR算法提升了在自动驾驶场景中对远距离小尺度行人和严重遮挡行人的检测能力，并在复杂环境下也表现出更强的鲁棒性，其性能优势在多组对比实验中得到了充分验证，能有效应对自动驾驶中的行人检测挑战，为自动驾驶系统的安全提供支持。
Published: 2025-12-05T08:29:44+00:00
Venue: Journal of Image and Graphics
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liu Tao; OuYang Hui; Gao Yimeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250437"&gt;10.11834/jig.250437&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;目的在自动驾驶场景中，行人尺度变化剧烈、遮挡现象频繁和复杂环境干扰等问题会导致检测性能显著下降。针对这些挑战，本文提出了一种结合多重频域增强策略与边界感知机制的实时行人检测算法FEBA-DETR（Frequency-Enhanced and Bound-Aware Detection Transformer）。方法FEBA-DETR基于RT-DETR架构进行改进，通过使用频域增强、边界感知机制以及优化损失函数，提升对小目标的检测能力和遮挡情况下的检测能力。最后再结合频域子带数据增强方法，进一步提升算法在雨、雾、雪和低光照等复杂环境下的检测性能。结果与原RT-DETR算法相比，FEBA-DETR在CityPerosns数据集上，AP50与AP50：95分别提升2.3%和2%，引入频域子带增强后分别提升3%和2.4%；在代表部分遮挡和严重遮挡的场景中，MR -2 分别下降4.92%和2.82%。结论FEBA-DETR算法提升了在自动驾驶场景中对远距离小尺度行人和严重遮挡行人的检测能力，并在复杂环境下也表现出更强的鲁棒性，其性能优势在多组对比实验中得到了充分验证，能有效应对自动驾驶中的行人检测挑战，为自动驾驶系统的安全提供支持。&lt;/p&gt;</content:encoded></item><item><title>Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection</title><link>https://arxiv.org/abs/2512.04413v1</link><guid>http://arxiv.org/abs/2512.04413v1</guid><pubDate>Thu, 04 Dec 2025 03:18:42 +0000</pubDate><dc:creator>Xiangyi Gao</dc:creator><dc:creator>Danpei Zhao</dc:creator><dc:creator>Bo Yuan</dc:creator><dc:creator>Wentao Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TGRS.2025.3600098</prism:doi><description>Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.
Published: 2025-12-04T03:18:42+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangyi Gao; Danpei Zhao; Bo Yuan; Wentao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TGRS.2025.3600098"&gt;10.1109/TGRS.2025.3600098&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.&lt;/p&gt;</content:encoded></item><item><title>基于视觉与深度学习的非合作空间目标识别与位姿估计方法研究进展</title><link>https://doi.org/10.11834/jig.250435</link><guid>10.11834/jig.250435</guid><pubDate>Fri, 05 Dec 2025 08:29:39 +0000</pubDate><dc:creator>Jiang Jianxiang</dc:creator><dc:creator>Wu Yiquan</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250435</prism:doi><description>为提升非合作空间目标在轨服务与态势感知中的感知能力，系统综述了基于视觉与深度学习的非合作空间目标识别与位姿估计方法研究进展。首先阐明非合作空间目标识别与位姿估计的任务特点，简要回顾了各自的发展历程。其次围绕非合作空间目标识别与位姿估计等关键任务，梳理了当前主流方法。在目标识别方面，重点分析了基于多模态视觉融合、目标检测与分割、迁移学习与少样本学习的识别模型及其在复杂空间环境中的适应性与鲁棒性；位姿估计方面主要包括直接回归、关键点检测、无监督与域自适应、多模态视觉融合等技术。随后梳理了空间目标公开数据集及自建数据集，评估了目标识别与位姿估计方法的性能，并分别对其优缺点进行比较。结果表明，深度学习方法相较于传统方法在弱纹理、遮挡和姿态变化等复杂空间环境下具有更强的鲁棒性，多模态融合与无监督策略进一步提升了跨域泛化能力。最后总结了目标识别与位姿估计方法面临的主要挑战，并展望了未来的发展方向，旨在为该领域的技术研究与工程应用提供参考，提及的算法、数据集和评价指标已汇总至https：//github.com/viskyll/openResource/tree/main。
Published: 2025-12-05T08:29:39+00:00
Venue: Journal of Image and Graphics
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiang Jianxiang; Wu Yiquan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250435"&gt;10.11834/jig.250435&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;为提升非合作空间目标在轨服务与态势感知中的感知能力，系统综述了基于视觉与深度学习的非合作空间目标识别与位姿估计方法研究进展。首先阐明非合作空间目标识别与位姿估计的任务特点，简要回顾了各自的发展历程。其次围绕非合作空间目标识别与位姿估计等关键任务，梳理了当前主流方法。在目标识别方面，重点分析了基于多模态视觉融合、目标检测与分割、迁移学习与少样本学习的识别模型及其在复杂空间环境中的适应性与鲁棒性；位姿估计方面主要包括直接回归、关键点检测、无监督与域自适应、多模态视觉融合等技术。随后梳理了空间目标公开数据集及自建数据集，评估了目标识别与位姿估计方法的性能，并分别对其优缺点进行比较。结果表明，深度学习方法相较于传统方法在弱纹理、遮挡和姿态变化等复杂空间环境下具有更强的鲁棒性，多模态融合与无监督策略进一步提升了跨域泛化能力。最后总结了目标识别与位姿估计方法面临的主要挑战，并展望了未来的发展方向，旨在为该领域的技术研究与工程应用提供参考，提及的算法、数据集和评价指标已汇总至https：//github.com/viskyll/openResource/tree/main。&lt;/p&gt;</content:encoded></item><item><title>CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning</title><link>https://doi.org/10.1016/j.inffus.2025.104027</link><guid>10.1016/j.inffus.2025.104027</guid><pubDate>Fri, 05 Dec 2025 00:44:23 +0000</pubDate><dc:creator>Wenjie Li</dc:creator><dc:creator>Yujie Zhang</dc:creator><dc:creator>Haoran Sun</dc:creator><dc:creator>Yueqi Li</dc:creator><dc:creator>Fanrui Zhang</dc:creator><dc:creator>Mengzhe Xu</dc:creator><dc:creator>Victoria Borja Clausich</dc:creator><dc:creator>Sade Mellin</dc:creator><dc:creator>Renhao Yang</dc:creator><dc:creator>Chenrun Wang</dc:creator><dc:creator>Jethro Zih-Shuo Wang</dc:creator><dc:creator>Shiyi Yao</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Yidong Xu</dc:creator><dc:creator>Hanyu Wang</dc:creator><dc:creator>Yilin Huang</dc:creator><dc:creator>Angela Lin Wang</dc:creator><dc:creator>Chen Shi</dc:creator><dc:creator>Yin Zhang</dc:creator><dc:creator>Jianan Guo</dc:creator><dc:creator>Luqi Yang</dc:creator><dc:creator>Renxuan Li</dc:creator><dc:creator>Yang Xu</dc:creator><dc:creator>Jiawei Liu</dc:creator><dc:creator>Yao Zhang</dc:creator><dc:creator>Lei Liu</dc:creator><dc:creator>Carlos Gutiérrez Sanromán</dc:creator><dc:creator>Lei Wang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104027</prism:doi><description>Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on ”one-time” diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind , the first generative model to achieve interleaved ”think-answer” reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On a real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. CX-Mind establishes a new paradigm for constructing interpretable, and high-performing medical MLLMs.
Published: 2025-12-05T00:44:23+00:00
Venue: Information Fusion
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenjie Li; Yujie Zhang; Haoran Sun; Yueqi Li; Fanrui Zhang; Mengzhe Xu; Victoria Borja Clausich; Sade Mellin; Renhao Yang; Chenrun Wang; Jethro Zih-Shuo Wang; Shiyi Yao; Gen Li; Yidong Xu; Hanyu Wang; Yilin Huang; Angela Lin Wang; Chen Shi; Yin Zhang; Jianan Guo; Luqi Yang; Renxuan Li; Yang Xu; Jiawei Liu; Yao Zhang; Lei Liu; Carlos Gutiérrez Sanromán; Lei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104027"&gt;10.1016/j.inffus.2025.104027&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Chest X-ray (CXR) imaging is one of the most widely used diagnostic modalities in clinical practice, encompassing a broad spectrum of diagnostic tasks. Recent advancements have seen the extensive application of reasoning-based multimodal large language models (MLLMs) in medical imaging to enhance diagnostic efficiency and interpretability. However, existing multimodal models predominantly rely on ”one-time” diagnostic approaches, lacking verifiable supervision of the reasoning process. This leads to challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse rewards, and frequent hallucinations. To address these issues, we propose CX-Mind , the first generative model to achieve interleaved ”think-answer” reasoning for CXR tasks, driven by curriculum-based reinforcement learning and verifiable process rewards (CuRL-VPR). Specifically, we constructed an instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148 samples, and generated 42,828 high-quality interleaved reasoning data points supervised by clinical reports. Optimization was conducted in two stages under the Group Relative Policy Optimization framework: initially stabilizing basic reasoning with closed-domain tasks, followed by transfer to open-domain diagnostics, incorporating rule-based conditional process rewards to bypass the need for pretrained reward models. Extensive experimental results demonstrate that CX-Mind significantly outperforms existing medical and general-domain MLLMs in visual understanding, text generation, and spatiotemporal alignment, achieving an average performance improvement of 25.1% over comparable CXR-specific models. On a real-world clinical dataset (Rui-CXR), CX-Mind achieves a mean recall@1 across 14 diseases that substantially surpasses the second-best results, with multi-center expert evaluations further confirming its clinical utility across multiple dimensions. CX-Mind establishes a new paradigm for constructing interpretable, and high-performing medical MLLMs.&lt;/p&gt;</content:encoded></item><item><title>DSKFuse: Passive-Active Distillation Learning for Multi-modal Image Fusion via Dynamic Sparse Kansformer</title><link>https://doi.org/10.1016/j.eswa.2025.130610</link><guid>10.1016/j.eswa.2025.130610</guid><pubDate>Fri, 05 Dec 2025 00:35:51 +0000</pubDate><dc:creator>Zhaisheng Ding</dc:creator><dc:creator>Ruichao Hou</dc:creator><dc:creator>Yunzhe Men</dc:creator><dc:creator>Shengyang Luan</dc:creator><dc:creator>Yanyu Liu</dc:creator><dc:creator>Kangjian He</dc:creator><dc:creator>Shidong Xie</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130610</prism:doi><description>An effective knowledge learning strategy combined with a lightweight network architecture is crucial for the practical deployment of multi-modal image fusion. While existing methods have made significant progress in the visual perception of fused results, their model complexity and generalization capabilities still require further optimization. In this paper, we propose a novel passive-active distillation learning framework for multi-modal image fusion, termed DSKFuse, which integrates the Dynamic Sparse Transformer and the latent Kolmogorov-Arnold Network (KAN). Specifically, we design an efficient fusion architecture trained via a two-stage knowledge distillation strategy, seamlessly integrating passive and active learning methodologies. In the first stage, passive distillation learning enhances the fusion network by extracting valuable knowledge from complex fusion models. In the second stage, an active knowledge distillation approach is implemented, enabling the model to autonomously capture discriminative features from source images, thereby improving the robustness and generalization of DSKFuse. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance in both image fusion and downstream tasks, including detection and segmentation. The code will be released at https://github.com/DZSYUNNAN/DSKFuse .
Published: 2025-12-05T00:35:51+00:00
Venue: Expert Systems with Applications
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaisheng Ding; Ruichao Hou; Yunzhe Men; Shengyang Luan; Yanyu Liu; Kangjian He; Shidong Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130610"&gt;10.1016/j.eswa.2025.130610&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;An effective knowledge learning strategy combined with a lightweight network architecture is crucial for the practical deployment of multi-modal image fusion. While existing methods have made significant progress in the visual perception of fused results, their model complexity and generalization capabilities still require further optimization. In this paper, we propose a novel passive-active distillation learning framework for multi-modal image fusion, termed DSKFuse, which integrates the Dynamic Sparse Transformer and the latent Kolmogorov-Arnold Network (KAN). Specifically, we design an efficient fusion architecture trained via a two-stage knowledge distillation strategy, seamlessly integrating passive and active learning methodologies. In the first stage, passive distillation learning enhances the fusion network by extracting valuable knowledge from complex fusion models. In the second stage, an active knowledge distillation approach is implemented, enabling the model to autonomously capture discriminative features from source images, thereby improving the robustness and generalization of DSKFuse. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance in both image fusion and downstream tasks, including detection and segmentation. The code will be released at https://github.com/DZSYUNNAN/DSKFuse .&lt;/p&gt;</content:encoded></item><item><title>Background noise suppression for advanced fine-grained visual classification</title><link>https://doi.org/10.1016/j.neucom.2025.132183</link><guid>10.1016/j.neucom.2025.132183</guid><pubDate>Fri, 05 Dec 2025 16:20:53 +0000</pubDate><dc:creator>Zhi-Gang Wang</dc:creator><dc:creator>Si-Bao Chen</dc:creator><dc:creator>Bin Luo</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132183</prism:doi><description>In fine-grained visual classification (FGVC), complex backgrounds exacerbate the challenge of discerning subtle inter-class variations while amplifying intra-class differences. While vision transformer (ViT) has demonstrated remarkable potential in global feature modeling through the multi-head self-attention (MHSA) mechanism, its direct application to FGVC reveals critical limitations. In particular, in a standard ViT, the classification token aggregates global information from less-important background patches may introduce noise, especially when dealing with fine-grained images with diverse backgrounds. To address these issues, we propose a method including three key modules: the masked-guided encoder (MGE) module, the attention random patch combination (ARPC) module, and the spatial-aware feature fusion (SAFF) module. We refer to this method as MAS-ViT. The MGE utilizes the segment anything model (SAM) to generate masks to suppress background noise and enhance discriminative tokens focus. ARPC integrates critical patches from paired images along with their corresponding masks, guided by a learnable weight map. SAFF leverages the spatial relationships of significant patches during feature fusion and uses knowledge distillation to refine the learning process. Extensive experiments demonstrate that MAS-ViT outperforms existing ViT-based methods on popular FGVC benchmarks.
Published: 2025-12-05T16:20:53+00:00
Venue: Neurocomputing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhi-Gang Wang; Si-Bao Chen; Bin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132183"&gt;10.1016/j.neucom.2025.132183&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;In fine-grained visual classification (FGVC), complex backgrounds exacerbate the challenge of discerning subtle inter-class variations while amplifying intra-class differences. While vision transformer (ViT) has demonstrated remarkable potential in global feature modeling through the multi-head self-attention (MHSA) mechanism, its direct application to FGVC reveals critical limitations. In particular, in a standard ViT, the classification token aggregates global information from less-important background patches may introduce noise, especially when dealing with fine-grained images with diverse backgrounds. To address these issues, we propose a method including three key modules: the masked-guided encoder (MGE) module, the attention random patch combination (ARPC) module, and the spatial-aware feature fusion (SAFF) module. We refer to this method as MAS-ViT. The MGE utilizes the segment anything model (SAM) to generate masks to suppress background noise and enhance discriminative tokens focus. ARPC integrates critical patches from paired images along with their corresponding masks, guided by a learnable weight map. SAFF leverages the spatial relationships of significant patches during feature fusion and uses knowledge distillation to refine the learning process. Extensive experiments demonstrate that MAS-ViT outperforms existing ViT-based methods on popular FGVC benchmarks.&lt;/p&gt;</content:encoded></item></channel></rss>