<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 09 Feb 2026 03:37:27 +0000</lastBuildDate><item><title>MuDeNet: a Multi-patch Descriptor Network for Anomaly Modeling</title><link>https://doi.org/10.1016/j.inffus.2026.104214</link><guid>10.1016/j.inffus.2026.104214</guid><pubDate>Sat, 07 Feb 2026 23:36:51 +0000</pubDate><dc:creator>Miguel Campos-Romero</dc:creator><dc:creator>Manuel Carranza-Garcia</dc:creator><dc:creator>Robert-Jan Sips</dc:creator><dc:creator>Jose C. Riquelme</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104214</prism:doi><description>Visual anomaly detection is a crucial task in industrial manufacturing, enabling early defect identification and minimizing production bottlenecks. Existing methods often struggle to effectively detect both structural anomalies, which appear as unexpected local patterns, and logical anomalies, which arise from violations of global contextual constraints. To address this challenge, we propose MuDeNet, an unsupervised Multi-patch Descriptor Network that performs multi-scale fusion of local structural features and global contextual information for comprehensive anomaly modeling. MuDeNet employs a lightweight teacher-student framework that jointly extracts and fuses local and global patch descriptors across multiple receptive fields within a single forward pass. Knowledge is first distilled from a pre-trained CNN to efficiently obtain semantic representations, which are then processed by two complementary modules: the structural module, targeting fine-grained defects at small receptive fields, and the logical module, modeling long-range contextual dependencies. Their outputs are fused at the decision level, yielding a unified anomaly score that integrates local and global evidence. Extensive experiments on three state-of-the-art datasets position MuDeNet as an efficient and scalable solution for real-time industrial anomaly detection and segmentation, consistently outperforming existing approaches.
Published: 2026-02-07T23:36:51+00:00
Venue: Information Fusion
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Miguel Campos-Romero; Manuel Carranza-Garcia; Robert-Jan Sips; Jose C. Riquelme&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104214"&gt;10.1016/j.inffus.2026.104214&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Visual anomaly detection is a crucial task in industrial manufacturing, enabling early defect identification and minimizing production bottlenecks. Existing methods often struggle to effectively detect both structural anomalies, which appear as unexpected local patterns, and logical anomalies, which arise from violations of global contextual constraints. To address this challenge, we propose MuDeNet, an unsupervised Multi-patch Descriptor Network that performs multi-scale fusion of local structural features and global contextual information for comprehensive anomaly modeling. MuDeNet employs a lightweight teacher-student framework that jointly extracts and fuses local and global patch descriptors across multiple receptive fields within a single forward pass. Knowledge is first distilled from a pre-trained CNN to efficiently obtain semantic representations, which are then processed by two complementary modules: the structural module, targeting fine-grained defects at small receptive fields, and the logical module, modeling long-range contextual dependencies. Their outputs are fused at the decision level, yielding a unified anomaly score that integrates local and global evidence. Extensive experiments on three state-of-the-art datasets position MuDeNet as an efficient and scalable solution for real-time industrial anomaly detection and segmentation, consistently outperforming existing approaches.&lt;/p&gt;</content:encoded></item><item><title>MMP-Mapper: Multi-modal priors enhancing vectorized HD road map construction from aerial imagery</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.008</link><guid>10.1016/j.isprsjprs.2026.02.008</guid><pubDate>Sat, 07 Feb 2026 17:30:51 +0000</pubDate><dc:creator>Haofeng Xie</dc:creator><dc:creator>Huiwei Jiang</dc:creator><dc:creator>Yandi Yang</dc:creator><dc:creator>Xiangyun Hu</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.008</prism:doi><description>High-definition (HD) road maps are indispensable for autonomous driving, supporting tasks such as localization, planning, and navigation. The traditional construction of HD road maps heavily relies on manual annotation of data from LiDAR, cameras, and GPS/IMU, a process that is both costly and time-consuming. While recent work has explored automatic HD road map extraction from aerial imagery — a data source offering broad-area coverage and superior robustness — existing methods face a critical limitation. They often process only a single, isolated image tile, failing to leverage crucial spatial context and semantic priors from multi-modal data sources. This shortage severely impacts map accuracy and continuity, especially at complex intersections and in occluded areas. To overcome these challenges, we propose MMP-Mapper, a novel framework that enhances HD road map construction with multi-modal priors. MMP-Mapper introduces two key modules: (1) the Contextual Image Fusion (CIF) module, which selects and fuses features from neighbor image tiles to provide spatial continuity; and (2) the Map-Guided Fusion (MGF) module, which uses a Transformer module to fuse the encoded semantic attributes from standard-definition (SD) road maps with geometric priors, guiding HD road map construction. We validate our framework on the Aerial Argoverse 2 and OpenSatMap datasets. Our results demonstrate that MMP-Mapper outperforms state-of-the-art baselines in both accuracy and generalization for aerial-imagery-based HD road map construction.
Published: 2026-02-07T17:30:51+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haofeng Xie; Huiwei Jiang; Yandi Yang; Xiangyun Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.008"&gt;10.1016/j.isprsjprs.2026.02.008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;High-definition (HD) road maps are indispensable for autonomous driving, supporting tasks such as localization, planning, and navigation. The traditional construction of HD road maps heavily relies on manual annotation of data from LiDAR, cameras, and GPS/IMU, a process that is both costly and time-consuming. While recent work has explored automatic HD road map extraction from aerial imagery — a data source offering broad-area coverage and superior robustness — existing methods face a critical limitation. They often process only a single, isolated image tile, failing to leverage crucial spatial context and semantic priors from multi-modal data sources. This shortage severely impacts map accuracy and continuity, especially at complex intersections and in occluded areas. To overcome these challenges, we propose MMP-Mapper, a novel framework that enhances HD road map construction with multi-modal priors. MMP-Mapper introduces two key modules: (1) the Contextual Image Fusion (CIF) module, which selects and fuses features from neighbor image tiles to provide spatial continuity; and (2) the Map-Guided Fusion (MGF) module, which uses a Transformer module to fuse the encoded semantic attributes from standard-definition (SD) road maps with geometric priors, guiding HD road map construction. We validate our framework on the Aerial Argoverse 2 and OpenSatMap datasets. Our results demonstrate that MMP-Mapper outperforms state-of-the-art baselines in both accuracy and generalization for aerial-imagery-based HD road map construction.&lt;/p&gt;</content:encoded></item><item><title>CMVF: Cross-Modal Unregistered Video Fusion via Spatio-Temporal Consistency</title><link>https://doi.org/10.1016/j.inffus.2026.104212</link><guid>10.1016/j.inffus.2026.104212</guid><pubDate>Sat, 07 Feb 2026 23:36:48 +0000</pubDate><dc:creator>Jianfeng Ding</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Zhongyuan Wang</dc:creator><dc:creator>Jinsheng Xiao</dc:creator><dc:creator>Xin Tian</dc:creator><dc:creator>Zhen Han</dc:creator><dc:creator>Jiayi Ma</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104212</prism:doi><description>Current multi-modal image fusion methods focus solely on single-frame fusion and are unable to handle spatio-temporal information in videos, which often results in flickering artifacts and abrupt brightness transitions between consecutive frames. This fundamentally hinders the practical deployment of image fusion technology in downstream applications that rely on video streams as the primary data modality. To fill the technological gap in multi-modal video fusion, we propose a cross-modal unregistered video fusion framework based on spatio-temporal consistency. First, by designing a video fusion network and spatial aggregation module, this study robustly integrates spatial information across modalities, preserving visible details and infrared salient features, thereby preventing information loss. Second, we introduce an optical flow network and a temporal consistency loss function. By analyzing the dynamic relationships between frames in video sequences, these methods effectively address flickering issues caused by inter-frame brightness variations. Third, we develop a deformation estimation network and a cross-modal consistency loss function. This strategy effectively aligns unregistered cross-modal videos by analyzing the consistency of spatio-temporal information across modalities. Finally, this paper approaches the issue from spatial, temporal, and modal dimensions, employing a cyclic consistency strategy to ensure the alternating updates and mutual enhancement of each module, thereby robustly generating high-quality, stable, and aligned video fusion results. Through extensive qualitative and quantitative experiments, we validate the state-of-the-art performance of the proposed method.
Published: 2026-02-07T23:36:48+00:00
Venue: Information Fusion
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianfeng Ding; Hao Zhang; Zhongyuan Wang; Jinsheng Xiao; Xin Tian; Zhen Han; Jiayi Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104212"&gt;10.1016/j.inffus.2026.104212&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Current multi-modal image fusion methods focus solely on single-frame fusion and are unable to handle spatio-temporal information in videos, which often results in flickering artifacts and abrupt brightness transitions between consecutive frames. This fundamentally hinders the practical deployment of image fusion technology in downstream applications that rely on video streams as the primary data modality. To fill the technological gap in multi-modal video fusion, we propose a cross-modal unregistered video fusion framework based on spatio-temporal consistency. First, by designing a video fusion network and spatial aggregation module, this study robustly integrates spatial information across modalities, preserving visible details and infrared salient features, thereby preventing information loss. Second, we introduce an optical flow network and a temporal consistency loss function. By analyzing the dynamic relationships between frames in video sequences, these methods effectively address flickering issues caused by inter-frame brightness variations. Third, we develop a deformation estimation network and a cross-modal consistency loss function. This strategy effectively aligns unregistered cross-modal videos by analyzing the consistency of spatio-temporal information across modalities. Finally, this paper approaches the issue from spatial, temporal, and modal dimensions, employing a cyclic consistency strategy to ensure the alternating updates and mutual enhancement of each module, thereby robustly generating high-quality, stable, and aligned video fusion results. Through extensive qualitative and quantitative experiments, we validate the state-of-the-art performance of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Self-Supervised Learning with a Multi-Task Latent Space Objective</title><link>https://arxiv.org/abs/2602.05845v1</link><guid>http://arxiv.org/abs/2602.05845v1</guid><pubDate>Thu, 05 Feb 2026 16:33:30 +0000</pubDate><dc:creator>Pierre-François De Plaen</dc:creator><dc:creator>Abhishek Jha</dc:creator><dc:creator>Luc Van Gool</dc:creator><dc:creator>Tinne Tuytelaars</dc:creator><dc:creator>Marc Proesmans</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.
Published: 2026-02-05T16:33:30+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pierre-François De Plaen; Abhishek Jha; Luc Van Gool; Tinne Tuytelaars; Marc Proesmans&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.&lt;/p&gt;</content:encoded></item><item><title>LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection</title><link>https://arxiv.org/abs/2602.06474v1</link><guid>http://arxiv.org/abs/2602.06474v1</guid><pubDate>Fri, 06 Feb 2026 08:03:04 +0000</pubDate><dc:creator>Xu Zhang</dc:creator><dc:creator>Zhe Chen</dc:creator><dc:creator>Jing Zhang</dc:creator><dc:creator>Dacheng Tao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.
Published: 2026-02-06T08:03:04+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xu Zhang; Zhe Chen; Jing Zhang; Dacheng Tao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.&lt;/p&gt;</content:encoded></item><item><title>Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification</title><link>https://arxiv.org/abs/2602.05218v1</link><guid>http://arxiv.org/abs/2602.05218v1</guid><pubDate>Thu, 05 Feb 2026 02:17:38 +0000</pubDate><dc:creator>Jiahao Nie</dc:creator><dc:creator>Yun Xing</dc:creator><dc:creator>Wenbin An</dc:creator><dc:creator>Qingsong Zhao</dc:creator><dc:creator>Jiawei Shao</dc:creator><dc:creator>Yap-Peng Tan</dc:creator><dc:creator>Alex C. Kot</dc:creator><dc:creator>Shijian Lu</dc:creator><dc:creator>Xuelong Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.
Published: 2026-02-05T02:17:38+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Nie; Yun Xing; Wenbin An; Qingsong Zhao; Jiawei Shao; Yap-Peng Tan; Alex C. Kot; Shijian Lu; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.&lt;/p&gt;</content:encoded></item><item><title>Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains</title><link>https://arxiv.org/abs/2602.05527v2</link><guid>http://arxiv.org/abs/2602.05527v2</guid><pubDate>Thu, 05 Feb 2026 10:39:00 +0000</pubDate><dc:creator>Ben Isselmann</dc:creator><dc:creator>Dilara Göksu</dc:creator><dc:creator>Andreas Weinmann</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 $\pm$ 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 $\pm$ 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.
Published: 2026-02-05T10:39:00+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ben Isselmann; Dilara Göksu; Andreas Weinmann&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 $\pm$ 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 $\pm$ 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.&lt;/p&gt;</content:encoded></item><item><title>Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation</title><link>https://arxiv.org/abs/2602.05217v1</link><guid>http://arxiv.org/abs/2602.05217v1</guid><pubDate>Thu, 05 Feb 2026 02:16:44 +0000</pubDate><dc:creator>Jiahao Nie</dc:creator><dc:creator>Guanqiao Fu</dc:creator><dc:creator>Wenbin An</dc:creator><dc:creator>Yap-Peng Tan</dc:creator><dc:creator>Alex C. Kot</dc:creator><dc:creator>Shijian Lu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model's initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).
Published: 2026-02-05T02:16:44+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Nie; Guanqiao Fu; Wenbin An; Yap-Peng Tan; Alex C. Kot; Shijian Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model&amp;#x27;s initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).&lt;/p&gt;</content:encoded></item><item><title>Few-Shot Incremental Food Recognition via Cross-Domain Guided Pseudo-Targets</title><link>https://doi.org/10.1016/j.patcog.2026.113280</link><guid>10.1016/j.patcog.2026.113280</guid><pubDate>Sat, 07 Feb 2026 23:29:53 +0000</pubDate><dc:creator>Minkang Chai</dc:creator><dc:creator>Lu Wei</dc:creator><dc:creator>Zheng Qian</dc:creator><dc:creator>Ran Zhang</dc:creator><dc:creator>Ye Zhu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113280</prism:doi><description>The explosive growth of global food culture has expanded the application scope of visual recognition; however, it has introduced complex challenges arising from high intra-class variability and inter-class similarity. However, existing systems struggle to address fine-grained confusion and the trade-off between retaining old knowledge and adapting to new information. Traditional methods are constrained by a heavy reliance on large-scale datasets, whereas emerging zero-shot techniques are prone to semantic hallucination when encountering unseen dishes, thereby posing a severe challenge to precise recognition. To address these challenges, we propose the Cross-domain Guided Food Pseudo-Target Estimation (CFPE) framework, establishing a novel paradigm that is vision-led and semantically enhanced. First, to tackle the scarcity of incremental data, we utilize cross-domain adversarial training and an adaptive mask generator to synthesize high-quality pseudo-targets, thus establishing stable geometric anchors within the feature space. Second, by integrating Bessel Estimation Loss of Hypersphere (BELH) and Perturbation Margin Enhanced Prototype Regularization (PMEPR), we geometrically reconstruct the hyperspherical manifold distribution of features, effectively correcting estimation biases induced by few-shot samples. Crucially, we introduce a Food Factor-based Visual Semantic Consistency (FVSC) constraint, which explicitly decouples fine-grained visual confusion by injecting structured semantics. This is complemented by a depth-aware feature decoupling strategy to dynamically balance the plasticity and stability of the model. Experimental results demonstrate that CFPE achieves state-of-the-art performance across multiple benchmark datasets. It not only significantly improves incremental learning accuracy but also exhibits exceptional robustness in recognizing high-entropy food images.
Published: 2026-02-07T23:29:53+00:00
Venue: Pattern Recognition
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minkang Chai; Lu Wei; Zheng Qian; Ran Zhang; Ye Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113280"&gt;10.1016/j.patcog.2026.113280&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;The explosive growth of global food culture has expanded the application scope of visual recognition; however, it has introduced complex challenges arising from high intra-class variability and inter-class similarity. However, existing systems struggle to address fine-grained confusion and the trade-off between retaining old knowledge and adapting to new information. Traditional methods are constrained by a heavy reliance on large-scale datasets, whereas emerging zero-shot techniques are prone to semantic hallucination when encountering unseen dishes, thereby posing a severe challenge to precise recognition. To address these challenges, we propose the Cross-domain Guided Food Pseudo-Target Estimation (CFPE) framework, establishing a novel paradigm that is vision-led and semantically enhanced. First, to tackle the scarcity of incremental data, we utilize cross-domain adversarial training and an adaptive mask generator to synthesize high-quality pseudo-targets, thus establishing stable geometric anchors within the feature space. Second, by integrating Bessel Estimation Loss of Hypersphere (BELH) and Perturbation Margin Enhanced Prototype Regularization (PMEPR), we geometrically reconstruct the hyperspherical manifold distribution of features, effectively correcting estimation biases induced by few-shot samples. Crucially, we introduce a Food Factor-based Visual Semantic Consistency (FVSC) constraint, which explicitly decouples fine-grained visual confusion by injecting structured semantics. This is complemented by a depth-aware feature decoupling strategy to dynamically balance the plasticity and stability of the model. Experimental results demonstrate that CFPE achieves state-of-the-art performance across multiple benchmark datasets. It not only significantly improves incremental learning accuracy but also exhibits exceptional robustness in recognizing high-entropy food images.&lt;/p&gt;</content:encoded></item><item><title>Towards Generalizable Reasoning: Group Causal Counterfactual Policy Optimization for LLM Reasoning</title><link>https://arxiv.org/abs/2602.06475v1</link><guid>http://arxiv.org/abs/2602.06475v1</guid><pubDate>Fri, 06 Feb 2026 08:03:11 +0000</pubDate><dc:creator>Jingyao Wang</dc:creator><dc:creator>Peizheng Guo</dc:creator><dc:creator>Wenwen Qiang</dc:creator><dc:creator>Jiahuan Zhou</dc:creator><dc:creator>Huijie Guo</dc:creator><dc:creator>Changwen Zheng</dc:creator><dc:creator>Hui Xiong</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) excel at complex tasks with advances in reasoning capabilities. However, existing reward mechanisms remain tightly coupled to final correctness and pay little attention to the underlying reasoning process: trajectories with sound reasoning but wrong answers receive low credit, while lucky guesses with flawed logic may be highly rewarded, affecting reasoning generalization. From a causal perspective, we interpret multi-candidate reasoning for a fixed question as a family of counterfactual experiments with theoretical supports. Building on this, we propose Group Causal Counterfactual Policy Optimization to explicitly train LLMs to learn generalizable reasoning patterns. It proposes an episodic causal counterfactual reward that jointly captures (i) robustness, encouraging the answer distribution induced by a reasoning step to remain stable under counterfactual perturbations; and (ii) effectiveness, enforcing sufficient variability so that the learned reasoning strategy can transfer across questions. We then construct token-level advantages from this reward and optimize the policy, encouraging LLMs to favor reasoning patterns that are process-valid and counterfactually robust. Extensive experiments on diverse benchmarks demonstrate its advantages.
Published: 2026-02-06T08:03:11+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingyao Wang; Peizheng Guo; Wenwen Qiang; Jiahuan Zhou; Huijie Guo; Changwen Zheng; Hui Xiong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) excel at complex tasks with advances in reasoning capabilities. However, existing reward mechanisms remain tightly coupled to final correctness and pay little attention to the underlying reasoning process: trajectories with sound reasoning but wrong answers receive low credit, while lucky guesses with flawed logic may be highly rewarded, affecting reasoning generalization. From a causal perspective, we interpret multi-candidate reasoning for a fixed question as a family of counterfactual experiments with theoretical supports. Building on this, we propose Group Causal Counterfactual Policy Optimization to explicitly train LLMs to learn generalizable reasoning patterns. It proposes an episodic causal counterfactual reward that jointly captures (i) robustness, encouraging the answer distribution induced by a reasoning step to remain stable under counterfactual perturbations; and (ii) effectiveness, enforcing sufficient variability so that the learned reasoning strategy can transfer across questions. We then construct token-level advantages from this reward and optimize the policy, encouraging LLMs to favor reasoning patterns that are process-valid and counterfactually robust. Extensive experiments on diverse benchmarks demonstrate its advantages.&lt;/p&gt;</content:encoded></item><item><title>ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation</title><link>https://arxiv.org/abs/2602.05472v1</link><guid>http://arxiv.org/abs/2602.05472v1</guid><pubDate>Thu, 05 Feb 2026 09:20:23 +0000</pubDate><dc:creator>Yiwen Duan</dc:creator><dc:creator>Jing Ye</dc:creator><dc:creator>Xinpei Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.
Published: 2026-02-05T09:20:23+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiwen Duan; Jing Ye; Xinpei Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.&lt;/p&gt;</content:encoded></item><item><title>CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion</title><link>https://arxiv.org/abs/2602.05598v1</link><guid>http://arxiv.org/abs/2602.05598v1</guid><pubDate>Thu, 05 Feb 2026 12:33:09 +0000</pubDate><dc:creator>Aon Safdar</dc:creator><dc:creator>Mohamed Saadeldin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.
Published: 2026-02-05T12:33:09+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aon Safdar; Mohamed Saadeldin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce &amp;#x27;CAViT&amp;#x27;, a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.&lt;/p&gt;</content:encoded></item><item><title>MoSE: Mixture of Slimmable Experts for Efficient and Adaptive Language Models</title><link>https://arxiv.org/abs/2602.06154v1</link><guid>http://arxiv.org/abs/2602.06154v1</guid><pubDate>Thu, 05 Feb 2026 19:48:41 +0000</pubDate><dc:creator>Nurbek Tastan</dc:creator><dc:creator>Stefanos Laskaridis</dc:creator><dc:creator>Karthik Nandakumar</dc:creator><dc:creator>Samuel Horvath</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Mixture-of-Experts (MoE) models scale large language models efficiently by sparsely activating experts, but once an expert is selected, it is executed fully. Hence, the trade-off between accuracy and computation in an MoE model typically exhibits large discontinuities. We propose Mixture of Slimmable Experts (MoSE), an MoE architecture in which each expert has a nested, slimmable structure that can be executed at variable widths. This enables conditional computation not only over which experts are activated, but also over how much of each expert is utilized. Consequently, a single pretrained MoSE model can support a more continuous spectrum of accuracy-compute trade-offs at inference time. We present a simple and stable training recipe for slimmable experts under sparse routing, combining multi-width training with standard MoE objectives. During inference, we explore strategies for runtime width determination, including a lightweight test-time training mechanism that learns how to map router confidence/probabilities to expert widths under a fixed budget. Experiments on GPT models trained on OpenWebText demonstrate that MoSE matches or improves upon standard MoE at full width and consistently shifts the Pareto frontier for accuracy vs. cost, achieving comparable performance with significantly fewer FLOPs.
Published: 2026-02-05T19:48:41+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nurbek Tastan; Stefanos Laskaridis; Karthik Nandakumar; Samuel Horvath&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Mixture-of-Experts (MoE) models scale large language models efficiently by sparsely activating experts, but once an expert is selected, it is executed fully. Hence, the trade-off between accuracy and computation in an MoE model typically exhibits large discontinuities. We propose Mixture of Slimmable Experts (MoSE), an MoE architecture in which each expert has a nested, slimmable structure that can be executed at variable widths. This enables conditional computation not only over which experts are activated, but also over how much of each expert is utilized. Consequently, a single pretrained MoSE model can support a more continuous spectrum of accuracy-compute trade-offs at inference time. We present a simple and stable training recipe for slimmable experts under sparse routing, combining multi-width training with standard MoE objectives. During inference, we explore strategies for runtime width determination, including a lightweight test-time training mechanism that learns how to map router confidence/probabilities to expert widths under a fixed budget. Experiments on GPT models trained on OpenWebText demonstrate that MoSE matches or improves upon standard MoE at full width and consistently shifts the Pareto frontier for accuracy vs. cost, achieving comparable performance with significantly fewer FLOPs.&lt;/p&gt;</content:encoded></item><item><title>Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning</title><link>https://arxiv.org/abs/2602.05271v1</link><guid>http://arxiv.org/abs/2602.05271v1</guid><pubDate>Thu, 05 Feb 2026 03:50:53 +0000</pubDate><dc:creator>Shengqin Jiang</dc:creator><dc:creator>Xiaoran Feng</dc:creator><dc:creator>Yuankai Qi</dc:creator><dc:creator>Haokui Zhang</dc:creator><dc:creator>Renlong Hang</dc:creator><dc:creator>Qingshan Liu</dc:creator><dc:creator>Lina Yao</dc:creator><dc:creator>Quan Z. Sheng</dc:creator><dc:creator>Ming-Hsuan Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model's capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.
Published: 2026-02-05T03:50:53+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengqin Jiang; Xiaoran Feng; Yuankai Qi; Haokui Zhang; Renlong Hang; Qingshan Liu; Lina Yao; Quan Z. Sheng; Ming-Hsuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model&amp;#x27;s capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.&lt;/p&gt;</content:encoded></item><item><title>Attention Retention for Continual Learning with Vision Transformers</title><link>https://arxiv.org/abs/2602.05454v1</link><guid>http://arxiv.org/abs/2602.05454v1</guid><pubDate>Thu, 05 Feb 2026 08:55:58 +0000</pubDate><dc:creator>Yue Lu</dc:creator><dc:creator>Xiangyu Zhou</dc:creator><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Yinghui Xing</dc:creator><dc:creator>Guoqiang Liang</dc:creator><dc:creator>Wencong Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.
Published: 2026-02-05T08:55:58+00:00
Venue: arXiv
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Lu; Xiangyu Zhou; Shizhou Zhang; Yinghui Xing; Guoqiang Liang; Wencong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.&lt;/p&gt;</content:encoded></item><item><title>SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing</title><link>https://arxiv.org/abs/2602.05480v1</link><guid>http://arxiv.org/abs/2602.05480v1</guid><pubDate>Thu, 05 Feb 2026 09:39:49 +0000</pubDate><dc:creator>Peihao Wu</dc:creator><dc:creator>Yongxiang Yao</dc:creator><dc:creator>Yi Wan</dc:creator><dc:creator>Wenfei Zhang</dc:creator><dc:creator>Ruipeng Zhao</dc:creator><dc:creator>Jiayuan Li</dc:creator><dc:creator>Yongjun Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.
Published: 2026-02-05T09:39:49+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peihao Wu; Yongxiang Yao; Yi Wan; Wenfei Zhang; Ruipeng Zhao; Jiayuan Li; Yongjun Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.&lt;/p&gt;</content:encoded></item><item><title>Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs</title><link>https://arxiv.org/abs/2602.06914v1</link><guid>http://arxiv.org/abs/2602.06914v1</guid><pubDate>Fri, 06 Feb 2026 18:13:01 +0000</pubDate><dc:creator>Darryl Hannan</dc:creator><dc:creator>John Cooper</dc:creator><dc:creator>Dylan White</dc:creator><dc:creator>Yijing Watkins</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.
Published: 2026-02-06T18:13:01+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Darryl Hannan; John Cooper; Dylan White; Yijing Watkins&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.&lt;/p&gt;</content:encoded></item><item><title>Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs</title><link>https://arxiv.org/abs/2602.05191v1</link><guid>http://arxiv.org/abs/2602.05191v1</guid><pubDate>Thu, 05 Feb 2026 01:37:10 +0000</pubDate><dc:creator>Wentao Ni</dc:creator><dc:creator>Kangqi Zhang</dc:creator><dc:creator>Zhongming Yu</dc:creator><dc:creator>Oren Nelson</dc:creator><dc:creator>Mingu Lee</dc:creator><dc:creator>Hong Cai</dc:creator><dc:creator>Fatih Porikli</dc:creator><dc:creator>Jongryool Kim</dc:creator><dc:creator>Zhijian Liu</dc:creator><dc:creator>Jishen Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.
Published: 2026-02-05T01:37:10+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wentao Ni; Kangqi Zhang; Zhongming Yu; Oren Nelson; Mingu Lee; Hong Cai; Fatih Porikli; Jongryool Kim; Zhijian Liu; Jishen Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.&lt;/p&gt;</content:encoded></item><item><title>Visual Implicit Geometry Transformer for Autonomous Driving</title><link>https://arxiv.org/abs/2602.05573v1</link><guid>http://arxiv.org/abs/2602.05573v1</guid><pubDate>Thu, 05 Feb 2026 11:54:38 +0000</pubDate><dc:creator>Arsenii Shirokov</dc:creator><dc:creator>Mikhail Kuznetsov</dc:creator><dc:creator>Danila Stepochkin</dc:creator><dc:creator>Egor Evdokimov</dc:creator><dc:creator>Daniil Glazkov</dc:creator><dc:creator>Nikolay Patakin</dc:creator><dc:creator>Anton Konushin</dc:creator><dc:creator>Dmitry Senushkin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.
Published: 2026-02-05T11:54:38+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Arsenii Shirokov; Mikhail Kuznetsov; Danila Stepochkin; Egor Evdokimov; Daniil Glazkov; Nikolay Patakin; Anton Konushin; Dmitry Senushkin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.&lt;/p&gt;</content:encoded></item><item><title>LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation</title><link>https://arxiv.org/abs/2602.05578v1</link><guid>http://arxiv.org/abs/2602.05578v1</guid><pubDate>Thu, 05 Feb 2026 12:03:11 +0000</pubDate><dc:creator>Junyang Chen</dc:creator><dc:creator>Xiangbo Lv</dc:creator><dc:creator>Zhiqiang Kou</dc:creator><dc:creator>Xingdong Sheng</dc:creator><dc:creator>Ning Xu</dc:creator><dc:creator>Yiguo Qiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.
Published: 2026-02-05T12:03:11+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junyang Chen; Xiangbo Lv; Zhiqiang Kou; Xingdong Sheng; Ning Xu; Yiguo Qiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.&lt;/p&gt;</content:encoded></item><item><title>Fast-SAM3D: 3Dfy Anything in Images but Faster</title><link>https://arxiv.org/abs/2602.05293v1</link><guid>http://arxiv.org/abs/2602.05293v1</guid><pubDate>Thu, 05 Feb 2026 04:27:59 +0000</pubDate><dc:creator>Weilun Feng</dc:creator><dc:creator>Mingqiang Wu</dc:creator><dc:creator>Zhiliang Chen</dc:creator><dc:creator>Chuanguang Yang</dc:creator><dc:creator>Haotong Qin</dc:creator><dc:creator>Yuqi Li</dc:creator><dc:creator>Xiaokun Liu</dc:creator><dc:creator>Guoxin Fan</dc:creator><dc:creator>Zhulin An</dc:creator><dc:creator>Libo Huang</dc:creator><dc:creator>Yulun Zhang</dc:creator><dc:creator>Michele Magno</dc:creator><dc:creator>Yongjun Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.
Published: 2026-02-05T04:27:59+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weilun Feng; Mingqiang Wu; Zhiliang Chen; Chuanguang Yang; Haotong Qin; Yuqi Li; Xiaokun Liu; Guoxin Fan; Zhulin An; Libo Huang; Yulun Zhang; Michele Magno; Yongjun Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline&amp;#x27;s inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.&lt;/p&gt;</content:encoded></item><item><title>Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs</title><link>https://arxiv.org/abs/2602.05275v1</link><guid>http://arxiv.org/abs/2602.05275v1</guid><pubDate>Thu, 05 Feb 2026 04:01:01 +0000</pubDate><dc:creator>Qi Li</dc:creator><dc:creator>Yanzhe Zhao</dc:creator><dc:creator>Yongxin Zhou</dc:creator><dc:creator>Yameng Wang</dc:creator><dc:creator>Yandong Yang</dc:creator><dc:creator>Yuanjia Zhou</dc:creator><dc:creator>Jue Wang</dc:creator><dc:creator>Zuojian Wang</dc:creator><dc:creator>Jinxiang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.
Published: 2026-02-05T04:01:01+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qi Li; Yanzhe Zhao; Yongxin Zhou; Yameng Wang; Yandong Yang; Yuanjia Zhou; Jue Wang; Zuojian Wang; Jinxiang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.&lt;/p&gt;</content:encoded></item><item><title>BrokenBind: Universal Modality Exploration beyond Dataset Boundaries</title><link>https://arxiv.org/abs/2602.06451v1</link><guid>http://arxiv.org/abs/2602.06451v1</guid><pubDate>Fri, 06 Feb 2026 07:26:49 +0000</pubDate><dc:creator>Zhuo Huang</dc:creator><dc:creator>Runnan Chen</dc:creator><dc:creator>Bo Han</dc:creator><dc:creator>Gang Niu</dc:creator><dc:creator>Masashi Sugiyama</dc:creator><dc:creator>Tongliang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-modal learning combines various modalities to provide a comprehensive understanding of real-world problems. A common strategy is to directly bind different modalities together in a specific joint embedding space. However, the capability of existing methods is restricted within the modalities presented in the given dataset, thus they are biased when generalizing to unpresented modalities in downstream tasks. As a result, due to such inflexibility, the viability of previous methods is seriously hindered by the cost of acquiring multi-modal datasets. In this paper, we introduce BrokenBind, which focuses on binding modalities that are presented from different datasets. To achieve this, BrokenBind simultaneously leverages multiple datasets containing the modalities of interest and one shared modality. Though the two datasets do not correspond to each other due to distribution mismatch, we can capture their relationship to generate pseudo embeddings to fill in the missing modalities of interest, enabling flexible and generalized multi-modal learning. Under our framework, any two modalities can be bound together, free from the dataset limitation, to achieve universal modality exploration. Further, to reveal the capability of our method, we study intensified scenarios where more than two datasets are needed for modality binding and show the effectiveness of BrokenBind in low-data regimes. Through extensive evaluation, we carefully justify the superiority of BrokenBind compared to well-known multi-modal baseline methods.
Published: 2026-02-06T07:26:49+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhuo Huang; Runnan Chen; Bo Han; Gang Niu; Masashi Sugiyama; Tongliang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal learning combines various modalities to provide a comprehensive understanding of real-world problems. A common strategy is to directly bind different modalities together in a specific joint embedding space. However, the capability of existing methods is restricted within the modalities presented in the given dataset, thus they are biased when generalizing to unpresented modalities in downstream tasks. As a result, due to such inflexibility, the viability of previous methods is seriously hindered by the cost of acquiring multi-modal datasets. In this paper, we introduce BrokenBind, which focuses on binding modalities that are presented from different datasets. To achieve this, BrokenBind simultaneously leverages multiple datasets containing the modalities of interest and one shared modality. Though the two datasets do not correspond to each other due to distribution mismatch, we can capture their relationship to generate pseudo embeddings to fill in the missing modalities of interest, enabling flexible and generalized multi-modal learning. Under our framework, any two modalities can be bound together, free from the dataset limitation, to achieve universal modality exploration. Further, to reveal the capability of our method, we study intensified scenarios where more than two datasets are needed for modality binding and show the effectiveness of BrokenBind in low-data regimes. Through extensive evaluation, we carefully justify the superiority of BrokenBind compared to well-known multi-modal baseline methods.&lt;/p&gt;</content:encoded></item><item><title>Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings</title><link>https://arxiv.org/abs/2602.06218v1</link><guid>http://arxiv.org/abs/2602.06218v1</guid><pubDate>Thu, 05 Feb 2026 21:56:26 +0000</pubDate><dc:creator>Grégoire Dhimoïla</dc:creator><dc:creator>Thomas Fel</dc:creator><dc:creator>Victor Boutin</dc:creator><dc:creator>Agustin Picard</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.
Published: 2026-02-05T21:56:26+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Grégoire Dhimoïla; Thomas Fel; Victor Boutin; Agustin Picard&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.&lt;/p&gt;</content:encoded></item><item><title>Dynamic transformer architecture for continual learning of multimodal tasks</title><link>https://doi.org/10.1016/j.neucom.2026.132977</link><guid>10.1016/j.neucom.2026.132977</guid><pubDate>Sat, 07 Feb 2026 16:26:36 +0000</pubDate><dc:creator>Yuliang Cai</dc:creator><dc:creator>Mohammad Rostami</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132977</prism:doi><description>Transformer neural networks are increasingly replacing prior architectures in a wide range of applications in different data modalities. The increasing size and computational demands of fine-tuning large pre-trained transformer neural networks pose significant challenges for the widespread adoption of these models for applications that demand on-edge computing. To tackle this challenge, continual learning (CL) emerges as a solution by facilitating the transfer of knowledge across tasks that arrive sequentially for an autonomously learning agent. However, current CL methods mainly focus on learning tasks that are exclusively vision-based or language-based. We propose a transformer-based CL framework focusing on learning tasks that involve both vision and language, known as Vision-and-Language (VaL) tasks. In our framework, we benefit from the novel task-attention block and the introduced extra parameters to a base transformer to specialize the network for each task. As a result, we enable dynamic model expansion to learn several tasks in a sequence. We also use knowledge distillation and experience replay to benefit from relevant past experiences to learn the current task more efficiently. Our proposed method, Task Attentive Multimodal Continual Learning (TAM-CL), allows for the exchange of information between tasks while mitigating the problem of catastrophic forgetting. Notably, our approach is scalable, incurring minimal memory overhead. TAM-CL achieves 4.62% accuracy higher than the state-of-the-art (SOTA) accuracy on challenging multimodal tasks. 1
Published: 2026-02-07T16:26:36+00:00
Venue: Neurocomputing
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuliang Cai; Mohammad Rostami&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132977"&gt;10.1016/j.neucom.2026.132977&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer neural networks are increasingly replacing prior architectures in a wide range of applications in different data modalities. The increasing size and computational demands of fine-tuning large pre-trained transformer neural networks pose significant challenges for the widespread adoption of these models for applications that demand on-edge computing. To tackle this challenge, continual learning (CL) emerges as a solution by facilitating the transfer of knowledge across tasks that arrive sequentially for an autonomously learning agent. However, current CL methods mainly focus on learning tasks that are exclusively vision-based or language-based. We propose a transformer-based CL framework focusing on learning tasks that involve both vision and language, known as Vision-and-Language (VaL) tasks. In our framework, we benefit from the novel task-attention block and the introduced extra parameters to a base transformer to specialize the network for each task. As a result, we enable dynamic model expansion to learn several tasks in a sequence. We also use knowledge distillation and experience replay to benefit from relevant past experiences to learn the current task more efficiently. Our proposed method, Task Attentive Multimodal Continual Learning (TAM-CL), allows for the exchange of information between tasks while mitigating the problem of catastrophic forgetting. Notably, our approach is scalable, incurring minimal memory overhead. TAM-CL achieves 4.62% accuracy higher than the state-of-the-art (SOTA) accuracy on challenging multimodal tasks. 1&lt;/p&gt;</content:encoded></item><item><title>OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention</title><link>https://arxiv.org/abs/2602.05847v1</link><guid>http://arxiv.org/abs/2602.05847v1</guid><pubDate>Thu, 05 Feb 2026 16:35:19 +0000</pubDate><dc:creator>Zhangquan Chen</dc:creator><dc:creator>Jiale Tao</dc:creator><dc:creator>Ruihuang Li</dc:creator><dc:creator>Yihao Hu</dc:creator><dc:creator>Ruitao Chen</dc:creator><dc:creator>Zhantao Yang</dc:creator><dc:creator>Xinlei Yu</dc:creator><dc:creator>Haodong Jing</dc:creator><dc:creator>Manyuan Zhang</dc:creator><dc:creator>Shuai Shao</dc:creator><dc:creator>Biao Wang</dc:creator><dc:creator>Qinglin Lu</dc:creator><dc:creator>Ruqi Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to "think with omnimodal cues" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.
Published: 2026-02-05T16:35:19+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhangquan Chen; Jiale Tao; Ruihuang Li; Yihao Hu; Ruitao Chen; Zhantao Yang; Xinlei Yu; Haodong Jing; Manyuan Zhang; Shuai Shao; Biao Wang; Qinglin Lu; Ruqi Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to &amp;quot;think with omnimodal cues&amp;quot; by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.&lt;/p&gt;</content:encoded></item><item><title>Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition</title><link>https://doi.org/10.1016/j.patcog.2026.113237</link><guid>10.1016/j.patcog.2026.113237</guid><pubDate>Sat, 07 Feb 2026 23:30:06 +0000</pubDate><dc:creator>Congqi Cao</dc:creator><dc:creator>Peiheng Han</dc:creator><dc:creator>Yueran Zhang</dc:creator><dc:creator>Yating Yu</dc:creator><dc:creator>Qinyi Lv</dc:creator><dc:creator>Lingtong Min</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113237</prism:doi><description>Large-scale pre-trained models have achieved remarkable success in language and image tasks, leading an increasing number of studies to explore the application of pre-trained vision models, such as CLIP, in the domain of few-shot action recognition (FSAR). However, current methods generally suffer from several problems: 1) Direct fine-tuning often undermines the generalization capability of the pre-trained model; 2) The exploration of task-specific information is insufficient in the visual tasks; 3) The semantic order information is typically overlooked during text modeling; 4) Existing cross-modal alignment techniques ignore the temporal coupling of multimodal information. To address these, we propose Task-Adapter++, a parameter-efficient dual adaptation method for both image and text encoders. Specifically, to make full use of the variations across different few-shot learning tasks, we design a task-specific adaptation for the image encoder so that the most discriminative information can be well noticed during feature extraction. Furthermore, we leverage large language models (LLMs) to generate detailed sequential sub-action descriptions for each action class, and introduce semantic order adapters into the text encoder to effectively model the sequential relationships between these sub-actions. Finally, we develop an innovative fine-grained cross-modal alignment strategy that actively maps visual features to reside in the same temporal stage as semantic descriptions. Extensive experiments fully demonstrate the effectiveness and superiority of the proposed method, which achieves state-of-the-art performance on five benchmarks consistently. The code is open-sourced at https://github.com/Jaulin-Bage/Task-Adapter-pp .
Published: 2026-02-07T23:30:06+00:00
Venue: Pattern Recognition
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Congqi Cao; Peiheng Han; Yueran Zhang; Yating Yu; Qinyi Lv; Lingtong Min; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113237"&gt;10.1016/j.patcog.2026.113237&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale pre-trained models have achieved remarkable success in language and image tasks, leading an increasing number of studies to explore the application of pre-trained vision models, such as CLIP, in the domain of few-shot action recognition (FSAR). However, current methods generally suffer from several problems: 1) Direct fine-tuning often undermines the generalization capability of the pre-trained model; 2) The exploration of task-specific information is insufficient in the visual tasks; 3) The semantic order information is typically overlooked during text modeling; 4) Existing cross-modal alignment techniques ignore the temporal coupling of multimodal information. To address these, we propose Task-Adapter++, a parameter-efficient dual adaptation method for both image and text encoders. Specifically, to make full use of the variations across different few-shot learning tasks, we design a task-specific adaptation for the image encoder so that the most discriminative information can be well noticed during feature extraction. Furthermore, we leverage large language models (LLMs) to generate detailed sequential sub-action descriptions for each action class, and introduce semantic order adapters into the text encoder to effectively model the sequential relationships between these sub-actions. Finally, we develop an innovative fine-grained cross-modal alignment strategy that actively maps visual features to reside in the same temporal stage as semantic descriptions. Extensive experiments fully demonstrate the effectiveness and superiority of the proposed method, which achieves state-of-the-art performance on five benchmarks consistently. The code is open-sourced at https://github.com/Jaulin-Bage/Task-Adapter-pp .&lt;/p&gt;</content:encoded></item><item><title>Self-Improved Holistic Alignment for Preference Enhancement</title><link>https://doi.org/10.1016/j.patcog.2026.113238</link><guid>10.1016/j.patcog.2026.113238</guid><pubDate>Sat, 07 Feb 2026 15:51:25 +0000</pubDate><dc:creator>Kejia Chen</dc:creator><dc:creator>Jiawen Zhang</dc:creator><dc:creator>Jiazhen Yang</dc:creator><dc:creator>Mingli Song</dc:creator><dc:creator>Zunlei Feng</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113238</prism:doi><description>Large Visual-Language Models (LVLMs) increasingly rely on preference alignment to ensure reliability, which steers the model behavior via preference fine-tuning on preference data structured as “image - question - winner text - loser text” tuplets. However, existing approaches often [revise: suffer from → constrained by] limited diversity and high [add: acquisition] costs [revise: associated with → of] human-annotated preference data, hindering LVLMs from fully achieving their intended alignment capabilities. We present SHAPE , a self-supervised framework capable of transforming the already abundant supervised text-image pairs into holistic preference tuplets for more effective and cheaper LVLM alignment, eliminating the need for human preference annotations. Our approach facilitates LVLMs in progressively enhancing alignment capabilities through iterative self-improvement. The key design rationale is to devise preference tuplets where the winner text consistently improves in holisticness and outperforms the loser response in quality, thereby pushing the model to “strive to the utmost” of alignment performance through preference fine-tuning. For each given text-image pair, SHAPE introduces multiple visual augmentations and pairs them with a summarized text to serve as the winner response, while designating the original text as the loser response.
Published: 2026-02-07T15:51:25+00:00
Venue: Pattern Recognition
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kejia Chen; Jiawen Zhang; Jiazhen Yang; Mingli Song; Zunlei Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113238"&gt;10.1016/j.patcog.2026.113238&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Large Visual-Language Models (LVLMs) increasingly rely on preference alignment to ensure reliability, which steers the model behavior via preference fine-tuning on preference data structured as “image - question - winner text - loser text” tuplets. However, existing approaches often [revise: suffer from → constrained by] limited diversity and high [add: acquisition] costs [revise: associated with → of] human-annotated preference data, hindering LVLMs from fully achieving their intended alignment capabilities. We present SHAPE , a self-supervised framework capable of transforming the already abundant supervised text-image pairs into holistic preference tuplets for more effective and cheaper LVLM alignment, eliminating the need for human preference annotations. Our approach facilitates LVLMs in progressively enhancing alignment capabilities through iterative self-improvement. The key design rationale is to devise preference tuplets where the winner text consistently improves in holisticness and outperforms the loser response in quality, thereby pushing the model to “strive to the utmost” of alignment performance through preference fine-tuning. For each given text-image pair, SHAPE introduces multiple visual augmentations and pairs them with a summarized text to serve as the winner response, while designating the original text as the loser response.&lt;/p&gt;</content:encoded></item><item><title>Depth as Prior Knowledge for Object Detection</title><link>https://arxiv.org/abs/2602.05730v1</link><guid>http://arxiv.org/abs/2602.05730v1</guid><pubDate>Thu, 05 Feb 2026 14:52:39 +0000</pubDate><dc:creator>Moussa Kassem Sbeyti</dc:creator><dc:creator>Nadja Klein</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.
Published: 2026-02-05T14:52:39+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Moussa Kassem Sbeyti; Nadja Klein&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.&lt;/p&gt;</content:encoded></item><item><title>Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities</title><link>https://arxiv.org/abs/2602.05281v2</link><guid>http://arxiv.org/abs/2602.05281v2</guid><pubDate>Thu, 05 Feb 2026 04:06:55 +0000</pubDate><dc:creator>Pengyi Li</dc:creator><dc:creator>Elizaveta Goncharova</dc:creator><dc:creator>Andrey Kuznetsov</dc:creator><dc:creator>Ivan Oseledets</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.
Published: 2026-02-05T04:06:55+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengyi Li; Elizaveta Goncharova; Andrey Kuznetsov; Ivan Oseledets&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.&lt;/p&gt;</content:encoded></item></channel></rss>