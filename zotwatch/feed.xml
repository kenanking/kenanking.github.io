<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 16 Dec 2025 02:49:20 +0000</lastBuildDate><item><title>Vision-Language Models Empowered Nighttime Object Detection with Consistency Sampler and Hallucination Feature Generator</title><link>https://doi.org/10.1109/tip.2025.3641316</link><guid>10.1109/tip.2025.3641316</guid><pubDate>Mon, 15 Dec 2025 18:40:43 +0000</pubDate><dc:creator>Lihuo He</dc:creator><dc:creator>Junjie Ke</dc:creator><dc:creator>Zhenghao Wang</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Kai Zhou</dc:creator><dc:creator>Qi Wang</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3641316</prism:doi><description>Current object detectors often suffer performance degradation when applied to cross-domain scenarios, particularly under challenging visual conditions such as nighttime scenes. This is primarily due to the I3 problems: Inadequate sampling of instance-level features, Indistinguishable feature representation across domains and Inaccurate generation for identical category participation. To address these challenges, we propose a domain-adaptive detection framework that enables robust generalization across different visual domains without introducing any additional inference overhead. The framework comprises three key components. Specifically, the centerness–category consistency sampler alleviates inadequate sampling by selecting representative instance-level features, while the paired centerness consistency loss enforces alignment between classification and localization. Second, VLM-based orthogonality enhancement leverages frozen vision–language encoders with an orthogonal projection loss to improve cross-domain feature distinguishability. Third, hallucination feature generator synthesizes robust instance-level features for missing categories, ensuring balanced category participation across domains. Extensive experiments on multiple datasets covering various domain adaptation and generalization settings demonstrate that our method consistently outperforms state-of-the-art detectors, achieving up to 5.5 mAP improvement, with particularly strong gains in nighttime adaptation.
Published: 2025-12-15T18:40:43+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.838 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihuo He; Junjie Ke; Zhenghao Wang; Jie Li; Kai Zhou; Qi Wang; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3641316"&gt;10.1109/tip.2025.3641316&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.838 (must_read)&lt;/p&gt;
&lt;p&gt;Current object detectors often suffer performance degradation when applied to cross-domain scenarios, particularly under challenging visual conditions such as nighttime scenes. This is primarily due to the I3 problems: Inadequate sampling of instance-level features, Indistinguishable feature representation across domains and Inaccurate generation for identical category participation. To address these challenges, we propose a domain-adaptive detection framework that enables robust generalization across different visual domains without introducing any additional inference overhead. The framework comprises three key components. Specifically, the centerness–category consistency sampler alleviates inadequate sampling by selecting representative instance-level features, while the paired centerness consistency loss enforces alignment between classification and localization. Second, VLM-based orthogonality enhancement leverages frozen vision–language encoders with an orthogonal projection loss to improve cross-domain feature distinguishability. Third, hallucination feature generator synthesizes robust instance-level features for missing categories, ensuring balanced category participation across domains. Extensive experiments on multiple datasets covering various domain adaptation and generalization settings demonstrate that our method consistently outperforms state-of-the-art detectors, achieving up to 5.5 mAP improvement, with particularly strong gains in nighttime adaptation.&lt;/p&gt;</content:encoded></item><item><title>PLPFusion: Plane-Line-Pixel Fully Sparse Fusion for Robust Multi-Modal 3D Object Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3644414</link><guid>10.1109/tcsvt.2025.3644414</guid><pubDate>Mon, 15 Dec 2025 18:40:24 +0000</pubDate><dc:creator>Jingfu Hou</dc:creator><dc:creator>Hong Song</dc:creator><dc:creator>Jinfu Li</dc:creator><dc:creator>Yucong Lin</dc:creator><dc:creator>Tianyu Huang</dc:creator><dc:creator>Jugang He</dc:creator><dc:creator>Xiuwei He</dc:creator><dc:creator>Jian Yang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3644414</prism:doi><description>Fully sparse fusion makes an excellent balance between efficiency and accuracy in multi-modal 3D object detection. However, most existing methods focus on foreground objects while overlooking background context. This oversight compromises detection robustness, especially for occluded or small-sized objects, leading to suboptimal detection performance. To address this limitation, we propose a novel fully sparse fusion framework (PLPFusion), which introduces a hierarchical Plane-Line-Pixel representation to progressively model the object-context relationships. PLPFusion comprises three key modules: the Plane Enhancement Module (PEM), the Line Alignment Module (LAM) and the Pixel-Level Aggregation Module (PLAM). Firstly, PEM utilizes geometric cues from LiDAR feature planes to generate spatially-aware object queries. Secondly, LAM further refines these queries with geometric priors for semantic awareness. Lastly, PLAM aggregates pixel-level context to enhance discriminative completeness by leveraging the semantically-aware object queries. On the nuScenes benchmark, PLPFusion achieves 71.9% mAP and 74.0% NDS, outperforming the baseline method FUTR3D by +2.5% mAP and +1.9% NDS, respectively. On the KITTI benchmark, it achieves 72.68% BEV mAP and 67.39% 3D mAP. These results confirm its robustness and effectiveness in diverse multi-modal 3D scenarios. The code of PLPFusion is available on the https://github.com/Text357/PLPFusion.
Published: 2025-12-15T18:40:24+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingfu Hou; Hong Song; Jinfu Li; Yucong Lin; Tianyu Huang; Jugang He; Xiuwei He; Jian Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3644414"&gt;10.1109/tcsvt.2025.3644414&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Fully sparse fusion makes an excellent balance between efficiency and accuracy in multi-modal 3D object detection. However, most existing methods focus on foreground objects while overlooking background context. This oversight compromises detection robustness, especially for occluded or small-sized objects, leading to suboptimal detection performance. To address this limitation, we propose a novel fully sparse fusion framework (PLPFusion), which introduces a hierarchical Plane-Line-Pixel representation to progressively model the object-context relationships. PLPFusion comprises three key modules: the Plane Enhancement Module (PEM), the Line Alignment Module (LAM) and the Pixel-Level Aggregation Module (PLAM). Firstly, PEM utilizes geometric cues from LiDAR feature planes to generate spatially-aware object queries. Secondly, LAM further refines these queries with geometric priors for semantic awareness. Lastly, PLAM aggregates pixel-level context to enhance discriminative completeness by leveraging the semantically-aware object queries. On the nuScenes benchmark, PLPFusion achieves 71.9% mAP and 74.0% NDS, outperforming the baseline method FUTR3D by +2.5% mAP and +1.9% NDS, respectively. On the KITTI benchmark, it achieves 72.68% BEV mAP and 67.39% 3D mAP. These results confirm its robustness and effectiveness in diverse multi-modal 3D scenarios. The code of PLPFusion is available on the https://github.com/Text357/PLPFusion.&lt;/p&gt;</content:encoded></item><item><title>MESA: Effective Matching Redundancy Reduction by Semantic Area Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3644296</link><guid>10.1109/tpami.2025.3644296</guid><pubDate>Mon, 15 Dec 2025 18:38:22 +0000</pubDate><dc:creator>Yesheng Zhang</dc:creator><dc:creator>Shuhan Shen</dc:creator><dc:creator>Xu Zhao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644296</prism:doi><description>Matching redundancy, which refers to fine-grained feature comparison between irrelevant image areas, is a prevalent limitation in current feature matching approaches. It leads to unnecessary and error-prone computations, ultimately diminishing matching accuracy. To reduce matching redundancy, we propose MESA and DMESA, both leveraging advanced image understanding of Segment Anything Model (SAM) to establish semantic area matches prior to point matching. These informative area matches, then, can undergo effective internal feature comparison, facilitating precise inside-area point matching. Specifically, MESA adopts a sparse matching framework, while DMESA applies a dense one. Both of them first obtain candidate areas from SAM results through a novel Area Graph (AG). In MESA, matching the candidates is formulated as a graph energy minimization and solved by graphical models derived from AG. In contrast, DMESA performs area matching by generating dense matching distributions on the entire image, aiming at enhancing efficiency. The distributions are produced from off-the-shelf patch matching, modeled as the Gaussian Mixture Model, and refined via the Expectation Maximization. With less repetitive computation, DMESA showcases an area matching speed improvement of nearly five times compared to MESA, while maintaining competitive accuracy. Our methods are extensively evaluated on four different tasks across six datasets, encompassing both indoor and outdoor scenes. The results suggest that our method achieves notable accuracy improvements for nine baselines of point matching in most cases. Furthermore, our methods exhibit promise generalization and improved robustness against image resolution. Code is available at github.com/Easonyesheng/A2PM-MESA.
Published: 2025-12-15T18:38:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yesheng Zhang; Shuhan Shen; Xu Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644296"&gt;10.1109/tpami.2025.3644296&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Matching redundancy, which refers to fine-grained feature comparison between irrelevant image areas, is a prevalent limitation in current feature matching approaches. It leads to unnecessary and error-prone computations, ultimately diminishing matching accuracy. To reduce matching redundancy, we propose MESA and DMESA, both leveraging advanced image understanding of Segment Anything Model (SAM) to establish semantic area matches prior to point matching. These informative area matches, then, can undergo effective internal feature comparison, facilitating precise inside-area point matching. Specifically, MESA adopts a sparse matching framework, while DMESA applies a dense one. Both of them first obtain candidate areas from SAM results through a novel Area Graph (AG). In MESA, matching the candidates is formulated as a graph energy minimization and solved by graphical models derived from AG. In contrast, DMESA performs area matching by generating dense matching distributions on the entire image, aiming at enhancing efficiency. The distributions are produced from off-the-shelf patch matching, modeled as the Gaussian Mixture Model, and refined via the Expectation Maximization. With less repetitive computation, DMESA showcases an area matching speed improvement of nearly five times compared to MESA, while maintaining competitive accuracy. Our methods are extensively evaluated on four different tasks across six datasets, encompassing both indoor and outdoor scenes. The results suggest that our method achieves notable accuracy improvements for nine baselines of point matching in most cases. Furthermore, our methods exhibit promise generalization and improved robustness against image resolution. Code is available at github.com/Easonyesheng/A2PM-MESA.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Image Captioning by Ranking Diffusion Transformer</title><link>https://doi.org/10.1109/tip.2025.3641303</link><guid>10.1109/tip.2025.3641303</guid><pubDate>Mon, 15 Dec 2025 18:40:43 +0000</pubDate><dc:creator>Jun Wan</dc:creator><dc:creator>Min Gan</dc:creator><dc:creator>Lefei Zhang</dc:creator><dc:creator>Jie Zhou</dc:creator><dc:creator>Jun Liu</dc:creator><dc:creator>Bo Du</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3641303</prism:doi><description>The CLIP visual feature-based image captioning models have developed rapidly and achieved remarkable results. However, existing models still struggle to produce descriptive and discriminative captions because they insufficiently exploit fine-grained visual cues and fail to model complex vision–language alignment. To address these limitations, we propose a Ranking Diffusion Transformer (RDT), which integrates a Ranking Visual Encoder (RVE) and a Ranking Loss (RL) for fine-grained image captioning. The RVE introduces a novel ranking attention mechanism that effectively mines diverse and discriminative visual information from CLIP features. Meanwhile, the RL leverages the ranking of generated caption quality as a global semantic supervisory signal, thereby enhancing the diffusion process and strengthening vision–language semantic alignment. We show that by collaborating RVE and RL via the novel RDT—and by gradually adding and removing noise in the diffusion process—more discriminative visual features are learned and precisely aligned with the language features. Experimental results on popular benchmark datasets demonstrate that our proposed RDT surpasses existing state-of-the-art image captioning models in the literature. The code is publicly available at: https://github.com/junwan2014/RDT.
Published: 2025-12-15T18:40:43+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jun Wan; Min Gan; Lefei Zhang; Jie Zhou; Jun Liu; Bo Du; C. L. Philip Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3641303"&gt;10.1109/tip.2025.3641303&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;The CLIP visual feature-based image captioning models have developed rapidly and achieved remarkable results. However, existing models still struggle to produce descriptive and discriminative captions because they insufficiently exploit fine-grained visual cues and fail to model complex vision–language alignment. To address these limitations, we propose a Ranking Diffusion Transformer (RDT), which integrates a Ranking Visual Encoder (RVE) and a Ranking Loss (RL) for fine-grained image captioning. The RVE introduces a novel ranking attention mechanism that effectively mines diverse and discriminative visual information from CLIP features. Meanwhile, the RL leverages the ranking of generated caption quality as a global semantic supervisory signal, thereby enhancing the diffusion process and strengthening vision–language semantic alignment. We show that by collaborating RVE and RL via the novel RDT—and by gradually adding and removing noise in the diffusion process—more discriminative visual features are learned and precisely aligned with the language features. Experimental results on popular benchmark datasets demonstrate that our proposed RDT surpasses existing state-of-the-art image captioning models in the literature. The code is publicly available at: https://github.com/junwan2014/RDT.&lt;/p&gt;</content:encoded></item><item><title>Development and evolution of YOLO in object detection: A survey</title><link>https://doi.org/10.1016/j.neucom.2025.132436</link><guid>10.1016/j.neucom.2025.132436</guid><pubDate>Mon, 15 Dec 2025 17:05:47 +0000</pubDate><dc:creator>Ying Tian</dc:creator><dc:creator>Wenbo Xu</dc:creator><dc:creator>Bo Yang</dc:creator><dc:creator>Xinglong Yang</dc:creator><dc:creator>Hongliang Guo</dc:creator><dc:creator>Gaige Wang</dc:creator><dc:creator>Helong Yu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132436</prism:doi><description>As a classic problem in computer vision, object detection has become one of the essential challenges that researchers continue to explore. The emergence of You Only Look Once (YOLO) has transformed object detection from two-stage to single-stage detection, enhancing real-time performance. By transforming the object detection task into a regression problem, the detection speed and efficiency have also been significantly improved. This article elaborates on the development history of YOLO object detection algorithm in the past decade, with a focus on the technological evolution, evaluation indicators, dataset selection, and variant improvement from 2016–2025. We have systematically reviewed the technological innovations and major contributions from YOLOv1 to YOLOv13, including the anchor box mechanism, multi-scale prediction, attention module, lightweight design, and anchor-free architecture. Meanwhile, the frequency of use of evaluation metrics for object detection, containing Frames Per Second (FPS), Giga Floating-Point Operations Per Second (GFLOPs), Precision (P), Recall (R), Receiver Operating Characteristic (ROC), Intersection over Union (IoU), F1-score, PR curve, Average Precision (AP), and Mean Average Precision (mAP), was analyzed using statistical literature methods. YOLO algorithm was analyzed for its proportion of utilization in object detection, image classification, and semantic segmentation on various datasets through commonly used datasets, PASCAL VOC, MS COCO, and ImageNet. Finally, the article summarizes the technological innovation and future development trends of the YOLO series, providing a reference for researchers.
Published: 2025-12-15T17:05:47+00:00
Venue: Neurocomputing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ying Tian; Wenbo Xu; Bo Yang; Xinglong Yang; Hongliang Guo; Gaige Wang; Helong Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132436"&gt;10.1016/j.neucom.2025.132436&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;As a classic problem in computer vision, object detection has become one of the essential challenges that researchers continue to explore. The emergence of You Only Look Once (YOLO) has transformed object detection from two-stage to single-stage detection, enhancing real-time performance. By transforming the object detection task into a regression problem, the detection speed and efficiency have also been significantly improved. This article elaborates on the development history of YOLO object detection algorithm in the past decade, with a focus on the technological evolution, evaluation indicators, dataset selection, and variant improvement from 2016–2025. We have systematically reviewed the technological innovations and major contributions from YOLOv1 to YOLOv13, including the anchor box mechanism, multi-scale prediction, attention module, lightweight design, and anchor-free architecture. Meanwhile, the frequency of use of evaluation metrics for object detection, containing Frames Per Second (FPS), Giga Floating-Point Operations Per Second (GFLOPs), Precision (P), Recall (R), Receiver Operating Characteristic (ROC), Intersection over Union (IoU), F1-score, PR curve, Average Precision (AP), and Mean Average Precision (mAP), was analyzed using statistical literature methods. YOLO algorithm was analyzed for its proportion of utilization in object detection, image classification, and semantic segmentation on various datasets through commonly used datasets, PASCAL VOC, MS COCO, and ImageNet. Finally, the article summarizes the technological innovation and future development trends of the YOLO series, providing a reference for researchers.&lt;/p&gt;</content:encoded></item><item><title>Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models</title><link>https://doi.org/10.1109/tpami.2025.3644016</link><guid>10.1109/tpami.2025.3644016</guid><pubDate>Mon, 15 Dec 2025 18:38:22 +0000</pubDate><dc:creator>Zhongqi Wang</dc:creator><dc:creator>Jie Zhang</dc:creator><dc:creator>Shiguang Shan</dc:creator><dc:creator>Xilin Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644016</prism:doi><description>Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the \lt \lt EOS \gt \gt token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens' attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across six representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.27% and an AUC of 86.27%. The code is available at https://github.com/Robin-WZQ/DAA.
Published: 2025-12-15T18:38:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhongqi Wang; Jie Zhang; Shiguang Shan; Xilin Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644016"&gt;10.1109/tpami.2025.3644016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the \lt \lt EOS \gt \gt token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens&amp;#x27; attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across six representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.27% and an AUC of 86.27%. The code is available at https://github.com/Robin-WZQ/DAA.&lt;/p&gt;</content:encoded></item><item><title>Multi-Scale Local-Global Fusion for Camouflaged Object Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3644658</link><guid>10.1109/tcsvt.2025.3644658</guid><pubDate>Mon, 15 Dec 2025 18:40:24 +0000</pubDate><dc:creator>Boran Yang</dc:creator><dc:creator>Min Zhang</dc:creator><dc:creator>Yong Wang</dc:creator><dc:creator>Duoqian Miao</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3644658</prism:doi><description>Camouflaged Object Detection (COD) is a formidable computer vision challenge due to the striking resemblance between camouflaged objects and their surroundings. Despite progress in existing methods, they still face significant limitations, particularly in addressing the issues of fuzzy boundaries and the inadequate fusion of local and global features. To address these challenges, we present a multi-scale COD network named Multi-Scale Local-Global Fusion (MSLGF). MSLGF incorporates a Multi-Scale Fusion Module (MSFM), which skillfully integrates feature maps at multiple scales to produce high-fidelity edge features. Additionally, to refine the detection process, a Local-Global Feature Fusion Module (LGFFM) combines the local edge details with global semantic information of camouflaged targets, significantly enhancing the accuracy of COD. Experimental results show that MSLGF achieves remarkable performance across 3 benchmark datasets, i.e., Camouflaged Object Dataset (CAMO), Camouflaged Object Dataset with 10,000 Images (COD10K), and NC4K. Specifically, MSLGF attains a structure-measure from 0.879 to 0.894 and a weighted F-measure between 0.817 and 0.856. The source code is publicly available at https://github.com/tc-fro/MSLGF.
Published: 2025-12-15T18:40:24+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boran Yang; Min Zhang; Yong Wang; Duoqian Miao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3644658"&gt;10.1109/tcsvt.2025.3644658&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Camouflaged Object Detection (COD) is a formidable computer vision challenge due to the striking resemblance between camouflaged objects and their surroundings. Despite progress in existing methods, they still face significant limitations, particularly in addressing the issues of fuzzy boundaries and the inadequate fusion of local and global features. To address these challenges, we present a multi-scale COD network named Multi-Scale Local-Global Fusion (MSLGF). MSLGF incorporates a Multi-Scale Fusion Module (MSFM), which skillfully integrates feature maps at multiple scales to produce high-fidelity edge features. Additionally, to refine the detection process, a Local-Global Feature Fusion Module (LGFFM) combines the local edge details with global semantic information of camouflaged targets, significantly enhancing the accuracy of COD. Experimental results show that MSLGF achieves remarkable performance across 3 benchmark datasets, i.e., Camouflaged Object Dataset (CAMO), Camouflaged Object Dataset with 10,000 Images (COD10K), and NC4K. Specifically, MSLGF attains a structure-measure from 0.879 to 0.894 and a weighted F-measure between 0.817 and 0.856. The source code is publicly available at https://github.com/tc-fro/MSLGF.&lt;/p&gt;</content:encoded></item><item><title>CompleMatch: Boosting Time-Series Semi-Supervised Classification With Temporal-Frequency Complementarity</title><link>https://doi.org/10.1109/tpami.2025.3644603</link><guid>10.1109/tpami.2025.3644603</guid><pubDate>Mon, 15 Dec 2025 18:38:22 +0000</pubDate><dc:creator>Zhen Liu</dc:creator><dc:creator>Kun Zeng</dc:creator><dc:creator>Qianli Ma</dc:creator><dc:creator>James T. Kwok</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644603</prism:doi><description>Time series Semi-Supervised Classification (SSC) aims to improve model performance by utilizing abundant unlabeled data in scenarios where labeled samples are limited. Previous approaches mainly focus on exploiting temporal dependencies within the time domain for SSC. However, these temporal dependencies are susceptible to sampling noise and may not effectively capture the global periodicity of features across categories. To this end, we propose a time series SSC framework called CompleMatch, leveraging the complementary information from both temporal and frequency representations for unlabeled data learning. CompleMatch simultaneously trains two deep neural networks based on time-domain and frequency-domain views, with pseudo-labels generated via label propagation in the representation space guiding the training of the opposing view's classifier. In this co-training paradigm, we incorporate a constraint term to harness the complementary nature of temporal-frequency representations, thereby enhancing the model's robustness under limited labeled data. In addition, we design a temporal-frequency contrastive learning module that integrates supervised and self-supervised signals to enhance pseudo-label quality by learning more discriminative representations. Extensive experiments demonstrate that CompleMatch surpasses state-of-the-art methods. Furthermore, analyses of model behavior (i.e., ablation studies and visualization) underscore the effectiveness of our proposed approach.
Published: 2025-12-15T18:38:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhen Liu; Kun Zeng; Qianli Ma; James T. Kwok&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644603"&gt;10.1109/tpami.2025.3644603&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Time series Semi-Supervised Classification (SSC) aims to improve model performance by utilizing abundant unlabeled data in scenarios where labeled samples are limited. Previous approaches mainly focus on exploiting temporal dependencies within the time domain for SSC. However, these temporal dependencies are susceptible to sampling noise and may not effectively capture the global periodicity of features across categories. To this end, we propose a time series SSC framework called CompleMatch, leveraging the complementary information from both temporal and frequency representations for unlabeled data learning. CompleMatch simultaneously trains two deep neural networks based on time-domain and frequency-domain views, with pseudo-labels generated via label propagation in the representation space guiding the training of the opposing view&amp;#x27;s classifier. In this co-training paradigm, we incorporate a constraint term to harness the complementary nature of temporal-frequency representations, thereby enhancing the model&amp;#x27;s robustness under limited labeled data. In addition, we design a temporal-frequency contrastive learning module that integrates supervised and self-supervised signals to enhance pseudo-label quality by learning more discriminative representations. Extensive experiments demonstrate that CompleMatch surpasses state-of-the-art methods. Furthermore, analyses of model behavior (i.e., ablation studies and visualization) underscore the effectiveness of our proposed approach.&lt;/p&gt;</content:encoded></item><item><title>DiffORSINet: Salient Object Detection in Optical Remote Sensing Images via Conditional Diffusion Model</title><link>https://doi.org/10.1109/tgrs.2025.3644383</link><guid>10.1109/tgrs.2025.3644383</guid><pubDate>Mon, 15 Dec 2025 18:38:25 +0000</pubDate><dc:creator>Yaoyao Hou</dc:creator><dc:creator>Ting Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3644383</prism:doi><description>Salient Object Detection in optical remote sensing images (ORSI-SOD) has received increasing attention in recent years. Although some progress has been made in existing methods, there are still challenges such as ambiguous and irregular boundaries of salient targets and complex backgrounds. The existing ORSI-SOD methods have difficulty in finely dividing the boundaries of salient targets and dealing with chaotic backgrounds. To solve these problems, we propose a new network based on the diffusion model, termed DiffORSINet, which describes the ORSI-SOD task as a conditional mask generation problem. By combining RGB images and the guidance of time steps, it can gradually and accurately locate and refine the segmentation of salient targets during the denoising process. Furthermore, we design a dedicated denoising network, which includes a Fourier frequency awareness module (FFAM) and a multi-level feature fusion module (MFFM), which significantly improves the refinement ability of the network. FFAM captures and fuses the frequency-domain features by combining the Fourier transform operation and the cross-attention mechanism, enhances the intensity of some signals, and thereby refines the image details. MFFM reduces the interference of chaotic backgrounds by coordinating and fusing multi-level features and suppressing irrelevant regions. Finally, the comparative experimental results on three widely used ORSI-SOD datasets show that the method proposed in this paper is superior to other existing methods. Our code and results are available at https://github.com/hyy-qd/DiffORSINet/.
Published: 2025-12-15T18:38:25+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaoyao Hou; Ting Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3644383"&gt;10.1109/tgrs.2025.3644383&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Salient Object Detection in optical remote sensing images (ORSI-SOD) has received increasing attention in recent years. Although some progress has been made in existing methods, there are still challenges such as ambiguous and irregular boundaries of salient targets and complex backgrounds. The existing ORSI-SOD methods have difficulty in finely dividing the boundaries of salient targets and dealing with chaotic backgrounds. To solve these problems, we propose a new network based on the diffusion model, termed DiffORSINet, which describes the ORSI-SOD task as a conditional mask generation problem. By combining RGB images and the guidance of time steps, it can gradually and accurately locate and refine the segmentation of salient targets during the denoising process. Furthermore, we design a dedicated denoising network, which includes a Fourier frequency awareness module (FFAM) and a multi-level feature fusion module (MFFM), which significantly improves the refinement ability of the network. FFAM captures and fuses the frequency-domain features by combining the Fourier transform operation and the cross-attention mechanism, enhances the intensity of some signals, and thereby refines the image details. MFFM reduces the interference of chaotic backgrounds by coordinating and fusing multi-level features and suppressing irrelevant regions. Finally, the comparative experimental results on three widely used ORSI-SOD datasets show that the method proposed in this paper is superior to other existing methods. Our code and results are available at https://github.com/hyy-qd/DiffORSINet/.&lt;/p&gt;</content:encoded></item><item><title>Unrolling operator splitting in learning PDEs for object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132172</link><guid>10.1016/j.neucom.2025.132172</guid><pubDate>Mon, 15 Dec 2025 17:05:34 +0000</pubDate><dc:creator>Banu Wirawan Yohanes</dc:creator><dc:creator>Philip O. Ogunbona</dc:creator><dc:creator>Wanqing Li</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132172</prism:doi><description>Object detection presents significant challenges due to the variability in object scale, location, and orientation within images. Most state-of-the-art detectors are based on convolutional or Transformer architectures, which, while effective, often result in deep, opaque models that generalise poorly and lack interpretability. In contrast, iterative algorithms offer greater transparency and generalisation, albeit at the cost of efficiency and accuracy. In this work, we reformulate object detection as a partial differential equation (PDE)-constrained optimal control problem. This formulation exploits linear combinations of fundamental differential invariants—such as translation and rotation invariance—to embed structural priors into the learning process. We solve this problem using operator splitting via the Alternating Direction Method of Multipliers (ADMM), and unroll each optimisation step into a network layer, yielding a novel architecture: ADMM-ODNet. This approach provides a principled and interpretable alternative to conventional deep networks. Experimental results on the Corel, Pascal VOC and COCO datasets demonstrate that ADMM-ODNet outperforms leading models such as Cascade Mask R-CNN, Swin Transformer, Deformable DETR, DINO, DN-and RT-DETR, and achieves performance comparable to Plain DETR and YOLO, while requiring significantly fewer parameters.
Published: 2025-12-15T17:05:34+00:00
Venue: Neurocomputing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Banu Wirawan Yohanes; Philip O. Ogunbona; Wanqing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132172"&gt;10.1016/j.neucom.2025.132172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection presents significant challenges due to the variability in object scale, location, and orientation within images. Most state-of-the-art detectors are based on convolutional or Transformer architectures, which, while effective, often result in deep, opaque models that generalise poorly and lack interpretability. In contrast, iterative algorithms offer greater transparency and generalisation, albeit at the cost of efficiency and accuracy. In this work, we reformulate object detection as a partial differential equation (PDE)-constrained optimal control problem. This formulation exploits linear combinations of fundamental differential invariants—such as translation and rotation invariance—to embed structural priors into the learning process. We solve this problem using operator splitting via the Alternating Direction Method of Multipliers (ADMM), and unroll each optimisation step into a network layer, yielding a novel architecture: ADMM-ODNet. This approach provides a principled and interpretable alternative to conventional deep networks. Experimental results on the Corel, Pascal VOC and COCO datasets demonstrate that ADMM-ODNet outperforms leading models such as Cascade Mask R-CNN, Swin Transformer, Deformable DETR, DINO, DN-and RT-DETR, and achieves performance comparable to Plain DETR and YOLO, while requiring significantly fewer parameters.&lt;/p&gt;</content:encoded></item><item><title>TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder</title><link>https://arxiv.org/abs/2512.11926v1</link><guid>http://arxiv.org/abs/2512.11926v1</guid><pubDate>Fri, 12 Dec 2025 00:08:03 +0000</pubDate><dc:creator>Qinghao Meng</dc:creator><dc:creator>Chenming Wu</dc:creator><dc:creator>Liangjun Zhang</dc:creator><dc:creator>Jianbing Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TITS.2025.3617527</prism:doi><description>3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.
Published: 2025-12-12T00:08:03+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qinghao Meng; Chenming Wu; Liangjun Zhang; Jianbing Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TITS.2025.3617527"&gt;10.1109/TITS.2025.3617527&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.&lt;/p&gt;</content:encoded></item><item><title>Towards Deeper Emotional Reflection: Crafting Affective Image Filters With Generative Priors</title><link>https://doi.org/10.1109/tpami.2025.3643911</link><guid>10.1109/tpami.2025.3643911</guid><pubDate>Mon, 15 Dec 2025 18:38:22 +0000</pubDate><dc:creator>Peixuan Zhang</dc:creator><dc:creator>Shuchen Weng</dc:creator><dc:creator>Jiajun Tang</dc:creator><dc:creator>Si Li</dc:creator><dc:creator>Boxin Shi</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643911</prism:doi><description>Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.
Published: 2025-12-15T18:38:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peixuan Zhang; Shuchen Weng; Jiajun Tang; Si Li; Boxin Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643911"&gt;10.1109/tpami.2025.3643911&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.&lt;/p&gt;</content:encoded></item><item><title>Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers</title><link>https://arxiv.org/abs/2512.11260v1</link><guid>http://arxiv.org/abs/2512.11260v1</guid><pubDate>Fri, 12 Dec 2025 03:49:55 +0000</pubDate><dc:creator>Ali El Bellaj</dc:creator><dc:creator>Mohammed-Amine Cheddadi</dc:creator><dc:creator>Rhassan Berber</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.
Published: 2025-12-12T03:49:55+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ali El Bellaj; Mohammed-Amine Cheddadi; Rhassan Berber&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.&lt;/p&gt;</content:encoded></item><item><title>Multi-modal Integration with Adversarial Mutual Distribution Matching</title><link>https://doi.org/10.1016/j.patcog.2025.112818</link><guid>10.1016/j.patcog.2025.112818</guid><pubDate>Sun, 14 Dec 2025 23:00:13 +0000</pubDate><dc:creator>Ouhan Huang</dc:creator><dc:creator>Jianyang Shi</dc:creator><dc:creator>Ziwei Li</dc:creator><dc:creator>Siyuan Ye</dc:creator><dc:creator>Chao Shen</dc:creator><dc:creator>Junwen Zhang</dc:creator><dc:creator>Haiwen Cai</dc:creator><dc:creator>Nan Chi</dc:creator><dc:creator>Feng Bao</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112818</prism:doi><description>Integrating multimodal data requires learning representations that are invariant and complementary across modalities. Most existing approaches focus on instance-level alignment by explicitly matching paired samples, but they often fail to capture the global structure and are sensitive to noise or missing modalities. This paper presents Adversarial Mutual Distribution Matching (adMDM), a unified framework that jointly enforces sample-level and distribution-level consistency for robust multimodal integration. The proposed method leverages the Wasserstein distance to align latent distributions while maintaining instance-wise correspondence through cosine similarity and reconstruction constraints. A mutual adversarial optimization strategy is introduced to dynamically adapt both modality-specific encoders, achieving symmetric and stable distribution matching. Extensive experiments on synthetic, transformed MNIST, and real-world CITE-seq datasets demonstrate that adMDM not only enhances cross-modal correlation and semantic consistency but also shows superior robustness against data degradation compared with ten state-of-the-art baselines. These results highlight adMDM as a principled and scalable approach to multimodal representation learning under heterogeneous and noisy conditions.
Published: 2025-12-14T23:00:13+00:00
Venue: Pattern Recognition
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ouhan Huang; Jianyang Shi; Ziwei Li; Siyuan Ye; Chao Shen; Junwen Zhang; Haiwen Cai; Nan Chi; Feng Bao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112818"&gt;10.1016/j.patcog.2025.112818&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Integrating multimodal data requires learning representations that are invariant and complementary across modalities. Most existing approaches focus on instance-level alignment by explicitly matching paired samples, but they often fail to capture the global structure and are sensitive to noise or missing modalities. This paper presents Adversarial Mutual Distribution Matching (adMDM), a unified framework that jointly enforces sample-level and distribution-level consistency for robust multimodal integration. The proposed method leverages the Wasserstein distance to align latent distributions while maintaining instance-wise correspondence through cosine similarity and reconstruction constraints. A mutual adversarial optimization strategy is introduced to dynamically adapt both modality-specific encoders, achieving symmetric and stable distribution matching. Extensive experiments on synthetic, transformed MNIST, and real-world CITE-seq datasets demonstrate that adMDM not only enhances cross-modal correlation and semantic consistency but also shows superior robustness against data degradation compared with ten state-of-the-art baselines. These results highlight adMDM as a principled and scalable approach to multimodal representation learning under heterogeneous and noisy conditions.&lt;/p&gt;</content:encoded></item><item><title>SSM-Det: State Space Model-Based Object Detector for Intelligent Transportation System</title><link>https://doi.org/10.1109/tits.2025.3640934</link><guid>10.1109/tits.2025.3640934</guid><pubDate>Mon, 15 Dec 2025 18:39:47 +0000</pubDate><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Chunmian Lin</dc:creator><dc:creator>Kan Guo</dc:creator><dc:creator>Jiangang Guo</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3640934</prism:doi><description>The State Space Model (SSM) has been a growth of interest in computer vision due to its long-term dependency modeling with linear complexity. Despite massive endeavor, it has not been extensively explored in intelligent transportation system (ITS) yet. In this paper, we propose State Space Model-based object Detector (SSM-Det), that is meticulously curated with Direction-aware Visual State Space Encoder (D-VSSE). Specifically, it customizes multi-path pixel exchange and patch re-arrangement via four-direction scanning mechanism, promoting for information communication. To bridge the information bottleneck across high-low level, we further design Split-Fusion (SF) and Skip-Connection (SC) modules for contextual feature propagation before decoding: SF performs multi-channel semantic separation and re-weighting in global-local scope, while SC is responsible for cross-layer feature interaction in a cascaded manner. Empirical studies is conducted on both VisDrone2019-DET and SEU_PML benchmarks, and our proposed SSM-Det reports the state-of-the-art performance against all counterparts by a substantial margin, while maintaining the real-time inference speed. We hope this work contributes to the in-depth investigation of SSM-based detector for intelligent transportation applications. The code is available at https://buaawjq.github.io/SSM-Det/.
Published: 2025-12-15T18:39:47+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Wang; Chunmian Lin; Kan Guo; Jiangang Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3640934"&gt;10.1109/tits.2025.3640934&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;The State Space Model (SSM) has been a growth of interest in computer vision due to its long-term dependency modeling with linear complexity. Despite massive endeavor, it has not been extensively explored in intelligent transportation system (ITS) yet. In this paper, we propose State Space Model-based object Detector (SSM-Det), that is meticulously curated with Direction-aware Visual State Space Encoder (D-VSSE). Specifically, it customizes multi-path pixel exchange and patch re-arrangement via four-direction scanning mechanism, promoting for information communication. To bridge the information bottleneck across high-low level, we further design Split-Fusion (SF) and Skip-Connection (SC) modules for contextual feature propagation before decoding: SF performs multi-channel semantic separation and re-weighting in global-local scope, while SC is responsible for cross-layer feature interaction in a cascaded manner. Empirical studies is conducted on both VisDrone2019-DET and SEU_PML benchmarks, and our proposed SSM-Det reports the state-of-the-art performance against all counterparts by a substantial margin, while maintaining the real-time inference speed. We hope this work contributes to the in-depth investigation of SSM-based detector for intelligent transportation applications. The code is available at https://buaawjq.github.io/SSM-Det/.&lt;/p&gt;</content:encoded></item><item><title>Disentangled Image-Text Classification: Enhancing Visual Representations with MLLM-driven Knowledge Transfer</title><link>https://doi.org/10.1016/j.eswa.2025.130790</link><guid>10.1016/j.eswa.2025.130790</guid><pubDate>Sun, 14 Dec 2025 15:19:32 +0000</pubDate><dc:creator>Qianjun Shuai</dc:creator><dc:creator>Xiaohao Chen</dc:creator><dc:creator>Yongqiang Cheng</dc:creator><dc:creator>Fang Miao</dc:creator><dc:creator>Libiao Jin</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130790</prism:doi><description>Multimodal image-text classification plays a critical role in applications such as content moderation, news recommendation, and multimedia understanding. Despite recent advances, visual modality faces higher representation learning complexity than textual modality in semantic extraction, which often leads to a semantic gap between visual and textual representations. In addition, conventional fusion strategies introduce cross-modal redundancy, further limiting classification performance. To address these issues, we propose MD-MLLM , a novel image-text classification framework that leverages large multimodal language models (MLLMs) to generate semantically enhanced visual representations. To mitigate redundancy introduced by direct MLLM feature integration, we introduce a hierarchical disentanglement mechanism based on the Hilbert-Schmidt Independence Criterion (HSIC) and orthogonality constraints, which explicitly separates modality-specific and shared representations. Furthermore, a hierarchical fusion strategy combines original unimodal features with disentangled shared semantics, promoting discriminative feature learning and cross-modal complementarity. Extensive experiments on two benchmark datasets, N24News and Food101 , show that MD-MLLM achieves consistently stable improvements in classification accuracy and exhibits competitive performance compared with various representative multimodal baselines. The framework also demonstrates good generalization ability and robustness across different multimodal scenarios. The code is available at https://github.com/xiaohaochen0308/MD-MLLM .
Published: 2025-12-14T15:19:32+00:00
Venue: Expert Systems with Applications
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qianjun Shuai; Xiaohao Chen; Yongqiang Cheng; Fang Miao; Libiao Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130790"&gt;10.1016/j.eswa.2025.130790&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal image-text classification plays a critical role in applications such as content moderation, news recommendation, and multimedia understanding. Despite recent advances, visual modality faces higher representation learning complexity than textual modality in semantic extraction, which often leads to a semantic gap between visual and textual representations. In addition, conventional fusion strategies introduce cross-modal redundancy, further limiting classification performance. To address these issues, we propose MD-MLLM , a novel image-text classification framework that leverages large multimodal language models (MLLMs) to generate semantically enhanced visual representations. To mitigate redundancy introduced by direct MLLM feature integration, we introduce a hierarchical disentanglement mechanism based on the Hilbert-Schmidt Independence Criterion (HSIC) and orthogonality constraints, which explicitly separates modality-specific and shared representations. Furthermore, a hierarchical fusion strategy combines original unimodal features with disentangled shared semantics, promoting discriminative feature learning and cross-modal complementarity. Extensive experiments on two benchmark datasets, N24News and Food101 , show that MD-MLLM achieves consistently stable improvements in classification accuracy and exhibits competitive performance compared with various representative multimodal baselines. The framework also demonstrates good generalization ability and robustness across different multimodal scenarios. The code is available at https://github.com/xiaohaochen0308/MD-MLLM .&lt;/p&gt;</content:encoded></item><item><title>APR-BiCA: LiDAR-based absolute pose regression with bidirectional cross attention and gating unit</title><link>https://doi.org/10.1016/j.neucom.2025.132404</link><guid>10.1016/j.neucom.2025.132404</guid><pubDate>Mon, 15 Dec 2025 16:04:44 +0000</pubDate><dc:creator>Jianlong Dai</dc:creator><dc:creator>Hui Wang</dc:creator><dc:creator>Yuqian Zhao</dc:creator><dc:creator>Zhihua Liu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132404</prism:doi><description>LiDAR localization is a critical component in fields like intelligent robots and autonomous driving. Absolute pose regression (APR) techniques directly infer global poses from input point clouds through end-to-end regression, achieving superior computational efficiency. However, APR struggles with dynamic objects and environmental noise in large-scale scenarios. To address this, we propose an APR network called APR-BiCA to fuse complementary information from raw point clouds and range images, which aims to improve the localization accuracy of robots in large-scale autonomous driving scenarios. The APR-BiCA incorporates two distinct branches: one extracts features from the raw point cloud to capture key features and build global point relationships, while the other processes the range image derived from the point cloud to extracts robust structural features. Additionally, a bidirectional cross attention mechanism combined with a gating unit-based fusion module is designed to facilitate effective inter-modal feature interaction, thereby enhancing the feature representational capability to support efficient handling of large-scale environments. Experimental results on the Oxford RobotCar and NCLT datasets demonstrate the superior performance of APR-BiCA, while maintaining exceptional efficiency. This well-balanced combination of accuracy and efficiency underscores its potential to advance LiDAR-based localization technology and drive its practical application in real-world autonomous driving systems.
Published: 2025-12-15T16:04:44+00:00
Venue: Neurocomputing
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianlong Dai; Hui Wang; Yuqian Zhao; Zhihua Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132404"&gt;10.1016/j.neucom.2025.132404&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;LiDAR localization is a critical component in fields like intelligent robots and autonomous driving. Absolute pose regression (APR) techniques directly infer global poses from input point clouds through end-to-end regression, achieving superior computational efficiency. However, APR struggles with dynamic objects and environmental noise in large-scale scenarios. To address this, we propose an APR network called APR-BiCA to fuse complementary information from raw point clouds and range images, which aims to improve the localization accuracy of robots in large-scale autonomous driving scenarios. The APR-BiCA incorporates two distinct branches: one extracts features from the raw point cloud to capture key features and build global point relationships, while the other processes the range image derived from the point cloud to extracts robust structural features. Additionally, a bidirectional cross attention mechanism combined with a gating unit-based fusion module is designed to facilitate effective inter-modal feature interaction, thereby enhancing the feature representational capability to support efficient handling of large-scale environments. Experimental results on the Oxford RobotCar and NCLT datasets demonstrate the superior performance of APR-BiCA, while maintaining exceptional efficiency. This well-balanced combination of accuracy and efficiency underscores its potential to advance LiDAR-based localization technology and drive its practical application in real-world autonomous driving systems.&lt;/p&gt;</content:encoded></item><item><title>Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation</title><link>https://arxiv.org/abs/2512.12595v1</link><guid>http://arxiv.org/abs/2512.12595v1</guid><pubDate>Sun, 14 Dec 2025 08:28:50 +0000</pubDate><dc:creator>Karthikeya KV</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.
Published: 2025-12-14T08:28:50+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Karthikeya KV&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.&lt;/p&gt;</content:encoded></item><item><title>UDMMColor: A Unified Diffusion Model for Multi-Modal Colorization</title><link>https://doi.org/10.1109/tcsvt.2025.3643915</link><guid>10.1109/tcsvt.2025.3643915</guid><pubDate>Mon, 15 Dec 2025 18:40:24 +0000</pubDate><dc:creator>Yan Zhai</dc:creator><dc:creator>Zerui Han</dc:creator><dc:creator>Zhulin Tao</dc:creator><dc:creator>Xianglin Huang</dc:creator><dc:creator>Jinshan Pan</dc:creator><dc:creator>Jinhui Tang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643915</prism:doi><description>Diffusion model-based networks have been widely applied in the field of image generation and have gradually demonstrated a strong potential in image colorization tasks. However, despite the emergence of various colorization diffusion models, two major challenges remain: (1) the lack of effective control over the colorization process and (2) the prevalent issue of color bleeding. Integrating suitable conditional control can effectively alleviate these challenges. To this end, we propose a unified multi-modal diffusion model that harnesses diverse modality information to achieve flexible and high-quality colorization. Specifically, we introduce a Stroke-Adapter that extracts and integrates stroke prompt, enhancing user control over color distribution. Additionally, we design an Edge-Guided Attention mechanism to effectively inject edge information into the colorization process, significantly reducing color bleeding artifacts. Extensive comparative experiments demonstrate that our method outperforms state-of-the-art image colorization approaches in both qualitative and quantitative evaluations, achieving superior colorization results with enhanced controllability.
Published: 2025-12-15T18:40:24+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yan Zhai; Zerui Han; Zhulin Tao; Xianglin Huang; Jinshan Pan; Jinhui Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643915"&gt;10.1109/tcsvt.2025.3643915&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion model-based networks have been widely applied in the field of image generation and have gradually demonstrated a strong potential in image colorization tasks. However, despite the emergence of various colorization diffusion models, two major challenges remain: (1) the lack of effective control over the colorization process and (2) the prevalent issue of color bleeding. Integrating suitable conditional control can effectively alleviate these challenges. To this end, we propose a unified multi-modal diffusion model that harnesses diverse modality information to achieve flexible and high-quality colorization. Specifically, we introduce a Stroke-Adapter that extracts and integrates stroke prompt, enhancing user control over color distribution. Additionally, we design an Edge-Guided Attention mechanism to effectively inject edge information into the colorization process, significantly reducing color bleeding artifacts. Extensive comparative experiments demonstrate that our method outperforms state-of-the-art image colorization approaches in both qualitative and quantitative evaluations, achieving superior colorization results with enhanced controllability.&lt;/p&gt;</content:encoded></item><item><title>Triple-Supervised Progressive Contrastive Learning for Heterogeneous Graph Embedding</title><link>https://doi.org/10.1016/j.inffus.2025.104051</link><guid>10.1016/j.inffus.2025.104051</guid><pubDate>Mon, 15 Dec 2025 07:15:52 +0000</pubDate><dc:creator>Huan Xu</dc:creator><dc:creator>Jia Liu</dc:creator><dc:creator>Xipeng Yuan</dc:creator><dc:creator>Wei Huang</dc:creator><dc:creator>Yajun Du</dc:creator><dc:creator>Tianrui Li</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104051</prism:doi><description>Heterogeneous graph embedding aims to map nodes in a heterogeneous graph into low-dimensional representations by capturing its rich structural and semantic information. Due to the scarcity of labeled data, contrastive learning has emerged as an effective approach in heterogeneous graph representation learning. However, existing heterogeneous graph contrastive learning methods frequently rely on random or static negative sampling strategies, failing to adapt to the dynamic nature of sample difficulty, which ultimately degrades model performance and training stability. Furthermore, they typically focus on node features and implicit semantic aggregation during message passing, lacking explicit structured modeling of heterogeneous relations. To address these issues, we propose a T riple- S upervised P rogressive C ontrastive L earning (TSPCL) method for heterogeneous graph embedding. Specifically, a progressive contrastive learning strategy is designed to enhance the model’s discriminative ability and stability. In this strategy, coarse-grained positive and negative samples are contrasted, and fine-grained contrastive learning is achieved through dynamic hard negative sample mining and data augmentation techniques. In addition, explicit modeling of meta-paths in heterogeneous graphs is performed through path decomposition. Furthermore, the structural semantics are represented in the form of triples, where an auxiliary task is introduced to optimize the relation vector. Finally, extensive experiments on three public datasets demonstrate that TSPCL achieves competitive performance compared to state-of-the-art methods on downstream tasks.
Published: 2025-12-15T07:15:52+00:00
Venue: Information Fusion
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huan Xu; Jia Liu; Xipeng Yuan; Wei Huang; Yajun Du; Tianrui Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104051"&gt;10.1016/j.inffus.2025.104051&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Heterogeneous graph embedding aims to map nodes in a heterogeneous graph into low-dimensional representations by capturing its rich structural and semantic information. Due to the scarcity of labeled data, contrastive learning has emerged as an effective approach in heterogeneous graph representation learning. However, existing heterogeneous graph contrastive learning methods frequently rely on random or static negative sampling strategies, failing to adapt to the dynamic nature of sample difficulty, which ultimately degrades model performance and training stability. Furthermore, they typically focus on node features and implicit semantic aggregation during message passing, lacking explicit structured modeling of heterogeneous relations. To address these issues, we propose a T riple- S upervised P rogressive C ontrastive L earning (TSPCL) method for heterogeneous graph embedding. Specifically, a progressive contrastive learning strategy is designed to enhance the model’s discriminative ability and stability. In this strategy, coarse-grained positive and negative samples are contrasted, and fine-grained contrastive learning is achieved through dynamic hard negative sample mining and data augmentation techniques. In addition, explicit modeling of meta-paths in heterogeneous graphs is performed through path decomposition. Furthermore, the structural semantics are represented in the form of triples, where an auxiliary task is introduced to optimize the relation vector. Finally, extensive experiments on three public datasets demonstrate that TSPCL achieves competitive performance compared to state-of-the-art methods on downstream tasks.&lt;/p&gt;</content:encoded></item><item><title>Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing</title><link>https://arxiv.org/abs/2512.11680v1</link><guid>http://arxiv.org/abs/2512.11680v1</guid><pubDate>Fri, 12 Dec 2025 15:59:49 +0000</pubDate><dc:creator>Xu Zhang</dc:creator><dc:creator>Jiabin Fang</dc:creator><dc:creator>Zhuoming Ding</dc:creator><dc:creator>Jin Yuan</dc:creator><dc:creator>Xuan Liu</dc:creator><dc:creator>Qianjun Zhang</dc:creator><dc:creator>Zhiyong Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.
Published: 2025-12-12T15:59:49+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xu Zhang; Jiabin Fang; Zhuoming Ding; Jin Yuan; Xuan Liu; Qianjun Zhang; Zhiyong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.&lt;/p&gt;</content:encoded></item><item><title>$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment</title><link>https://arxiv.org/abs/2512.12678v1</link><guid>http://arxiv.org/abs/2512.12678v1</guid><pubDate>Sun, 14 Dec 2025 13:03:20 +0000</pubDate><dc:creator>Fatimah Zohra</dc:creator><dc:creator>Chen Zhao</dc:creator><dc:creator>Hani Itani</dc:creator><dc:creator>Bernard Ghanem</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.
Published: 2025-12-14T13:03:20+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fatimah Zohra; Chen Zhao; Hani Itani; Bernard Ghanem&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.&lt;/p&gt;</content:encoded></item><item><title>Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts</title><link>https://arxiv.org/abs/2512.11360v1</link><guid>http://arxiv.org/abs/2512.11360v1</guid><pubDate>Fri, 12 Dec 2025 08:20:11 +0000</pubDate><dc:creator>Mohammad Sadegh Gholizadeh</dc:creator><dc:creator>Amir Arsalan Rezapour</dc:creator><dc:creator>Hamidreza Shayegh</dc:creator><dc:creator>Ehsan Pazouki</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model's generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.
Published: 2025-12-12T08:20:11+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohammad Sadegh Gholizadeh; Amir Arsalan Rezapour; Hamidreza Shayegh; Ehsan Pazouki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model&amp;#x27;s generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.&lt;/p&gt;</content:encoded></item><item><title>GATformer-Transformer Based Progressive Triplet Network for Hyperspectral Target Detection</title><link>https://doi.org/10.1109/jstars.2025.3644429</link><guid>10.1109/jstars.2025.3644429</guid><pubDate>Mon, 15 Dec 2025 18:38:50 +0000</pubDate><dc:creator>Songqi Li</dc:creator><dc:creator>Xudong Sun</dc:creator><dc:creator>Lingyu Kong</dc:creator><dc:creator>Jiahua Zhang</dc:creator><dc:creator>Xiaodi Shang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3644429</prism:doi><description>Deep learning (DL) has achieved significant progress in hyperspectral target detection (HTD), yet several challenges remain. The limited prior information in HTD hinders effective network training, while the high dimensionality of hyperspectral image and the complex relationships between bands impose greater demands on the model's ability to capture spectral features and variations. To address these issues, this paper proposes a GATformer-Transformer based progressive triplet network (GTPTN) for HTD. First, a clustering-driven linear mixing sample construction strategy is proposed to generate high-quality background and target samples for model training. Subsequently, the spectra are segmented to construct an enhanced chain spectral graph (ECSG), and we introduce a local-global progressive learning network to thoroughly explore representative spectral information of targets. In addition, a novel strong separation-aggregation (SSA) loss is designed by combining batch hard positive mining (BHPM) loss with binary cross-entropy (BCE) loss, further enhance the network's target recognition ability. Finally, experiments on four public HTD datasets demonstrate that GTPTN achieves excellent detection accuracy and strong stability. Ablation studies confirm the effectiveness of each proposed module.
Published: 2025-12-15T18:38:50+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Songqi Li; Xudong Sun; Lingyu Kong; Jiahua Zhang; Xiaodi Shang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3644429"&gt;10.1109/jstars.2025.3644429&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning (DL) has achieved significant progress in hyperspectral target detection (HTD), yet several challenges remain. The limited prior information in HTD hinders effective network training, while the high dimensionality of hyperspectral image and the complex relationships between bands impose greater demands on the model&amp;#x27;s ability to capture spectral features and variations. To address these issues, this paper proposes a GATformer-Transformer based progressive triplet network (GTPTN) for HTD. First, a clustering-driven linear mixing sample construction strategy is proposed to generate high-quality background and target samples for model training. Subsequently, the spectra are segmented to construct an enhanced chain spectral graph (ECSG), and we introduce a local-global progressive learning network to thoroughly explore representative spectral information of targets. In addition, a novel strong separation-aggregation (SSA) loss is designed by combining batch hard positive mining (BHPM) loss with binary cross-entropy (BCE) loss, further enhance the network&amp;#x27;s target recognition ability. Finally, experiments on four public HTD datasets demonstrate that GTPTN achieves excellent detection accuracy and strong stability. Ablation studies confirm the effectiveness of each proposed module.&lt;/p&gt;</content:encoded></item><item><title>WeDetect: Fast Open-Vocabulary Object Detection as Retrieval</title><link>https://arxiv.org/abs/2512.12309v1</link><guid>http://arxiv.org/abs/2512.12309v1</guid><pubDate>Sat, 13 Dec 2025 12:40:28 +0000</pubDate><dc:creator>Shenghao Fu</dc:creator><dc:creator>Yukun Su</dc:creator><dc:creator>Fengyun Rao</dc:creator><dc:creator>Jing Lyu</dc:creator><dc:creator>Xiaohua Xie</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.
Published: 2025-12-13T12:40:28+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shenghao Fu; Yukun Su; Fengyun Rao; Jing Lyu; Xiaohua Xie; Wei-Shi Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.&lt;/p&gt;</content:encoded></item><item><title>GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search</title><link>https://arxiv.org/abs/2512.12296v1</link><guid>http://arxiv.org/abs/2512.12296v1</guid><pubDate>Sat, 13 Dec 2025 11:40:21 +0000</pubDate><dc:creator>Hyunju Lee</dc:creator><dc:creator>Youngmin Oh</dc:creator><dc:creator>Jeimin Jeon</dc:creator><dc:creator>Donghyeon Baek</dc:creator><dc:creator>Bumsub Ham</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods
Published: 2025-12-13T11:40:21+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyunju Lee; Youngmin Oh; Jeimin Jeon; Donghyeon Baek; Bumsub Ham&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods&lt;/p&gt;</content:encoded></item><item><title>Spatial-Temporal Self-Supervised Learning for Audio Classification</title><link>https://doi.org/10.1016/j.eswa.2025.130641</link><guid>10.1016/j.eswa.2025.130641</guid><pubDate>Mon, 15 Dec 2025 17:07:42 +0000</pubDate><dc:creator>Manal Alsuwat</dc:creator><dc:creator>Sarah Al-Shareef</dc:creator><dc:creator>Manal AlGhamdi</dc:creator><dc:creator>Miada AlMasre</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130641</prism:doi><description>AI developers utilize multimodal learning incorporating video, text, and audio to mimic human perception and enhance world representations. This approach has progressed with deep supervised learning that depends on large-scale, expensive, and error-prone human annotation. This paper explores self-supervised learning (SSL) in audio-visual modalities as an alternative to extensive data labeling in domains with limited pre-trained models. This paper presents four key contributions: first, proposes a novel self-supervised training approach blending different dataset domains during pretext task training to improve the model’s generalization. Second, exploring the network’s spatial and temporal attention mechanisms and their impact. Third, training on the recent, unlabeled ACAV100M dataset. Finally, evaluate the pre-trained models in new domains in downstream tasks like emotion recognition and Arabic music classification in the audio-visual correspondence (AVC) pretext task context. The experimental findings indicate that the proposed single-level attention model was the most effective, significantly improving performance and generalization.
Published: 2025-12-15T17:07:42+00:00
Venue: Expert Systems with Applications
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Manal Alsuwat; Sarah Al-Shareef; Manal AlGhamdi; Miada AlMasre&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130641"&gt;10.1016/j.eswa.2025.130641&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;AI developers utilize multimodal learning incorporating video, text, and audio to mimic human perception and enhance world representations. This approach has progressed with deep supervised learning that depends on large-scale, expensive, and error-prone human annotation. This paper explores self-supervised learning (SSL) in audio-visual modalities as an alternative to extensive data labeling in domains with limited pre-trained models. This paper presents four key contributions: first, proposes a novel self-supervised training approach blending different dataset domains during pretext task training to improve the model’s generalization. Second, exploring the network’s spatial and temporal attention mechanisms and their impact. Third, training on the recent, unlabeled ACAV100M dataset. Finally, evaluate the pre-trained models in new domains in downstream tasks like emotion recognition and Arabic music classification in the audio-visual correspondence (AVC) pretext task context. The experimental findings indicate that the proposed single-level attention model was the most effective, significantly improving performance and generalization.&lt;/p&gt;</content:encoded></item><item><title>OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation</title><link>https://arxiv.org/abs/2512.12303v1</link><guid>http://arxiv.org/abs/2512.12303v1</guid><pubDate>Sat, 13 Dec 2025 12:01:23 +0000</pubDate><dc:creator>Yang Ou</dc:creator><dc:creator>Xiongwei Zhao</dc:creator><dc:creator>Xinye Yang</dc:creator><dc:creator>Yihan Wang</dc:creator><dc:creator>Yicheng Di</dc:creator><dc:creator>Rong Yuan</dc:creator><dc:creator>Xieyuanli Chen</dc:creator><dc:creator>Xu Zhu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA-&gt;Cityscapes and GTA5-&gt;Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.
Published: 2025-12-13T12:01:23+00:00
Venue: arXiv
Score: 0.769 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Ou; Xiongwei Zhao; Xinye Yang; Yihan Wang; Yicheng Di; Rong Yuan; Xieyuanli Chen; Xu Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (consider)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA-&amp;gt;Cityscapes and GTA5-&amp;gt;Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Zero-Shot Learning with Attribute-Centric Representations</title><link>https://arxiv.org/abs/2512.12219v1</link><guid>http://arxiv.org/abs/2512.12219v1</guid><pubDate>Sat, 13 Dec 2025 07:12:09 +0000</pubDate><dc:creator>Zhi Chen</dc:creator><dc:creator>Jingcai Guo</dc:creator><dc:creator>Taotao Cai</dc:creator><dc:creator>Yuxiang Cai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.
Published: 2025-12-13T07:12:09+00:00
Venue: arXiv
Score: 0.769 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhi Chen; Jingcai Guo; Taotao Cai; Yuxiang Cai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (consider)&lt;/p&gt;
&lt;p&gt;Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.&lt;/p&gt;</content:encoded></item><item><title>Quadruplet-Attention Transformer for Scale-Invariant Robot Place Recognition</title><link>https://doi.org/10.1016/j.eswa.2025.130756</link><guid>10.1016/j.eswa.2025.130756</guid><pubDate>Sun, 14 Dec 2025 23:03:28 +0000</pubDate><dc:creator>Zhenyu Li</dc:creator><dc:creator>Pengjie Xu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130756</prism:doi><description>Place recognition is a key task in robotics and artificial intelligence, especially for visual localization and navigation in difficult environments such as low-light or dynamic scenes. Many existing methods fail to capture reliable visual cues because of environmental changes and occlusions. To address this issue, we propose the Aggregated Quadruplet Pyramid Transformer (AQPT) for large-scale robot place recognition. AQPT employs a multi-scale attention mechanism to extract robust features at different resolutions. We further enhance these features with masked features, where parts of the image are intentionally hidden during training to simulate occlusions and improve resilience. The model is trained with a quadruplet loss, comparing an anchor with a positive match and two negatives, to achieve better feature separation and generalization. For efficient retrieval, we generate compact binary codes through hash coding and refine candidate matches using a Bayesian re-ranking module. Experiments on benchmark datasets and real-world scenarios show that AQPT outperforms existing methods, offering superior robustness and scalability. Our code is available at https://github.com/CV4RA/AQPT-VPR .
Published: 2025-12-14T23:03:28+00:00
Venue: Expert Systems with Applications
Score: 0.769 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenyu Li; Pengjie Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130756"&gt;10.1016/j.eswa.2025.130756&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (consider)&lt;/p&gt;
&lt;p&gt;Place recognition is a key task in robotics and artificial intelligence, especially for visual localization and navigation in difficult environments such as low-light or dynamic scenes. Many existing methods fail to capture reliable visual cues because of environmental changes and occlusions. To address this issue, we propose the Aggregated Quadruplet Pyramid Transformer (AQPT) for large-scale robot place recognition. AQPT employs a multi-scale attention mechanism to extract robust features at different resolutions. We further enhance these features with masked features, where parts of the image are intentionally hidden during training to simulate occlusions and improve resilience. The model is trained with a quadruplet loss, comparing an anchor with a positive match and two negatives, to achieve better feature separation and generalization. For efficient retrieval, we generate compact binary codes through hash coding and refine candidate matches using a Bayesian re-ranking module. Experiments on benchmark datasets and real-world scenarios show that AQPT outperforms existing methods, offering superior robustness and scalability. Our code is available at https://github.com/CV4RA/AQPT-VPR .&lt;/p&gt;</content:encoded></item></channel></rss>