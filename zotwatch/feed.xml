<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 04 Dec 2025 03:20:58 +0000</lastBuildDate><item><title>Constituency-Tree-Induced Vision-Language Alignment for Multimodal Large Language Models</title><link>https://doi.org/10.1109/tcsvt.2025.3639574</link><guid>10.1109/tcsvt.2025.3639574</guid><pubDate>Wed, 03 Dec 2025 18:44:30 +0000</pubDate><dc:creator>Yingchen Zhai</dc:creator><dc:creator>Ning Xu</dc:creator><dc:creator>Hongshuo Tian</dc:creator><dc:creator>Bolun Zheng</dc:creator><dc:creator>Chenggang Yan</dc:creator><dc:creator>Jinbo Cao</dc:creator><dc:creator>Rongbao Kang</dc:creator><dc:creator>An-An Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3639574</prism:doi><description>Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.
Published: 2025-12-03T18:44:30+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.840 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yingchen Zhai; Ning Xu; Hongshuo Tian; Bolun Zheng; Chenggang Yan; Jinbo Cao; Rongbao Kang; An-An Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3639574"&gt;10.1109/tcsvt.2025.3639574&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.840 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.&lt;/p&gt;</content:encoded></item><item><title>Evaluating SAM2 for Video Semantic Segmentation</title><link>https://arxiv.org/abs/2512.01774v1</link><guid>http://arxiv.org/abs/2512.01774v1</guid><pubDate>Mon, 01 Dec 2025 15:15:16 +0000</pubDate><dc:creator>Syed Hesham Syed Ariff</dc:creator><dc:creator>Yun Liu</dc:creator><dc:creator>Guolei Sun</dc:creator><dc:creator>Jing Yang</dc:creator><dc:creator>Henghui Ding</dc:creator><dc:creator>Xue Geng</dc:creator><dc:creator>Xudong Jiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.
Published: 2025-12-01T15:15:16+00:00
Venue: arXiv
Score: 0.837 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Syed Hesham Syed Ariff; Yun Liu; Guolei Sun; Jing Yang; Henghui Ding; Xue Geng; Xudong Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.837 (must_read)&lt;/p&gt;
&lt;p&gt;The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.&lt;/p&gt;</content:encoded></item><item><title>BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection</title><link>https://arxiv.org/abs/2512.02972v1</link><guid>http://arxiv.org/abs/2512.02972v1</guid><pubDate>Tue, 02 Dec 2025 17:50:33 +0000</pubDate><dc:creator>Guowen Zhang</dc:creator><dc:creator>Chenhang He</dc:creator><dc:creator>Liyi Chen</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.
Published: 2025-12-02T17:50:33+00:00
Venue: arXiv
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guowen Zhang; Chenhang He; Liyi Chen; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;Integrating LiDAR and camera information in the bird&amp;#x27;s eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.&lt;/p&gt;</content:encoded></item><item><title>Optical Context Compression Is Just (Bad) Autoencoding</title><link>https://arxiv.org/abs/2512.03643v1</link><guid>http://arxiv.org/abs/2512.03643v1</guid><pubDate>Wed, 03 Dec 2025 10:27:27 +0000</pubDate><dc:creator>Ivan Yee Lee</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Taylor Berg-Kirkpatrick</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding
Published: 2025-12-03T10:27:27+00:00
Venue: arXiv
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ivan Yee Lee; Cheng Yang; Taylor Berg-Kirkpatrick&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR&amp;#x27;s reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding&lt;/p&gt;</content:encoded></item><item><title>Unlocking Pseudolabel Potential and Alignment for Unpaired Cross-Modality Adaptation in Remote Sensing Image Segmentation</title><link>https://doi.org/10.1109/tnnls.2025.3635883</link><guid>10.1109/tnnls.2025.3635883</guid><pubDate>Tue, 02 Dec 2025 18:49:00 +0000</pubDate><dc:creator>Zhengyi Xu</dc:creator><dc:creator>Jie Geng</dc:creator><dc:creator>Wen Jiang</dc:creator><dc:creator>Shuai Song</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3635883</prism:doi><description>With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.
Published: 2025-12-02T18:49:00+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhengyi Xu; Jie Geng; Wen Jiang; Shuai Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3635883"&gt;10.1109/tnnls.2025.3635883&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.&lt;/p&gt;</content:encoded></item><item><title>CrossHypergraph: Consistent High-order Semantic Network for Few-shot Image Classification</title><link>https://doi.org/10.1109/tmm.2025.3639903</link><guid>10.1109/tmm.2025.3639903</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Yucheng Zhang</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Shuo Zhang</dc:creator><dc:creator>Biao Leng</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639903</prism:doi><description>Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet → ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yucheng Zhang; Hao Wang; Shuo Zhang; Biao Leng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639903"&gt;10.1109/tmm.2025.3639903&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet → ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.&lt;/p&gt;</content:encoded></item><item><title>MambaFusion: State-Space Model-Driven Object-Scene Fusion for Multi-Modal 3D Object Detection</title><link>https://doi.org/10.1016/j.patcog.2025.112820</link><guid>10.1016/j.patcog.2025.112820</guid><pubDate>Tue, 02 Dec 2025 00:17:20 +0000</pubDate><dc:creator>Tong Ning</dc:creator><dc:creator>Ke Lu</dc:creator><dc:creator>Xirui Jiang</dc:creator><dc:creator>Jian Xue</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112820</prism:doi><description>MambaFusion provides a hierarchical framework for highly efficient multi-modal 3D object detection. The method first achieves object-level fusion by integrating cross-modal object features. These fused features are then projected into the scene-level feature space to enable object-scene interaction, yielding the accurate 3D bounding boxes.
Existing multi-modal 3D detection struggles with geometric discrepancies between LiDAR/camera data and imbalanced feature alignment in Bird’s Eye View (BEV) space, where sparse foreground objects and scene-context gaps degrade performance. We propose MambaFusion, a novel framework unifying object-level fusion and scene-object interaction for robust 3D perception. Unlike scene-centric BEV fusion methods, MambaFusion introduces two modules: Object-Mamba, aligning 2D and 3D object candidates via grid-sorting and state-space models (SSM) to resolve modality inconsistencies, and Scene-Mamba, integrating image patches with object features and bidirectional SSM to model scene-object topological relationships. This dual-branch approach mitigates foreground-background imbalance and geometric misalignment while capturing holistic context. MambaFusion has achieved promising performance on both nuScenes and Waymo benchmarks.
Published: 2025-12-02T00:17:20+00:00
Venue: Pattern Recognition
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tong Ning; Ke Lu; Xirui Jiang; Jian Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112820"&gt;10.1016/j.patcog.2025.112820&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;MambaFusion provides a hierarchical framework for highly efficient multi-modal 3D object detection. The method first achieves object-level fusion by integrating cross-modal object features. These fused features are then projected into the scene-level feature space to enable object-scene interaction, yielding the accurate 3D bounding boxes.
Existing multi-modal 3D detection struggles with geometric discrepancies between LiDAR/camera data and imbalanced feature alignment in Bird’s Eye View (BEV) space, where sparse foreground objects and scene-context gaps degrade performance. We propose MambaFusion, a novel framework unifying object-level fusion and scene-object interaction for robust 3D perception. Unlike scene-centric BEV fusion methods, MambaFusion introduces two modules: Object-Mamba, aligning 2D and 3D object candidates via grid-sorting and state-space models (SSM) to resolve modality inconsistencies, and Scene-Mamba, integrating image patches with object features and bidirectional SSM to model scene-object topological relationships. This dual-branch approach mitigates foreground-background imbalance and geometric misalignment while capturing holistic context. MambaFusion has achieved promising performance on both nuScenes and Waymo benchmarks.&lt;/p&gt;</content:encoded></item><item><title>TranSTD: A Wavelet-Driven Transformer-Based SAR Target Detection Framework With Adaptive Feature Enhancement and Fusion</title><link>https://doi.org/10.1109/jstars.2025.3639785</link><guid>10.1109/jstars.2025.3639785</guid><pubDate>Wed, 03 Dec 2025 18:42:40 +0000</pubDate><dc:creator>Bobo Xi</dc:creator><dc:creator>Jiaqi Chen</dc:creator><dc:creator>Yan Huang</dc:creator><dc:creator>Jiaojiao Li</dc:creator><dc:creator>Yunsong Li</dc:creator><dc:creator>Zan Li</dc:creator><dc:creator>Xiang-Gen Xia</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3639785</prism:doi><description>Target detection in Synthetic Aperture Radar (SAR) images is of great importance in civilian monitoring and military reconnaissance. However, the unique speckle noise inherent in SAR images leads to semantic information loss, while traditional CNN downsampling methods exacerbate this issue, impacting detection accuracy and robustness. Moreover, some dense target scenarios and weak scattering features of targets make it challenging to achieve sufficient feature discriminability, adding complexity to the detection task. Additionally, the multi-scale characteristic of SAR targets presents difficulties in balancing detection performance with computational efficiency in complex scenes. To tackle these difficulties, this paper introduces a wavelet-driven transformer-based SAR target detection framework called TranSTD. Specifically, it incorporates the Haar wavelet dynamic downsampling (HWDD) and semantic preserving dynamic downsampling (SPDD) modules, which effectively suppress noise and preserve semantic information using techniques such as Haar wavelet denoise (HW Denoise) and input-driven dynamic pooling downsampling (IDPD). Furthermore, the SAR adaptive convolution bottleneck (SAC Bottleneck) is proposed for enhancing the discrimination of features. To optimize performance and efficiency across varying scene complexities, a multiscale SAR attention fusion encoder (MSAF Encoder) is developed. Extensive experiments are carried out on three datasets, showing that our proposed algorithm outperforms the current state-of-the-art benchmarks in SAR target detection, offering a robust solution for the detection of targets in complex SAR scenes.
Published: 2025-12-03T18:42:40+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bobo Xi; Jiaqi Chen; Yan Huang; Jiaojiao Li; Yunsong Li; Zan Li; Xiang-Gen Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3639785"&gt;10.1109/jstars.2025.3639785&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Target detection in Synthetic Aperture Radar (SAR) images is of great importance in civilian monitoring and military reconnaissance. However, the unique speckle noise inherent in SAR images leads to semantic information loss, while traditional CNN downsampling methods exacerbate this issue, impacting detection accuracy and robustness. Moreover, some dense target scenarios and weak scattering features of targets make it challenging to achieve sufficient feature discriminability, adding complexity to the detection task. Additionally, the multi-scale characteristic of SAR targets presents difficulties in balancing detection performance with computational efficiency in complex scenes. To tackle these difficulties, this paper introduces a wavelet-driven transformer-based SAR target detection framework called TranSTD. Specifically, it incorporates the Haar wavelet dynamic downsampling (HWDD) and semantic preserving dynamic downsampling (SPDD) modules, which effectively suppress noise and preserve semantic information using techniques such as Haar wavelet denoise (HW Denoise) and input-driven dynamic pooling downsampling (IDPD). Furthermore, the SAR adaptive convolution bottleneck (SAC Bottleneck) is proposed for enhancing the discrimination of features. To optimize performance and efficiency across varying scene complexities, a multiscale SAR attention fusion encoder (MSAF Encoder) is developed. Extensive experiments are carried out on three datasets, showing that our proposed algorithm outperforms the current state-of-the-art benchmarks in SAR target detection, offering a robust solution for the detection of targets in complex SAR scenes.&lt;/p&gt;</content:encoded></item><item><title>Adaptive iterative retrieval for enhanced retrieval-augmented generation</title><link>https://doi.org/10.1016/j.neucom.2025.132272</link><guid>10.1016/j.neucom.2025.132272</guid><pubDate>Tue, 02 Dec 2025 07:52:56 +0000</pubDate><dc:creator>Wenhan Han</dc:creator><dc:creator>Xiao Xiao</dc:creator><dc:creator>Yaohang Li</dc:creator><dc:creator>Jun Wang</dc:creator><dc:creator>Mykola Pechenizkiy</dc:creator><dc:creator>Meng Fang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132272</prism:doi><description>Existing retrieval-augmented generation (RAG) methods often treat retrieval as a one-off operation, yet recent work suggests that iteratively refining the retrieval step can yield substantial gains in relevance and downstream generation quality. However, prior iterative-retrieval approaches typically optimize only the retriever’s ranking function or only post-hoc document refinement, and they require expensive retriever retraining or complex multi-stage pipelines. To address these challenges, we propose Adaptive Iterative Retrieval for Retrieval-Augmented Generation (AIR-RAG), an adaptive, iterative retrieval framework designed to optimize both document relevance and LLM alignment within the RAG pipeline. By leveraging adaptive feedback, AIR-RAG simultaneously enhances retrieval ranking and document refinement across multiple iterations, eliminating the need for complex retraining pipelines and enabling seamless integration with existing systems. In extensive evaluations against state-of-the-art RAG methods across several benchmark datasets including TriviaQA, PopQA, HotpotQA, WikiMultiHop, PubHealth, and StrategyQA, AIR-RAG consistently demonstrates superior performance, underscoring its effectiveness in enhancing retrieval-augmented generation systems. Our code and data are available anonymously at https://github.com/aialt/AIR-RAG .
Published: 2025-12-02T07:52:56+00:00
Venue: Neurocomputing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenhan Han; Xiao Xiao; Yaohang Li; Jun Wang; Mykola Pechenizkiy; Meng Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132272"&gt;10.1016/j.neucom.2025.132272&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Existing retrieval-augmented generation (RAG) methods often treat retrieval as a one-off operation, yet recent work suggests that iteratively refining the retrieval step can yield substantial gains in relevance and downstream generation quality. However, prior iterative-retrieval approaches typically optimize only the retriever’s ranking function or only post-hoc document refinement, and they require expensive retriever retraining or complex multi-stage pipelines. To address these challenges, we propose Adaptive Iterative Retrieval for Retrieval-Augmented Generation (AIR-RAG), an adaptive, iterative retrieval framework designed to optimize both document relevance and LLM alignment within the RAG pipeline. By leveraging adaptive feedback, AIR-RAG simultaneously enhances retrieval ranking and document refinement across multiple iterations, eliminating the need for complex retraining pipelines and enabling seamless integration with existing systems. In extensive evaluations against state-of-the-art RAG methods across several benchmark datasets including TriviaQA, PopQA, HotpotQA, WikiMultiHop, PubHealth, and StrategyQA, AIR-RAG consistently demonstrates superior performance, underscoring its effectiveness in enhancing retrieval-augmented generation systems. Our code and data are available anonymously at https://github.com/aialt/AIR-RAG .&lt;/p&gt;</content:encoded></item><item><title>ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers</title><link>https://arxiv.org/abs/2512.03673v1</link><guid>http://arxiv.org/abs/2512.03673v1</guid><pubDate>Wed, 03 Dec 2025 11:02:16 +0000</pubDate><dc:creator>Feice Huang</dc:creator><dc:creator>Zuliang Han</dc:creator><dc:creator>Xing Zhou</dc:creator><dc:creator>Yihuang Chen</dc:creator><dc:creator>Lifei Zhu</dc:creator><dc:creator>Haoqian Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.
Published: 2025-12-03T11:02:16+00:00
Venue: arXiv
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feice Huang; Zuliang Han; Xing Zhou; Yihuang Chen; Lifei Zhu; Haoqian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.&lt;/p&gt;</content:encoded></item><item><title>Balanced Multi-modality Knowledge Mining for RGB-Infrared Object Detection</title><link>https://doi.org/10.1016/j.neunet.2025.108421</link><guid>10.1016/j.neunet.2025.108421</guid><pubDate>Tue, 02 Dec 2025 07:52:46 +0000</pubDate><dc:creator>You Ma</dc:creator><dc:creator>Yucheng Zhang</dc:creator><dc:creator>Shihan Mao</dc:creator><dc:creator>Lin Chai</dc:creator><dc:creator>Qingling Wang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108421</prism:doi><description>RGB-Infrared object detection aims to fuse the complementary information of two modalities to improve the accuracy and robustness of the detector. Given the advantages of transformer in modeling long-range dependencies, transformer-based cross-modality fusion methods have been continuously proposed and achieved satisfactory results. However, existing methods face two major challenges: 1) it is difficult to balance the mining of intra-modality specific knowledge and inter-modality complementary knowledge; 2) a single attention layer only models the relationship between token features of the same receptive field, thus failing to capture the intrinsic relationship between objects at different scales and lacking the ability to focus on both local and global information. To this end, we propose a balanced multi-modality knowledge mining method. Specifically, we design a dual attention knowledge mining (DAKM) module, which explicitly mines intra- and inter-modality key knowledge through self-attention and cross-attention, respectively. In addition, we introduce multi-scale information into the attention layer of DAKM, which not only extracts multi-scale object features but also retains both local and global information. Then, we fuse the intra- and inter-modality features obtained by DAKM using the scene-aware adaptive interaction module. The module employs differential and scene information to focus on object-related feature fusion. Finally, the cross-layer feature refinement module is utilized to aggregate different fusion layers to further enhance the feature representation. Extensive experiments in multiple scenes demonstrate that our method outperforms existing state-of-the-art RGB-Infrared object detection methods.
Published: 2025-12-02T07:52:46+00:00
Venue: Neural Networks
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; You Ma; Yucheng Zhang; Shihan Mao; Lin Chai; Qingling Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108421"&gt;10.1016/j.neunet.2025.108421&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;RGB-Infrared object detection aims to fuse the complementary information of two modalities to improve the accuracy and robustness of the detector. Given the advantages of transformer in modeling long-range dependencies, transformer-based cross-modality fusion methods have been continuously proposed and achieved satisfactory results. However, existing methods face two major challenges: 1) it is difficult to balance the mining of intra-modality specific knowledge and inter-modality complementary knowledge; 2) a single attention layer only models the relationship between token features of the same receptive field, thus failing to capture the intrinsic relationship between objects at different scales and lacking the ability to focus on both local and global information. To this end, we propose a balanced multi-modality knowledge mining method. Specifically, we design a dual attention knowledge mining (DAKM) module, which explicitly mines intra- and inter-modality key knowledge through self-attention and cross-attention, respectively. In addition, we introduce multi-scale information into the attention layer of DAKM, which not only extracts multi-scale object features but also retains both local and global information. Then, we fuse the intra- and inter-modality features obtained by DAKM using the scene-aware adaptive interaction module. The module employs differential and scene information to focus on object-related feature fusion. Finally, the cross-layer feature refinement module is utilized to aggregate different fusion layers to further enhance the feature representation. Extensive experiments in multiple scenes demonstrate that our method outperforms existing state-of-the-art RGB-Infrared object detection methods.&lt;/p&gt;</content:encoded></item><item><title>TransGOP-R: Transformer-based Real-World Gaze Object Prediction</title><link>https://doi.org/10.1109/tmm.2025.3639891</link><guid>10.1109/tmm.2025.3639891</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Guangyu Guo</dc:creator><dc:creator>Chenxi Guo</dc:creator><dc:creator>Zhaozhong Wang</dc:creator><dc:creator>Binglu Wang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639891</prism:doi><description>The goal of gaze object prediction (GOP) is to predict human gaze objects and categories. However, existing methods require additional head priors or filter the results before evaluation, which is an obstacle for real-world applications. To this end, this paper proposes a Transformer-based Gaze Object Prediction under Real-world setting (TransGOP-R), which does not rely on any head prior input and evaluates end-to-end. We first design a head location module to generate human head location information from a head query. Then, an error analysis demonstrates that the primary error source of the existing GOP model is in gaze estimation, which is caused by the difficulty in predicting gaze points by directly regressing heatmaps. Therefore, we introduce cone prediction into the model training stage, allowing the middle-layer features of the gaze regressor to build the relationship between the target human and objects before regressing the gaze point. An oriented gradient mechanism is proposed in this process to ensure the object detection performance is not affected by cone information. Finally, we conducted very detailed and sufficient experiments to verify the superiority of our method on the GOO-Synth and GOO-Real datasets. At the same time, we also achieve advantages compared to the human-target gaze estimation methods on the GazeFollowing, VideoAttentionTarget, and ChildPlay datasets.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guangyu Guo; Chenxi Guo; Zhaozhong Wang; Binglu Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639891"&gt;10.1109/tmm.2025.3639891&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;The goal of gaze object prediction (GOP) is to predict human gaze objects and categories. However, existing methods require additional head priors or filter the results before evaluation, which is an obstacle for real-world applications. To this end, this paper proposes a Transformer-based Gaze Object Prediction under Real-world setting (TransGOP-R), which does not rely on any head prior input and evaluates end-to-end. We first design a head location module to generate human head location information from a head query. Then, an error analysis demonstrates that the primary error source of the existing GOP model is in gaze estimation, which is caused by the difficulty in predicting gaze points by directly regressing heatmaps. Therefore, we introduce cone prediction into the model training stage, allowing the middle-layer features of the gaze regressor to build the relationship between the target human and objects before regressing the gaze point. An oriented gradient mechanism is proposed in this process to ensure the object detection performance is not affected by cone information. Finally, we conducted very detailed and sufficient experiments to verify the superiority of our method on the GOO-Synth and GOO-Real datasets. At the same time, we also achieve advantages compared to the human-target gaze estimation methods on the GazeFollowing, VideoAttentionTarget, and ChildPlay datasets.&lt;/p&gt;</content:encoded></item><item><title>UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking</title><link>https://arxiv.org/abs/2512.02668v1</link><guid>http://arxiv.org/abs/2512.02668v1</guid><pubDate>Tue, 02 Dec 2025 11:47:13 +0000</pubDate><dc:creator>Qionglin Ren</dc:creator><dc:creator>Dawei Zhang</dc:creator><dc:creator>Chunxu Tian</dc:creator><dc:creator>Dan Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.
Published: 2025-12-02T11:47:13+00:00
Venue: arXiv
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qionglin Ren; Dawei Zhang; Chunxu Tian; Dan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.&lt;/p&gt;</content:encoded></item><item><title>FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention</title><link>https://arxiv.org/abs/2512.01540v1</link><guid>http://arxiv.org/abs/2512.01540v1</guid><pubDate>Mon, 01 Dec 2025 11:12:37 +0000</pubDate><dc:creator>Zipeng Wang</dc:creator><dc:creator>Dan Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.
Published: 2025-12-01T11:12:37+00:00
Venue: arXiv
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zipeng Wang; Dan Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.&lt;/p&gt;</content:encoded></item><item><title>A Fusion-Enhanced Network for Infrared and Visible High-Level Vision Tasks</title><link>https://doi.org/10.1109/tmm.2025.3640020</link><guid>10.1109/tmm.2025.3640020</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Fangcen Liu</dc:creator><dc:creator>Chenqiang Gao</dc:creator><dc:creator>Fang Chen</dc:creator><dc:creator>Pengcheng Li</dc:creator><dc:creator>Junjie Guo</dc:creator><dc:creator>Deyu Meng</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3640020</prism:doi><description>Infrared and visible dual-modality vision tasks such as semantic segmentation, object detection, and salient object detection can achieve robust performance even in extreme scenes by leveraging complementary information. However, most existing image fusion-based methods and task-specific frameworks exhibit limited generalization across multiple tasks. Moreover, summing the general representations obtained from foundation models poses challenges, including insufficient semantic information mining and feature fusion. In this paper, we propose a fusion-enhanced network, which effectively enriches semantic information and integrates features based on the complementary characteristics of infrared and visible modalities. The proposed network can extend to high-level vision tasks, showing strong generalization capabilities. Firstly, we adopt the infrared and visible foundation models to extract the general representations. Then, to enrich the semantic information of these general representations for high-level vision tasks, we design the feature enhancement module and the token enhancement module for feature maps and tokens, respectively. Besides, the attention-guided fusion module is proposed for effective fusion by exploring the complementary information of two modalities. Moreover, we adopt the cutout&amp;mix augmentation strategy to conduct the data augmentation, which further improves the ability of the model to mine the regional complementarity between the two modalities. Extensive experiments show that the proposed method outperforms state-of-the-art dual-modality methods in the semantic segmentation, object detection, and salient object detection tasks.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fangcen Liu; Chenqiang Gao; Fang Chen; Pengcheng Li; Junjie Guo; Deyu Meng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3640020"&gt;10.1109/tmm.2025.3640020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared and visible dual-modality vision tasks such as semantic segmentation, object detection, and salient object detection can achieve robust performance even in extreme scenes by leveraging complementary information. However, most existing image fusion-based methods and task-specific frameworks exhibit limited generalization across multiple tasks. Moreover, summing the general representations obtained from foundation models poses challenges, including insufficient semantic information mining and feature fusion. In this paper, we propose a fusion-enhanced network, which effectively enriches semantic information and integrates features based on the complementary characteristics of infrared and visible modalities. The proposed network can extend to high-level vision tasks, showing strong generalization capabilities. Firstly, we adopt the infrared and visible foundation models to extract the general representations. Then, to enrich the semantic information of these general representations for high-level vision tasks, we design the feature enhancement module and the token enhancement module for feature maps and tokens, respectively. Besides, the attention-guided fusion module is proposed for effective fusion by exploring the complementary information of two modalities. Moreover, we adopt the cutout&amp;amp;mix augmentation strategy to conduct the data augmentation, which further improves the ability of the model to mine the regional complementarity between the two modalities. Extensive experiments show that the proposed method outperforms state-of-the-art dual-modality methods in the semantic segmentation, object detection, and salient object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>Accelerating Long-Context Inference of Large Language Models via Dynamic Attention Load Balancing</title><link>https://doi.org/10.1016/j.knosys.2025.115018</link><guid>10.1016/j.knosys.2025.115018</guid><pubDate>Wed, 03 Dec 2025 00:20:34 +0000</pubDate><dc:creator>Jie Ou</dc:creator><dc:creator>Jinyu Guo</dc:creator><dc:creator>Shuaihong Jiang</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Ruini Xue</dc:creator><dc:creator>Wenhong Tian</dc:creator><dc:creator>Rajkumar Buyya</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115018</prism:doi><description>Large language models (LLMs) have demonstrated exceptional performance across various natural language processing tasks. However, their quadratic complexity of attention mechanisms results in inefficiency during long-context inference. Although existing research has employed sparse attention techniques to enhance LLM efficiency in long-context scenarios, we observe that these methods introduce heterogeneous attention computation patterns across different heads, leading to GPU load imbalance and resource idling during practical deployments. To address this challenge, we propose FlexAttn, a novel inference framework that dynamically generates attention load balancing strategies tailored to input context lengths. Our framework enhances resource utilization during long-context prefilling by scheduling attention heads within each layer according to the searched strategies. Specifically, FlexAttn first conducts head-level profiling to collect computational characteristics and then searches for a load balancing strategy based on the current context length and profiling data. To minimize runtime overhead, we partition and reorganize the weights before inference execution. Furthermore, as the computational overhead is considerably larger than the I/O overhead in long-context inference, we employ a cross-prefetch strategy for each transformer layer to enhance efficiency. Extensive experiments demonstrate that when applied to state-of-the-art long-context techniques, our framework achieves a throughput improvement of 34.95% to 40.9% on LLaMA3-8B across context lengths ranging from 160k to 768k tokens. Notably, our proposed approach remains orthogonal to conventional model parallelism and sparse attention techniques, enabling complementary performance enhancements when integrated with existing accelerating methods.
Published: 2025-12-03T00:20:34+00:00
Venue: Knowledge-Based Systems
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Ou; Jinyu Guo; Shuaihong Jiang; Xu Li; Ruini Xue; Wenhong Tian; Rajkumar Buyya&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115018"&gt;10.1016/j.knosys.2025.115018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) have demonstrated exceptional performance across various natural language processing tasks. However, their quadratic complexity of attention mechanisms results in inefficiency during long-context inference. Although existing research has employed sparse attention techniques to enhance LLM efficiency in long-context scenarios, we observe that these methods introduce heterogeneous attention computation patterns across different heads, leading to GPU load imbalance and resource idling during practical deployments. To address this challenge, we propose FlexAttn, a novel inference framework that dynamically generates attention load balancing strategies tailored to input context lengths. Our framework enhances resource utilization during long-context prefilling by scheduling attention heads within each layer according to the searched strategies. Specifically, FlexAttn first conducts head-level profiling to collect computational characteristics and then searches for a load balancing strategy based on the current context length and profiling data. To minimize runtime overhead, we partition and reorganize the weights before inference execution. Furthermore, as the computational overhead is considerably larger than the I/O overhead in long-context inference, we employ a cross-prefetch strategy for each transformer layer to enhance efficiency. Extensive experiments demonstrate that when applied to state-of-the-art long-context techniques, our framework achieves a throughput improvement of 34.95% to 40.9% on LLaMA3-8B across context lengths ranging from 160k to 768k tokens. Notably, our proposed approach remains orthogonal to conventional model parallelism and sparse attention techniques, enabling complementary performance enhancements when integrated with existing accelerating methods.&lt;/p&gt;</content:encoded></item><item><title>MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms</title><link>https://arxiv.org/abs/2512.03640v1</link><guid>http://arxiv.org/abs/2512.03640v1</guid><pubDate>Wed, 03 Dec 2025 10:22:27 +0000</pubDate><dc:creator>Jiahao Zhang</dc:creator><dc:creator>Xiao Zhao</dc:creator><dc:creator>Guangyu Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1007/978-981-96-2061-6_29</prism:doi><description>Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.
Published: 2025-12-03T10:22:27+00:00
Venue: arXiv
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Zhang; Xiao Zhao; Guangyu Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/978-981-96-2061-6_29"&gt;10.1007/978-981-96-2061-6_29&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network&amp;#x27;s ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet&amp;#x27;s superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.&lt;/p&gt;</content:encoded></item><item><title>ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning</title><link>https://arxiv.org/abs/2512.00831v1</link><guid>http://arxiv.org/abs/2512.00831v1</guid><pubDate>Sun, 30 Nov 2025 10:39:53 +0000</pubDate><dc:creator>Yuchen Zeng</dc:creator><dc:creator>Shuibai Zhang</dc:creator><dc:creator>Wonjun Kang</dc:creator><dc:creator>Shutong Wu</dc:creator><dc:creator>Lynnix Zou</dc:creator><dc:creator>Ying Fan</dc:creator><dc:creator>Heeju Kim</dc:creator><dc:creator>Ziqian Lin</dc:creator><dc:creator>Jungtaek Kim</dc:creator><dc:creator>Hyung Il Koo</dc:creator><dc:creator>Dimitris Papailiopoulos</dc:creator><dc:creator>Kangwook Lee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning "algorithms" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.
Published: 2025-11-30T10:39:53+00:00
Venue: arXiv
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchen Zeng; Shuibai Zhang; Wonjun Kang; Shutong Wu; Lynnix Zou; Ying Fan; Heeju Kim; Ziqian Lin; Jungtaek Kim; Hyung Il Koo; Dimitris Papailiopoulos; Kangwook Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning &amp;quot;algorithms&amp;quot; remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.&lt;/p&gt;</content:encoded></item><item><title>GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection</title><link>https://arxiv.org/abs/2512.02991v1</link><guid>http://arxiv.org/abs/2512.02991v1</guid><pubDate>Tue, 02 Dec 2025 18:05:02 +0000</pubDate><dc:creator>Md Sohag Mia</dc:creator><dc:creator>Md Nahid Hasan</dc:creator><dc:creator>Tawhid Ahmed</dc:creator><dc:creator>Muhammad Abdullah Adnan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.
Published: 2025-12-02T18:05:02+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Md Sohag Mia; Md Nahid Hasan; Tawhid Ahmed; Muhammad Abdullah Adnan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.&lt;/p&gt;</content:encoded></item><item><title>Difference Decomposition Networks for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2512.03470v1</link><guid>http://arxiv.org/abs/2512.03470v1</guid><pubDate>Wed, 03 Dec 2025 05:52:06 +0000</pubDate><dc:creator>Chen Hu</dc:creator><dc:creator>Mingyu Zhou</dc:creator><dc:creator>Shuai Yuan</dc:creator><dc:creator>Hongbo Hu</dc:creator><dc:creator>Xiangyu Qiu</dc:creator><dc:creator>Junhai Luo</dc:creator><dc:creator>Tian Pu</dc:creator><dc:creator>Xiyin Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.
Published: 2025-12-03T05:52:06+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Hu; Mingyu Zhou; Shuai Yuan; Hongbo Hu; Xiangyu Qiu; Junhai Luo; Tian Pu; Xiyin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.&lt;/p&gt;</content:encoded></item><item><title>Enhancing multi-label zero-shot learning with dual-contrastive image-text alignment</title><link>https://doi.org/10.1016/j.neucom.2025.132251</link><guid>10.1016/j.neucom.2025.132251</guid><pubDate>Tue, 02 Dec 2025 00:22:55 +0000</pubDate><dc:creator>Zhongchen Ma</dc:creator><dc:creator>Junjie Yang</dc:creator><dc:creator>Ahmed Belloul</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132251</prism:doi><description>Prompt learning has emerged as a prevalent strategy for adapting vision-language models like CLIP to multi-label zero-shot learning (ML-ZSL). However, these methods primarily rely on global image-text alignment, lacking the fine-grained mechanisms necessary to link specific image regions with their textual counterparts, which is crucial for complex multi-label scenes. To address these issues, we propose a unified framework that integrates three key components: a Dual Contrastive Alignment (DCA) regularization, a Multi-Granularity Data Augmentation (MGDA) strategy, and a Cross-Attention Alignment Module (CAM). The DCA regularization introduces two complementary constraints—Contrastive Image Content (CIC) and Contrastive Text Content (CTC)—to enhance both image-to-text and text-to-image alignment through mutual contrastive learning. The MGDA strategy synthesizes composite images and unified label sets to enrich supervisory signals and improve feature discriminability. The CAM module leverages cross-modal attention to dynamically focus on relevant image regions guided by text embeddings, ensuring precise local alignment. Extensive experiments on NUS-WIDE and MS-COCO datasets demonstrate that our approach achieves state-of-the-art performance, with mAP improvements of 3.1 % and 6.8 %, respectively, over previous best results. These advancements underscore the effectiveness of our method in enhancing fine-grained visual-textual alignment and facilitating robust multi-label zero-shot recognition.
Published: 2025-12-02T00:22:55+00:00
Venue: Neurocomputing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhongchen Ma; Junjie Yang; Ahmed Belloul&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132251"&gt;10.1016/j.neucom.2025.132251&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Prompt learning has emerged as a prevalent strategy for adapting vision-language models like CLIP to multi-label zero-shot learning (ML-ZSL). However, these methods primarily rely on global image-text alignment, lacking the fine-grained mechanisms necessary to link specific image regions with their textual counterparts, which is crucial for complex multi-label scenes. To address these issues, we propose a unified framework that integrates three key components: a Dual Contrastive Alignment (DCA) regularization, a Multi-Granularity Data Augmentation (MGDA) strategy, and a Cross-Attention Alignment Module (CAM). The DCA regularization introduces two complementary constraints—Contrastive Image Content (CIC) and Contrastive Text Content (CTC)—to enhance both image-to-text and text-to-image alignment through mutual contrastive learning. The MGDA strategy synthesizes composite images and unified label sets to enrich supervisory signals and improve feature discriminability. The CAM module leverages cross-modal attention to dynamically focus on relevant image regions guided by text embeddings, ensuring precise local alignment. Extensive experiments on NUS-WIDE and MS-COCO datasets demonstrate that our approach achieves state-of-the-art performance, with mAP improvements of 3.1 % and 6.8 %, respectively, over previous best results. These advancements underscore the effectiveness of our method in enhancing fine-grained visual-textual alignment and facilitating robust multi-label zero-shot recognition.&lt;/p&gt;</content:encoded></item><item><title>ICDSR: Integrated Conditional Diffusion Model for Single Image Super-Resolution</title><link>https://doi.org/10.1109/tmm.2025.3639910</link><guid>10.1109/tmm.2025.3639910</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Cong Hu</dc:creator><dc:creator>Xiao-Zhong Wei</dc:creator><dc:creator>Xiao-Jun Wu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639910</prism:doi><description>Diffusion Probabilistic Models (DPMs) have recently demonstrated considerable potential for single image super-resolution (SISR) by utilizing a conditional generation process that transforms Gaussian noise into high-resolution (HR) images based on low-resolution (LR) inputs. Current Image-Conditional DPMs (icDPMs) have demonstrated promising results by leveraging LR images as a condition to guide the generation of HR images. However, icDPMs fail to effectively integrate LR images and other conditional information to generate accurate and natural output. To address this issue, we propose an Integrated Conditional Diffusion Model for Single Image Super-Resolution (ICDSR). Our approach encodes the LR image as a condition to generate the prior feature, simultaneously integrating it with timestep information to establish intermediate constraints. To further enhance these constraints, we designed a multi-scale guidance structure for the U-shaped concatenation of the diffusion model during the integration of conditions. This constraint serves as multi-scale guidance specifically designed for the U-shaped concatenation of the diffusion model during the integration of conditions. Specifically, multi-scale integrated information is injected into the diffusion model basic block, informing about the coarse structure of the sharp image at the intermediate layers with spatially adaptive conditions. Additionally, ICDSR employs a lightweight U-Net to provide initial guidance and leverages the diffusion model to learn residual guidance for faster convergence. Extensive experiments on facial and general benchmarks, including the CelebA and DIV2K datasets, demonstrate that ICDSR surpasses existing methods, achieving state-of-the-art perceptual quality while maintaining competitive distortion metrics.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cong Hu; Xiao-Zhong Wei; Xiao-Jun Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639910"&gt;10.1109/tmm.2025.3639910&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion Probabilistic Models (DPMs) have recently demonstrated considerable potential for single image super-resolution (SISR) by utilizing a conditional generation process that transforms Gaussian noise into high-resolution (HR) images based on low-resolution (LR) inputs. Current Image-Conditional DPMs (icDPMs) have demonstrated promising results by leveraging LR images as a condition to guide the generation of HR images. However, icDPMs fail to effectively integrate LR images and other conditional information to generate accurate and natural output. To address this issue, we propose an Integrated Conditional Diffusion Model for Single Image Super-Resolution (ICDSR). Our approach encodes the LR image as a condition to generate the prior feature, simultaneously integrating it with timestep information to establish intermediate constraints. To further enhance these constraints, we designed a multi-scale guidance structure for the U-shaped concatenation of the diffusion model during the integration of conditions. This constraint serves as multi-scale guidance specifically designed for the U-shaped concatenation of the diffusion model during the integration of conditions. Specifically, multi-scale integrated information is injected into the diffusion model basic block, informing about the coarse structure of the sharp image at the intermediate layers with spatially adaptive conditions. Additionally, ICDSR employs a lightweight U-Net to provide initial guidance and leverages the diffusion model to learn residual guidance for faster convergence. Extensive experiments on facial and general benchmarks, including the CelebA and DIV2K datasets, demonstrate that ICDSR surpasses existing methods, achieving state-of-the-art perceptual quality while maintaining competitive distortion metrics.&lt;/p&gt;</content:encoded></item><item><title>ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection</title><link>https://arxiv.org/abs/2512.02696v1</link><guid>http://arxiv.org/abs/2512.02696v1</guid><pubDate>Tue, 02 Dec 2025 12:28:07 +0000</pubDate><dc:creator>Omid Reza Heidari</dc:creator><dc:creator>Yang Wang</dc:creator><dc:creator>Xinxin Zuo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.
Published: 2025-12-02T12:28:07+00:00
Venue: arXiv
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Omid Reza Heidari; Yang Wang; Xinxin Zuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.&lt;/p&gt;</content:encoded></item><item><title>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</title><link>https://arxiv.org/abs/2512.03004v1</link><guid>http://arxiv.org/abs/2512.03004v1</guid><pubDate>Tue, 02 Dec 2025 18:29:18 +0000</pubDate><dc:creator>Xiaoxue Chen</dc:creator><dc:creator>Ziyi Xiong</dc:creator><dc:creator>Yuantao Chen</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Nan Wang</dc:creator><dc:creator>Hongcheng Luo</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Haiyang Sun</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Guang Chen</dc:creator><dc:creator>Hangjun Ye</dc:creator><dc:creator>Hongyang Li</dc:creator><dc:creator>Ya-Qin Zhang</dc:creator><dc:creator>Hao Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.
Published: 2025-12-02T18:29:18+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxue Chen; Ziyi Xiong; Yuantao Chen; Gen Li; Nan Wang; Hongcheng Luo; Long Chen; Haiyang Sun; Bing Wang; Guang Chen; Hangjun Ye; Hongyang Li; Ya-Qin Zhang; Hao Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.&lt;/p&gt;</content:encoded></item><item><title>Exploring Cross-Modal Mutual Prompt Learning for Video Quality Assessment</title><link>https://doi.org/10.1109/tmm.2025.3639890</link><guid>10.1109/tmm.2025.3639890</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Pengfei Chen</dc:creator><dc:creator>Leida Li</dc:creator><dc:creator>Jinjian Wu</dc:creator><dc:creator>Jiebin Yan</dc:creator><dc:creator>Vinit Jakhetiya</dc:creator><dc:creator>Aladine Chetouani</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639890</prism:doi><description>Enhancing video quality assessment (VQA) through semantic information integration is a critical research focus. Recent research has employed the Contrastive Language-Image Pre-training (CLIP) model as a foundation to improve semantic perception. However, the image-text alignment inherent in these pre-trained Vision-Language (VL) models frequently results in suboptimal VQA performance. While prompt engineering has recently targeted the language component to address this alignment issue, the unique insights resided in visual analysis is still overlooked for further advancing VQA tasks. Additionally, seeking a trade-off between quality separability and domain invariance in VQA remains largely unresolved within the VL paradigm. In this paper, we introduce a novel cross-modal prompt-based approach to tackle these challenges. Specifically, we propose learnable prompts within the vision branch to foster synergy between visual and language modalities through a language-to-vision coupling function. The multi-view backbone is then carefully crafted with content enhancement and distortion-aware temporal modulation to ensure quality separability. The language prompts, derived from visual representations, are further supported by adaptive weighting mechanisms to optimize the balance between quality separability and domain invariance. Experimental results demonstrate the effectiveness of our proposed method over leading VQA models, showing significant improvements in generalization across diverse datasets. The source code for this work is publicly available at https://github.com/cpf0079/CM2PL.
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pengfei Chen; Leida Li; Jinjian Wu; Jiebin Yan; Vinit Jakhetiya; Aladine Chetouani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639890"&gt;10.1109/tmm.2025.3639890&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Enhancing video quality assessment (VQA) through semantic information integration is a critical research focus. Recent research has employed the Contrastive Language-Image Pre-training (CLIP) model as a foundation to improve semantic perception. However, the image-text alignment inherent in these pre-trained Vision-Language (VL) models frequently results in suboptimal VQA performance. While prompt engineering has recently targeted the language component to address this alignment issue, the unique insights resided in visual analysis is still overlooked for further advancing VQA tasks. Additionally, seeking a trade-off between quality separability and domain invariance in VQA remains largely unresolved within the VL paradigm. In this paper, we introduce a novel cross-modal prompt-based approach to tackle these challenges. Specifically, we propose learnable prompts within the vision branch to foster synergy between visual and language modalities through a language-to-vision coupling function. The multi-view backbone is then carefully crafted with content enhancement and distortion-aware temporal modulation to ensure quality separability. The language prompts, derived from visual representations, are further supported by adaptive weighting mechanisms to optimize the balance between quality separability and domain invariance. Experimental results demonstrate the effectiveness of our proposed method over leading VQA models, showing significant improvements in generalization across diverse datasets. The source code for this work is publicly available at https://github.com/cpf0079/CM2PL.&lt;/p&gt;</content:encoded></item><item><title>StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for Superior 3D Gaussian Splatting</title><link>https://doi.org/10.1109/tmm.2025.3639991</link><guid>10.1109/tmm.2025.3639991</guid><pubDate>Wed, 03 Dec 2025 18:43:18 +0000</pubDate><dc:creator>Zexu Huang</dc:creator><dc:creator>Min Xu</dc:creator><dc:creator>Stuart Perry</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639991</prism:doi><description>Recent advancements in 3D reconstruction coupled with neural rendering techniques have greatly improved the creation of photo-realistic 3D scenes, influencing both academic research and industry applications. The technique of 3D Gaussian Splatting and its variants incorporate the strengths of both primitive-based and volumetric representations, achieving superior rendering quality. While 3D Geometric Scattering (3DGS) and its variants have advanced the field of 3D representation, they fall short in capturing the stochastic properties of non-local structural information during the training process. Additionally, the initialisation of spherical functions in 3DGS-based methods often fails to engage higher-order terms in early training rounds, leading to unnecessary computational overhead as training progresses. Furthermore, current 3DGS-based approaches require training on higher resolution images to render higher resolution outputs, significantly increasing memory demands and prolonging training durations. We introduce StructGS, a framework that enhances 3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D reconstruction. StructGS innovatively incorporates a patch-based SSIM loss, dynamic spherical harmonics initialisation and a Multi-scale Residual Network (MSRN) to address the above-mentioned limitations, respectively. Our framework significantly reduces computational redundancy, enhances detail capture and supports high-resolution rendering from low-resolution inputs. Experimentally, StructGS demonstrates superior performance over state-of-the-art (SOTA) models, achieving higher quality and more detailed renderings with fewer artifacts. (The link to the code will be made available after publication.).
Published: 2025-12-03T18:43:18+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zexu Huang; Min Xu; Stuart Perry&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639991"&gt;10.1109/tmm.2025.3639991&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in 3D reconstruction coupled with neural rendering techniques have greatly improved the creation of photo-realistic 3D scenes, influencing both academic research and industry applications. The technique of 3D Gaussian Splatting and its variants incorporate the strengths of both primitive-based and volumetric representations, achieving superior rendering quality. While 3D Geometric Scattering (3DGS) and its variants have advanced the field of 3D representation, they fall short in capturing the stochastic properties of non-local structural information during the training process. Additionally, the initialisation of spherical functions in 3DGS-based methods often fails to engage higher-order terms in early training rounds, leading to unnecessary computational overhead as training progresses. Furthermore, current 3DGS-based approaches require training on higher resolution images to render higher resolution outputs, significantly increasing memory demands and prolonging training durations. We introduce StructGS, a framework that enhances 3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D reconstruction. StructGS innovatively incorporates a patch-based SSIM loss, dynamic spherical harmonics initialisation and a Multi-scale Residual Network (MSRN) to address the above-mentioned limitations, respectively. Our framework significantly reduces computational redundancy, enhances detail capture and supports high-resolution rendering from low-resolution inputs. Experimentally, StructGS demonstrates superior performance over state-of-the-art (SOTA) models, achieving higher quality and more detailed renderings with fewer artifacts. (The link to the code will be made available after publication.).&lt;/p&gt;</content:encoded></item><item><title>Forget Less, Retain More: A Lightweight Regularizer for Rehearsal-Based Continual Learning</title><link>https://arxiv.org/abs/2512.01818v1</link><guid>http://arxiv.org/abs/2512.01818v1</guid><pubDate>Mon, 01 Dec 2025 15:56:00 +0000</pubDate><dc:creator>Lama Alssum</dc:creator><dc:creator>Hasan Abed Al Kader Hammoud</dc:creator><dc:creator>Motasem Alfarra</dc:creator><dc:creator>Juan C Leon Alcazar</dc:creator><dc:creator>Bernard Ghanem</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep neural networks suffer from catastrophic forgetting, where performance on previous tasks degrades after training on a new task. This issue arises due to the model's tendency to overwrite previously acquired knowledge with new information. We present a novel approach to address this challenge, focusing on the intersection of memory-based methods and regularization approaches. We formulate a regularization strategy, termed Information Maximization (IM) regularizer, for memory-based continual learning methods, which is based exclusively on the expected label distribution, thus making it class-agnostic. As a consequence, IM regularizer can be directly integrated into various rehearsal-based continual learning methods, reducing forgetting and favoring faster convergence. Our empirical validation shows that, across datasets and regardless of the number of tasks, our proposed regularization strategy consistently improves baseline performance at the expense of a minimal computational overhead. The lightweight nature of IM ensures that it remains a practical and scalable solution, making it applicable to real-world continual learning scenarios where efficiency is paramount. Finally, we demonstrate the data-agnostic nature of our regularizer by applying it to video data, which presents additional challenges due to its temporal structure and higher memory requirements. Despite the significant domain gap, our experiments show that IM regularizer also improves the performance of video continual learning methods.
Published: 2025-12-01T15:56:00+00:00
Venue: arXiv
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lama Alssum; Hasan Abed Al Kader Hammoud; Motasem Alfarra; Juan C Leon Alcazar; Bernard Ghanem&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Deep neural networks suffer from catastrophic forgetting, where performance on previous tasks degrades after training on a new task. This issue arises due to the model&amp;#x27;s tendency to overwrite previously acquired knowledge with new information. We present a novel approach to address this challenge, focusing on the intersection of memory-based methods and regularization approaches. We formulate a regularization strategy, termed Information Maximization (IM) regularizer, for memory-based continual learning methods, which is based exclusively on the expected label distribution, thus making it class-agnostic. As a consequence, IM regularizer can be directly integrated into various rehearsal-based continual learning methods, reducing forgetting and favoring faster convergence. Our empirical validation shows that, across datasets and regardless of the number of tasks, our proposed regularization strategy consistently improves baseline performance at the expense of a minimal computational overhead. The lightweight nature of IM ensures that it remains a practical and scalable solution, making it applicable to real-world continual learning scenarios where efficiency is paramount. Finally, we demonstrate the data-agnostic nature of our regularizer by applying it to video data, which presents additional challenges due to its temporal structure and higher memory requirements. Despite the significant domain gap, our experiments show that IM regularizer also improves the performance of video continual learning methods.&lt;/p&gt;</content:encoded></item><item><title>Diminishing Returns in Self-Supervised Learning</title><link>https://arxiv.org/abs/2512.03862v1</link><guid>http://arxiv.org/abs/2512.03862v1</guid><pubDate>Wed, 03 Dec 2025 15:11:44 +0000</pubDate><dc:creator>Oli Bridge</dc:creator><dc:creator>Huey Sun</dc:creator><dc:creator>Botond Branyicskai-Nagy</dc:creator><dc:creator>Charles D'Ornano</dc:creator><dc:creator>Shomit Basu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.
Published: 2025-12-03T15:11:44+00:00
Venue: arXiv
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Oli Bridge; Huey Sun; Botond Branyicskai-Nagy; Charles D&amp;#x27;Ornano; Shomit Basu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;While transformer-based architectures have taken computer vision and NLP by storm, they often require a vast amount of parameters and training data to attain strong performance. In this work, we experiment with three distinct pre-training, intermediate fine-tuning, and downstream datasets and training objectives to explore their marginal benefits on a small 5M-parameter vision transformer. We find that while pre-training and fine-tuning always help our model but have diminishing returns, intermediate fine-tuning can actually show harmful impact on downstream performance, potentially due to dissimilarity in task mechanics. Taken together, our results suggest that small-scale ViTs benefit most from targeted pre-training and careful data selection, while indiscriminate stacking of intermediate tasks can waste compute and even degrade performance.&lt;/p&gt;</content:encoded></item><item><title>MDCA-Net: a multi-directional alignment and dynamic context aggregation network for optical and SAR image fusion</title><link>https://doi.org/10.1080/10095020.2025.2589611</link><guid>10.1080/10095020.2025.2589611</guid><pubDate>Wed, 03 Dec 2025 16:33:06 +0000</pubDate><dc:creator>Tao Chen</dc:creator><dc:creator>Jun Pan</dc:creator><dc:creator>Jiangong Xu</dc:creator><dc:creator>Jiarui Hu</dc:creator><dc:creator>Liwen Cao</dc:creator><dc:creator>Junli Li</dc:creator><prism:publicationName>Geo-spatial Information Science</prism:publicationName><prism:doi>10.1080/10095020.2025.2589611</prism:doi><description>Optical images and synthetic aperture radar (SAR) data exhibit complementary advantages, offering rich spectral and spatial information. The fusion of these modalities to enhance the quality of remote sensing images has garnered increasing attention in recent years. However, fusing optical and SAR images remains challenging due to differences in imaging mechanisms, speckle noise in SAR data, and the difficulty in jointly preserving details, structure, and spectral fidelity. To address these challenges, this paper proposes a multi-directional alignment and dynamic context aggregation network (MDCA-Net) for optical and SAR image fusion, designed to effectively exploit the complementary features of both modalities to generate information-rich fused images. Specifically, the cross-modal multi-directional alignment (CMDA) module is designed to mitigate discrepancies caused by differing imaging mechanisms. To suppress speckle noise and enhance structural details in SAR images, SAR dynamic context detail enhancement (SDCE) module is developed. Furthermore, a globally shift-aware context aggregation (GSCA) module is designed to jointly preserve detail, structural coherence, and spectral fidelity in the fused image. Compared with seven representative fusion methods, MDCA-Net demonstrates superior visual quality and achieves more outstanding quantitative and qualitative evaluation results. In addition, MDCA-Net significantly improves classification accuracy across multiple land cover types, including wetland, built-up area, grassland, forest, and cropland. On the Hunan dataset, MDCA-Net achieves gains of 4.48%, 2.63%, and 4.81% in mean intersection over union (mIoU), mean producer’s accuracy (mPA), and mean precision (mPrecision), respectively.
Published: 2025-12-03T16:33:06+00:00
Venue: Geo-spatial Information Science
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Chen; Jun Pan; Jiangong Xu; Jiarui Hu; Liwen Cao; Junli Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Geo-spatial Information Science&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1080/10095020.2025.2589611"&gt;10.1080/10095020.2025.2589611&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Optical images and synthetic aperture radar (SAR) data exhibit complementary advantages, offering rich spectral and spatial information. The fusion of these modalities to enhance the quality of remote sensing images has garnered increasing attention in recent years. However, fusing optical and SAR images remains challenging due to differences in imaging mechanisms, speckle noise in SAR data, and the difficulty in jointly preserving details, structure, and spectral fidelity. To address these challenges, this paper proposes a multi-directional alignment and dynamic context aggregation network (MDCA-Net) for optical and SAR image fusion, designed to effectively exploit the complementary features of both modalities to generate information-rich fused images. Specifically, the cross-modal multi-directional alignment (CMDA) module is designed to mitigate discrepancies caused by differing imaging mechanisms. To suppress speckle noise and enhance structural details in SAR images, SAR dynamic context detail enhancement (SDCE) module is developed. Furthermore, a globally shift-aware context aggregation (GSCA) module is designed to jointly preserve detail, structural coherence, and spectral fidelity in the fused image. Compared with seven representative fusion methods, MDCA-Net demonstrates superior visual quality and achieves more outstanding quantitative and qualitative evaluation results. In addition, MDCA-Net significantly improves classification accuracy across multiple land cover types, including wetland, built-up area, grassland, forest, and cropland. On the Hunan dataset, MDCA-Net achieves gains of 4.48%, 2.63%, and 4.81% in mean intersection over union (mIoU), mean producer’s accuracy (mPA), and mean precision (mPrecision), respectively.&lt;/p&gt;</content:encoded></item><item><title>Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback</title><link>https://arxiv.org/abs/2512.03208v1</link><guid>http://arxiv.org/abs/2512.03208v1</guid><pubDate>Tue, 02 Dec 2025 20:22:25 +0000</pubDate><dc:creator>Pangpang Liu</dc:creator><dc:creator>Junwei Lu</dc:creator><dc:creator>Will Wei Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.
Published: 2025-12-02T20:22:25+00:00
Venue: arXiv
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pangpang Liu; Junwei Lu; Will Wei Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.&lt;/p&gt;</content:encoded></item></channel></rss>