<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 06 Jan 2026 02:51:50 +0000</lastBuildDate><item><title>Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models</title><link>https://doi.org/10.1109/tpami.2026.3650761</link><guid>10.1109/tpami.2026.3650761</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>YiFan Zhang</dc:creator><dc:creator>Qingsong Wen</dc:creator><dc:creator>Chaoyou Fu</dc:creator><dc:creator>Kun Wang</dc:creator><dc:creator>Xue Wang</dc:creator><dc:creator>Zhang Zhang</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Rong Jin</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650761</prism:doi><description>Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.844 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; YiFan Zhang; Qingsong Wen; Chaoyou Fu; Kun Wang; Xue Wang; Zhang Zhang; Liang Wang; Rong Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650761"&gt;10.1109/tpami.2026.3650761&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.844 (must_read)&lt;/p&gt;
&lt;p&gt;Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.&lt;/p&gt;</content:encoded></item><item><title>LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting</title><link>https://doi.org/10.1109/tpami.2026.3650769</link><guid>10.1109/tpami.2026.3650769</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Yuchen Su</dc:creator><dc:creator>Zhineng Chen</dc:creator><dc:creator>Yongkun Du</dc:creator><dc:creator>Zuxuan Wu</dc:creator><dc:creator>Hongtao Xie</dc:creator><dc:creator>Yu-Gang Jiang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650769</prism:doi><description>End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.829 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchen Su; Zhineng Chen; Yongkun Du; Zuxuan Wu; Hongtao Xie; Yu-Gang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650769"&gt;10.1109/tpami.2026.3650769&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.829 (must_read)&lt;/p&gt;
&lt;p&gt;End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.&lt;/p&gt;</content:encoded></item><item><title>MADTP++: Bridge the Gap Between Token and Weight Pruning for Accelerating VLTs</title><link>https://doi.org/10.1109/tpami.2025.3650545</link><guid>10.1109/tpami.2025.3650545</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Jianjian Cao</dc:creator><dc:creator>Chong Yu</dc:creator><dc:creator>Peng Ye</dc:creator><dc:creator>Tao Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650545</prism:doi><description>Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianjian Cao; Chong Yu; Peng Ye; Tao Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650545"&gt;10.1109/tpami.2025.3650545&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...&lt;/p&gt;</content:encoded></item><item><title>A Feature-Enhanced Network-Based Target Detection Method for SAR Images of Ships in Complex Scenes</title><link>https://doi.org/10.3390/rs18010178</link><guid>10.3390/rs18010178</guid><pubDate>Mon, 05 Jan 2026 15:28:57 +0000</pubDate><dc:creator>Yunsheng Ba</dc:creator><dc:creator>Nan Xia</dc:creator><dc:creator>Weijia Lu</dc:creator><dc:creator>Junqiao Liu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010178</prism:doi><description>In the context of ship target detection with Synthetic Aperture Radar (SAR) images, misdetection and missed detection are often caused by complex background interference and the variability in target size. To address these challenges, this paper proposes an innovative method based on image enhancement and feature fusion to reduce background noise and effectively handle the detection confusion caused by differences in ship sizes. Firstly, a feature-aware enhancement network is introduced, which preserves and strengthens the edge information of the target objects. Secondly, during the feature extraction phase, a dynamic hierarchical extraction module is proposed, significantly improving the feature capture ability of convolutional neural networks and overcoming the limitations of traditional fixed kernel receptive fields. Finally, a feature fusion module based on attention gating is employed to fully leverage the complementary information between the original and enhanced images, achieving precise modeling and efficient fusion of inter-feature correlations. The proposed method is integrated with the YOLOv8 detection framework for target detection. Experimental results in the publicly available SSDD and HRSID datasets demonstrate detection accuracies of 97.9% and 93.2%, respectively, thus validating the superiority and robustness of the proposed method.
Published: 2026-01-05T15:28:57+00:00
Venue: Remote Sensing
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunsheng Ba; Nan Xia; Weijia Lu; Junqiao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010178"&gt;10.3390/rs18010178&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;In the context of ship target detection with Synthetic Aperture Radar (SAR) images, misdetection and missed detection are often caused by complex background interference and the variability in target size. To address these challenges, this paper proposes an innovative method based on image enhancement and feature fusion to reduce background noise and effectively handle the detection confusion caused by differences in ship sizes. Firstly, a feature-aware enhancement network is introduced, which preserves and strengthens the edge information of the target objects. Secondly, during the feature extraction phase, a dynamic hierarchical extraction module is proposed, significantly improving the feature capture ability of convolutional neural networks and overcoming the limitations of traditional fixed kernel receptive fields. Finally, a feature fusion module based on attention gating is employed to fully leverage the complementary information between the original and enhanced images, achieving precise modeling and efficient fusion of inter-feature correlations. The proposed method is integrated with the YOLOv8 detection framework for target detection. Experimental results in the publicly available SSDD and HRSID datasets demonstrate detection accuracies of 97.9% and 93.2%, respectively, thus validating the superiority and robustness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Target-Level SAR-to-Optical Image Translation Driven by Semantic Segmentation</title><link>https://doi.org/10.1109/taes.2025.3650511</link><guid>10.1109/taes.2025.3650511</guid><pubDate>Mon, 05 Jan 2026 18:42:42 +0000</pubDate><dc:creator>Huihui Li</dc:creator><dc:creator>Siyuan Liu</dc:creator><dc:creator>Zhou Liu</dc:creator><dc:creator>Hang Liu</dc:creator><dc:creator>Dawei Guo</dc:creator><dc:creator>Kun Liu</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3650511</prism:doi><description>Synthetic Aperture Radar (SAR) is widely used in military and civilian applications due to its all-weather, day-and-night imaging capability. However, interpreting SAR images is challenging for both experts and non-experts. Inspired by deep learning-based style transfer, researchers have employed Generative Adversarial Networks (GANs) to convert SAR images into more intuitive optical ones. Yet, current loss functions and evaluation metrics focus mainly on pixel-level differences, overlooking the structural coherence required for target recognition and downstream tasks. To address this, we propose a semantic segmentation-driven framework for target-level SAR-to-optical image translation. Compatible with various supervised models, it incorporates segmentation loss and uses SAR segmentation maps as additional inputs to preserve target structure. Experiments on custom datasets, built from Sentinel 1-2 imagery with road binary segmentation labels, as well as public datasets, confirm the method's effectiveness across different base translation models. The source code and the datasets used will be published at the following URL https://github.com/NWPU-LHH/SOIT-Seg-Driven.
Published: 2026-01-05T18:42:42+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huihui Li; Siyuan Liu; Zhou Liu; Hang Liu; Dawei Guo; Kun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3650511"&gt;10.1109/taes.2025.3650511&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is widely used in military and civilian applications due to its all-weather, day-and-night imaging capability. However, interpreting SAR images is challenging for both experts and non-experts. Inspired by deep learning-based style transfer, researchers have employed Generative Adversarial Networks (GANs) to convert SAR images into more intuitive optical ones. Yet, current loss functions and evaluation metrics focus mainly on pixel-level differences, overlooking the structural coherence required for target recognition and downstream tasks. To address this, we propose a semantic segmentation-driven framework for target-level SAR-to-optical image translation. Compatible with various supervised models, it incorporates segmentation loss and uses SAR segmentation maps as additional inputs to preserve target structure. Experiments on custom datasets, built from Sentinel 1-2 imagery with road binary segmentation labels, as well as public datasets, confirm the method&amp;#x27;s effectiveness across different base translation models. The source code and the datasets used will be published at the following URL https://github.com/NWPU-LHH/SOIT-Seg-Driven.&lt;/p&gt;</content:encoded></item><item><title>MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.021</link><guid>10.1016/j.isprsjprs.2025.12.021</guid><pubDate>Mon, 05 Jan 2026 11:18:47 +0000</pubDate><dc:creator>Weipeng Jing</dc:creator><dc:creator>Peilun Kang</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Jian Wang</dc:creator><dc:creator>Yang Song</dc:creator><dc:creator>Chao Li</dc:creator><dc:creator>Lei Fan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.021</prism:doi><description>Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.
Published: 2026-01-05T11:18:47+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weipeng Jing; Peilun Kang; Donglin Di; Jian Wang; Yang Song; Chao Li; Lei Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021"&gt;10.1016/j.isprsjprs.2025.12.021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.&lt;/p&gt;</content:encoded></item><item><title>Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination</title><link>https://doi.org/10.1109/tpami.2026.3650770</link><guid>10.1109/tpami.2026.3650770</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Sida Peng</dc:creator><dc:creator>Jiarui Guo</dc:creator><dc:creator>Xi Chen</dc:creator><dc:creator>Yuan Liu</dc:creator><dc:creator>Dongchen Yang</dc:creator><dc:creator>Hujun Bao</dc:creator><dc:creator>Xiaowei Zhou</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650770</prism:doi><description>This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code is available at https://zju3dv.github.io/IntrinsicAnything/.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sida Peng; Jiarui Guo; Xi Chen; Yuan Liu; Dongchen Yang; Hujun Bao; Xiaowei Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650770"&gt;10.1109/tpami.2026.3650770&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code is available at https://zju3dv.github.io/IntrinsicAnything/.&lt;/p&gt;</content:encoded></item><item><title>Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing</title><link>https://doi.org/10.1109/tiv.2025.3650682</link><guid>10.1109/tiv.2025.3650682</guid><pubDate>Mon, 05 Jan 2026 18:40:39 +0000</pubDate><dc:creator>Sicen Guo</dc:creator><dc:creator>Tianyou Wen</dc:creator><dc:creator>Chuang-Wei Liu</dc:creator><dc:creator>Qijun Chen</dc:creator><dc:creator>Rui Fan</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Vehicles</prism:publicationName><prism:doi>10.1109/tiv.2025.3650682</prism:doi><description>Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.
Published: 2026-01-05T18:40:39+00:00
Venue: IEEE Transactions on Intelligent Vehicles
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sicen Guo; Tianyou Wen; Chuang-Wei Liu; Qijun Chen; Rui Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Vehicles&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tiv.2025.3650682"&gt;10.1109/tiv.2025.3650682&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Out-of-Distribution Detection in Real-time Object Detection: From Benchmark Pitfalls to a New Mitigation Paradigm</title><link>https://doi.org/10.1109/tpami.2025.3650695</link><guid>10.1109/tpami.2025.3650695</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Changshun Wu</dc:creator><dc:creator>Weicheng He</dc:creator><dc:creator>Chih-Hong Cheng</dc:creator><dc:creator>Xiaowei Huang</dc:creator><dc:creator>Saddek Bensalem</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650695</prism:doi><description>Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changshun Wu; Weicheng He; Chih-Hong Cheng; Xiaowei Huang; Saddek Bensalem&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650695"&gt;10.1109/tpami.2025.3650695&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction</title><link>https://doi.org/10.1109/tpami.2025.3650478</link><guid>10.1109/tpami.2025.3650478</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Bohan Li</dc:creator><dc:creator>Jiajun Deng</dc:creator><dc:creator>Yasheng Sun</dc:creator><dc:creator>Xiaofeng Wang</dc:creator><dc:creator>Xin Jin</dc:creator><dc:creator>Wenjun Zeng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650478</prism:doi><description>Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bohan Li; Jiajun Deng; Yasheng Sun; Xiaofeng Wang; Xin Jin; Wenjun Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650478"&gt;10.1109/tpami.2025.3650478&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp;amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.&lt;/p&gt;</content:encoded></item><item><title>Lifelong Learning of Large Language Model based Agents: A Roadmap</title><link>https://doi.org/10.1109/tpami.2025.3650546</link><guid>10.1109/tpami.2025.3650546</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Junhao Zheng</dc:creator><dc:creator>Chengming Shi</dc:creator><dc:creator>Xidi Cai</dc:creator><dc:creator>Qiuke Li</dc:creator><dc:creator>Duzhen Zhang</dc:creator><dc:creator>Chenxing Li</dc:creator><dc:creator>Dong Yu</dc:creator><dc:creator>Qianli Ma</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650546</prism:doi><description>Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at https://github.com/qianlimalab/ awesome-lifelong-llm-agent.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junhao Zheng; Chengming Shi; Xidi Cai; Qiuke Li; Duzhen Zhang; Chenxing Li; Dong Yu; Qianli Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650546"&gt;10.1109/tpami.2025.3650546&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at https://github.com/qianlimalab/ awesome-lifelong-llm-agent.&lt;/p&gt;</content:encoded></item><item><title>PVF-DectNet++: Adaptive Multi-Modal Fusion with Perspective Voxels for 3D Object Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3650671</link><guid>10.1109/tcsvt.2025.3650671</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Ke Wang</dc:creator><dc:creator>Weilin Gao</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Tianyi Shao</dc:creator><dc:creator>Liyang Li</dc:creator><dc:creator>Tianqiang Zhou</dc:creator><dc:creator>Jianbo Lu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3650671</prism:doi><description>To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ke Wang; Weilin Gao; Kai Chen; Tianyi Shao; Liyang Li; Tianqiang Zhou; Jianbo Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3650671"&gt;10.1109/tcsvt.2025.3650671&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.&lt;/p&gt;</content:encoded></item><item><title>Physical Attributes Embedded Prototypical Network for Incremental SAR Automatic Target Recognition</title><link>https://doi.org/10.1109/jstars.2025.3650513</link><guid>10.1109/jstars.2025.3650513</guid><pubDate>Mon, 05 Jan 2026 18:39:04 +0000</pubDate><dc:creator>Yanjie Xu</dc:creator><dc:creator>Hao Sun</dc:creator><dc:creator>Chenfang Liu</dc:creator><dc:creator>Kefeng Ji</dc:creator><dc:creator>Gangyao Kuang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650513</prism:doi><description>In Synthetic Aperture Radar (SAR) applications, the continuous emergence of new target classes poses a significant challenge to Automatic Target Recognition (ATR) systems. Adapting to the distribution of new data can induce drastic alterations in the feature space of deep models, resulting in a decline in their ability to recognize old data, termed catastrophic forgetting. To address this challenge, we propose a novel class-incremental SAR ATR method based on Physical Attributes Embedded Prototypical Network (PAEPN). PAEPN embeds physical attributes derived from electromagnetic scattering and geometric priors into the deep model to achieve stable representations. These physical attributes, determined by the target's shape, structure, and material composition, remain invariant throughout the incremental learning process, thereby enhancing the stability and interpretability of deep models. Specifically, PAEPN first extracts and integrates physical attribute priors to establish feature anchors, guiding the deep model in extracting physically consistent features and preventing drastic changes in the feature space. Second, a spatial attention enhancement strategy is introduced to enable the deep model to reliably focus on the key regions of SAR targets. Finally, feature relations that represent semantic similarity are distilled to further mitigate catastrophic forgetting. During testing, PAEPN employs the cosine distance between the sample feature and class prototypes for recognition. Comprehensive experiments on three datasets demonstrate that PAEPN outperforms existing state-of-the-art methods.
Published: 2026-01-05T18:39:04+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanjie Xu; Hao Sun; Chenfang Liu; Kefeng Ji; Gangyao Kuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650513"&gt;10.1109/jstars.2025.3650513&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;In Synthetic Aperture Radar (SAR) applications, the continuous emergence of new target classes poses a significant challenge to Automatic Target Recognition (ATR) systems. Adapting to the distribution of new data can induce drastic alterations in the feature space of deep models, resulting in a decline in their ability to recognize old data, termed catastrophic forgetting. To address this challenge, we propose a novel class-incremental SAR ATR method based on Physical Attributes Embedded Prototypical Network (PAEPN). PAEPN embeds physical attributes derived from electromagnetic scattering and geometric priors into the deep model to achieve stable representations. These physical attributes, determined by the target&amp;#x27;s shape, structure, and material composition, remain invariant throughout the incremental learning process, thereby enhancing the stability and interpretability of deep models. Specifically, PAEPN first extracts and integrates physical attribute priors to establish feature anchors, guiding the deep model in extracting physically consistent features and preventing drastic changes in the feature space. Second, a spatial attention enhancement strategy is introduced to enable the deep model to reliably focus on the key regions of SAR targets. Finally, feature relations that represent semantic similarity are distilled to further mitigate catastrophic forgetting. During testing, PAEPN employs the cosine distance between the sample feature and class prototypes for recognition. Comprehensive experiments on three datasets demonstrate that PAEPN outperforms existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Guiding Token-Sparse Diffusion Models</title><link>https://arxiv.org/abs/2601.01608v1</link><guid>http://arxiv.org/abs/2601.01608v1</guid><pubDate>Sun, 04 Jan 2026 17:18:27 +0000</pubDate><dc:creator>Felix Krause</dc:creator><dc:creator>Stefan Andreas Baumann</dc:creator><dc:creator>Johannes Schusterbauer</dc:creator><dc:creator>Olga Grebenkova</dc:creator><dc:creator>Ming Gui</dc:creator><dc:creator>Vincent Tao Hu</dc:creator><dc:creator>Björn Ommer</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.
Published: 2026-01-04T17:18:27+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Felix Krause; Stefan Andreas Baumann; Johannes Schusterbauer; Olga Grebenkova; Ming Gui; Vincent Tao Hu; Björn Ommer&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Multi-view Omnidirectional Depth Estimation with Semantic-Aware Cost Aggregation and Spatial Propagation</title><link>https://doi.org/10.1109/tcsvt.2026.3651056</link><guid>10.1109/tcsvt.2026.3651056</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Ming Li</dc:creator><dc:creator>Xuejiao Hu</dc:creator><dc:creator>Zihang Gao</dc:creator><dc:creator>Sidan Du</dc:creator><dc:creator>Yang Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651056</prism:doi><description>Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Li; Xuejiao Hu; Zihang Gao; Sidan Du; Yang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651056"&gt;10.1109/tcsvt.2026.3651056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.&lt;/p&gt;</content:encoded></item><item><title>MDADet: A Multimodal Dynamic Adaptation Framework for Efficient Small Object Detection in Aerial Images</title><link>https://doi.org/10.1109/tgrs.2026.3650963</link><guid>10.1109/tgrs.2026.3650963</guid><pubDate>Mon, 05 Jan 2026 18:38:39 +0000</pubDate><dc:creator>Jian Zhang</dc:creator><dc:creator>Jiarong Lv</dc:creator><dc:creator>Heng Zhang</dc:creator><dc:creator>Ming Li</dc:creator><dc:creator>Meng Huang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3650963</prism:doi><description>In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.
Published: 2026-01-05T18:38:39+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Zhang; Jiarong Lv; Heng Zhang; Ming Li; Meng Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3650963"&gt;10.1109/tgrs.2026.3650963&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.&lt;/p&gt;</content:encoded></item><item><title>A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.023</link><guid>10.1016/j.isprsjprs.2025.12.023</guid><pubDate>Mon, 05 Jan 2026 14:32:55 +0000</pubDate><dc:creator>Hyunho Lee</dc:creator><dc:creator>Wenwen Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.023</prism:doi><description>Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. This is because SAR sensors can observe through cloud cover and operate both day and night, whereas Multispectral Imaging (MSI) data, despite providing higher mapping accuracy, are only available under cloud-free and daytime conditions. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Specifically, SMAGNet achieved the highest IoU score of 86.47% using SAR and MSI data and maintained the highest performance with an IoU score of 79.53% even when MSI data were entirely missing. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios. The source code is available at https://github.com/ASUcicilab/SMAGNet .
Published: 2026-01-05T14:32:55+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyunho Lee; Wenwen Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.023"&gt;10.1016/j.isprsjprs.2025.12.023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. This is because SAR sensors can observe through cloud cover and operate both day and night, whereas Multispectral Imaging (MSI) data, despite providing higher mapping accuracy, are only available under cloud-free and daytime conditions. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Specifically, SMAGNet achieved the highest IoU score of 86.47% using SAR and MSI data and maintained the highest performance with an IoU score of 79.53% even when MSI data were entirely missing. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios. The source code is available at https://github.com/ASUcicilab/SMAGNet .&lt;/p&gt;</content:encoded></item><item><title>Few-shot object detection via dynamic feature enhancement and attention template matching</title><link>https://doi.org/10.1007/s10489-025-06997-y</link><guid>10.1007/s10489-025-06997-y</guid><pubDate>Mon, 05 Jan 2026 15:40:29 +0000</pubDate><dc:creator>Ruqi Su</dc:creator><dc:creator>Kai Zhang</dc:creator><dc:creator>Songhao Zhu</dc:creator><prism:publicationName>Applied Intelligence</prism:publicationName><prism:doi>10.1007/s10489-025-06997-y</prism:doi><description>With the rapid advancement of deep learning and computer vision, few-shot object detection (FSOD) has emerged as a critical research frontier. A key challenge in FSOD lies in extracting discriminative feature representations from limited samples, which severely degrades detection performance. To mitigate this issue, we propose a novel FSOD framework that integrates cross-domain adaptive feature enhancement and attention-guided proposal generation, effectively leveraging support set information to improve query set detection accuracy. Our method introduces three key innovations. (1) Dynamic Kernel Generation. A learnable kernel generator produces sample-specific convolutional kernels to adaptively enhance query features using support set cues. (2) Attention-Driven Region Proposals. An attention-based region proposal network (ARPN) suppresses irrelevant regions while prioritizing semantically relevant areas. (3) Template-Aware Scoring. A matching module evaluates candidate boxes against support templates to ensure geometric and semantic consistency. Extensive experiments on PASCAL VOC and MS COCO benchmarks demonstrate our method outperforming existing approaches by 3.2 AP50 on 10-shot tasks. The results validate the efficacy of cross-domain adaptation and attention mechanisms in addressing data scarcity challenges.
Published: 2026-01-05T15:40:29+00:00
Venue: Applied Intelligence
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruqi Su; Kai Zhang; Songhao Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Applied Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s10489-025-06997-y"&gt;10.1007/s10489-025-06997-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancement of deep learning and computer vision, few-shot object detection (FSOD) has emerged as a critical research frontier. A key challenge in FSOD lies in extracting discriminative feature representations from limited samples, which severely degrades detection performance. To mitigate this issue, we propose a novel FSOD framework that integrates cross-domain adaptive feature enhancement and attention-guided proposal generation, effectively leveraging support set information to improve query set detection accuracy. Our method introduces three key innovations. (1) Dynamic Kernel Generation. A learnable kernel generator produces sample-specific convolutional kernels to adaptively enhance query features using support set cues. (2) Attention-Driven Region Proposals. An attention-based region proposal network (ARPN) suppresses irrelevant regions while prioritizing semantically relevant areas. (3) Template-Aware Scoring. A matching module evaluates candidate boxes against support templates to ensure geometric and semantic consistency. Extensive experiments on PASCAL VOC and MS COCO benchmarks demonstrate our method outperforming existing approaches by 3.2 AP50 on 10-shot tasks. The results validate the efficacy of cross-domain adaptation and attention mechanisms in addressing data scarcity challenges.&lt;/p&gt;</content:encoded></item><item><title>Collaborative Refinement Guidance for High-Fidelity Defect Image Synthesis</title><link>https://doi.org/10.1109/tcsvt.2026.3650924</link><guid>10.1109/tcsvt.2026.3650924</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Weihang Luo</dc:creator><dc:creator>Xingen Gao</dc:creator><dc:creator>Zhijie Zhang</dc:creator><dc:creator>Jianxiongwen Huang</dc:creator><dc:creator>Hongyi Zhang</dc:creator><dc:creator>Juqiang Lin</dc:creator><dc:creator>Hua Shi</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3650924</prism:doi><description>Automated Visual Inspection is a cornerstone of modern manufacturing, yet the development of robust deep learning models is frequently impeded by the scarcity and imbalance of training data. This challenge is particularly acute for industrial defects, which often manifest as subtle anomalies intrinsically linked to complex, structured backgrounds. To address this challenge, CRG-DefectDiffuser is proposed as a collaborative generative framework for high-fidelity defect synthesis. At its core lies the Collaborative Refinement Guidance (CRG) mechanism, which orchestrates two specialized diffusion models: one trained on abundant defect-free images to master background context, and another trained on scarce defect patches to encode fine-grained defect semantics. The CRG mechanism steers the synthesis by dynamically generating a guidance map, which is refined through a four-stage process to ensure that morphologically accurate defects are seamlessly integrated into the appropriate background context. Augmenting training data with our method boosts the defect detection mAP@50-95 from a baseline of 0.496 to 0.557, corresponding to a 12.3 percentage point relative improvement. The framework also demonstrates superior scalability, with performance gains continuing up to the three-fold data augmentation evaluated in our experiments, a point where competing methods often falter. These results establish CRG-DefectDiffuser as an effective and practical solution to data scarcity in industrial visual inspection, with strong potential for generalization across diverse manufacturing scenarios. The source code is publicly available at https://github.com/weihang-luo/ CRG-DefectDiffuser.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weihang Luo; Xingen Gao; Zhijie Zhang; Jianxiongwen Huang; Hongyi Zhang; Juqiang Lin; Hua Shi; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3650924"&gt;10.1109/tcsvt.2026.3650924&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Automated Visual Inspection is a cornerstone of modern manufacturing, yet the development of robust deep learning models is frequently impeded by the scarcity and imbalance of training data. This challenge is particularly acute for industrial defects, which often manifest as subtle anomalies intrinsically linked to complex, structured backgrounds. To address this challenge, CRG-DefectDiffuser is proposed as a collaborative generative framework for high-fidelity defect synthesis. At its core lies the Collaborative Refinement Guidance (CRG) mechanism, which orchestrates two specialized diffusion models: one trained on abundant defect-free images to master background context, and another trained on scarce defect patches to encode fine-grained defect semantics. The CRG mechanism steers the synthesis by dynamically generating a guidance map, which is refined through a four-stage process to ensure that morphologically accurate defects are seamlessly integrated into the appropriate background context. Augmenting training data with our method boosts the defect detection mAP@50-95 from a baseline of 0.496 to 0.557, corresponding to a 12.3 percentage point relative improvement. The framework also demonstrates superior scalability, with performance gains continuing up to the three-fold data augmentation evaluated in our experiments, a point where competing methods often falter. These results establish CRG-DefectDiffuser as an effective and practical solution to data scarcity in industrial visual inspection, with strong potential for generalization across diverse manufacturing scenarios. The source code is publicly available at https://github.com/weihang-luo/ CRG-DefectDiffuser.&lt;/p&gt;</content:encoded></item><item><title>MHPE: Learning Morphology Relationships for Robust Head Pose Estimation with Facial Rotation Representation</title><link>https://doi.org/10.1109/tcsvt.2026.3651198</link><guid>10.1109/tcsvt.2026.3651198</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Tingting Liu</dc:creator><dc:creator>Jianping Ju</dc:creator><dc:creator>Zhixiong Song</dc:creator><dc:creator>Shijia Qian</dc:creator><dc:creator>Ning Rao</dc:creator><dc:creator>Hai Liu</dc:creator><dc:creator>You-Fu Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651198</prism:doi><description>Although accurate head pose estimation is critical for natural human-computer interaction, it remains challenging due to occlusion, extreme poses, illumination conditions, and data ambiguity issues. To address these challenges, a novel morphology aware Transformer framework (MHPE) is proposed, which can learn morphological relationships during facial rotation. The methodology is based on two key findings: cross-region geometric dependencies and angle-specific morphodynamic representations. The proposed framework incorporates two key components: adversarial feature generation, which generates robust rotation representations by adaptive multi-scale feature interaction; and morphology relationship inference, which establishes long-range dependencies between facial features through a cross-modal attention mechanism that incorporates morphological priors. Extensive evaluations on three demanding benchmarks (BIWI, AFLW2000, and 300W-LP) demonstrate state-of-the-art performance, particularly in demanding scenarios. The Python implementation will be available on request to facilitate reproducibility.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tingting Liu; Jianping Ju; Zhixiong Song; Shijia Qian; Ning Rao; Hai Liu; You-Fu Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651198"&gt;10.1109/tcsvt.2026.3651198&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Although accurate head pose estimation is critical for natural human-computer interaction, it remains challenging due to occlusion, extreme poses, illumination conditions, and data ambiguity issues. To address these challenges, a novel morphology aware Transformer framework (MHPE) is proposed, which can learn morphological relationships during facial rotation. The methodology is based on two key findings: cross-region geometric dependencies and angle-specific morphodynamic representations. The proposed framework incorporates two key components: adversarial feature generation, which generates robust rotation representations by adaptive multi-scale feature interaction; and morphology relationship inference, which establishes long-range dependencies between facial features through a cross-modal attention mechanism that incorporates morphological priors. Extensive evaluations on three demanding benchmarks (BIWI, AFLW2000, and 300W-LP) demonstrate state-of-the-art performance, particularly in demanding scenarios. The Python implementation will be available on request to facilitate reproducibility.&lt;/p&gt;</content:encoded></item><item><title>Dynamic Prompting Spatial Temporal Actor Transformer for Fine-grained Skeleton-based Action Recognition</title><link>https://doi.org/10.1109/tcsvt.2025.3650554</link><guid>10.1109/tcsvt.2025.3650554</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Yisheng Zhu</dc:creator><dc:creator>Guangcan Liu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3650554</prism:doi><description>The scarcity of flexibility and effectiveness in skeleton models, combined with the characteristic of limited information in skeleton data, has resulted in fine-grained modeling insufficiently explored in recent skeleton-based action recognition. Confronting these challenges, we propose Dynamic prompting Spatial Temporal Actor transFormer (DSTAFormer), a powerful framework which neatly unifies vision and language. Specifically, we introduce a decoupled vision transformer, which consists of three components: Spatial transFormer (SF), Temporal transFormer (TF), and Actor transFormer (AF), to account for numerous visual aspects of the human body, namely spatial, temporal, and interactive relations. Compared to the vanilla transformer, we reformulate self-attention using Statistically-inspired Attention Reconstruction (SAR) module and Local-specific constraints, thereby enabling a more explicit and interpretable exploration of the action’s fine-grained compositions. The skeleton sequences are processed by this decoupled structure to generate the visual embeddings. To encode environmental interactions that skeletal coordinates inherently lack, we utilize Dynamic Prompting (Dp) strategy to generate visual-based textual prompts. These prompts are transformed into discriminative textual embeddings via a pre-trained large language model (LLM). We also design a Semantic Adapter (SA) to bridge the modality gap. The cross-modality embeddings are projected into a unified feature space for contrastive co-training. This infusion of knowledge into the skeleton data enhances its semantic richness, pushing the boundaries of fine-grained understanding. We evaluate our framework on NTU RGB+D, NTU RGB+D 120, and Toyota Smarthome datasets. DSTAFormer achieves comparable performance against state-of-the-arts.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yisheng Zhu; Guangcan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3650554"&gt;10.1109/tcsvt.2025.3650554&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;The scarcity of flexibility and effectiveness in skeleton models, combined with the characteristic of limited information in skeleton data, has resulted in fine-grained modeling insufficiently explored in recent skeleton-based action recognition. Confronting these challenges, we propose Dynamic prompting Spatial Temporal Actor transFormer (DSTAFormer), a powerful framework which neatly unifies vision and language. Specifically, we introduce a decoupled vision transformer, which consists of three components: Spatial transFormer (SF), Temporal transFormer (TF), and Actor transFormer (AF), to account for numerous visual aspects of the human body, namely spatial, temporal, and interactive relations. Compared to the vanilla transformer, we reformulate self-attention using Statistically-inspired Attention Reconstruction (SAR) module and Local-specific constraints, thereby enabling a more explicit and interpretable exploration of the action’s fine-grained compositions. The skeleton sequences are processed by this decoupled structure to generate the visual embeddings. To encode environmental interactions that skeletal coordinates inherently lack, we utilize Dynamic Prompting (Dp) strategy to generate visual-based textual prompts. These prompts are transformed into discriminative textual embeddings via a pre-trained large language model (LLM). We also design a Semantic Adapter (SA) to bridge the modality gap. The cross-modality embeddings are projected into a unified feature space for contrastive co-training. This infusion of knowledge into the skeleton data enhances its semantic richness, pushing the boundaries of fine-grained understanding. We evaluate our framework on NTU RGB+D, NTU RGB+D 120, and Toyota Smarthome datasets. DSTAFormer achieves comparable performance against state-of-the-arts.&lt;/p&gt;</content:encoded></item><item><title>DPS-Net: Direction-Aware Pseudo-Stereo Network for Accurate Road Surface Reconstruction</title><link>https://doi.org/10.1109/tcsvt.2026.3650947</link><guid>10.1109/tcsvt.2026.3650947</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Shiyuan Han</dc:creator><dc:creator>Yidan Pei</dc:creator><dc:creator>Rui Wang</dc:creator><dc:creator>Tong Zhang</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3650947</prism:doi><description>The geometry of road surfaces plays a critical role in the performance of autonomous driving systems. Consequently, achieving accurate and efficient road surface reconstruction (RSR) is of paramount importance. However, due to the inherent effects of perspective projection, distant regions often exhibit geometric distortions and a long-tailed distribution, which pose significant challenges to existing reconstruction methods. To address these issues, we propose a novel framework, termed Direction-aware Pseudo-Stereo Road Reconstruction Network (DPS-Net), which incorporates two lightweight and plug-and-play modules: Direction-Aware Feature Enhancement (DFE) module and Pseudo-Stereo Fusion (PSF) module. The DFE module is designed to enhance the perception of sparse and geometry-invariant features by integrating directional context, while the PSF module captures global dependencies across spatial and channel dimensions through pseudo-stereo fusion. Both modules are constructed with an emphasis on maintaining low computational complexity. We conducted extensive experiments on the public RSRD dataset to evaluate the effectiveness and superiority of our proposed method. The code is available at https://github.com/yidanyi/DPS-Net.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shiyuan Han; Yidan Pei; Rui Wang; Tong Zhang; C. L. Philip Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3650947"&gt;10.1109/tcsvt.2026.3650947&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;The geometry of road surfaces plays a critical role in the performance of autonomous driving systems. Consequently, achieving accurate and efficient road surface reconstruction (RSR) is of paramount importance. However, due to the inherent effects of perspective projection, distant regions often exhibit geometric distortions and a long-tailed distribution, which pose significant challenges to existing reconstruction methods. To address these issues, we propose a novel framework, termed Direction-aware Pseudo-Stereo Road Reconstruction Network (DPS-Net), which incorporates two lightweight and plug-and-play modules: Direction-Aware Feature Enhancement (DFE) module and Pseudo-Stereo Fusion (PSF) module. The DFE module is designed to enhance the perception of sparse and geometry-invariant features by integrating directional context, while the PSF module captures global dependencies across spatial and channel dimensions through pseudo-stereo fusion. Both modules are constructed with an emphasis on maintaining low computational complexity. We conducted extensive experiments on the public RSRD dataset to evaluate the effectiveness and superiority of our proposed method. The code is available at https://github.com/yidanyi/DPS-Net.&lt;/p&gt;</content:encoded></item><item><title>Excluding the Interference for Open-Vocabulary Semantic Segmentation</title><link>https://doi.org/10.1109/tcsvt.2026.3650803</link><guid>10.1109/tcsvt.2026.3650803</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Shuai Shao</dc:creator><dc:creator>Shiyuan Zhao</dc:creator><dc:creator>Rui Xu</dc:creator><dc:creator>Yan Wang</dc:creator><dc:creator>Baodi Liu</dc:creator><dc:creator>Weifeng Liu</dc:creator><dc:creator>Yicong Zhou</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3650803</prism:doi><description>Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuai Shao; Shiyuan Zhao; Rui Xu; Yan Wang; Baodi Liu; Weifeng Liu; Yicong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3650803"&gt;10.1109/tcsvt.2026.3650803&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) is a hot research domain aimed at pixel-level categorization in dynamic environments, requiring the identification of both familiar categories and those known only by name but never visually encountered, offering significant practical value. Mainstream solutions integrate CLIP for category identification but often bias the model to misclassify novel categories as common ones (i.e., interference terms) due to inherent category imbalances within CLIP and exclusive reliance on known-class images for training. To address this issue, we introduce a novel approach named EXcluding the Interference Semantic SegmenTation Network (EXIST-Net), an extension of ELSE-Net, first presented at AAAI 2025. EXIST-Net transforms conventional single-step recognition into a nuanced two-stage process: initially filtering out interference terms to narrow the selection range, followed by enabling more precise identification of the sample’s specific category. In implementation, EXIST-Net consists of four blocks: (1) Mask Proposal Network (MPN) generates class-agnostic masks. (2) Mask Forward Classifier (MFC) assesses the inclusion probability (the likelihood that a mask belongs to a category). (3) Mask Reverse Classifier (MRC) is the cornerstone to implement the “Excluding the Interference” concept. It calculates high-quality exclusion probabilities (the likelihood that a mask does not belong to a specific category). (4) Probability Corrector (PCor) leverages exclusion probabilities to adjust inclusion probabilities, thereby improving the accuracy of semantic segmentation. Moreover, the MRC block is model-agnostic and entails low consumption, making it compatible with a wide range of mainstream approaches. Experimental results on five benchmark datasets validate the effectiveness of EXIST-Net and demonstrate the model-agnostic functionality and low resource usage of the MRC block.&lt;/p&gt;</content:encoded></item><item><title>Generalizable and Adaptive Continual Learning Framework for AI-generated Image Detection</title><link>https://doi.org/10.1109/tmm.2026.3651067</link><guid>10.1109/tmm.2026.3651067</guid><pubDate>Mon, 05 Jan 2026 18:39:22 +0000</pubDate><dc:creator>Hanyi Wang</dc:creator><dc:creator>Jun Lan</dc:creator><dc:creator>Yaoyu Kang</dc:creator><dc:creator>Huijia Zhu</dc:creator><dc:creator>Weiqiang Wang</dc:creator><dc:creator>Zhuosheng Zhang</dc:creator><dc:creator>Shilin Wang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651067</prism:doi><description>The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.
Published: 2026-01-05T18:39:22+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanyi Wang; Jun Lan; Yaoyu Kang; Huijia Zhu; Weiqiang Wang; Zhuosheng Zhang; Shilin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651067"&gt;10.1109/tmm.2026.3651067&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;The malicious misuse and widespread dissemination of AI-generated images pose a significant threat to the authenticity of online information. Current detection methods often struggle to generalize to unseen generative models, and the rapid evolution of generative techniques continuously exacerbates this challenge. Without adaptability, detection models risk becoming ineffective in real-world applications. To address this critical issue, we propose a novel three-stage domain continual learning framework designed for continuous adaptation to evolving generative models. In the first stage, we employ a strategic parameter-efficient fine-tuning approach to develop a transferable offline detection model with strong generalization capabilities. Building upon this foundation, the second stage integrates unseen data streams into a continual learning process. To efficiently learn from limited samples of novel generated models and mitigate overfitting, we design a data augmentation chain with progressively increasing complexity. Furthermore, we leverage the Kronecker-Factored Approximate Curvature (K-FAC) method to approximate the Hessian and alleviate catastrophic forgetting. Finally, the third stage utilizes a linear interpolation strategy based on Linear Mode Connectivity, effectively capturing commonalities across diverse generative models and further enhancing overall performance. We establish a comprehensive benchmark of 27 generative models, including GANs, deepfakes, and diffusion models, chronologically structured up to August 2024 to simulate real-world scenarios. Extensive experiments demonstrate that our initial offline detectors surpass the leading baseline by +5.51% in terms of mean average precision. Our continual learning strategy achieves an average accuracy of 92.20%, outperforming state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Relative depth knowledge distillation for generalizable monocular depth estimation</title><link>https://doi.org/10.1016/j.neucom.2026.132632</link><guid>10.1016/j.neucom.2026.132632</guid><pubDate>Mon, 05 Jan 2026 16:47:54 +0000</pubDate><dc:creator>Lulu Zhang</dc:creator><dc:creator>Mankun Li</dc:creator><dc:creator>Meng Yang</dc:creator><dc:creator>Xuguang Lan</dc:creator><dc:creator>Ce Zhu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132632</prism:doi><description>Monocular depth estimation provides an easily deployable solution for robots to perceive the 3D scene. Existing methods have achieved impressive performance on benchmark datasets. However, these methods tend to overfit to training domains, resulting in limited generalization in the real world. A dominant solution is to train on large-scale datasets featuring high-quality GT depth and precise camera intrinsics, both of which are often unavailable or difficult to obtain. To mitigate this issue, we propose a relative depth knowledge distillation framework to boost the generalization of monocular depth estimation with limited training data. It is based on the insight that recent relative depth foundation models can be trained efficiently on large-scale datasets to capture accurate object structure and general relative depth relationships. More specifically, in the teacher network, we generate relative depth from a pre-trained foundation model and introduce a scale alignment module to ensure its scale consistency with GT depth. In the student network, we infer the depth bin centers and corresponding probabilities to represent the scales and relative depth relationships, respectively, and compute the final depth via their linear combination. Furthermore, we design two novel response-based distillation modules to distill knowledge of relative depth and object structure, respectively, from the teacher to the student. For validation, our model is trained on widely used benchmark datasets in three settings, including indoor NYUDv2, outdoor KITTI, and a mixture of both. Extensive experiments on six unseen indoor and outdoor datasets verify that our model consistently reduces RMSE of base model by 3.5 %, 5.3 %, and 5.4 % on average, respectively, and achieves state-of-the-art in the three settings. Our model even achieves competitive accuracy when compared to recent models trained on very large-scale datasets.
Published: 2026-01-05T16:47:54+00:00
Venue: Neurocomputing
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lulu Zhang; Mankun Li; Meng Yang; Xuguang Lan; Ce Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132632"&gt;10.1016/j.neucom.2026.132632&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Monocular depth estimation provides an easily deployable solution for robots to perceive the 3D scene. Existing methods have achieved impressive performance on benchmark datasets. However, these methods tend to overfit to training domains, resulting in limited generalization in the real world. A dominant solution is to train on large-scale datasets featuring high-quality GT depth and precise camera intrinsics, both of which are often unavailable or difficult to obtain. To mitigate this issue, we propose a relative depth knowledge distillation framework to boost the generalization of monocular depth estimation with limited training data. It is based on the insight that recent relative depth foundation models can be trained efficiently on large-scale datasets to capture accurate object structure and general relative depth relationships. More specifically, in the teacher network, we generate relative depth from a pre-trained foundation model and introduce a scale alignment module to ensure its scale consistency with GT depth. In the student network, we infer the depth bin centers and corresponding probabilities to represent the scales and relative depth relationships, respectively, and compute the final depth via their linear combination. Furthermore, we design two novel response-based distillation modules to distill knowledge of relative depth and object structure, respectively, from the teacher to the student. For validation, our model is trained on widely used benchmark datasets in three settings, including indoor NYUDv2, outdoor KITTI, and a mixture of both. Extensive experiments on six unseen indoor and outdoor datasets verify that our model consistently reduces RMSE of base model by 3.5 %, 5.3 %, and 5.4 % on average, respectively, and achieves state-of-the-art in the three settings. Our model even achieves competitive accuracy when compared to recent models trained on very large-scale datasets.&lt;/p&gt;</content:encoded></item><item><title>A Multi-Modal Knowledge-Driven Approach for Generalized Zero-shot Video Classification</title><link>https://doi.org/10.1007/s11263-025-02584-3</link><guid>10.1007/s11263-025-02584-3</guid><pubDate>Sun, 04 Jan 2026 02:39:12 +0000</pubDate><dc:creator>Mingyao Hong</dc:creator><dc:creator>Xinfeng Zhang</dc:creator><dc:creator>Guorong Li</dc:creator><dc:creator>Qingming Huang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02584-3</prism:doi><description>Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.
Published: 2026-01-04T02:39:12+00:00
Venue: International Journal of Computer Vision
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingyao Hong; Xinfeng Zhang; Guorong Li; Qingming Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02584-3"&gt;10.1007/s11263-025-02584-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Learning video information only by their category names limited the development of the generalized zero-shot video classification (GZSVC) task. By analyzing the way that humans learn new things, we found that people can utilize knowledge such as textual concepts and visual fundamentals to construct new video cognition. Taking this as inspiration, we propose a multi-modal knowledge-driven approach to solve the GZSVC task by searching and learning various knowledge. In the real world, it is hard to guarantee that important components of new videos can be covered by existing knowledge. To bridge this knowledge gap, our method constructs a reliable knowledge supplement from multi-modal information for categories, which can also establish connections between classes. In order to fuse the information from different modalities, we propose a multi-modal generative model to synthesize visual features that are rich in content and closer to the true distribution of videos. Since training process lacks real unseen visual information, we propose that the model should pay more attention to semantic information in this task, and we strengthen the constraint and utilization of semantic information in the proposed framework. Extensive experimental results on various databases show that our proposed method outperforms the state-of-the-art GZSVC methods.&lt;/p&gt;</content:encoded></item><item><title>DreamJourney: Perpetual View Generation with Video Diffusion Models</title><link>https://doi.org/10.1109/tmm.2026.3651030</link><guid>10.1109/tmm.2026.3651030</guid><pubDate>Mon, 05 Jan 2026 18:39:22 +0000</pubDate><dc:creator>Bo Pan</dc:creator><dc:creator>Yang Chen</dc:creator><dc:creator>Yingwei Pan</dc:creator><dc:creator>Ting Yao</dc:creator><dc:creator>Wei Chen</dc:creator><dc:creator>Tao Mei</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651030</prism:doi><description>Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: https://dream-journey.vercel.app/.
Published: 2026-01-05T18:39:22+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.778 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Pan; Yang Chen; Yingwei Pan; Ting Yao; Wei Chen; Tao Mei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651030"&gt;10.1109/tmm.2026.3651030&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (consider)&lt;/p&gt;
&lt;p&gt;Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: https://dream-journey.vercel.app/.&lt;/p&gt;</content:encoded></item><item><title>Vision transformer with salience self-attention for underwater and aerial object recognition and tracking</title><link>https://doi.org/10.1016/j.neucom.2026.132631</link><guid>10.1016/j.neucom.2026.132631</guid><pubDate>Mon, 05 Jan 2026 16:10:35 +0000</pubDate><dc:creator>Sai Zhou</dc:creator><dc:creator>Meiqin Liu</dc:creator><dc:creator>Jing Zhou</dc:creator><dc:creator>Ronghao Zheng</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132631</prism:doi><description>The recognition and tracking of underwater and aerial objects are crucial for the perception of air-water cross-domain robots. This paper proposes a Vision Transformer (ViT) framework based on Salience Self-Attention (SSA) for underwater and aerial object detection and tracking tasks. By employing the " role="presentation"&gt; norm-based evaluation criterion of token importance, a small number of salient tokens are selected for self-attention computation, reducing computational burden while retaining global modeling capability. Next, non-salient tokens are aggregated to enhance the interaction between foreground and background tokens. To further improve the backbone capabilities, we restore the tokens to their original positions and incorporate a semantic complement module for sparse self-attention. The established S-ViT backbone is built and evaluated on the ImageNet-1K benchmark dataset, integrated with an advanced head for detection, and further extended into a Siamese framework for tracking tasks. Our proposed methods are validated across six underwater and aerial object detection and tracking datasets. For example, on the RUOD and VisDrone-DET object detection datasets, our S-ViT-DETR outperforms the baseline on mAP by 3.6 % and 1.9 %, respectively. On the UTB180 object tracking dataset, the S-ViT-Track surpasses OSTrack on the AUC metric by 2.8 % with a smaller model size. The code is available at: https://github.com/saizhou777/S-ViT .
Published: 2026-01-05T16:10:35+00:00
Venue: Neurocomputing
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sai Zhou; Meiqin Liu; Jing Zhou; Ronghao Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132631"&gt;10.1016/j.neucom.2026.132631&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;The recognition and tracking of underwater and aerial objects are crucial for the perception of air-water cross-domain robots. This paper proposes a Vision Transformer (ViT) framework based on Salience Self-Attention (SSA) for underwater and aerial object detection and tracking tasks. By employing the &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; norm-based evaluation criterion of token importance, a small number of salient tokens are selected for self-attention computation, reducing computational burden while retaining global modeling capability. Next, non-salient tokens are aggregated to enhance the interaction between foreground and background tokens. To further improve the backbone capabilities, we restore the tokens to their original positions and incorporate a semantic complement module for sparse self-attention. The established S-ViT backbone is built and evaluated on the ImageNet-1K benchmark dataset, integrated with an advanced head for detection, and further extended into a Siamese framework for tracking tasks. Our proposed methods are validated across six underwater and aerial object detection and tracking datasets. For example, on the RUOD and VisDrone-DET object detection datasets, our S-ViT-DETR outperforms the baseline on mAP by 3.6 % and 1.9 %, respectively. On the UTB180 object tracking dataset, the S-ViT-Track surpasses OSTrack on the AUC metric by 2.8 % with a smaller model size. The code is available at: https://github.com/saizhou777/S-ViT .&lt;/p&gt;</content:encoded></item><item><title>TransZSIS: Superpixel-guided Irregular Patch-Pair Features Learning with Transformer for Zero-Shot Instance Segmentation in Robotic Environments</title><link>https://doi.org/10.1109/tmm.2026.3651018</link><guid>10.1109/tmm.2026.3651018</guid><pubDate>Mon, 05 Jan 2026 18:39:22 +0000</pubDate><dc:creator>Ying Zhang</dc:creator><dc:creator>Haopeng Zhang</dc:creator><dc:creator>Maoliang Yin</dc:creator><dc:creator>Kai Ma</dc:creator><dc:creator>Cui-Hua Zhang</dc:creator><dc:creator>Changchun Hua</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651018</prism:doi><description>Object instance segmentation is a key prerequisite for service robots to perform daily chores in unstructured environments. Traditional supervised learning-based segmentation solutions rely on massive annotated datasets, which are impractical for the wide variety of objects in real-world scenarios. To this end, we propose a novel zero-shot instance segmentation approach (TransZSIS) that enables precise instance segmentation without relying on external semantic embeddings or auxiliary information to address the unseen object instance segmentation (UOIS) problem. First, the RGB and depth images are segmented into irregular patches based on a super-pixel segmentation algorithm to generate a unified segmentation map, and then the comprehensive feature vectors of each patch is extracted and paired. Further, a Transformer-based architecture is introduced to capture the correlation between different patch-pair and the intrinsic characteristics of each patch-pair. To predict patch-pair relationships, TransZSIS uses a four-layer fully connected neural network (FCNN) to classify the transformer-encoded features and refine them with a graph-based processing tactic to achieve object instance segmentation. Extensive evaluations on both synthetic and real datasets demonstrate that TransZSIS achieves superior performance compared with state-of-the-art baseline methods. Also, we implement real experiments to verify that our solution can achieve robot grasping by segmenting unseen objects.
Published: 2026-01-05T18:39:22+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ying Zhang; Haopeng Zhang; Maoliang Yin; Kai Ma; Cui-Hua Zhang; Changchun Hua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651018"&gt;10.1109/tmm.2026.3651018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Object instance segmentation is a key prerequisite for service robots to perform daily chores in unstructured environments. Traditional supervised learning-based segmentation solutions rely on massive annotated datasets, which are impractical for the wide variety of objects in real-world scenarios. To this end, we propose a novel zero-shot instance segmentation approach (TransZSIS) that enables precise instance segmentation without relying on external semantic embeddings or auxiliary information to address the unseen object instance segmentation (UOIS) problem. First, the RGB and depth images are segmented into irregular patches based on a super-pixel segmentation algorithm to generate a unified segmentation map, and then the comprehensive feature vectors of each patch is extracted and paired. Further, a Transformer-based architecture is introduced to capture the correlation between different patch-pair and the intrinsic characteristics of each patch-pair. To predict patch-pair relationships, TransZSIS uses a four-layer fully connected neural network (FCNN) to classify the transformer-encoded features and refine them with a graph-based processing tactic to achieve object instance segmentation. Extensive evaluations on both synthetic and real datasets demonstrate that TransZSIS achieves superior performance compared with state-of-the-art baseline methods. Also, we implement real experiments to verify that our solution can achieve robot grasping by segmenting unseen objects.&lt;/p&gt;</content:encoded></item><item><title>Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation</title><link>https://arxiv.org/abs/2601.01457v1</link><guid>http://arxiv.org/abs/2601.01457v1</guid><pubDate>Sun, 04 Jan 2026 09:59:43 +0000</pubDate><dc:creator>Mingxing Zhan</dc:creator><dc:creator>Li Zhang</dc:creator><dc:creator>Beibei Wang</dc:creator><dc:creator>Yingjie Wang</dc:creator><dc:creator>Zenglin Shi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.
Published: 2026-01-04T09:59:43+00:00
Venue: arXiv
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingxing Zhan; Li Zhang; Beibei Wang; Yingjie Wang; Zenglin Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.&lt;/p&gt;</content:encoded></item></channel></rss>