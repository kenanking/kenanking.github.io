<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 23 Jan 2026 02:48:46 +0000</lastBuildDate><item><title>LDFENet: A Lightweight Dilated Feature Enhanced Network for SAR Ship Detection</title><link>https://doi.org/10.1109/jstars.2026.3656551</link><guid>10.1109/jstars.2026.3656551</guid><pubDate>Wed, 21 Jan 2026 21:10:12 +0000</pubDate><dc:creator>Yi Zheng</dc:creator><dc:creator>Fengkai Lang</dc:creator><dc:creator>Jinqi Zhao</dc:creator><dc:creator>Zhangjie Chen</dc:creator><dc:creator>Zixuan Wang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3656551</prism:doi><description>Synthetic Aperture Radar (SAR) possesses all-weather and all-time imaging capabilities, which play a crucial role in ship monitoring. Meanwhile, convolutional neural networks (CNNs) are extensively employed in SAR ship detection because of their strong ability to extract features. However, existing CNN-based SAR ship detectors often exhibit insufficient sensitivity to small-scale ship targets, primarily due to inadequate multi-scale feature representation and limited feature expressiveness in complex environments. Moreover, some models rely on large network architectures, which restricts their deployment in real-time applications. To address these challenges, we propose a lightweight dilated feature enhanced network (LDFENet). First, we design the CSBS module, in which a space-to-depth convolution (SPDConv) is incorporated to reduce the loss of small ships information. Second, we introduce the augmented dilated efficient layer aggregation network module (AD-ELAN) to improve the model's adaptability for targets of varying scales. Third, we utilize a mixed attention module to enhance features while suppressing background noise. Finally, the Powerful-IoU loss function (PIoU) is adopted to improve target localization performance and accelerate model convergence. Experimental results on the HRSID and SSDD datasets demonstrate the effectiveness of LDFENet in SAR ship detection. With only 4.3M parameters and 10.8 GFLOPs, LDFENet achieves superior accuracy and efficiency compared to the latest methods. Furthermore, LDFENet exhibits strong generalization capability on large-scale SAR images, highlighting its potential for practical applications. The code will be available at https://github.com/Z-LM-10/LDFENet
Published: 2026-01-21T21:10:12+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Zheng; Fengkai Lang; Jinqi Zhao; Zhangjie Chen; Zixuan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3656551"&gt;10.1109/jstars.2026.3656551&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) possesses all-weather and all-time imaging capabilities, which play a crucial role in ship monitoring. Meanwhile, convolutional neural networks (CNNs) are extensively employed in SAR ship detection because of their strong ability to extract features. However, existing CNN-based SAR ship detectors often exhibit insufficient sensitivity to small-scale ship targets, primarily due to inadequate multi-scale feature representation and limited feature expressiveness in complex environments. Moreover, some models rely on large network architectures, which restricts their deployment in real-time applications. To address these challenges, we propose a lightweight dilated feature enhanced network (LDFENet). First, we design the CSBS module, in which a space-to-depth convolution (SPDConv) is incorporated to reduce the loss of small ships information. Second, we introduce the augmented dilated efficient layer aggregation network module (AD-ELAN) to improve the model&amp;#x27;s adaptability for targets of varying scales. Third, we utilize a mixed attention module to enhance features while suppressing background noise. Finally, the Powerful-IoU loss function (PIoU) is adopted to improve target localization performance and accelerate model convergence. Experimental results on the HRSID and SSDD datasets demonstrate the effectiveness of LDFENet in SAR ship detection. With only 4.3M parameters and 10.8 GFLOPs, LDFENet achieves superior accuracy and efficiency compared to the latest methods. Furthermore, LDFENet exhibits strong generalization capability on large-scale SAR images, highlighting its potential for practical applications. The code will be available at https://github.com/Z-LM-10/LDFENet&lt;/p&gt;</content:encoded></item><item><title>Prototype Decoupled Knowledge Distillation</title><link>https://doi.org/10.1109/tcsvt.2026.3656746</link><guid>10.1109/tcsvt.2026.3656746</guid><pubDate>Wed, 21 Jan 2026 21:12:03 +0000</pubDate><dc:creator>Yuanwei Liu</dc:creator><dc:creator>Zheng Qu</dc:creator><dc:creator>Nian Liu</dc:creator><dc:creator>Xiwen Yao</dc:creator><dc:creator>Junwei Han</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3656746</prism:doi><description>Knowledge distillation (KD) has emerged as a powerful technique for transferring knowledge from large, complex teacher models to smaller, more efficient student models. However, current KD methods primarily concentrate on mimicking instance-level predictions or feature representations, often overlooking the crucial role of class-level semantic structure in guiding effective knowledge transfer. This paper introduces Prototypical Decoupled Knowledge Distillation (PDKD), a novel framework designed to bridge the gap between instance-specific and class-discriminative knowledge by leveraging class prototypes. PDKD incorporates a prototype-aware supervision module that distills global class characteristics by aligning student predictions with both instance-level and prototype-based outputs from the teacher. This module dynamically harmonizes the logit scales of these two targets, effectively addressing the model size mismatches between teacher and student. Furthermore, a feature discrepancy alignment module is proposed to enforce consistency between the teacher and student in how they modulate features between the learned prototypes and individual samples. This alignment preserves the structural relationships between classes. By effectively unifying hierarchical class semantics with instance-level learning, PDKD establishes a new paradigm for training compact yet highly discriminative models. Extensive experiments on CIFAR-100 and ImageNet showcase the superior performance of PDKD compared to existing state-of-the-art methods.
Published: 2026-01-21T21:12:03+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanwei Liu; Zheng Qu; Nian Liu; Xiwen Yao; Junwei Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3656746"&gt;10.1109/tcsvt.2026.3656746&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Knowledge distillation (KD) has emerged as a powerful technique for transferring knowledge from large, complex teacher models to smaller, more efficient student models. However, current KD methods primarily concentrate on mimicking instance-level predictions or feature representations, often overlooking the crucial role of class-level semantic structure in guiding effective knowledge transfer. This paper introduces Prototypical Decoupled Knowledge Distillation (PDKD), a novel framework designed to bridge the gap between instance-specific and class-discriminative knowledge by leveraging class prototypes. PDKD incorporates a prototype-aware supervision module that distills global class characteristics by aligning student predictions with both instance-level and prototype-based outputs from the teacher. This module dynamically harmonizes the logit scales of these two targets, effectively addressing the model size mismatches between teacher and student. Furthermore, a feature discrepancy alignment module is proposed to enforce consistency between the teacher and student in how they modulate features between the learned prototypes and individual samples. This alignment preserves the structural relationships between classes. By effectively unifying hierarchical class semantics with instance-level learning, PDKD establishes a new paradigm for training compact yet highly discriminative models. Extensive experiments on CIFAR-100 and ImageNet showcase the superior performance of PDKD compared to existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Interpretable Few-Shot Image Classification via Prototypical Concept-Guided Mixture of LoRA Experts</title><link>https://doi.org/10.1109/tip.2026.3654473</link><guid>10.1109/tip.2026.3654473</guid><pubDate>Wed, 21 Jan 2026 21:12:53 +0000</pubDate><dc:creator>Zhong Ji</dc:creator><dc:creator>Rongshuai Wei</dc:creator><dc:creator>Jingren Liu</dc:creator><dc:creator>Yanwei Pang</dc:creator><dc:creator>Jungong Han</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654473</prism:doi><description>Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to enable their visual recognition processes more interpretable, but they often struggle in data-scarce settings where insufficient training samples lead to suboptimal performance. To address this limitation, we propose a Few-Shot Prototypical Concept Classification (FSPCC) framework that systematically mitigates two key challenges under low-data regimes: parametric imbalance and representation misalignment. Specifically, our approach leverages a Mixture of LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced allocation of trainable parameters between the backbone and the PCL module. Meanwhile, cross-module concept guidance enforces tight alignment between the backbone’s feature representations and the prototypical concept activation patterns. In addition, we incorporate a multi-level feature preservation strategy that fuses spatial and semantic cues across various layers, thereby enriching the learned representations and mitigating the challenges posed by limited data availability. Finally, to enhance interpretability and minimize concept overlap, we introduce a geometry-aware concept discrimination loss that enforces orthogonality among concepts, encouraging more disentangled and transparent decision boundaries. Experimental results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach consistently outperforms existing SEMs by a notable margin, with 4.2%–8.7% relative gains in 5-way 5-shot classification. These findings highlight the efficacy of coupling concept learning with few-shot adaptation to achieve both higher accuracy and clearer model interpretability, paving the way for more transparent visual recognition systems.
Published: 2026-01-21T21:12:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhong Ji; Rongshuai Wei; Jingren Liu; Yanwei Pang; Jungong Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654473"&gt;10.1109/tip.2026.3654473&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Self-Explainable Models (SEMs) rely on Prototypical Concept Learning (PCL) to enable their visual recognition processes more interpretable, but they often struggle in data-scarce settings where insufficient training samples lead to suboptimal performance. To address this limitation, we propose a Few-Shot Prototypical Concept Classification (FSPCC) framework that systematically mitigates two key challenges under low-data regimes: parametric imbalance and representation misalignment. Specifically, our approach leverages a Mixture of LoRA Experts (MoLE) for parameter-efficient adaptation, ensuring a balanced allocation of trainable parameters between the backbone and the PCL module. Meanwhile, cross-module concept guidance enforces tight alignment between the backbone’s feature representations and the prototypical concept activation patterns. In addition, we incorporate a multi-level feature preservation strategy that fuses spatial and semantic cues across various layers, thereby enriching the learned representations and mitigating the challenges posed by limited data availability. Finally, to enhance interpretability and minimize concept overlap, we introduce a geometry-aware concept discrimination loss that enforces orthogonality among concepts, encouraging more disentangled and transparent decision boundaries. Experimental results on six popular benchmarks (CUB-200-2011, mini-ImageNet, CIFAR-FS, Stanford Cars, FGVC-Aircraft, and DTD) demonstrate that our approach consistently outperforms existing SEMs by a notable margin, with 4.2%–8.7% relative gains in 5-way 5-shot classification. These findings highlight the efficacy of coupling concept learning with few-shot adaptation to achieve both higher accuracy and clearer model interpretability, paving the way for more transparent visual recognition systems.&lt;/p&gt;</content:encoded></item><item><title>PCL-Reasoner-V1.5: Advancing Math Reasoning with Offline Reinforcement Learning</title><link>https://arxiv.org/abs/2601.14716v1</link><guid>http://arxiv.org/abs/2601.14716v1</guid><pubDate>Wed, 21 Jan 2026 07:11:40 +0000</pubDate><dc:creator>Yao Lu</dc:creator><dc:creator>Dengdong Fan</dc:creator><dc:creator>Jianzheng Nie</dc:creator><dc:creator>Fan Xu</dc:creator><dc:creator>Jie Chen</dc:creator><dc:creator>Bin Zhou</dc:creator><dc:creator>Yonghong Tian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.
Published: 2026-01-21T07:11:40+00:00
Venue: arXiv
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yao Lu; Dengdong Fan; Jianzheng Nie; Fan Xu; Jie Chen; Bin Zhou; Yonghong Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;We present PCL-Reasoner-V1.5, a 32-billion-parameter large language model (LLM) for mathematical reasoning. The model is built upon Qwen2.5-32B and refined via supervised fine-tuning (SFT) followed by reinforcement learning (RL). A central innovation is our proposed offline RL method, which provides superior training stability and efficiency over standard online RL methods such as GRPO. Our model achieves state-of-the-art performance among models post-trained on Qwen2.5-32B, attaining average accuracies of 90.9% on AIME 2024 and 85.6% on AIME 2025. Our work demonstrates offline RL as a stable and efficient paradigm for advancing reasoning in LLMs. All experiments were conducted on Huawei Ascend 910C NPUs.&lt;/p&gt;</content:encoded></item><item><title>AMFS-Net: An Adaptive Multi-Scale Feature Fusion Framework for Efficient SAR Ship Detection</title><link>https://doi.org/10.1109/lgrs.2026.3656578</link><guid>10.1109/lgrs.2026.3656578</guid><pubDate>Wed, 21 Jan 2026 21:13:24 +0000</pubDate><dc:creator>Cheng Zhang</dc:creator><dc:creator>Jianping Xing</dc:creator><dc:creator>Yue Wang</dc:creator><dc:creator>Yufeng Liu</dc:creator><dc:creator>Yixuan Xing</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2026.3656578</prism:doi><description>Real-time deployment of Synthetic Aperture Radar (SAR) ship detection systems encounters substantial obstacles stemming from inherent speckle noise characteristics, intricate background interference patterns, and excessive computational demands in contemporary deep learning architectures. We present AMFS-Net (Adaptive Multi-scale Feature-fusion SAR detection Network), a streamlined framework that realizes efficient SAR vessel identification through three synergistic technological innovations. The C3k2 FasterWConv module leverages weighted convolution operations with selective quarter-channel processing, exploiting spatial density functions for dynamic weight adjustment while substantially diminishing computational complexity. A Dual-Scale Efficient Detection Head (DSED) implements a P3+P4 architecture featuring intelligent parameter sharing mechanisms, removing the computation-intensive P2 layer and yielding a 33% reduction in detection overhead. The Adaptive Parameter Scaling Optimization (APSO) framework executes joint depth-width-channel scaling coupled with sensitivity-guided preservation of critical layers, achieving 27-34% parameter reduction. Furthermore, a Hierarchical Adaptive Geometric Optimization IoU (HAGOIoU) loss function employing quality-adaptive weighting effectively addresses speckle noise and sea clutter interference. Extensive experimental validation on SSDD and HRSID benchmark datasets demonstrates that AMFS-Net attains 98.2% and 93.1% mAP50 respectively, while reducing parameters by 38.4% and accelerating inference speed by 48.0% relative to the baseline YOLOv11n architecture. The proposed framework establishes an effective paradigm for real-time SAR ship detection deployment in resource-constrained computational environments while preserving superior detection accuracy.
Published: 2026-01-21T21:13:24+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheng Zhang; Jianping Xing; Yue Wang; Yufeng Liu; Yixuan Xing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2026.3656578"&gt;10.1109/lgrs.2026.3656578&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time deployment of Synthetic Aperture Radar (SAR) ship detection systems encounters substantial obstacles stemming from inherent speckle noise characteristics, intricate background interference patterns, and excessive computational demands in contemporary deep learning architectures. We present AMFS-Net (Adaptive Multi-scale Feature-fusion SAR detection Network), a streamlined framework that realizes efficient SAR vessel identification through three synergistic technological innovations. The C3k2 FasterWConv module leverages weighted convolution operations with selective quarter-channel processing, exploiting spatial density functions for dynamic weight adjustment while substantially diminishing computational complexity. A Dual-Scale Efficient Detection Head (DSED) implements a P3+P4 architecture featuring intelligent parameter sharing mechanisms, removing the computation-intensive P2 layer and yielding a 33% reduction in detection overhead. The Adaptive Parameter Scaling Optimization (APSO) framework executes joint depth-width-channel scaling coupled with sensitivity-guided preservation of critical layers, achieving 27-34% parameter reduction. Furthermore, a Hierarchical Adaptive Geometric Optimization IoU (HAGOIoU) loss function employing quality-adaptive weighting effectively addresses speckle noise and sea clutter interference. Extensive experimental validation on SSDD and HRSID benchmark datasets demonstrates that AMFS-Net attains 98.2% and 93.1% mAP50 respectively, while reducing parameters by 38.4% and accelerating inference speed by 48.0% relative to the baseline YOLOv11n architecture. The proposed framework establishes an effective paradigm for real-time SAR ship detection deployment in resource-constrained computational environments while preserving superior detection accuracy.&lt;/p&gt;</content:encoded></item><item><title>Retrieval-augmented Pseudo-image Guided Alignment and Text Domain-aware Memory Recall for Continual Zero-shot Captioning</title><link>https://doi.org/10.1109/tcsvt.2026.3656817</link><guid>10.1109/tcsvt.2026.3656817</guid><pubDate>Wed, 21 Jan 2026 21:12:03 +0000</pubDate><dc:creator>Bing Liu</dc:creator><dc:creator>Wenjie Yang</dc:creator><dc:creator>Mingming Liu</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Peng Liu</dc:creator><dc:creator>Yong Zhou</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3656817</prism:doi><description>Zero-shot captioning aims to describe visual content without additional paired image-text data by leveraging the potential of Visual Language Models (VLMs). Although text-only training allows the model to leverage large-scale textual knowledge, current approaches suffer from two major challenges: (1) the modality gap between text-only training and image-based inference, and (2) catastrophic forgetting when adapting to new text domains. In this paper, we present a novel Continual Zero-shot Captioning framework (CZC), which contains two key components: Retrieval-augmented Pseudo-image Guided Alignment (RPGA) and Text domain-aware Memory Recall (TMR). RPGA synthesizes pseudo visuals to bridge the modality gap and perform the retrieval-augmented generation. The synthetic visuals serve as cross-modal anchors in the CZC where real unseen visuals are unavailable during training, while retrieval-augmented generation enriches them with additional semantic cues to produce more informative conditional prompts. TMR mitigates catastrophic forgetting through the text domain-aware parameter-efficient fine-tuning with adaptive weight replay. It selectively recalls previously text domain knowledge relevant to the input images, achieving stability on previous tasks and plasticity for new tasks. Extensive experiments on the ZCCL demonstrate that CZC effectively bridges the modality gap between training and inference and enables zero-shot captioning under cross-task continual learning scenarios. Particularly, it achieves up to +7.6% and +19.8% relative CIDEr improvements over state-of-the-art baselines on UCM-Captions and Sydney-Captions, respectively, while maintaining strong performance on previously learned tasks.
Published: 2026-01-21T21:12:03+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bing Liu; Wenjie Yang; Mingming Liu; Hao Liu; Peng Liu; Yong Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3656817"&gt;10.1109/tcsvt.2026.3656817&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-shot captioning aims to describe visual content without additional paired image-text data by leveraging the potential of Visual Language Models (VLMs). Although text-only training allows the model to leverage large-scale textual knowledge, current approaches suffer from two major challenges: (1) the modality gap between text-only training and image-based inference, and (2) catastrophic forgetting when adapting to new text domains. In this paper, we present a novel Continual Zero-shot Captioning framework (CZC), which contains two key components: Retrieval-augmented Pseudo-image Guided Alignment (RPGA) and Text domain-aware Memory Recall (TMR). RPGA synthesizes pseudo visuals to bridge the modality gap and perform the retrieval-augmented generation. The synthetic visuals serve as cross-modal anchors in the CZC where real unseen visuals are unavailable during training, while retrieval-augmented generation enriches them with additional semantic cues to produce more informative conditional prompts. TMR mitigates catastrophic forgetting through the text domain-aware parameter-efficient fine-tuning with adaptive weight replay. It selectively recalls previously text domain knowledge relevant to the input images, achieving stability on previous tasks and plasticity for new tasks. Extensive experiments on the ZCCL demonstrate that CZC effectively bridges the modality gap between training and inference and enables zero-shot captioning under cross-task continual learning scenarios. Particularly, it achieves up to +7.6% and +19.8% relative CIDEr improvements over state-of-the-art baselines on UCM-Captions and Sydney-Captions, respectively, while maintaining strong performance on previously learned tasks.&lt;/p&gt;</content:encoded></item><item><title>Exploring Generic Knowledge and Reactivating Source Model for Source-free Universal Domain Adaptation</title><link>https://doi.org/10.1109/tmm.2026.3651024</link><guid>10.1109/tmm.2026.3651024</guid><pubDate>Wed, 21 Jan 2026 21:10:50 +0000</pubDate><dc:creator>Yuwu Lu</dc:creator><dc:creator>Yifan Lan</dc:creator><dc:creator>Huan Yang</dc:creator><dc:creator>Zhihui Lai</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651024</prism:doi><description>Source-free universal domain adaptation (SF UniDA) aims to correctly classify known samples from shared categories while distinguishing them from target-private un known data. However, existing approaches predominantly focus on processing target domain data, overlooking the rich knowledge embedded in the pre-trained source model. This limitation often hampers the ability of the model to accurately identify shared categories. To overcome this limitation, we introduce a novel approach called Exploring Generic knowledge and Reactivating Source model (EGRS). EGRS leverages the knowledge encoded in a pre-trained source model to mitigate the impact of class space discrepancies between source and target domains. Specifically, we adversarially perturb target samples to align their embeddings with source class prototypes in the embedding space of the pre trained source model. Using these perturbed samples, we estimate the embedding shift from the source model to the target model and dynamically refine the prototypes. Furthermore, we design a novel pseudo-label clustering algorithm and propose a new strategy to update target-domain-specific parameters, instead of simply freezing all classifier parameters as in prior methods. Extensive experiments across universal adaptation scenarios demonstrate that EGRS significantly enhances classification accuracy and consistently outperforms existing state-of-the-art approaches.
Published: 2026-01-21T21:10:50+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuwu Lu; Yifan Lan; Huan Yang; Zhihui Lai; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651024"&gt;10.1109/tmm.2026.3651024&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Source-free universal domain adaptation (SF UniDA) aims to correctly classify known samples from shared categories while distinguishing them from target-private un known data. However, existing approaches predominantly focus on processing target domain data, overlooking the rich knowledge embedded in the pre-trained source model. This limitation often hampers the ability of the model to accurately identify shared categories. To overcome this limitation, we introduce a novel approach called Exploring Generic knowledge and Reactivating Source model (EGRS). EGRS leverages the knowledge encoded in a pre-trained source model to mitigate the impact of class space discrepancies between source and target domains. Specifically, we adversarially perturb target samples to align their embeddings with source class prototypes in the embedding space of the pre trained source model. Using these perturbed samples, we estimate the embedding shift from the source model to the target model and dynamically refine the prototypes. Furthermore, we design a novel pseudo-label clustering algorithm and propose a new strategy to update target-domain-specific parameters, instead of simply freezing all classifier parameters as in prior methods. Extensive experiments across universal adaptation scenarios demonstrate that EGRS significantly enhances classification accuracy and consistently outperforms existing state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>CoScale-RL: Efficient Post-Training by Co-Scaling Data and Computation</title><link>https://arxiv.org/abs/2601.14695v1</link><guid>http://arxiv.org/abs/2601.14695v1</guid><pubDate>Wed, 21 Jan 2026 06:17:52 +0000</pubDate><dc:creator>Yutong Chen</dc:creator><dc:creator>Jiandong Gao</dc:creator><dc:creator>Ji Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM's ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM's reasoning ability.
Published: 2026-01-21T06:17:52+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yutong Chen; Jiandong Gao; Ji Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Training Large Reasoning Model (LRM) is usually unstable and unpredictable, especially on hard problems or weak foundation models. We found that the current post-training scaling strategy can still improve on these cases. We propose CoScale-RL, a novel scaling strategy with better data and computational efficiency. We first scale up solutions to make problems solvable. The core idea is to collect multiple solutions for each problem, rather than simply enlarging the dataset. Then, we scale up rollout computation to stabilize Reinforcement Learning. We further leverage a model merge technique called Re-distillation to sustain or even improve computational efficiency when scaling up. Our method significantly improves data and computational efficiency, with an average 3.76$\times$ accuracy improvement on four benchmarks. CoScale-RL is able to improve an LRM&amp;#x27;s ability boundary without an extensive SFT dataset. Our method provides a new scaling direction to further improve LRM&amp;#x27;s reasoning ability.&lt;/p&gt;</content:encoded></item><item><title>Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models</title><link>https://arxiv.org/abs/2601.14327v1</link><guid>http://arxiv.org/abs/2601.14327v1</guid><pubDate>Tue, 20 Jan 2026 08:39:04 +0000</pubDate><dc:creator>YuanLab. ai</dc:creator><dc:creator>Shawn Wu</dc:creator><dc:creator>Jiangang Luo</dc:creator><dc:creator>Tong Yu</dc:creator><dc:creator>Darcy Chen</dc:creator><dc:creator>Sean Wang</dc:creator><dc:creator>Xudong Zhao</dc:creator><dc:creator>Louie Li</dc:creator><dc:creator>Claire Wang</dc:creator><dc:creator>Hunter He</dc:creator><dc:creator>Carol Wang</dc:creator><dc:creator>Allen Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.
Published: 2026-01-20T08:39:04+00:00
Venue: arXiv
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; YuanLab. ai; Shawn Wu; Jiangang Luo; Tong Yu; Darcy Chen; Sean Wang; Xudong Zhao; Louie Li; Claire Wang; Hunter He; Carol Wang; Allen Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Although Mixture-of-Experts (MoE) Large Language Models (LLMs) deliver superior accuracy with a reduced number of active parameters, their pre-training represents a significant computationally bottleneck due to underutilized experts and limited training efficiency. This work introduces a Layer-Adaptive Expert Pruning (LAEP) algorithm designed for the pre-training stage of MoE LLMs. In contrast to previous expert pruning approaches that operate primarily in the post-training phase, the proposed algorithm enhances training efficiency by selectively pruning underutilized experts and reorganizing experts across computing devices according to token distribution statistics. Comprehensive experiments demonstrate that LAEP effectively reduces model size and substantially improves pre-training efficiency. In particular, when pre-training the 1010B Base model from scratch, LAEP achieves a 48.3\% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance across multiple domains.&lt;/p&gt;</content:encoded></item><item><title>Jointly modeling cardiovascular biomarkers</title><link>https://doi.org/10.1038/s42256-025-01172-x</link><guid>10.1038/s42256-025-01172-x</guid><pubDate>Wed, 21 Jan 2026 10:03:02 +0000</pubDate><dc:creator>Sully F. Chen</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01172-x</prism:doi><description>Capturing the complexity of cardiovascular dynamics demands multiple monitoring modalities, each with inherent trade-offs. Diffusion-based modeling offers a promising route for synthesizing and generating cross-modal data.
Published: 2026-01-21T10:03:02+00:00
Venue: Nature Machine Intelligence
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sully F. Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01172-x"&gt;10.1038/s42256-025-01172-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Capturing the complexity of cardiovascular dynamics demands multiple monitoring modalities, each with inherent trade-offs. Diffusion-based modeling offers a promising route for synthesizing and generating cross-modal data.&lt;/p&gt;</content:encoded></item><item><title>Deep Learning-Based Object Pose Estimation: A Comprehensive Survey</title><link>https://doi.org/10.1007/s11263-025-02646-6</link><guid>10.1007/s11263-025-02646-6</guid><pubDate>Thu, 22 Jan 2026 05:23:16 +0000</pubDate><dc:creator>Jian Liu</dc:creator><dc:creator>Wei Sun</dc:creator><dc:creator>Hui Yang</dc:creator><dc:creator>Zhiwen Zeng</dc:creator><dc:creator>Chongpei Liu</dc:creator><dc:creator>Jin Zheng</dc:creator><dc:creator>Xingyu Liu</dc:creator><dc:creator>Hossein Rahmani</dc:creator><dc:creator>Nicu Sebe</dc:creator><dc:creator>Ajmal Mian</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02646-6</prism:doi><description>Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, i.e., instance-level, category-level, and unseen (including both instance-unseen and category-unseen cases) object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We cover the literature up to our submission date and will continue to follow the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation .
Published: 2026-01-22T05:23:16+00:00
Venue: International Journal of Computer Vision
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Liu; Wei Sun; Hui Yang; Zhiwen Zeng; Chongpei Liu; Jin Zheng; Xingyu Liu; Hossein Rahmani; Nicu Sebe; Ajmal Mian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02646-6"&gt;10.1007/s11263-025-02646-6&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Object pose estimation is a fundamental computer vision problem with broad applications in augmented reality and robotics. Over the past decade, deep learning models, due to their superior accuracy and robustness, have increasingly supplanted conventional algorithms reliant on engineered point pair features. Nevertheless, several challenges persist in contemporary methods, including their dependency on labeled training data, model compactness, robustness under challenging conditions, and their ability to generalize to novel unseen objects. A recent survey discussing the progress made on different aspects of this area, outstanding challenges, and promising future directions, is missing. To fill this gap, we discuss the recent advances in deep learning-based object pose estimation, covering all three formulations of the problem, i.e., instance-level, category-level, and unseen (including both instance-unseen and category-unseen cases) object pose estimation. Our survey also covers multiple input data modalities, degrees-of-freedom of output poses, object properties, and downstream tasks, providing the readers with a holistic understanding of this field. Additionally, it discusses training paradigms of different domains, inference modes, application areas, evaluation metrics, and benchmark datasets, as well as reports the performance of current state-of-the-art methods on these benchmarks, thereby facilitating the readers in selecting the most suitable method for their application. Finally, the survey identifies key challenges, reviews the prevailing trends along with their pros and cons, and identifies promising directions for future research. We cover the literature up to our submission date and will continue to follow the latest works at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation .&lt;/p&gt;</content:encoded></item><item><title>FeedbackSTS-Det: Sparse Frames-Based Spatio-Temporal Semantic Feedback Network for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2601.14690v1</link><guid>http://arxiv.org/abs/2601.14690v1</guid><pubDate>Wed, 21 Jan 2026 06:06:36 +0000</pubDate><dc:creator>Yian Huang</dc:creator><dc:creator>Qing Qin</dc:creator><dc:creator>Aji Mao</dc:creator><dc:creator>Xiangyu Qiu</dc:creator><dc:creator>Liang Xu</dc:creator><dc:creator>Xian Zhang</dc:creator><dc:creator>Zhenming Peng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.
Published: 2026-01-21T06:06:36+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yian Huang; Qing Qin; Aji Mao; Xiangyu Qiu; Liang Xu; Xian Zhang; Zhenming Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) under complex backgrounds remains a critical yet challenging task, primarily due to the extremely low signal-to-clutter ratio, persistent dynamic interference, and the lack of distinct target features. While multi-frame detection methods leverages temporal cues to improve upon single-frame approaches, existing methods still struggle with inefficient long-range dependency modeling and insufficient robustness. To overcome these issues, we propose a novel scheme for ISTD, realized through a sparse frames-based spatio-temporal semantic feedback network named FeedbackSTS-Det. The core of our approach is a novel spatio-temporal semantic feedback strategy with a closed-loop semantic association mechanism, which consists of paired forward and backward refinement modules that work cooperatively across the encoder and decoder. Moreover, both modules incorporate an embedded sparse semantic module (SSM), which performs structured sparse temporal modeling to capture long-range dependencies with low computational cost. This integrated design facilitates robust implicit inter-frame registration and continuous semantic refinement, effectively suppressing false alarms. Furthermore, our overall procedure maintains a consistent training-inference pipeline, which ensures reliable performance transfer and increases model robustness. Extensive experiments on multiple benchmark datasets confirm the effectiveness of FeedbackSTS-Det. Code and models are available at: https://github.com/IDIP-Lab/FeedbackSTS-Det.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Multi-Task Visual Representation Learning</title><link>https://arxiv.org/abs/2601.13886v1</link><guid>http://arxiv.org/abs/2601.13886v1</guid><pubDate>Tue, 20 Jan 2026 11:59:19 +0000</pubDate><dc:creator>Shangzhe Di</dc:creator><dc:creator>Zhonghua Zhai</dc:creator><dc:creator>Weidi Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity "expert" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves "best-of-both-worlds" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.
Published: 2026-01-20T11:59:19+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shangzhe Di; Zhonghua Zhai; Weidi Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &amp;quot;expert&amp;quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &amp;quot;best-of-both-worlds&amp;quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.&lt;/p&gt;</content:encoded></item><item><title>SigMa: Semantic Similarity-Guided Semi-Dense Feature Matching</title><link>https://doi.org/10.1109/tip.2026.3654367</link><guid>10.1109/tip.2026.3654367</guid><pubDate>Wed, 21 Jan 2026 21:12:53 +0000</pubDate><dc:creator>Xiang Fang</dc:creator><dc:creator>Zizhuo Li</dc:creator><dc:creator>Jiayi Ma</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654367</prism:doi><description>Recent advancements have led the image matching community to increasingly focus on obtaining subpixel-level correspondences in a detector-free manner, i.e., semi-dense feature matching. Existing methods tend to overfocus on low-level local features while ignoring equally important high-level semantic information. To tackle these shortcomings, we propose SigMa, a semantic similarity-guided semi-dense feature matching method, which leverages the strengths of both local features and high-level semantic features. First, we design a dual-branch feature extractor, comprising a convolutional network and a vision foundation model, to extract low-level local features and high-level semantic features, respectively. To fully retain the advantages of these two features and effectively integrate them, we also introduce a cross-domain feature adapter, which could overcome their spatial resolution mismatches, channel dimensionality variations, and inter-domain gaps. Furthermore, we observe that performing the transformer on the whole feature map is unnecessary because of the similarity of local representations. We design a guided pooling method based on semantic similarity. This strategy performs attention computation by selecting highly semantically similar regions, aiming to minimize information loss while maintaining computational efficiency. Extensive experiments on multiple datasets demonstrate that our method achieves a competitive accuracy-efficiency trade-off across various tasks and exhibits strong generalization capabilities across different datasets. Additionally, we conduct a series of ablation studies and analysis experiments to validate the effectiveness and rationality of our method’s design. Our code will be publicly available.
Published: 2026-01-21T21:12:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Fang; Zizhuo Li; Jiayi Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654367"&gt;10.1109/tip.2026.3654367&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements have led the image matching community to increasingly focus on obtaining subpixel-level correspondences in a detector-free manner, i.e., semi-dense feature matching. Existing methods tend to overfocus on low-level local features while ignoring equally important high-level semantic information. To tackle these shortcomings, we propose SigMa, a semantic similarity-guided semi-dense feature matching method, which leverages the strengths of both local features and high-level semantic features. First, we design a dual-branch feature extractor, comprising a convolutional network and a vision foundation model, to extract low-level local features and high-level semantic features, respectively. To fully retain the advantages of these two features and effectively integrate them, we also introduce a cross-domain feature adapter, which could overcome their spatial resolution mismatches, channel dimensionality variations, and inter-domain gaps. Furthermore, we observe that performing the transformer on the whole feature map is unnecessary because of the similarity of local representations. We design a guided pooling method based on semantic similarity. This strategy performs attention computation by selecting highly semantically similar regions, aiming to minimize information loss while maintaining computational efficiency. Extensive experiments on multiple datasets demonstrate that our method achieves a competitive accuracy-efficiency trade-off across various tasks and exhibits strong generalization capabilities across different datasets. Additionally, we conduct a series of ablation studies and analysis experiments to validate the effectiveness and rationality of our method’s design. Our code will be publicly available.&lt;/p&gt;</content:encoded></item><item><title>Towards Understanding Best Practices for Quantization of Vision-Language Models</title><link>https://arxiv.org/abs/2601.15287v1</link><guid>http://arxiv.org/abs/2601.15287v1</guid><pubDate>Wed, 21 Jan 2026 18:59:51 +0000</pubDate><dc:creator>Gautom Das</dc:creator><dc:creator>Vincent La</dc:creator><dc:creator>Ethan Lau</dc:creator><dc:creator>Abhinav Shrivastava</dc:creator><dc:creator>Matthew Gwilliam</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.
Published: 2026-01-21T18:59:51+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gautom Das; Vincent La; Ethan Lau; Abhinav Shrivastava; Matthew Gwilliam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.&lt;/p&gt;</content:encoded></item><item><title>Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning</title><link>https://arxiv.org/abs/2601.15160v1</link><guid>http://arxiv.org/abs/2601.15160v1</guid><pubDate>Wed, 21 Jan 2026 16:38:59 +0000</pubDate><dc:creator>Yuval Kansal</dc:creator><dc:creator>Niraj K. Jha</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a "compositional bridge", enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.
Published: 2026-01-21T16:38:59+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuval Kansal; Niraj K. Jha&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &amp;quot;compositional bridge&amp;quot;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.&lt;/p&gt;</content:encoded></item><item><title>What Makes Low-Bit Quantization-Aware Training Work for Reasoning LLMs? A Systematic Study</title><link>https://arxiv.org/abs/2601.14888v1</link><guid>http://arxiv.org/abs/2601.14888v1</guid><pubDate>Wed, 21 Jan 2026 11:22:29 +0000</pubDate><dc:creator>Keyu Lv</dc:creator><dc:creator>Manyi Zhang</dc:creator><dc:creator>Xiaobo Xia</dc:creator><dc:creator>Jingchen Ni</dc:creator><dc:creator>Shannan Yan</dc:creator><dc:creator>Xianzhi Yu</dc:creator><dc:creator>Lu Hou</dc:creator><dc:creator>Chun Yuan</dc:creator><dc:creator>Haoli Bai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.
Published: 2026-01-21T11:22:29+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keyu Lv; Manyi Zhang; Xiaobo Xia; Jingchen Ni; Shannan Yan; Xianzhi Yu; Lu Hou; Chun Yuan; Haoli Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Reasoning models excel at complex tasks such as coding and mathematics, yet their inference is often slow and token-inefficient. To improve the inference efficiency, post-training quantization (PTQ) usually comes with the cost of large accuracy drops, especially for reasoning tasks under low-bit settings. In this study, we present a systematic empirical study of quantization-aware training (QAT) for reasoning models. Our key findings include: (1) Knowledge distillation is a robust objective for reasoning models trained via either supervised fine-tuning or reinforcement learning; (2) PTQ provides a strong initialization for QAT, improving accuracy while reducing training cost; (3) Reinforcement learning remains feasible for quantized models given a viable cold start and yields additional gains; and (4) Aligning the PTQ calibration domain with the QAT training domain accelerates convergence and often improves the final accuracy. Finally, we consolidate these findings into an optimized workflow (Reasoning-QAT), and show that it consistently outperforms state-of-the-art PTQ methods across multiple LLM backbones and reasoning datasets. For instance, on Qwen3-0.6B, it surpasses GPTQ by 44.53% on MATH-500 and consistently recovers performance in the 2-bit regime.&lt;/p&gt;</content:encoded></item><item><title>Reliable Pseudo-supervision for Unsupervised Domain Adaptive Person Search</title><link>https://doi.org/10.1109/tip.2026.3654373</link><guid>10.1109/tip.2026.3654373</guid><pubDate>Wed, 21 Jan 2026 21:12:53 +0000</pubDate><dc:creator>Qixian Zhang</dc:creator><dc:creator>Duoqian Miao</dc:creator><dc:creator>Qi Zhang</dc:creator><dc:creator>Xuan Tan</dc:creator><dc:creator>Hongyun Zhang</dc:creator><dc:creator>Cairong Zhao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3654373</prism:doi><description>Unsupervised Domain Adaptation (UDA) person search aims to adapt models trained on labeled source data to unlabeled target domains. Existing approaches typically rely on clustering-based proxy learning, but their performance is often undermined by unreliable pseudo-supervision. This unreliability mainly stems from two challenges: (i) spectral shift bias, where low- and high-frequency components behave differently under domain shifts but are rarely considered, degrading feature stability; and (ii) static proxy updates, which make clustering proxies highly sensitive to noise and less adaptable to domain shifts. To address these challenges, we propose the Reliable Pseudo-supervision in UDA Person Search (RPPS) framework. At the feature level, a Dual-branch Wavelet Enhancement Module (DWEM) embedded in the backbone applies discrete wavelet transform (DWT) to decompose features into low- and high-frequency components, followed by differentiated enhancements that improve cross-domain robustness and discriminability. At the proxy level, a Dynamic Confidence-weighted Clustering Proxy (DCCP) employs confidence-guided initialization and a two-stage online–offline update strategy to stabilize proxy optimization and suppress proxy noise. Extensive experiments on the CUHK-SYSU and PRW benchmarks demonstrate that RPPS achieves state-of-the-art performance and strong robustness, underscoring the importance of enhancing pseudo-supervision reliability in UDA person search. Our code is accessible at https://github.com/zqx951102/RPPS.
Published: 2026-01-21T21:12:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qixian Zhang; Duoqian Miao; Qi Zhang; Xuan Tan; Hongyun Zhang; Cairong Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3654373"&gt;10.1109/tip.2026.3654373&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised Domain Adaptation (UDA) person search aims to adapt models trained on labeled source data to unlabeled target domains. Existing approaches typically rely on clustering-based proxy learning, but their performance is often undermined by unreliable pseudo-supervision. This unreliability mainly stems from two challenges: (i) spectral shift bias, where low- and high-frequency components behave differently under domain shifts but are rarely considered, degrading feature stability; and (ii) static proxy updates, which make clustering proxies highly sensitive to noise and less adaptable to domain shifts. To address these challenges, we propose the Reliable Pseudo-supervision in UDA Person Search (RPPS) framework. At the feature level, a Dual-branch Wavelet Enhancement Module (DWEM) embedded in the backbone applies discrete wavelet transform (DWT) to decompose features into low- and high-frequency components, followed by differentiated enhancements that improve cross-domain robustness and discriminability. At the proxy level, a Dynamic Confidence-weighted Clustering Proxy (DCCP) employs confidence-guided initialization and a two-stage online–offline update strategy to stabilize proxy optimization and suppress proxy noise. Extensive experiments on the CUHK-SYSU and PRW benchmarks demonstrate that RPPS achieves state-of-the-art performance and strong robustness, underscoring the importance of enhancing pseudo-supervision reliability in UDA person search. Our code is accessible at https://github.com/zqx951102/RPPS.&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Evaluation of LLM Reasoning: From Single-Model to Multi-Agent Paradigms</title><link>https://arxiv.org/abs/2601.13243v1</link><guid>http://arxiv.org/abs/2601.13243v1</guid><pubDate>Mon, 19 Jan 2026 17:23:45 +0000</pubDate><dc:creator>Yapeng Li</dc:creator><dc:creator>Jiakuo Yu</dc:creator><dc:creator>Zhixin Liu</dc:creator><dc:creator>Xinnan Liu</dc:creator><dc:creator>Jing Yu</dc:creator><dc:creator>Songze Li</dc:creator><dc:creator>Tonghua Su</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.
Published: 2026-01-19T17:23:45+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yapeng Li; Jiakuo Yu; Zhixin Liu; Xinnan Liu; Jing Yu; Songze Li; Tonghua Su&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) are increasingly deployed as reasoning systems, where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent systems (MAS) - play a critical role, yet their relative effectiveness and cost-accuracy trade-offs remain poorly understood. In this work, we conduct a comprehensive and unified evaluation of reasoning paradigms, spanning direct single-model generation, CoT-augmented single-model reasoning, and representative MAS workflows, characterizing their reasoning performance across a diverse suite of closed-form benchmarks. Beyond overall performance, we probe role-specific capability demands in MAS using targeted role isolation analyses, and analyze cost-accuracy trade-offs to identify which MAS workflows offer a favorable balance between cost and accuracy, and which incur prohibitive overhead for marginal gains. We further introduce MIMeBench, a new open-ended benchmark that targets two foundational yet underexplored semantic capabilities - semantic abstraction and contrastive discrimination - thereby providing an alternative evaluation axis beyond closed-form accuracy and enabling fine-grained assessment of semantic competence that is difficult to capture with existing benchmarks. Our results show that increased structural complexity does not consistently lead to improved reasoning performance, with its benefits being highly dependent on the properties and suitability of the reasoning paradigm itself. The codes are released at https://gitcode.com/HIT1920/OpenLLMBench.&lt;/p&gt;</content:encoded></item><item><title>YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection</title><link>https://arxiv.org/abs/2601.12882v1</link><guid>http://arxiv.org/abs/2601.12882v1</guid><pubDate>Mon, 19 Jan 2026 09:36:08 +0000</pubDate><dc:creator>Sudip Chakrabarty</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The "You Only Look Once" (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.
Published: 2026-01-19T09:36:08+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sudip Chakrabarty&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;The &amp;quot;You Only Look Once&amp;quot; (YOLO) framework has long served as the benchmark for real-time object detection, yet traditional iterations (YOLOv1 through YOLO11) remain constrained by the latency and hyperparameter sensitivity of Non-Maximum Suppression (NMS) post-processing. This paper analyzes a comprehensive analysis of YOLO26, an architecture that fundamentally redefines this paradigm by eliminating NMS in favor of a native end-to-end learning strategy. This study examines the critical innovations that enable this transition, specifically the introduction of the MuSGD optimizer for stabilizing lightweight backbones, STAL for small-target-aware assignment, and ProgLoss for dynamic supervision. Through a systematic review of official performance benchmarks, the results demonstrate that YOLO26 establishes a new Pareto front, outperforming a comprehensive suite of predecessors and state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference speed and detection accuracy. The analysis confirms that by decoupling representation learning from heuristic post-processing, YOLOv26 successfully resolves the historical trade-off between latency and precision, signaling the next evolutionary step in edge-based computer vision.&lt;/p&gt;</content:encoded></item><item><title>Practical Insights into Semi-Supervised Object Detection Approaches</title><link>https://arxiv.org/abs/2601.13380v1</link><guid>http://arxiv.org/abs/2601.13380v1</guid><pubDate>Mon, 19 Jan 2026 20:31:15 +0000</pubDate><dc:creator>Chaoxin Wang</dc:creator><dc:creator>Bharaneeshwar Balasubramaniyam</dc:creator><dc:creator>Anurag Sangem</dc:creator><dc:creator>Nicolais Guevara</dc:creator><dc:creator>Doina Caragea</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.
Published: 2026-01-19T20:31:15+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chaoxin Wang; Bharaneeshwar Balasubramaniyam; Anurag Sangem; Nicolais Guevara; Doina Caragea&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Learning in data-scarce settings has recently gained significant attention in the research community. Semi-supervised object detection(SSOD) aims to improve detection performance by leveraging a large number of unlabeled images alongside a limited number of labeled images(a.k.a.,few-shot learning). In this paper, we present a comprehensive comparison of three state-of-the-art SSOD approaches, including MixPL, Semi-DETR and Consistent-Teacher, with the goal of understanding how performance varies with the number of labeled images. We conduct experiments using the MS-COCO and Pascal VOC datasets, two popular object detection benchmarks which allow for standardized evaluation. In addition, we evaluate the SSOD approaches on a custom Beetle dataset which enables us to gain insights into their performance on specialized datasets with a smaller number of object categories. Our findings highlight the trade-offs between accuracy, model size, and latency, providing insights into which methods are best suited for low-data regimes.&lt;/p&gt;</content:encoded></item><item><title>Structural Entropy Guided Meta-Learning for Few-Shot Node Classification</title><link>https://doi.org/10.1109/tkde.2026.3656714</link><guid>10.1109/tkde.2026.3656714</guid><pubDate>Wed, 21 Jan 2026 21:11:50 +0000</pubDate><dc:creator>Xiang Chen</dc:creator><dc:creator>Kun Yue</dc:creator><dc:creator>Daliang Liu</dc:creator><dc:creator>Wenjie Liu</dc:creator><dc:creator>Liang Duan</dc:creator><dc:creator>Angsheng Li</dc:creator><prism:publicationName>IEEE Transactions on Knowledge and Data Engineering</prism:publicationName><prism:doi>10.1109/tkde.2026.3656714</prism:doi><description>Few-shot node classification (FSNC) is a challenging task in graph analysis, where the goal is to classify unlabeled nodes in a graph using only a few labeled nodes as references. To tackle the label shortage problem, many meta-learning methods have been proposed to extract meta-knowledge from base classes with abundant labeled nodes and transfer the learned knowledge to classify nodes from novel classes. However, the theoretical foundation of meta-knowledge remains unexplored, and existing solutions often struggle when dealing with complex or noisy graphs. To address these issues, we propose a novel and effective meta-learning framework for FSNC based on structural information theory. First, we introduce the concept of minimal sufficient meta-knowledge, a theoretical principle inherited from information bottleneck, which optimally balances the expressiveness and robustness of the learned meta-knowledge. Guided by this principle, we develop a meta-learning model, named SE-FSNC, that extracts the minimal sufficient meta-knowledge using an encoding tree derived from the input graph with minimal structural entropy. We then propose an effective algorithm to train SE-FSNC by incorporating the encoding tree with graph contrastive learning. Extensive experiments on several datasets demonstrate the superiority of our model compared with other state-of-the-art methods.
Published: 2026-01-21T21:11:50+00:00
Venue: IEEE Transactions on Knowledge and Data Engineering
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Chen; Kun Yue; Daliang Liu; Wenjie Liu; Liang Duan; Angsheng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Knowledge and Data Engineering&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tkde.2026.3656714"&gt;10.1109/tkde.2026.3656714&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot node classification (FSNC) is a challenging task in graph analysis, where the goal is to classify unlabeled nodes in a graph using only a few labeled nodes as references. To tackle the label shortage problem, many meta-learning methods have been proposed to extract meta-knowledge from base classes with abundant labeled nodes and transfer the learned knowledge to classify nodes from novel classes. However, the theoretical foundation of meta-knowledge remains unexplored, and existing solutions often struggle when dealing with complex or noisy graphs. To address these issues, we propose a novel and effective meta-learning framework for FSNC based on structural information theory. First, we introduce the concept of minimal sufficient meta-knowledge, a theoretical principle inherited from information bottleneck, which optimally balances the expressiveness and robustness of the learned meta-knowledge. Guided by this principle, we develop a meta-learning model, named SE-FSNC, that extracts the minimal sufficient meta-knowledge using an encoding tree derived from the input graph with minimal structural entropy. We then propose an effective algorithm to train SE-FSNC by incorporating the encoding tree with graph contrastive learning. Extensive experiments on several datasets demonstrate the superiority of our model compared with other state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</title><link>https://arxiv.org/abs/2601.14209v1</link><guid>http://arxiv.org/abs/2601.14209v1</guid><pubDate>Tue, 20 Jan 2026 18:15:38 +0000</pubDate><dc:creator>Matthew Y. R. Yang</dc:creator><dc:creator>Hao Bai</dc:creator><dc:creator>Ian Wu</dc:creator><dc:creator>Gene Yang</dc:creator><dc:creator>Amrith Setlur</dc:creator><dc:creator>Aviral Kumar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.
Published: 2026-01-20T18:15:38+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Matthew Y. R. Yang; Hao Bai; Ian Wu; Gene Yang; Amrith Setlur; Aviral Kumar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.&lt;/p&gt;</content:encoded></item><item><title>Video Decoupling Networks for Accurate, Efficient, Generalizable, and Robust Video Object Segmentation</title><link>https://doi.org/10.1109/tip.2025.3649360</link><guid>10.1109/tip.2025.3649360</guid><pubDate>Wed, 21 Jan 2026 21:12:53 +0000</pubDate><dc:creator>Jisheng Dang</dc:creator><dc:creator>Huicheng Zheng</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Jianhuang Lai</dc:creator><dc:creator>Bin Hu</dc:creator><dc:creator>Tat-Seng Chua</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3649360</prism:doi><description>object segmentation (VOS) is a fundamental task in video analysis, aiming to accurately recognize and segment objects of interest within video sequences. Conventional methods, relying on memory networks to store single-frame appearance features, face challenges in computational efficiency and capturing dynamic visual information effectively. To address these limitations, we present a Video Decoupling Network (VDN) with a per-clip memory updating mechanism. Our approach is inspired by the dual-stream hypothesis of the human visual cortex and decomposes multiple previous video frames into fundamental elements: scene, motion, and instance. We propose the Unified Prior-based Spatio-temporal Decoupler (UPSD) algorithm, which parses multiple frames into basic elements in a unified manner. UPSD continuously stores elements over time, enabling adaptive integration of different cues based on task requirements. This decomposition mechanism facilitates comprehensive spatial-temporal information capture and rapid updating, leading to notable enhancements in overall VOS performance. Extensive experiments conducted on multiple VOS benchmarks validate the state-of-the-art accuracy, efficiency, generalizability, and robustness of our approach. Remarkably, VDN demonstrates a significant performance improvement and a substantial speed-up compared to previous state-of-the-art methods on multiple VOS benchmarks. It also exhibits excellent generalizability under domain shift and robustness against various noise types.
Published: 2026-01-21T21:12:53+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jisheng Dang; Huicheng Zheng; Yulan Guo; Jianhuang Lai; Bin Hu; Tat-Seng Chua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3649360"&gt;10.1109/tip.2025.3649360&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;object segmentation (VOS) is a fundamental task in video analysis, aiming to accurately recognize and segment objects of interest within video sequences. Conventional methods, relying on memory networks to store single-frame appearance features, face challenges in computational efficiency and capturing dynamic visual information effectively. To address these limitations, we present a Video Decoupling Network (VDN) with a per-clip memory updating mechanism. Our approach is inspired by the dual-stream hypothesis of the human visual cortex and decomposes multiple previous video frames into fundamental elements: scene, motion, and instance. We propose the Unified Prior-based Spatio-temporal Decoupler (UPSD) algorithm, which parses multiple frames into basic elements in a unified manner. UPSD continuously stores elements over time, enabling adaptive integration of different cues based on task requirements. This decomposition mechanism facilitates comprehensive spatial-temporal information capture and rapid updating, leading to notable enhancements in overall VOS performance. Extensive experiments conducted on multiple VOS benchmarks validate the state-of-the-art accuracy, efficiency, generalizability, and robustness of our approach. Remarkably, VDN demonstrates a significant performance improvement and a substantial speed-up compared to previous state-of-the-art methods on multiple VOS benchmarks. It also exhibits excellent generalizability under domain shift and robustness against various noise types.&lt;/p&gt;</content:encoded></item><item><title>LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems</title><link>https://arxiv.org/abs/2601.14053v1</link><guid>http://arxiv.org/abs/2601.14053v1</guid><pubDate>Tue, 20 Jan 2026 15:06:19 +0000</pubDate><dc:creator>Badri N. Patro</dc:creator><dc:creator>Vijay S. Agneeswaran</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at &lt;$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.
Published: 2026-01-20T15:06:19+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Badri N. Patro; Vijay S. Agneeswaran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at &amp;lt;$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.&lt;/p&gt;</content:encoded></item><item><title>RayRoPE: Projective Ray Positional Encoding for Multi-view Attention</title><link>https://arxiv.org/abs/2601.15275v1</link><guid>http://arxiv.org/abs/2601.15275v1</guid><pubDate>Wed, 21 Jan 2026 18:55:51 +0000</pubDate><dc:creator>Yu Wu</dc:creator><dc:creator>Minsik Jeon</dc:creator><dc:creator>Jen-Hao Rick Chang</dc:creator><dc:creator>Oncel Tuzel</dc:creator><dc:creator>Shubham Tulsiani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the 'predicted' 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.
Published: 2026-01-21T18:55:51+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Wu; Minsik Jeon; Jen-Hao Rick Chang; Oncel Tuzel; Shubham Tulsiani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the &amp;#x27;predicted&amp;#x27; 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.&lt;/p&gt;</content:encoded></item><item><title>SAMTok: Representing Any Mask with Two Words</title><link>https://arxiv.org/abs/2601.16093v1</link><guid>http://arxiv.org/abs/2601.16093v1</guid><pubDate>Thu, 22 Jan 2026 16:44:09 +0000</pubDate><dc:creator>Yikang Zhou</dc:creator><dc:creator>Tao Zhang</dc:creator><dc:creator>Dengxian Gong</dc:creator><dc:creator>Yuanzheng Wu</dc:creator><dc:creator>Ye Tian</dc:creator><dc:creator>Haochen Wang</dc:creator><dc:creator>Haobo Yuan</dc:creator><dc:creator>Jiacong Wang</dc:creator><dc:creator>Lu Qi</dc:creator><dc:creator>Hao Fei</dc:creator><dc:creator>Anran Wang</dc:creator><dc:creator>Zhuochen Wang</dc:creator><dc:creator>Yujing Wang</dc:creator><dc:creator>Cheng Chen</dc:creator><dc:creator>Shunping Ji</dc:creator><dc:creator>Xiangtai Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.
Published: 2026-01-22T16:44:09+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yikang Zhou; Tao Zhang; Dengxian Gong; Yuanzheng Wu; Ye Tian; Haochen Wang; Haobo Yuan; Jiacong Wang; Lu Qi; Hao Fei; Anran Wang; Zhuochen Wang; Yujing Wang; Cheng Chen; Shunping Ji; Xiangtai Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.&lt;/p&gt;</content:encoded></item><item><title>Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision via Belief Engineering</title><link>https://arxiv.org/abs/2601.13752v1</link><guid>http://arxiv.org/abs/2601.13752v1</guid><pubDate>Tue, 20 Jan 2026 09:07:01 +0000</pubDate><dc:creator>Chak Tou Leong</dc:creator><dc:creator>Dingwei Chen</dc:creator><dc:creator>Heming Xia</dc:creator><dc:creator>Qingyu Yin</dc:creator><dc:creator>Sunbowen Lee</dc:creator><dc:creator>Jian Wang</dc:creator><dc:creator>Wenjie Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model's self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model's reasoning belief effectively shapes its actual behavior.
Published: 2026-01-20T09:07:01+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chak Tou Leong; Dingwei Chen; Heming Xia; Qingyu Yin; Sunbowen Lee; Jian Wang; Wenjie Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Large reasoning models (LRMs) have achieved remarkable success in complex problem-solving, yet they often suffer from computational redundancy or reasoning unfaithfulness. Current methods for shaping LRM behavior typically rely on reinforcement learning or fine-tuning with gold-standard reasoning traces, a paradigm that is both computationally expensive and difficult to scale. In this paper, we reveal that LRMs possess latent \textit{reasoning beliefs} that internally track their own reasoning traits, which can be captured through simple logit probing. Building upon this insight, we propose Reasoning Belief Engineering (RELIEF), a simple yet effective framework that shapes LRM behavior by aligning the model&amp;#x27;s self-concept with a target belief blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace supervision. It internalizes desired traits by fine-tuning on synthesized, self-reflective question-answering pairs that affirm the target belief. Extensive experiments on efficiency and faithfulness tasks demonstrate that RELIEF matches or outperforms behavior-supervised and preference-based baselines while requiring lower training costs. Further analysis validates that shifting a model&amp;#x27;s reasoning belief effectively shapes its actual behavior.&lt;/p&gt;</content:encoded></item><item><title>Rethinking Reinforcement fine-tuning of LLMs: A Multi-armed Bandit Learning Perspective</title><link>https://arxiv.org/abs/2601.14599v1</link><guid>http://arxiv.org/abs/2601.14599v1</guid><pubDate>Wed, 21 Jan 2026 02:37:44 +0000</pubDate><dc:creator>Xiao Hu</dc:creator><dc:creator>Hong Xie</dc:creator><dc:creator>Tao Tan</dc:creator><dc:creator>Defu Lian</dc:creator><dc:creator>Jianyu Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.
Published: 2026-01-21T02:37:44+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiao Hu; Hong Xie; Tao Tan; Defu Lian; Jianyu Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;A large number of heuristics have been proposed to optimize the reinforcement fine-tuning of LLMs. However, inconsistent claims are made from time to time, making this area elusive. Reflecting on this situation, two fundamental questions still lack a clear understanding: 1) what is the role of each optimizing choice? 2) which ones are the bottlenecks? This paper aims to shed light on them, and it faces the challenge of several entangled confounding factors in the fine-tuning process. To tackle this challenge, we propose a bottom-up experiment pipeline. The bottom layer is composed of a minimalist configuration: one training data, one rollout per round and the reward directly serve as the learning signal without advantage function design. This minimalist configuration connects to multi-armed bandit learning with extremely large discrete action space, which offers theories to corroborate the experiment findings. The up procedure of the experiment pipeline expanding the minimalist configuration layer by layer, examining the role of each design choice. Experimental results on three LLMs and two reasoning datasets not only reveal new understanding of the design choice but also yield essential insights to shape the area.&lt;/p&gt;</content:encoded></item><item><title>You Need Better Attention Priors</title><link>https://arxiv.org/abs/2601.15380v1</link><guid>http://arxiv.org/abs/2601.15380v1</guid><pubDate>Wed, 21 Jan 2026 19:00:08 +0000</pubDate><dc:creator>Elon Litman</dc:creator><dc:creator>Gabe Guo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.
Published: 2026-01-21T19:00:08+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Elon Litman; Gabe Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;We generalize the attention mechanism by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce Generalized Optimal transport Attention with Trainable priors (GOAT), a new attention mechanism that replaces this naive assumption with a learnable, continuous prior. This prior maintains full compatibility with optimized kernels such as FlashAttention. GOAT also provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. Finally, by absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings.&lt;/p&gt;</content:encoded></item></channel></rss>