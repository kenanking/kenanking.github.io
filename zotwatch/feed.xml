<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 30 Jan 2026 03:35:08 +0000</lastBuildDate><item><title>Vocabulary-free Image Classification and Semantic Segmentation</title><link>https://doi.org/10.1109/tpami.2026.3657989</link><guid>10.1109/tpami.2026.3657989</guid><pubDate>Wed, 28 Jan 2026 20:59:22 +0000</pubDate><dc:creator>Alessandro Conti</dc:creator><dc:creator>Enrico Fini</dc:creator><dc:creator>Massimiliano Mancini</dc:creator><dc:creator>Paolo Rota</dc:creator><dc:creator>Yiming Wang</dc:creator><dc:creator>Elisa Ricci</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3657989</prism:doi><description>Large vision-language models revolutionized image classification and semantic segmentation paradigms. However, they typically assume a pre-defined set of categories, or vocabulary, at test time for composing textual prompts. This assumption is impractical in scenarios with unknown or evolving semantic context. Here, we address this issue and introduce the Vocabulary-free Image Classification (VIC) task, which aims to assign a class from an unconstrained language-induced semantic space to an input image without needing a known vocabulary. VIC is challenging due to the vastness of the semantic space, which contains millions of concepts, including fine-grained categories. To address VIC, we propose Category Search from External Databases (CaSED), a training-free method that leverages a pre-trained vision-language model and an external database. CaSED first extracts the set of candidate categories from the most semantically similar captions in the database and then assigns the image to the best-matching candidate category according to the same vision-language model. Furthermore, we demonstrate that CaSED can be applied locally to generate a coarse segmentation mask that classifies image regions, introducing the task of Vocabulary-free Semantic Segmentation. CaSED and its variants outperform other more complex vision-language models, on classification and semantic segmentation benchmarks, while using much fewer parameters.
Published: 2026-01-28T20:59:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.833 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alessandro Conti; Enrico Fini; Massimiliano Mancini; Paolo Rota; Yiming Wang; Elisa Ricci&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3657989"&gt;10.1109/tpami.2026.3657989&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.833 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision-language models revolutionized image classification and semantic segmentation paradigms. However, they typically assume a pre-defined set of categories, or vocabulary, at test time for composing textual prompts. This assumption is impractical in scenarios with unknown or evolving semantic context. Here, we address this issue and introduce the Vocabulary-free Image Classification (VIC) task, which aims to assign a class from an unconstrained language-induced semantic space to an input image without needing a known vocabulary. VIC is challenging due to the vastness of the semantic space, which contains millions of concepts, including fine-grained categories. To address VIC, we propose Category Search from External Databases (CaSED), a training-free method that leverages a pre-trained vision-language model and an external database. CaSED first extracts the set of candidate categories from the most semantically similar captions in the database and then assigns the image to the best-matching candidate category according to the same vision-language model. Furthermore, we demonstrate that CaSED can be applied locally to generate a coarse segmentation mask that classifies image regions, introducing the task of Vocabulary-free Semantic Segmentation. CaSED and its variants outperform other more complex vision-language models, on classification and semantic segmentation benchmarks, while using much fewer parameters.&lt;/p&gt;</content:encoded></item><item><title>Dissecting RGB-D Learning for Improved Multi-modal Fusion</title><link>https://doi.org/10.1109/tip.2026.3657171</link><guid>10.1109/tip.2026.3657171</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Hao Chen</dc:creator><dc:creator>Haoran Zhou</dc:creator><dc:creator>Yunshu Zhang</dc:creator><dc:creator>Zheng Lin</dc:creator><dc:creator>Yongjian Deng</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657171</prism:doi><description>In the RGB-D vision community, extensive research has been focused on designing multi-modal learning strategies and fusion structures. However, the complementary and fusion mechanisms in RGB-D models remain a black box. In this paper, we present an analytical framework and a novel score to dissect the RGB-D vision community. Our approach involves measuring proposed semantic variance and feature similarity across modalities and levels, conducting visual and quantitative analyzes on multi-modal learning through comprehensive experiments. Specifically, we investigate the consistency and specialty of features across modalities, evolution rules within each modality, and the collaboration logic used when optimizing a RGB-D model. Our studies reveal/verify several important findings, such as the discrepancy in cross-modal features and the hybrid multi-modal cooperation rule, which highlights consistency and specialty simultaneously for complementary inference. We also showcase the versatility of the proposed RGB-D dissection method and introduce a straightforward fusion strategy based on our findings, which delivers significant enhancements across various tasks and even other multi-modal data.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.832 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Chen; Haoran Zhou; Yunshu Zhang; Zheng Lin; Yongjian Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657171"&gt;10.1109/tip.2026.3657171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.832 (must_read)&lt;/p&gt;
&lt;p&gt;In the RGB-D vision community, extensive research has been focused on designing multi-modal learning strategies and fusion structures. However, the complementary and fusion mechanisms in RGB-D models remain a black box. In this paper, we present an analytical framework and a novel score to dissect the RGB-D vision community. Our approach involves measuring proposed semantic variance and feature similarity across modalities and levels, conducting visual and quantitative analyzes on multi-modal learning through comprehensive experiments. Specifically, we investigate the consistency and specialty of features across modalities, evolution rules within each modality, and the collaboration logic used when optimizing a RGB-D model. Our studies reveal/verify several important findings, such as the discrepancy in cross-modal features and the hybrid multi-modal cooperation rule, which highlights consistency and specialty simultaneously for complementary inference. We also showcase the versatility of the proposed RGB-D dissection method and introduce a straightforward fusion strategy based on our findings, which delivers significant enhancements across various tasks and even other multi-modal data.&lt;/p&gt;</content:encoded></item><item><title>Domain-aware Adversarial Domain Augmentation Network for Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tip.2026.3657203</link><guid>10.1109/tip.2026.3657203</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Yi Huang</dc:creator><dc:creator>Jiangtao Peng</dc:creator><dc:creator>Weiwei Sun</dc:creator><dc:creator>Na Chen</dc:creator><dc:creator>Zhijing Ye</dc:creator><dc:creator>Qian Du</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657203</prism:doi><description>Classifying hyperspectral remote sensing images across different scenes has recently emerged as a significant challenge. When only historical labeled images (source domain, SD) are available, it is crucial to leverage these images effectively to train a model with strong generalization ability that can be directly applied to classify unseen samples (target domain, TD). To address these challenges, this paper proposes a novel single-domain generalization (SDG) network, termed the domain-aware adversarial domain augmentation network (DADAnet) for cross-scene hyperspectral image classification (HSIC). DADAnet involves two stages: adversarial domain augmentation (ADA) and task-specific training. ADA employs a progressive adversarial generation strategy to construct an augmented domain (AD). To enhance variability in both spatial and spectral dimensions, a domain-aware spatial-spectral mask (DSSM) encoder is constructed to increase the diversity of the generated adversarial samples. Furthermore, a two-level contrastive loss (TCC) is designed and incorporated into the ADA to ensure both the diversity and effectiveness of AD samples. Finally, DADAnet performs supervised learning jointly on the SD and AD during the task-specific training stage. Experimental results on two public hyperspectral image datasets and a new Hangzhouwan (HZW) dataset demonstrate that the proposed DADAnet outperforms existing domain adaptation (DA) and domain generalization (DG) methods, achieving overall accuracies of 80.69%, 63.75%, and 87.61% on three datasets, respectively.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Huang; Jiangtao Peng; Weiwei Sun; Na Chen; Zhijing Ye; Qian Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657203"&gt;10.1109/tip.2026.3657203&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;Classifying hyperspectral remote sensing images across different scenes has recently emerged as a significant challenge. When only historical labeled images (source domain, SD) are available, it is crucial to leverage these images effectively to train a model with strong generalization ability that can be directly applied to classify unseen samples (target domain, TD). To address these challenges, this paper proposes a novel single-domain generalization (SDG) network, termed the domain-aware adversarial domain augmentation network (DADAnet) for cross-scene hyperspectral image classification (HSIC). DADAnet involves two stages: adversarial domain augmentation (ADA) and task-specific training. ADA employs a progressive adversarial generation strategy to construct an augmented domain (AD). To enhance variability in both spatial and spectral dimensions, a domain-aware spatial-spectral mask (DSSM) encoder is constructed to increase the diversity of the generated adversarial samples. Furthermore, a two-level contrastive loss (TCC) is designed and incorporated into the ADA to ensure both the diversity and effectiveness of AD samples. Finally, DADAnet performs supervised learning jointly on the SD and AD during the task-specific training stage. Experimental results on two public hyperspectral image datasets and a new Hangzhouwan (HZW) dataset demonstrate that the proposed DADAnet outperforms existing domain adaptation (DA) and domain generalization (DG) methods, achieving overall accuracies of 80.69%, 63.75%, and 87.61% on three datasets, respectively.&lt;/p&gt;</content:encoded></item><item><title>Domain-Adaptive Mamba for Cross-Scene Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tip.2026.3657209</link><guid>10.1109/tip.2026.3657209</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Puhong Duan</dc:creator><dc:creator>Shiyu Jin</dc:creator><dc:creator>Xiaotian Lu</dc:creator><dc:creator>Lianhui Liang</dc:creator><dc:creator>Xudong Kang</dc:creator><dc:creator>Antonio Plaza</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657209</prism:doi><description>Cross-scene hyperspectral image classification aims to identify a new scene in target domain via learned knowledge from source domain using limited training samples. Existing cross-scene alignment approaches focus on aligning the global feature distribution between the source and target domains while overlooking the fine-grained alignment at different levels. Moreover, they mainly use Transformer architectures to model long-range dependencies across different channels but confront efficiency challenges due to their quadratic complexity, which limits classification performance in unsupervised domain adaptation tasks. To address these issues, a new domain-adaptive Mamba (DAMamba) is proposed for cross-scene hyperspectral image classification. First, a spectral-spatial Mamba is developed to extract high-order semantic features from the input data. Then, a domain-invariant prototype alignment method is proposed from three perspectives, i.e., intra-domain, inter-domain, and mini-batch, to produce reliable pseudo-labels and mitigate the spectral shift between the source and target domains. Finally, a fully connected layer is applied to the aligned features in the target domain to obtain the final classification results. Extensive evaluations across diverse cross-scene datasets demonstrate that our DAMamba outperforms existing state-of-the-art methods in classification accuracy and computing time. The code of this paper is available at https://github.com/PuhongDuan/DAMamba.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Puhong Duan; Shiyu Jin; Xiaotian Lu; Lianhui Liang; Xudong Kang; Antonio Plaza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657209"&gt;10.1109/tip.2026.3657209&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-scene hyperspectral image classification aims to identify a new scene in target domain via learned knowledge from source domain using limited training samples. Existing cross-scene alignment approaches focus on aligning the global feature distribution between the source and target domains while overlooking the fine-grained alignment at different levels. Moreover, they mainly use Transformer architectures to model long-range dependencies across different channels but confront efficiency challenges due to their quadratic complexity, which limits classification performance in unsupervised domain adaptation tasks. To address these issues, a new domain-adaptive Mamba (DAMamba) is proposed for cross-scene hyperspectral image classification. First, a spectral-spatial Mamba is developed to extract high-order semantic features from the input data. Then, a domain-invariant prototype alignment method is proposed from three perspectives, i.e., intra-domain, inter-domain, and mini-batch, to produce reliable pseudo-labels and mitigate the spectral shift between the source and target domains. Finally, a fully connected layer is applied to the aligned features in the target domain to obtain the final classification results. Extensive evaluations across diverse cross-scene datasets demonstrate that our DAMamba outperforms existing state-of-the-art methods in classification accuracy and computing time. The code of this paper is available at https://github.com/PuhongDuan/DAMamba.&lt;/p&gt;</content:encoded></item><item><title>YOLO-RSD: Enhanced Rotated Ship Detection in SAR Images</title><link>https://doi.org/10.1109/lgrs.2026.3657680</link><guid>10.1109/lgrs.2026.3657680</guid><pubDate>Wed, 28 Jan 2026 21:01:47 +0000</pubDate><dc:creator>Ye Wu</dc:creator><dc:creator>Yuhu Shi</dc:creator><dc:creator>Hongyu Chen</dc:creator><dc:creator>Xingyu Hu</dc:creator><dc:creator>Xue Yang</dc:creator><dc:creator>Yang Cai</dc:creator><dc:creator>Yuxin Liu</dc:creator><dc:creator>Weiming Zeng</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2026.3657680</prism:doi><description>Rotated ship detection in synthetic aperture radar (SAR) imagery remains challenging due to arbitrary orientations, large scale variations, and strong sea clutter. This paper presents YOLO-RSD, a lightweight yet effective oriented object detector tailored for maritime surveillance. We first introduce a Feature-Aligned Spatial Attention (FASA) backbone that integrates deformable convolution with spatial attention to enhance orientation- and scale-adaptive feature extraction. Then, a FuseDiff neck module is designed to achieve consistent cross-scale semantic fusion via multi-branch convolution and context diffusion. Finally, a Task-Aligned Dynamic Detection Head (TDDH) is constructed using task-specific attention to reconcile classification–localization conflicts. Experiments on SSDD+ and RSDD-SAR datasets demonstrate that YOLO-RSD achieves state-of-the-art performance with only 2.61M parameters, outperforming recent YOLO variants and other comparative models in both accuracy and efficiency. The proposed framework offers a general and hardware-friendly paradigm for robust rotated target detection in SAR imagery.
Published: 2026-01-28T21:01:47+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ye Wu; Yuhu Shi; Hongyu Chen; Xingyu Hu; Xue Yang; Yang Cai; Yuxin Liu; Weiming Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2026.3657680"&gt;10.1109/lgrs.2026.3657680&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Rotated ship detection in synthetic aperture radar (SAR) imagery remains challenging due to arbitrary orientations, large scale variations, and strong sea clutter. This paper presents YOLO-RSD, a lightweight yet effective oriented object detector tailored for maritime surveillance. We first introduce a Feature-Aligned Spatial Attention (FASA) backbone that integrates deformable convolution with spatial attention to enhance orientation- and scale-adaptive feature extraction. Then, a FuseDiff neck module is designed to achieve consistent cross-scale semantic fusion via multi-branch convolution and context diffusion. Finally, a Task-Aligned Dynamic Detection Head (TDDH) is constructed using task-specific attention to reconcile classification–localization conflicts. Experiments on SSDD+ and RSDD-SAR datasets demonstrate that YOLO-RSD achieves state-of-the-art performance with only 2.61M parameters, outperforming recent YOLO variants and other comparative models in both accuracy and efficiency. The proposed framework offers a general and hardware-friendly paradigm for robust rotated target detection in SAR imagery.&lt;/p&gt;</content:encoded></item><item><title>DENet: Dual-Path Edge Network with Global-Local Attention for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3658092</link><guid>10.1109/tgrs.2026.3658092</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Jiayi Zuo</dc:creator><dc:creator>Songwei Pei</dc:creator><dc:creator>Qian Li</dc:creator><dc:creator>Yuanzhuo Huang</dc:creator><dc:creator>Shangguang Wang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658092</prism:doi><description>Infrared small target detection (IRSTD) is crucial for remote sensing applications like disaster warning and maritime surveillance. However, due to the lack of distinctive texture and morphological features, infrared small targets are highly susceptible to blending into cluttered and noisy backgrounds. Existing methods often rely on fixed gradient operators (e.g., Sobel, Canny) or simplistic attention mechanisms, which are inadequate for accurately extracting target edges under low contrast and high noise. In this paper, we propose an enhanced dual-path edge network (DENet) that explicitly addresses this challenge by decoupling edge enhancement and semantic modeling into two deliberately designed processing paths. The first path employs a Bidirectional-Interaction Module (BIM), which uses both Local Self-Attention and Global Self-Attention to capture multi-scale local and global feature dependencies. The global attention mechanism, based on a Transformer architecture, integrates long-range semantic relationships and contextual information, ensuring robust scene understanding. The second path introduces the Multi-Edge Refiner (Multi-ER), which enhances fine-grained edge details through multi-scale cascaded refinement. Coupled with attention-driven gating, it improves edge localization for targets of varying sizes and suppresses noise effectively. Extensive experiments on the IRSTD-1K, NUDT-SIRST and NUAA-SIRST benchmarks demonstrate that DENet significantly outperforms state-of-the-art methods, achieving superior Mean Intersection over Union and pixel-level accuracy, while maintaining lower false alarm rates. The proposed framework effectively improves infrared small target detection and localization through joint semantic modeling and edge refinement.
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayi Zuo; Songwei Pei; Qian Li; Yuanzhuo Huang; Shangguang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658092"&gt;10.1109/tgrs.2026.3658092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is crucial for remote sensing applications like disaster warning and maritime surveillance. However, due to the lack of distinctive texture and morphological features, infrared small targets are highly susceptible to blending into cluttered and noisy backgrounds. Existing methods often rely on fixed gradient operators (e.g., Sobel, Canny) or simplistic attention mechanisms, which are inadequate for accurately extracting target edges under low contrast and high noise. In this paper, we propose an enhanced dual-path edge network (DENet) that explicitly addresses this challenge by decoupling edge enhancement and semantic modeling into two deliberately designed processing paths. The first path employs a Bidirectional-Interaction Module (BIM), which uses both Local Self-Attention and Global Self-Attention to capture multi-scale local and global feature dependencies. The global attention mechanism, based on a Transformer architecture, integrates long-range semantic relationships and contextual information, ensuring robust scene understanding. The second path introduces the Multi-Edge Refiner (Multi-ER), which enhances fine-grained edge details through multi-scale cascaded refinement. Coupled with attention-driven gating, it improves edge localization for targets of varying sizes and suppresses noise effectively. Extensive experiments on the IRSTD-1K, NUDT-SIRST and NUAA-SIRST benchmarks demonstrate that DENet significantly outperforms state-of-the-art methods, achieving superior Mean Intersection over Union and pixel-level accuracy, while maintaining lower false alarm rates. The proposed framework effectively improves infrared small target detection and localization through joint semantic modeling and edge refinement.&lt;/p&gt;</content:encoded></item><item><title>DFFormer: UAV Object Detection via Feature Scaling and Interaction</title><link>https://doi.org/10.1109/tgrs.2026.3658082</link><guid>10.1109/tgrs.2026.3658082</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Hanyun Li</dc:creator><dc:creator>Linsong Xiao</dc:creator><dc:creator>Lihua Cao</dc:creator><dc:creator>Sai Yao</dc:creator><dc:creator>Minghao Wang</dc:creator><dc:creator>Yi Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658082</prism:doi><description>Machine vision-based anti-drone detection systems enable long-range, cost-effective target monitoring in complex environments. However, small drones typically occupy only a few pixels in captured images. Existing detectors suffer from semantic loss and insufficient fusion during feature extraction and cross-scale interaction, resulting in limited detection accuracy. To address these challenges, this paper proposes Diffusion Focusing Former (DFFormer), a detection framework specifically designed for small target identification. The framework employs a backbone network to extract multi-layer features, which are enhanced through an Advanced Feature Processing Layer (AFPL) to strengthen semantic representation. A Feature Scaling Layer (FSL) then organically fuses shallow and high-level information before encoder processing, preserving fine-grained cues while minimizing computational overhead. Subsequently, the Multi-Scale Focusing Diffusion Network (MSFDN) processes scaled features for cross-scale interaction and progressive fusion. The Focusing Fusion Module (FFM) injects comprehensive contextual information into each scale throughout this process. Experimental results on three anti-drone datasets (DUT-Anti-UAV, Bird-UAV, and Anti-UAV (Inf)) demonstrate that DFFormer consistently outperforms existing state-of-the-art methods across multiple evaluation metrics. Generalization validation on the VisDrone2019 aerial dataset further confirms the method’s applicability to diverse scenarios and configurations.
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanyun Li; Linsong Xiao; Lihua Cao; Sai Yao; Minghao Wang; Yi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658082"&gt;10.1109/tgrs.2026.3658082&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Machine vision-based anti-drone detection systems enable long-range, cost-effective target monitoring in complex environments. However, small drones typically occupy only a few pixels in captured images. Existing detectors suffer from semantic loss and insufficient fusion during feature extraction and cross-scale interaction, resulting in limited detection accuracy. To address these challenges, this paper proposes Diffusion Focusing Former (DFFormer), a detection framework specifically designed for small target identification. The framework employs a backbone network to extract multi-layer features, which are enhanced through an Advanced Feature Processing Layer (AFPL) to strengthen semantic representation. A Feature Scaling Layer (FSL) then organically fuses shallow and high-level information before encoder processing, preserving fine-grained cues while minimizing computational overhead. Subsequently, the Multi-Scale Focusing Diffusion Network (MSFDN) processes scaled features for cross-scale interaction and progressive fusion. The Focusing Fusion Module (FFM) injects comprehensive contextual information into each scale throughout this process. Experimental results on three anti-drone datasets (DUT-Anti-UAV, Bird-UAV, and Anti-UAV (Inf)) demonstrate that DFFormer consistently outperforms existing state-of-the-art methods across multiple evaluation metrics. Generalization validation on the VisDrone2019 aerial dataset further confirms the method’s applicability to diverse scenarios and configurations.&lt;/p&gt;</content:encoded></item><item><title>CAS-ViT: Convolutional Additive Self-attention Vision Transformers for Efficient Mobile Applications</title><link>https://doi.org/10.1109/tip.2026.3655121</link><guid>10.1109/tip.2026.3655121</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Tianfang Zhang</dc:creator><dc:creator>Lei Li</dc:creator><dc:creator>Yang Zhou</dc:creator><dc:creator>Wentao Liu</dc:creator><dc:creator>Chen Qian</dc:creator><dc:creator>Jenq-Neng Hwang</dc:creator><dc:creator>Xiangyang Ji</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3655121</prism:doi><description>Vision Transformers (ViTs) mark a revolutionary advance in neural networks with their token mixer’s powerful global context capability. However, the pairwise token affinity and complex matrix operations limit its deployment on resource-constrained scenarios and real-time applications, such as mobile devices, although considerable efforts have been made in previous works. In this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision Transformers, to achieve a balance between efficiency and performance in mobile applications. Firstly, we argue that the capability of token mixers to obtain global contextual information hinges on multiple information interactions, such as spatial and channel domains. Subsequently, we propose Convolutional Additive Token Mixer (CATM) employing underlying spatial and channel attention as novel interaction forms. This module eliminates troublesome complex operations such as matrix multiplication and Softmax. We introduce Convolutional Additive Self-attention(CAS) block hybrid architecture and utilize CATM for each block. And further, we build a family of lightweight networks, which can be easily extended to various downstream tasks. Finally, we evaluate CAS-ViT across a variety of vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. Our M and T model achieves 83.0%/84.1% top-1 with only 12M/21M parameters on ImageNet- 1K. Meanwhile, throughput evaluations on GPUs, ONNX, and iPhones also demonstrate superior results compared to other state-of-the-art backbones. Extensive experiments demonstrate that our approach achieves a better balance of performance, efficient inference and easy-to-deploy. Our code and model are available at: https://github.com/Tianfang-Zhang/CAS-ViT.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianfang Zhang; Lei Li; Yang Zhou; Wentao Liu; Chen Qian; Jenq-Neng Hwang; Xiangyang Ji&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3655121"&gt;10.1109/tip.2026.3655121&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Transformers (ViTs) mark a revolutionary advance in neural networks with their token mixer’s powerful global context capability. However, the pairwise token affinity and complex matrix operations limit its deployment on resource-constrained scenarios and real-time applications, such as mobile devices, although considerable efforts have been made in previous works. In this paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision Transformers, to achieve a balance between efficiency and performance in mobile applications. Firstly, we argue that the capability of token mixers to obtain global contextual information hinges on multiple information interactions, such as spatial and channel domains. Subsequently, we propose Convolutional Additive Token Mixer (CATM) employing underlying spatial and channel attention as novel interaction forms. This module eliminates troublesome complex operations such as matrix multiplication and Softmax. We introduce Convolutional Additive Self-attention(CAS) block hybrid architecture and utilize CATM for each block. And further, we build a family of lightweight networks, which can be easily extended to various downstream tasks. Finally, we evaluate CAS-ViT across a variety of vision tasks, including image classification, object detection, instance segmentation, and semantic segmentation. Our M and T model achieves 83.0%/84.1% top-1 with only 12M/21M parameters on ImageNet- 1K. Meanwhile, throughput evaluations on GPUs, ONNX, and iPhones also demonstrate superior results compared to other state-of-the-art backbones. Extensive experiments demonstrate that our approach achieves a better balance of performance, efficient inference and easy-to-deploy. Our code and model are available at: https://github.com/Tianfang-Zhang/CAS-ViT.&lt;/p&gt;</content:encoded></item><item><title>All-in-One Transformer for Image Restoration Under Adverse Weather Degradations</title><link>https://doi.org/10.1109/tpami.2026.3658598</link><guid>10.1109/tpami.2026.3658598</guid><pubDate>Wed, 28 Jan 2026 20:59:22 +0000</pubDate><dc:creator>Jiawei Mao</dc:creator><dc:creator>Yu Yang</dc:creator><dc:creator>Xuesong Yin</dc:creator><dc:creator>Ling Shao</dc:creator><dc:creator>Hao Tang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3658598</prism:doi><description>Severe weather restoration models often face the simultaneous interaction of multiple degradations in real-world scenarios. Existing approaches typically handle single or composite degradations based on scene descriptors derived from text or image embeddings. However, due to the varying proportions of different degradations within an image, these scene descriptors may not accurately differentiate between degradations, leading to suboptimal restoration in practical applications. To address this issue, we propose a novel Transformer-based restoration framework, AllRestorer, for dealing with four physical severe weather impairments: low-light, haze, rain, and snow. In AllRestorer, we enable the model to adaptively consider all weather impairments, thereby avoiding errors from scene descriptor misdirection. Specifically, we introduce the All-in-One Transformer Block (AiOTB), the core innovation of which is the ability to adaptively handle multiple degradations in a single image, beyond the limitation of existing Transformers that can only handle one type of degradation at a time. To accurately address different variations potentially present within the same type of degradation and minimize ambiguity, AiOTB utilizes a Composite Scene Embedding consisting of both image and text embeddings to define the degradation. Moreover, AiOTB includes an adaptive weight for each degradation, allowing for precise control of the restoration intensity. By leveraging AiOTB, AllRestorer avoids misdirection caused by inaccurate scene descriptors, achieving a 5.00 dB increase in PSNR compared to the baseline on the CDD-11 dataset.
Published: 2026-01-28T20:59:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Mao; Yu Yang; Xuesong Yin; Ling Shao; Hao Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3658598"&gt;10.1109/tpami.2026.3658598&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Severe weather restoration models often face the simultaneous interaction of multiple degradations in real-world scenarios. Existing approaches typically handle single or composite degradations based on scene descriptors derived from text or image embeddings. However, due to the varying proportions of different degradations within an image, these scene descriptors may not accurately differentiate between degradations, leading to suboptimal restoration in practical applications. To address this issue, we propose a novel Transformer-based restoration framework, AllRestorer, for dealing with four physical severe weather impairments: low-light, haze, rain, and snow. In AllRestorer, we enable the model to adaptively consider all weather impairments, thereby avoiding errors from scene descriptor misdirection. Specifically, we introduce the All-in-One Transformer Block (AiOTB), the core innovation of which is the ability to adaptively handle multiple degradations in a single image, beyond the limitation of existing Transformers that can only handle one type of degradation at a time. To accurately address different variations potentially present within the same type of degradation and minimize ambiguity, AiOTB utilizes a Composite Scene Embedding consisting of both image and text embeddings to define the degradation. Moreover, AiOTB includes an adaptive weight for each degradation, allowing for precise control of the restoration intensity. By leveraging AiOTB, AllRestorer avoids misdirection caused by inaccurate scene descriptors, achieving a 5.00 dB increase in PSNR compared to the baseline on the CDD-11 dataset.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Monocular 3D Object Detection with Depth Thickness Field</title><link>https://doi.org/10.1109/tcsvt.2026.3658133</link><guid>10.1109/tcsvt.2026.3658133</guid><pubDate>Wed, 28 Jan 2026 21:01:12 +0000</pubDate><dc:creator>Qiude Zhang</dc:creator><dc:creator>Chunyu Lin</dc:creator><dc:creator>Zhijie Shen</dc:creator><dc:creator>Lang Nie</dc:creator><dc:creator>Yao Zhao</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3658133</prism:doi><description>Monocular 3D object detection is challenging due to the lack of accurate depth. However, existing depth-assisted solutions still exhibit inferior performance, whose reason is universally acknowledged as the unsatisfactory accuracy of monocular depth estimation models. In this paper, we revisit monocular 3D object detection from the depth perspective and formulate an additional issue as the limited 3D structure-aware capability of existing depth representations (e.g., depth one-hot encoding or depth distribution). To address this issue, we introduce a novel Depth Thickness Field approach to embed clear 3D structures of the scenes. Specifically, we present MonoDTF, a scene-to-instance depth-adapted network comprising a Scene-Level Depth Retargeting (SDR) module and an Instance-Level Spatial Refinement (ISR) module. The former retargets traditional depth representations to the proposed depth thickness field, incorporating the scene-level perception of 3D structures. The latter refines the voxel space with the guidance of instances, enhancing the 3D instance-aware capability of the depth thickness field and thus improving detection accuracy. Extensive experiments on the KITTI and Waymo datasets demonstrate our superiority to existing state-of-the-art (SoTA) methods and the universality when equipped with different depth estimation models. The source codes are available at https://github.com/QiuDeZhang/MonoDTF.
Published: 2026-01-28T21:01:12+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiude Zhang; Chunyu Lin; Zhijie Shen; Lang Nie; Yao Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3658133"&gt;10.1109/tcsvt.2026.3658133&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection is challenging due to the lack of accurate depth. However, existing depth-assisted solutions still exhibit inferior performance, whose reason is universally acknowledged as the unsatisfactory accuracy of monocular depth estimation models. In this paper, we revisit monocular 3D object detection from the depth perspective and formulate an additional issue as the limited 3D structure-aware capability of existing depth representations (e.g., depth one-hot encoding or depth distribution). To address this issue, we introduce a novel Depth Thickness Field approach to embed clear 3D structures of the scenes. Specifically, we present MonoDTF, a scene-to-instance depth-adapted network comprising a Scene-Level Depth Retargeting (SDR) module and an Instance-Level Spatial Refinement (ISR) module. The former retargets traditional depth representations to the proposed depth thickness field, incorporating the scene-level perception of 3D structures. The latter refines the voxel space with the guidance of instances, enhancing the 3D instance-aware capability of the depth thickness field and thus improving detection accuracy. Extensive experiments on the KITTI and Waymo datasets demonstrate our superiority to existing state-of-the-art (SoTA) methods and the universality when equipped with different depth estimation models. The source codes are available at https://github.com/QiuDeZhang/MonoDTF.&lt;/p&gt;</content:encoded></item><item><title>Phase-Guided Cross-Frequency Integration Network for ISAR and Optical Image Fusion</title><link>https://doi.org/10.1109/tcsvt.2026.3657411</link><guid>10.1109/tcsvt.2026.3657411</guid><pubDate>Wed, 28 Jan 2026 21:01:12 +0000</pubDate><dc:creator>Ze Wang</dc:creator><dc:creator>Lei Liu</dc:creator><dc:creator>Zhenxi Zhang</dc:creator><dc:creator>Rongzhen Du</dc:creator><dc:creator>Wanting Zhou</dc:creator><dc:creator>Man Zhou</dc:creator><dc:creator>Jingjing Cai</dc:creator><dc:creator>Feng Zhou</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3657411</prism:doi><description>Inverse synthetic aperture radar (ISAR) and optical image fusion aims to generate a composite image that simultaneously emphasizes the prominent contours of spacecraft from optical images and preserves the rich texture information inherent in ISAR images. However, the limited receptive fields of spatial-domain methods restrict their ability to capture global contextual dependencies among strong scattering points in ISAR images and to effectively integrate complementary optical features. To tackle this challenge, we propose a phase-guided cross-frequency integration module (PGCFIM), which exploits the intrinsic global modeling capability of the frequency domain and the semantic expressiveness of the phase spectrum. Specifically, a deep Fourier transform is employed to establish an image-wide receptive field for intra-domain global modeling. Subsequently, phase components are explicitly aggregated, and a gating mechanism is introduced to guide the integration of inter-domain long-range dependencies, enabling effective learning of complementary cross-modal representations. To eliminate reliance on hand-crafted fusion strategies, we design an end-to-end network, named PGCFINet. By jointly enhancing cross-domain interaction, frequency-domain global awareness, and explicit complementary feature integration, PGCFINet significantly strengthens cross-domain and cross-modal information interaction representation. Furthermore, to mitigate the current lack of ISAR and optical image datasets, we construct a new dataset comprising various spacecraft models, offering an alternative benchmark for evaluation. Extensive experiments demonstrate show that PGCFINet achieves superior performance than state-of-the-art methods in both qualitative and quantitative assessments. Moreover, PGCFINet is extended to infrared and visible image fusion, and the favorable results further validate its robust generalization ability. The codes of our fusion method and the dataset are forthcoming at http...
Published: 2026-01-28T21:01:12+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ze Wang; Lei Liu; Zhenxi Zhang; Rongzhen Du; Wanting Zhou; Man Zhou; Jingjing Cai; Feng Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3657411"&gt;10.1109/tcsvt.2026.3657411&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Inverse synthetic aperture radar (ISAR) and optical image fusion aims to generate a composite image that simultaneously emphasizes the prominent contours of spacecraft from optical images and preserves the rich texture information inherent in ISAR images. However, the limited receptive fields of spatial-domain methods restrict their ability to capture global contextual dependencies among strong scattering points in ISAR images and to effectively integrate complementary optical features. To tackle this challenge, we propose a phase-guided cross-frequency integration module (PGCFIM), which exploits the intrinsic global modeling capability of the frequency domain and the semantic expressiveness of the phase spectrum. Specifically, a deep Fourier transform is employed to establish an image-wide receptive field for intra-domain global modeling. Subsequently, phase components are explicitly aggregated, and a gating mechanism is introduced to guide the integration of inter-domain long-range dependencies, enabling effective learning of complementary cross-modal representations. To eliminate reliance on hand-crafted fusion strategies, we design an end-to-end network, named PGCFINet. By jointly enhancing cross-domain interaction, frequency-domain global awareness, and explicit complementary feature integration, PGCFINet significantly strengthens cross-domain and cross-modal information interaction representation. Furthermore, to mitigate the current lack of ISAR and optical image datasets, we construct a new dataset comprising various spacecraft models, offering an alternative benchmark for evaluation. Extensive experiments demonstrate show that PGCFINet achieves superior performance than state-of-the-art methods in both qualitative and quantitative assessments. Moreover, PGCFINet is extended to infrared and visible image fusion, and the favorable results further validate its robust generalization ability. The codes of our fusion method and the dataset are forthcoming at http...&lt;/p&gt;</content:encoded></item><item><title>Dynamic Mutual Learning for Object Detection in Aerial Imagery</title><link>https://doi.org/10.1109/tgrs.2026.3658431</link><guid>10.1109/tgrs.2026.3658431</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Cong Zhang</dc:creator><dc:creator>Chuang Yang</dc:creator><dc:creator>Yakun Ju</dc:creator><dc:creator>Jun Xiao</dc:creator><dc:creator>Muwei Jian</dc:creator><dc:creator>Kin-Man Lam</dc:creator><dc:creator>Qi Wang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658431</prism:doi><description>Object detection in aerial imagery is a pivotal task for various Earth observation systems, composed of two separate yet interdependent subtasks: classification and localization. However, existing methods face two fundamental limitations: 1) inconsistency in prediction distributions, where these two subtasks lack spatial distribution alignment, and 2) impracticability of cross-scale representations, where fixed-scale representations impede representation capacity and accuracy for objects of varying sizes in aerial scenarios. To overcome these challenges, this paper proposes a novel dynamic mutual learning paradigm that synergizes representation-wise and supervision-wise interactions within a unified detection head. It consists of two learning schemes: 1) dynamic learning, which introduces the dynamic routing mechanism to enable cross-scale fine-grained representation aggregation, significantly benefiting representational efficiency and flexibility, and 2) mutual learning, which establishes prediction alignment by explicitly performing subtask-consistent supervision and collaborative optimization. Moreover, within the entire enhanced detection head, these schemes can be jointly optimized and mutually reinforced. Extensive experimental results on different datasets have demonstrated the effectiveness and superiority of this proposed learning paradigm for object detection in aerial imagery, achieving competitive performance in both detection accuracy and computational efficiency.
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cong Zhang; Chuang Yang; Yakun Ju; Jun Xiao; Muwei Jian; Kin-Man Lam; Qi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658431"&gt;10.1109/tgrs.2026.3658431&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection in aerial imagery is a pivotal task for various Earth observation systems, composed of two separate yet interdependent subtasks: classification and localization. However, existing methods face two fundamental limitations: 1) inconsistency in prediction distributions, where these two subtasks lack spatial distribution alignment, and 2) impracticability of cross-scale representations, where fixed-scale representations impede representation capacity and accuracy for objects of varying sizes in aerial scenarios. To overcome these challenges, this paper proposes a novel dynamic mutual learning paradigm that synergizes representation-wise and supervision-wise interactions within a unified detection head. It consists of two learning schemes: 1) dynamic learning, which introduces the dynamic routing mechanism to enable cross-scale fine-grained representation aggregation, significantly benefiting representational efficiency and flexibility, and 2) mutual learning, which establishes prediction alignment by explicitly performing subtask-consistent supervision and collaborative optimization. Moreover, within the entire enhanced detection head, these schemes can be jointly optimized and mutually reinforced. Extensive experimental results on different datasets have demonstrated the effectiveness and superiority of this proposed learning paradigm for object detection in aerial imagery, achieving competitive performance in both detection accuracy and computational efficiency.&lt;/p&gt;</content:encoded></item><item><title>Large Multimodal Models for Low-Resource Languages: A Survey</title><link>https://doi.org/10.1016/j.inffus.2026.104189</link><guid>10.1016/j.inffus.2026.104189</guid><pubDate>Wed, 28 Jan 2026 00:39:35 +0000</pubDate><dc:creator>Marian Lupaşcu</dc:creator><dc:creator>Ana-Cristina Rogoz</dc:creator><dc:creator>Mihai Sorin Stupariu</dc:creator><dc:creator>Radu Tudor Ionescu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104189</prism:doi><description>In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey .
Published: 2026-01-28T00:39:35+00:00
Venue: Information Fusion
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Marian Lupaşcu; Ana-Cristina Rogoz; Mihai Sorin Stupariu; Radu Tudor Ionescu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104189"&gt;10.1016/j.inffus.2026.104189&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey .&lt;/p&gt;</content:encoded></item><item><title>DMPFDet: An End-to-End Deformable Mutual-Promotion Learning Network for Multispectral Visible–Infrared Fusion Detection</title><link>https://doi.org/10.1016/j.neunet.2026.108654</link><guid>10.1016/j.neunet.2026.108654</guid><pubDate>Wed, 28 Jan 2026 16:09:01 +0000</pubDate><dc:creator>Shangpo Zheng</dc:creator><dc:creator>Junfeng Liu</dc:creator><dc:creator>Jun Zeng</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108654</prism:doi><description>Multimodal image fusion and object detection significantly enhance detection accuracy and robustness in complex environments, which is crucial for autonomous driving. Despite the progress in multimodal fusion methods, most of them focus on generating visually appealing images while neglecting their effectiveness for downstream tasks. Meanwhile, existing multimodal detection methods often fail to fully exploit modality-specific features and cross-modal complementary information. Although some recent studies have attempted to integrate fusion and detection, they typically rely on multi-stage training pipelines and overlook the potential of mutual guidance between fused features and detection representations, leading to suboptimal performance. To address these challenges, this study proposes DMPFDet, an End-to-End Deformable Mutual-Promotion Learning Network for Multispectral Visible-Infrared Fusion Detection, which achieves both high-quality fusion and accurate detection within a unified training pipeline. It comprises two main components: a Multimodal Deformable Detection Transformer (MDDT) module for detection and a Cross-Modal Attention Fusion (CMAF) module for fusion. The MDDT is designed based on the RT-DETR architecture and is tailored to efficiently extract both modality-specific features and cross-modal complementary information. The CMAF effectively captures local texture details, global contextual information, as well as channel and spatial information. It is worth noting that the two modules are designed to mutually provide feature-level guidance, enabling joint optimization and reinforcing each other's learning processes. Experimental results on multiple datasets demonstrate the outstanding performance of the proposed method, outperforming state-of-the-art approaches. It not only produces effective fusion results for object detection but also delivers impressive detection outcomes.
Published: 2026-01-28T16:09:01+00:00
Venue: Neural Networks
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shangpo Zheng; Junfeng Liu; Jun Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108654"&gt;10.1016/j.neunet.2026.108654&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal image fusion and object detection significantly enhance detection accuracy and robustness in complex environments, which is crucial for autonomous driving. Despite the progress in multimodal fusion methods, most of them focus on generating visually appealing images while neglecting their effectiveness for downstream tasks. Meanwhile, existing multimodal detection methods often fail to fully exploit modality-specific features and cross-modal complementary information. Although some recent studies have attempted to integrate fusion and detection, they typically rely on multi-stage training pipelines and overlook the potential of mutual guidance between fused features and detection representations, leading to suboptimal performance. To address these challenges, this study proposes DMPFDet, an End-to-End Deformable Mutual-Promotion Learning Network for Multispectral Visible-Infrared Fusion Detection, which achieves both high-quality fusion and accurate detection within a unified training pipeline. It comprises two main components: a Multimodal Deformable Detection Transformer (MDDT) module for detection and a Cross-Modal Attention Fusion (CMAF) module for fusion. The MDDT is designed based on the RT-DETR architecture and is tailored to efficiently extract both modality-specific features and cross-modal complementary information. The CMAF effectively captures local texture details, global contextual information, as well as channel and spatial information. It is worth noting that the two modules are designed to mutually provide feature-level guidance, enabling joint optimization and reinforcing each other&amp;#x27;s learning processes. Experimental results on multiple datasets demonstrate the outstanding performance of the proposed method, outperforming state-of-the-art approaches. It not only produces effective fusion results for object detection but also delivers impressive detection outcomes.&lt;/p&gt;</content:encoded></item><item><title>Individual &amp;amp; Common Attack: Enhancing Transferability in VLP Models through Modal Feature Exploitation</title><link>https://doi.org/10.1109/tip.2026.3651982</link><guid>10.1109/tip.2026.3651982</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Yaguan Qian</dc:creator><dc:creator>Yaxin Kong</dc:creator><dc:creator>Qiqi Bao</dc:creator><dc:creator>Zhaoquan Gu</dc:creator><dc:creator>Bin Wang</dc:creator><dc:creator>Shouling Ji</dc:creator><dc:creator>Jianping Zhang</dc:creator><dc:creator>Zhen Lei</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3651982</prism:doi><description>Vision–Language Pretrained (VLP) models exhibit strong multimodal understanding and reasoning capabilities, finding wide application in tasks such as image–text retrieval and visual grounding. However, they remain highly vulnerable to adversarial attacks, posing serious reliability concerns in safety-critical scenarios. We observe that existing adversarial examples optimization methods typically rely on individual features from the other modality as guidance, causing the crafted adversarial examples to overfit that modality’s learning preferences and thus limiting their transferability. In order to further enhance the transferability of adversarial examples, we propose a novel adversarial attack framework, I&amp;CA (Individual &amp; Common feature Attack), which simultaneously considers individual features within each modality and common features cross-modal interactions. Concretely, I&amp;CA first drives divergence among individual features within each modality to disrupt single-modality learning, and then suppresses the expression of common features during cross-modal interactions, thereby undermining the robustness of the fusion mechanism. In addition, to prevent adversarial perturbations from overfitting to the learning bias of the other modality, which may distort the representation of common features, we simultaneously introduce augmentation strategies to both modalities. Across various experimental settings and widely recognized multimodal benchmarks, the I&amp;CA framework achieves an average transferability improvement of 6.15% over the state-of-the-art DRA method, delivering significant performance gains in both cross-model and cross-task attack scenarios.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaguan Qian; Yaxin Kong; Qiqi Bao; Zhaoquan Gu; Bin Wang; Shouling Ji; Jianping Zhang; Zhen Lei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3651982"&gt;10.1109/tip.2026.3651982&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Vision–Language Pretrained (VLP) models exhibit strong multimodal understanding and reasoning capabilities, finding wide application in tasks such as image–text retrieval and visual grounding. However, they remain highly vulnerable to adversarial attacks, posing serious reliability concerns in safety-critical scenarios. We observe that existing adversarial examples optimization methods typically rely on individual features from the other modality as guidance, causing the crafted adversarial examples to overfit that modality’s learning preferences and thus limiting their transferability. In order to further enhance the transferability of adversarial examples, we propose a novel adversarial attack framework, I&amp;amp;CA (Individual &amp;amp; Common feature Attack), which simultaneously considers individual features within each modality and common features cross-modal interactions. Concretely, I&amp;amp;CA first drives divergence among individual features within each modality to disrupt single-modality learning, and then suppresses the expression of common features during cross-modal interactions, thereby undermining the robustness of the fusion mechanism. In addition, to prevent adversarial perturbations from overfitting to the learning bias of the other modality, which may distort the representation of common features, we simultaneously introduce augmentation strategies to both modalities. Across various experimental settings and widely recognized multimodal benchmarks, the I&amp;amp;CA framework achieves an average transferability improvement of 6.15% over the state-of-the-art DRA method, delivering significant performance gains in both cross-model and cross-task attack scenarios.&lt;/p&gt;</content:encoded></item><item><title>Domain-Complementary Prior with Fine-Grained Feedback for Scene Text Image Super-Resolution</title><link>https://doi.org/10.1109/tip.2026.3657246</link><guid>10.1109/tip.2026.3657246</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Shen Zhang</dc:creator><dc:creator>Yang Li</dc:creator><dc:creator>Pengwen Dai</dc:creator><dc:creator>Xiaozhou Zhou</dc:creator><dc:creator>Guotao Xie</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657246</prism:doi><description>Enhancing the resolution of scene text images is a critical preprocessing step that can substantially improve the accuracy of downstream text recognition in low-quality images. Existing methods primarily rely on auxiliary text features to guide the super-resolution process. However, these features often lack rich low-level information, making them insufficient for faithfully reconstructing both the global structure and fine-grained details of text. Moreover, previous methods often learn suboptimal feature representations from the original low-quality landmark images, which cannot provide precise guidance for super-resolution. In this study, we propose a Fine-Grained Feedback Domain-Complementary Network (FDNet) for scene text image super-resolution. Specifically, we first employ a fine-grained feedback mechanism to selectively refine landmark images, thereby enhancing feature representations. Then, we introduce a novel domain-trace prior interaction generator, which integrates domain-specific traces with a text prior to comprehensively complement the clear edges and structural coverage of the text. Finally, motivated by the limitations of existing datasets, which often exhibit limited scene scales and insufficient challenging scenarios, we introduce a new dataset, MDRText. The proposed dataset MDRText features multi-scale and diverse characteristics and is designed to support challenging text image recognition and super-resolution tasks. Extensive experiments on the MDRText and TextZoom datasets demonstrate that our method achieves superior performance in scene text image super-resolution and further improves the accuracy of subsequent recognition tasks.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shen Zhang; Yang Li; Pengwen Dai; Xiaozhou Zhou; Guotao Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657246"&gt;10.1109/tip.2026.3657246&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Enhancing the resolution of scene text images is a critical preprocessing step that can substantially improve the accuracy of downstream text recognition in low-quality images. Existing methods primarily rely on auxiliary text features to guide the super-resolution process. However, these features often lack rich low-level information, making them insufficient for faithfully reconstructing both the global structure and fine-grained details of text. Moreover, previous methods often learn suboptimal feature representations from the original low-quality landmark images, which cannot provide precise guidance for super-resolution. In this study, we propose a Fine-Grained Feedback Domain-Complementary Network (FDNet) for scene text image super-resolution. Specifically, we first employ a fine-grained feedback mechanism to selectively refine landmark images, thereby enhancing feature representations. Then, we introduce a novel domain-trace prior interaction generator, which integrates domain-specific traces with a text prior to comprehensively complement the clear edges and structural coverage of the text. Finally, motivated by the limitations of existing datasets, which often exhibit limited scene scales and insufficient challenging scenarios, we introduce a new dataset, MDRText. The proposed dataset MDRText features multi-scale and diverse characteristics and is designed to support challenging text image recognition and super-resolution tasks. Extensive experiments on the MDRText and TextZoom datasets demonstrate that our method achieves superior performance in scene text image super-resolution and further improves the accuracy of subsequent recognition tasks.&lt;/p&gt;</content:encoded></item><item><title>BP-NeRF: End-to-End Neural Radiance Fields for Sparse Images without Camera Pose in Complex Scenes</title><link>https://doi.org/10.1109/tip.2026.3657188</link><guid>10.1109/tip.2026.3657188</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Yaru Qiu</dc:creator><dc:creator>Guoxia Wu</dc:creator><dc:creator>Yuanyuan Sun</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657188</prism:doi><description>Synthesizing novel perspectives of complex scenes in high quality using sparse image sequences, especially for those without camera poses, is a challenging task. The key to enhancing accuracy in such scenarios lies in sufficient prior knowledge and accurate camera motion constraints. Therefore, we propose an end-to-end novel view synthesis network named BP-NeRF. It is capable of using sequences of sparse images captured in indoor and outdoor complex scenes to estimate camera motion trajectories and generate novel view images. Firstly, to address the issue of inaccurate prediction of depth map caused by insufficient overlapping features in sparse images, we designed the RDP-Net module to generate depth maps for sparse image sequences and calculate the depth accuracy of these maps, providing the network with a reliable depth prior. Secondly, to enhance the accuracy of camera pose estimation, we construct a loss function based on the geometric consistency of 2D and 3D feature variations between frames, improving the accuracy and robustness of the network’s estimations. We conducted experimental evaluations on the LLFF and Tanks datasets, and the results show that, compared to the current mainstream methods, BP-NeRF can generate more accurate novel views without camera poses.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaru Qiu; Guoxia Wu; Yuanyuan Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657188"&gt;10.1109/tip.2026.3657188&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Synthesizing novel perspectives of complex scenes in high quality using sparse image sequences, especially for those without camera poses, is a challenging task. The key to enhancing accuracy in such scenarios lies in sufficient prior knowledge and accurate camera motion constraints. Therefore, we propose an end-to-end novel view synthesis network named BP-NeRF. It is capable of using sequences of sparse images captured in indoor and outdoor complex scenes to estimate camera motion trajectories and generate novel view images. Firstly, to address the issue of inaccurate prediction of depth map caused by insufficient overlapping features in sparse images, we designed the RDP-Net module to generate depth maps for sparse image sequences and calculate the depth accuracy of these maps, providing the network with a reliable depth prior. Secondly, to enhance the accuracy of camera pose estimation, we construct a loss function based on the geometric consistency of 2D and 3D feature variations between frames, improving the accuracy and robustness of the network’s estimations. We conducted experimental evaluations on the LLFF and Tanks datasets, and the results show that, compared to the current mainstream methods, BP-NeRF can generate more accurate novel views without camera poses.&lt;/p&gt;</content:encoded></item><item><title>A Few-Shot Class Incremental Learning Method Using Graph Neural Networks</title><link>https://doi.org/10.1109/tip.2026.3657170</link><guid>10.1109/tip.2026.3657170</guid><pubDate>Wed, 28 Jan 2026 21:01:28 +0000</pubDate><dc:creator>Yuqian Ma</dc:creator><dc:creator>Youfa Liu</dc:creator><dc:creator>Bo Du</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657170</prism:doi><description>Few-shot class incremental learning (FSCIL) aims to continuously learn new classes from limited training samples while retaining previously acquired knowledge. Existing approaches are not fully capable of balancing stability and plasticity in dynamic scenarios. To overcome this limitation, we introduce a novel FSCIL framework that leverages graph neural networks (GNNs) to model interdependencies between different categories and enhance cross-modal alignment. Our framework incorporates three key components: (1) a Graph Isomorphism Network (GIN) to propagate contextual relationships among prompts; (2) a Hamiltonian Graph Network with Energy Conservation (HGN-EC) to stabilize training dynamics via energy conservation constraints; and (3) an Adversarially Constrained Graph Autoencoder (ACGA) to enforce latent space consistency. By integrating these components with a parameter-efficient CLIP backbone, our method dynamically adapts graph structures to model semantic correlations between textual and visual modalities. Additionally, contrastive learning with energy-based regularization is employed to mitigate catastrophic forgetting and improve generalization. Comprehensive experiments on benchmark datasets validate the framework’s incremental accuracy and stability compared to state-of-the-art baselines. This work advances FSCIL by unifying graph-based relational reasoning with physics-inspired optimization, offering a scalable and interpretable framework.
Published: 2026-01-28T21:01:28+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuqian Ma; Youfa Liu; Bo Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657170"&gt;10.1109/tip.2026.3657170&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot class incremental learning (FSCIL) aims to continuously learn new classes from limited training samples while retaining previously acquired knowledge. Existing approaches are not fully capable of balancing stability and plasticity in dynamic scenarios. To overcome this limitation, we introduce a novel FSCIL framework that leverages graph neural networks (GNNs) to model interdependencies between different categories and enhance cross-modal alignment. Our framework incorporates three key components: (1) a Graph Isomorphism Network (GIN) to propagate contextual relationships among prompts; (2) a Hamiltonian Graph Network with Energy Conservation (HGN-EC) to stabilize training dynamics via energy conservation constraints; and (3) an Adversarially Constrained Graph Autoencoder (ACGA) to enforce latent space consistency. By integrating these components with a parameter-efficient CLIP backbone, our method dynamically adapts graph structures to model semantic correlations between textual and visual modalities. Additionally, contrastive learning with energy-based regularization is employed to mitigate catastrophic forgetting and improve generalization. Comprehensive experiments on benchmark datasets validate the framework’s incremental accuracy and stability compared to state-of-the-art baselines. This work advances FSCIL by unifying graph-based relational reasoning with physics-inspired optimization, offering a scalable and interpretable framework.&lt;/p&gt;</content:encoded></item><item><title>DeSPAR: Depth-Guided Semantic-Prompted Adaptive Refinement for ORSI Salient Object Detection</title><link>https://doi.org/10.1109/tgrs.2026.3658823</link><guid>10.1109/tgrs.2026.3658823</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Xiaoli Zhang</dc:creator><dc:creator>Ping Liufu</dc:creator><dc:creator>Xihang Hu</dc:creator><dc:creator>Xiongfei Li</dc:creator><dc:creator>Chuanmin Jia</dc:creator><dc:creator>Siwei Ma</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658823</prism:doi><description>Optical remote sensing image salient object detection (ORSI-SOD) currently faces two major challenges: (1) existing RGB-based methods primarily depend on color and texture cues, which makes it difficult to obtain robust representations of object spatial structure under extreme imaging conditions; (2) significant morphological variations across object categories lead to semantic feature confusion in existing methods. To address these issues, we propose Depth-Guided Semantic-Prompted Adaptive Refinement (DeSPAR), a progressive refinement framework with geometric–semantic decoupling. To avoid excessive coupling between geometric and semantic signals in an end-to-end architecture, which would cause semantic priors to interfere too early with the construction of generic geometric representations, DeSPAR adopts a two-stage design for feature learning. In Stage 1, Depth-Guided Geometric Learning (DGL) employs a novel lightweight Depth-Guided Refiner (DGR) to build a generic geometric foundation. DGR utilizes RGB features to guide pseudo-depth denoising and injects geometric cues from pseudo-depth to enhance spatial feature representations. In Stage 2, Depth-Guided Semantic-Adaptive Refinement (DSR) inherits the encoder weights from DGL and introduces category-specific constraints. Under the guidance of a Semantic Prompt Bank constructed from DGL, DSR adaptively optimizes the representations of different categories through a prompt-guided mechanism. Experimental results demonstrate that DeSPAR surpasses 22 state-of-the-art methods on three public ORSI-SOD benchmarks, achieving superior performance with only 26.4M parameters while attaining an inference speed of 161 FPS.
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoli Zhang; Ping Liufu; Xihang Hu; Xiongfei Li; Chuanmin Jia; Siwei Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658823"&gt;10.1109/tgrs.2026.3658823&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Optical remote sensing image salient object detection (ORSI-SOD) currently faces two major challenges: (1) existing RGB-based methods primarily depend on color and texture cues, which makes it difficult to obtain robust representations of object spatial structure under extreme imaging conditions; (2) significant morphological variations across object categories lead to semantic feature confusion in existing methods. To address these issues, we propose Depth-Guided Semantic-Prompted Adaptive Refinement (DeSPAR), a progressive refinement framework with geometric–semantic decoupling. To avoid excessive coupling between geometric and semantic signals in an end-to-end architecture, which would cause semantic priors to interfere too early with the construction of generic geometric representations, DeSPAR adopts a two-stage design for feature learning. In Stage 1, Depth-Guided Geometric Learning (DGL) employs a novel lightweight Depth-Guided Refiner (DGR) to build a generic geometric foundation. DGR utilizes RGB features to guide pseudo-depth denoising and injects geometric cues from pseudo-depth to enhance spatial feature representations. In Stage 2, Depth-Guided Semantic-Adaptive Refinement (DSR) inherits the encoder weights from DGL and introduces category-specific constraints. Under the guidance of a Semantic Prompt Bank constructed from DGL, DSR adaptively optimizes the representations of different categories through a prompt-guided mechanism. Experimental results demonstrate that DeSPAR surpasses 22 state-of-the-art methods on three public ORSI-SOD benchmarks, achieving superior performance with only 26.4M parameters while attaining an inference speed of 161 FPS.&lt;/p&gt;</content:encoded></item><item><title>SBSR-Net: Multi-type Parameters Learning-based Real Aperture Radar Forward-looking Superresolution Imaging Framework</title><link>https://doi.org/10.1109/tgrs.2026.3658105</link><guid>10.1109/tgrs.2026.3658105</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Mingjie Yang</dc:creator><dc:creator>Deqing Mao</dc:creator><dc:creator>Lu Jiao</dc:creator><dc:creator>Yin Zhang</dc:creator><dc:creator>Yunfei Zhu</dc:creator><dc:creator>Yulin Huang</dc:creator><dc:creator>Shuifeng Yang</dc:creator><dc:creator>Yi Han</dc:creator><dc:creator>Jianyu Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658105</prism:doi><description>Regularization methods are widely employed in real aperture radar forward-looking superresolution imaging. However, these methods suffer from the problem of selecting multi-type parameters. To mitigate this issue, based on the real aperture radar forward-looking superresolution imaging application, we propose a multi-type parameters learning-based Split Bregman superresolution network (SBSR-Net) by establishing a projection relationship between the multi-type parameters of the Split Bregman algorithm and the real beam data. First, before the modular network design, we analyze the impact of various types of parameters on imaging performance in conventional algorithms. Second, based on the characteristics of different types of parameters, including penalty parameters, regularization parameters, threshold parameters, and step parameters, we develop differentiated strategies by formulating them as learnable parameters within network layers. Finally, we construct a dataset for forward-looking superresolution imaging and conduct network training to select these multi-type parameters adaptively. The proposed model-data driven architecture demonstrates superior performance in multi-type parameters selection. Simulations and experiments are given to verify the performance of the proposed framework.
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingjie Yang; Deqing Mao; Lu Jiao; Yin Zhang; Yunfei Zhu; Yulin Huang; Shuifeng Yang; Yi Han; Jianyu Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658105"&gt;10.1109/tgrs.2026.3658105&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Regularization methods are widely employed in real aperture radar forward-looking superresolution imaging. However, these methods suffer from the problem of selecting multi-type parameters. To mitigate this issue, based on the real aperture radar forward-looking superresolution imaging application, we propose a multi-type parameters learning-based Split Bregman superresolution network (SBSR-Net) by establishing a projection relationship between the multi-type parameters of the Split Bregman algorithm and the real beam data. First, before the modular network design, we analyze the impact of various types of parameters on imaging performance in conventional algorithms. Second, based on the characteristics of different types of parameters, including penalty parameters, regularization parameters, threshold parameters, and step parameters, we develop differentiated strategies by formulating them as learnable parameters within network layers. Finally, we construct a dataset for forward-looking superresolution imaging and conduct network training to select these multi-type parameters adaptively. The proposed model-data driven architecture demonstrates superior performance in multi-type parameters selection. Simulations and experiments are given to verify the performance of the proposed framework.&lt;/p&gt;</content:encoded></item><item><title>MonoTDF: Temporal Deep Feature Learning for Generalizable Monocular 3D Object Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113184</link><guid>10.1016/j.patcog.2026.113184</guid><pubDate>Wed, 28 Jan 2026 16:04:12 +0000</pubDate><dc:creator>Xiu-Zhi Chen</dc:creator><dc:creator>Yi-Kai Chiu</dc:creator><dc:creator>Chih-Sheng Huang</dc:creator><dc:creator>Yen-Lin Chen</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113184</prism:doi><description>Monocular 3D object detection has gained significant attention due to its cost-effectiveness and practicality in real-world applications. However, existing monocular methods often struggle with depth estimation and spatial consistency, limiting their accuracy in complex environments. In this work, we introduce a Temporal Deep Feature Learning framework, which enhances monocular 3D object detection by integrating temporal features across sequential frames. Our approach leverages a novel deep feature auxiliary module based on convolutional recurrent structures, effectively capturing spatiotemporal information to improve depth perception and detection robustness. The proposed module is model-agnostic and can be seamlessly integrated into various existing monocular detection frameworks. Extensive experiments across multiple state-of-the-art monocular 3D object detection models demonstrate consistent performance improvements, particularly in detecting small or partially occluded objects. Our results highlight the effectiveness and generalizability of the proposed approach, making it a promising solution for real-world autonomous perception systems. The source code of this work is at: https://github.com/Shuray36/MonoTDF-Temporal-Deep-Feature-Learning-for-Generalizable-Monocular-3D-Object-Detection .
Published: 2026-01-28T16:04:12+00:00
Venue: Pattern Recognition
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiu-Zhi Chen; Yi-Kai Chiu; Chih-Sheng Huang; Yen-Lin Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113184"&gt;10.1016/j.patcog.2026.113184&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection has gained significant attention due to its cost-effectiveness and practicality in real-world applications. However, existing monocular methods often struggle with depth estimation and spatial consistency, limiting their accuracy in complex environments. In this work, we introduce a Temporal Deep Feature Learning framework, which enhances monocular 3D object detection by integrating temporal features across sequential frames. Our approach leverages a novel deep feature auxiliary module based on convolutional recurrent structures, effectively capturing spatiotemporal information to improve depth perception and detection robustness. The proposed module is model-agnostic and can be seamlessly integrated into various existing monocular detection frameworks. Extensive experiments across multiple state-of-the-art monocular 3D object detection models demonstrate consistent performance improvements, particularly in detecting small or partially occluded objects. Our results highlight the effectiveness and generalizability of the proposed approach, making it a promising solution for real-world autonomous perception systems. The source code of this work is at: https://github.com/Shuray36/MonoTDF-Temporal-Deep-Feature-Learning-for-Generalizable-Monocular-3D-Object-Detection .&lt;/p&gt;</content:encoded></item><item><title>EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery</title><link>https://arxiv.org/abs/2601.18597v1</link><guid>http://arxiv.org/abs/2601.18597v1</guid><pubDate>Mon, 26 Jan 2026 15:41:37 +0000</pubDate><dc:creator>Yu Xia</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Tianqi Xiang</dc:creator><dc:creator>Zhigang Tu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.
Published: 2026-01-26T15:41:37+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Xia; Chang Liu; Tianqi Xiang; Zhigang Tu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.&lt;/p&gt;</content:encoded></item><item><title>Data-Efficient Generalization for Zero-shot Composed Image Retrieval</title><link>https://doi.org/10.1016/j.patcog.2026.113187</link><guid>10.1016/j.patcog.2026.113187</guid><pubDate>Wed, 28 Jan 2026 16:04:06 +0000</pubDate><dc:creator>Zining Chen</dc:creator><dc:creator>Zhicheng Zhao</dc:creator><dc:creator>Fei Su</dc:creator><dc:creator>Shijian Lu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113187</prism:doi><description>Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image based on a reference image and a text description without requiring in-distribution triplets for training. One prevalent approach follows the vision-language pretraining paradigm that employs a mapping network to transfer the image embedding to a pseudo-word token in the text embedding space. However, this approach tends to impede network generalization due to modality discrepancy and distribution shift between training and inference. To this end, we propose a Data-efficient Generalization (DeG) framework, including two novel designs, namely, Textual Supplement (TS) module and Semantic Sample Pool (SSP) module. The TS module exploits compositional textual semantics during training, enhancing the pseudo-word token with more linguistic semantics and thus mitigating the modality discrepancy effectively. The SSP module exploits the zero-shot capability of pretrained Vision-Language Models (VLMs), alleviating the distribution shift and mitigating the overfitting issue from the redundancy of the large-scale image-text data. Extensive experiments over four ZS-CIR benchmarks show that DeG outperforms the state-of-the-art (SOTA) methods with much less training data, and saves substantial training and inference time for practical usage.
Published: 2026-01-28T16:04:06+00:00
Venue: Pattern Recognition
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zining Chen; Zhicheng Zhao; Fei Su; Shijian Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113187"&gt;10.1016/j.patcog.2026.113187&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-shot Composed Image Retrieval (ZS-CIR) aims to retrieve the target image based on a reference image and a text description without requiring in-distribution triplets for training. One prevalent approach follows the vision-language pretraining paradigm that employs a mapping network to transfer the image embedding to a pseudo-word token in the text embedding space. However, this approach tends to impede network generalization due to modality discrepancy and distribution shift between training and inference. To this end, we propose a Data-efficient Generalization (DeG) framework, including two novel designs, namely, Textual Supplement (TS) module and Semantic Sample Pool (SSP) module. The TS module exploits compositional textual semantics during training, enhancing the pseudo-word token with more linguistic semantics and thus mitigating the modality discrepancy effectively. The SSP module exploits the zero-shot capability of pretrained Vision-Language Models (VLMs), alleviating the distribution shift and mitigating the overfitting issue from the redundancy of the large-scale image-text data. Extensive experiments over four ZS-CIR benchmarks show that DeG outperforms the state-of-the-art (SOTA) methods with much less training data, and saves substantial training and inference time for practical usage.&lt;/p&gt;</content:encoded></item><item><title>SFSR: Spectral Fusion Super-Resolution for Multi-Sensor Remote Sensing with Degraded References</title><link>https://doi.org/10.1109/tgrs.2026.3658541</link><guid>10.1109/tgrs.2026.3658541</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Seunghyun Gwak</dc:creator><dc:creator>Sooyoung Yang</dc:creator><dc:creator>Myungjoo Kang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658541</prism:doi><description>Reference-based super-resolution (RefSR) aims to enhance low-resolution (LR) imagery by leveraging auxiliary reference observations. While effective under controlled settings, most existing RefSR methods implicitly assume that reference images are clean, well-aligned, and geometrically consistent with the target input. In real-world remote sensing systems, however, reference observations are frequently degraded by sensor noise, atmospheric blur, and geometric inconsistencies caused by different viewing angles and acquisition times. Moreover, due to the inherent resolution gap between LR and reference images, strict spectral consistency is difficult to guarantee in practice. These factors substantially reduce the reliability of reference cues and limit the applicability of RefSR in multi-sensor satellite imaging scenarios. To address these challenges, we propose Spectral Fusion Super-Resolution (SFSR), a diffusion-based RefSR framework designed to operate robustly under degraded reference conditions. At its core, SFSR introduces the Spectral Swin Cross-Attention Module (S2CAM), which enables frequency-aware reference utilization and integrates the refined reference features as conditional guidance within the reverse diffusion process. By explicitly redistributing spectral components and suppressing unreliable high-frequency responses introduced by noise, SFSR enables stable and effective use of reference information that conventional RefSR methods struggle to exploit. Extensive experiments on synthetic and benchmark satellite datasets demonstrate that SFSR consistently outperforms state-of-the-art RefSR approaches in terms of PSNR, SSIM, and perceptual metrics, while maintaining high visual fidelity under severe degradation. In addition, evaluations on downstream tasks such as object detection and semantic segmentation show that SFSR leads to clear performance improvements, confirming its robustness and practical value for real-world multi-sensor remote sensing appli...
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seunghyun Gwak; Sooyoung Yang; Myungjoo Kang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658541"&gt;10.1109/tgrs.2026.3658541&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Reference-based super-resolution (RefSR) aims to enhance low-resolution (LR) imagery by leveraging auxiliary reference observations. While effective under controlled settings, most existing RefSR methods implicitly assume that reference images are clean, well-aligned, and geometrically consistent with the target input. In real-world remote sensing systems, however, reference observations are frequently degraded by sensor noise, atmospheric blur, and geometric inconsistencies caused by different viewing angles and acquisition times. Moreover, due to the inherent resolution gap between LR and reference images, strict spectral consistency is difficult to guarantee in practice. These factors substantially reduce the reliability of reference cues and limit the applicability of RefSR in multi-sensor satellite imaging scenarios. To address these challenges, we propose Spectral Fusion Super-Resolution (SFSR), a diffusion-based RefSR framework designed to operate robustly under degraded reference conditions. At its core, SFSR introduces the Spectral Swin Cross-Attention Module (S2CAM), which enables frequency-aware reference utilization and integrates the refined reference features as conditional guidance within the reverse diffusion process. By explicitly redistributing spectral components and suppressing unreliable high-frequency responses introduced by noise, SFSR enables stable and effective use of reference information that conventional RefSR methods struggle to exploit. Extensive experiments on synthetic and benchmark satellite datasets demonstrate that SFSR consistently outperforms state-of-the-art RefSR approaches in terms of PSNR, SSIM, and perceptual metrics, while maintaining high visual fidelity under severe degradation. In addition, evaluations on downstream tasks such as object detection and semantic segmentation show that SFSR leads to clear performance improvements, confirming its robustness and practical value for real-world multi-sensor remote sensing appli...&lt;/p&gt;</content:encoded></item><item><title>Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification</title><link>https://arxiv.org/abs/2601.18088v1</link><guid>http://arxiv.org/abs/2601.18088v1</guid><pubDate>Mon, 26 Jan 2026 02:52:35 +0000</pubDate><dc:creator>Jianshu Chao</dc:creator><dc:creator>Tianhua Lv</dc:creator><dc:creator>Qiqiong Ma</dc:creator><dc:creator>Yunfei Qiu</dc:creator><dc:creator>Li Fang</dc:creator><dc:creator>Huifang Shen</dc:creator><dc:creator>Wei Yao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.
Published: 2026-01-26T02:52:35+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianshu Chao; Tianhua Lv; Qiqiong Ma; Yunfei Qiu; Li Fang; Huifang Shen; Wei Yao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model&amp;#x27;s capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method&amp;#x27;s effectiveness under resource-constrained conditions.&lt;/p&gt;</content:encoded></item><item><title>Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion</title><link>https://arxiv.org/abs/2601.22045v1</link><guid>http://arxiv.org/abs/2601.22045v1</guid><pubDate>Thu, 29 Jan 2026 17:47:07 +0000</pubDate><dc:creator>Da Li</dc:creator><dc:creator>Chen Yao</dc:creator><dc:creator>Tong Mao</dc:creator><dc:creator>Jiacheng Bao</dc:creator><dc:creator>Houjun Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.
Published: 2026-01-29T17:47:07+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Da Li; Chen Yao; Tong Mao; Jiacheng Bao; Houjun Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.&lt;/p&gt;</content:encoded></item><item><title>Instance-Guided Radar Depth Estimation for 3D Object Detection</title><link>https://arxiv.org/abs/2601.19314v1</link><guid>http://arxiv.org/abs/2601.19314v1</guid><pubDate>Tue, 27 Jan 2026 07:53:24 +0000</pubDate><dc:creator>Chen-Chou Lo</dc:creator><dc:creator>Patrick Vandewalle</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.
Published: 2026-01-27T07:53:24+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen-Chou Lo; Patrick Vandewalle&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.&lt;/p&gt;</content:encoded></item><item><title>AutoRoadSAM: Multimodal Remote Sensing Road Extraction with Structure-Semantic Awareness via Auto-Prompting Vision Foundation Models</title><link>https://doi.org/10.1109/tgrs.2026.3658664</link><guid>10.1109/tgrs.2026.3658664</guid><pubDate>Wed, 28 Jan 2026 20:59:24 +0000</pubDate><dc:creator>Jiayuan Li</dc:creator><dc:creator>Zhen Wang</dc:creator><dc:creator>Xiao Sun</dc:creator><dc:creator>Zhiyong Lv</dc:creator><dc:creator>Nan Xu</dc:creator><dc:creator>Zhuhong You</dc:creator><dc:creator>Deshuang Huang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3658664</prism:doi><description>The integration of multimodal data holds great promise for advancing road extraction in remote sensing. However, existing approaches are limited by the lack of unified end-to-end frameworks for diverse modality combinations, suboptimal multimodal feature fusion, and challenges in capturing the slender, winding, and complex topological structures of roads. In this paper, we propose AutoRoadSAM, a novel end-to-end framework for multimodal road extraction that fully exploits the powerful visual representation capabilities of the Segment Anything Model (SAM) and, for the first time, introduces an Auto-Prompting Mechanism via a Dynamic Snake Convolution-based Decoder. This decoder adaptively generates task-specific prompts by capturing fine-grained local geometric features from auxiliary modality branches, enabling precise alignment with complex road structures. To further enhance multimodal feature fusion and topological perception, we design the Cross-Modal Information Interaction (CMII) module, which facilitates global context modeling and cross-modal interaction, while strengthening the representation of intricate road topology through multidirectional snake scanning. Moreover, we incorporate a Mask Decoder with Cross Polarity-aware Linear Attention to boost decoding efficiency and effectively address pixel imbalance. Together, these innovations enable AutoRoadSAM to achieve superior structure- and semantic-aware road extraction across diverse modality combinations. Extensive experiments on six public datasets and four modality combinations demonstrate that AutoRoadSAM consistently outperforms state-of-the-art methods, validating the effectiveness and generalization capability of each proposed component. The code is available at https: //github.com/NWPUFranklee/AutoRoadSAM.git.
Published: 2026-01-28T20:59:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayuan Li; Zhen Wang; Xiao Sun; Zhiyong Lv; Nan Xu; Zhuhong You; Deshuang Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3658664"&gt;10.1109/tgrs.2026.3658664&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;The integration of multimodal data holds great promise for advancing road extraction in remote sensing. However, existing approaches are limited by the lack of unified end-to-end frameworks for diverse modality combinations, suboptimal multimodal feature fusion, and challenges in capturing the slender, winding, and complex topological structures of roads. In this paper, we propose AutoRoadSAM, a novel end-to-end framework for multimodal road extraction that fully exploits the powerful visual representation capabilities of the Segment Anything Model (SAM) and, for the first time, introduces an Auto-Prompting Mechanism via a Dynamic Snake Convolution-based Decoder. This decoder adaptively generates task-specific prompts by capturing fine-grained local geometric features from auxiliary modality branches, enabling precise alignment with complex road structures. To further enhance multimodal feature fusion and topological perception, we design the Cross-Modal Information Interaction (CMII) module, which facilitates global context modeling and cross-modal interaction, while strengthening the representation of intricate road topology through multidirectional snake scanning. Moreover, we incorporate a Mask Decoder with Cross Polarity-aware Linear Attention to boost decoding efficiency and effectively address pixel imbalance. Together, these innovations enable AutoRoadSAM to achieve superior structure- and semantic-aware road extraction across diverse modality combinations. Extensive experiments on six public datasets and four modality combinations demonstrate that AutoRoadSAM consistently outperforms state-of-the-art methods, validating the effectiveness and generalization capability of each proposed component. The code is available at https: //github.com/NWPUFranklee/AutoRoadSAM.git.&lt;/p&gt;</content:encoded></item><item><title>SONIC: Spectral Oriented Neural Invariant Convolutions</title><link>https://arxiv.org/abs/2601.19884v1</link><guid>http://arxiv.org/abs/2601.19884v1</guid><pubDate>Tue, 27 Jan 2026 18:51:11 +0000</pubDate><dc:creator>Gijs Joppe Moens</dc:creator><dc:creator>Regina Beets-Tan</dc:creator><dc:creator>Eduardo H. P. Pooch</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.
Published: 2026-01-27T18:51:11+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gijs Joppe Moens; Regina Beets-Tan; Eduardo H. P. Pooch&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.&lt;/p&gt;</content:encoded></item><item><title>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</title><link>https://arxiv.org/abs/2601.22054v1</link><guid>http://arxiv.org/abs/2601.22054v1</guid><pubDate>Thu, 29 Jan 2026 17:52:41 +0000</pubDate><dc:creator>Baorui Ma</dc:creator><dc:creator>Jiahui Yang</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Xuancheng Zhang</dc:creator><dc:creator>Jianxun Cui</dc:creator><dc:creator>Hao Li</dc:creator><dc:creator>Yan Xie</dc:creator><dc:creator>Wei Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.
Published: 2026-01-29T17:52:41+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baorui Ma; Jiahui Yang; Donglin Di; Xuancheng Zhang; Jianxun Cui; Hao Li; Yan Xie; Wei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.&lt;/p&gt;</content:encoded></item></channel></rss>