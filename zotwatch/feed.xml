<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Mon, 29 Dec 2025 02:47:40 +0000</lastBuildDate><item><title>GCEPANet: A Lightweight and Efficient Remote Sensing Image Cloud Removal Network Model for Optical-SAR Image Fusion</title><link>https://doi.org/10.1016/j.inffus.2025.104090</link><guid>10.1016/j.inffus.2025.104090</guid><pubDate>Sat, 27 Dec 2025 16:17:57 +0000</pubDate><dc:creator>Qinglong Zhou</dc:creator><dc:creator>Xing Wang</dc:creator><dc:creator>Jiahao Fang</dc:creator><dc:creator>Wenbo Wu</dc:creator><dc:creator>Bingxian Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104090</prism:doi><description>To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.
Published: 2025-12-27T16:17:57+00:00
Venue: Information Fusion
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qinglong Zhou; Xing Wang; Jiahao Fang; Wenbo Wu; Bingxian Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104090"&gt;10.1016/j.inffus.2025.104090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.&lt;/p&gt;</content:encoded></item><item><title>Deep learning-based astronomical multimodal data fusion: A comprehensive review</title><link>https://doi.org/10.1016/j.inffus.2025.104103</link><guid>10.1016/j.inffus.2025.104103</guid><pubDate>Sat, 27 Dec 2025 16:17:58 +0000</pubDate><dc:creator>Wujun Shao</dc:creator><dc:creator>Dongwei Fan</dc:creator><dc:creator>Chenzhou Cui</dc:creator><dc:creator>Yunfei Xu</dc:creator><dc:creator>Shirui Wei</dc:creator><dc:creator>Xin Lyu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104103</prism:doi><description>With the rapid advancements in observational technologies and the widespread implementation of large-scale sky surveys, diverse electromagnetic wave data (e.g., optical and infrared) and non-electromagnetic wave data (e.g., gravitational waves) have become increasingly accessible. Astronomy has thus entered an unprecedented era of data abundance and complexity. Astronomers have long relied on unimodal data analysis to perceive the universe, but these efforts often provide only limited insights when confronted with the current massive and heterogeneous astronomical data. In this context, multimodal data fusion (MDF), as an emerging method, provides new opportunities to enhance the value of astronomical data and deepening the understanding of the universe by integrating information from different modalities. Recent progress in artificial intelligence (AI), particularly in deep learning (DL), has greatly accelerated the development of multimodal research in astronomy. Therefore, a timely review of this field is essential. This paper begins by discussing the motivation and necessity of astronomical MDF, followed by an overview of astronomical data sources and major data modalities. It then introduces representative DL models commonly used in astronomical multimodal studies, the general fusion process as well as various fusion strategies, emphasizing their characteristics, applicability, advantages, and limitations. Subsequently, the paper surveys existing astronomical multimodal studies and datasets. Finally, the discussion section synthesizes key findings, identifies potential challenges, and suggests promising directions for future research. By offering a structured overview and critical analysis, this review aims to inspire and guide researchers engaged in DL-based MDF in astronomy.
Published: 2025-12-27T16:17:58+00:00
Venue: Information Fusion
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wujun Shao; Dongwei Fan; Chenzhou Cui; Yunfei Xu; Shirui Wei; Xin Lyu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104103"&gt;10.1016/j.inffus.2025.104103&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancements in observational technologies and the widespread implementation of large-scale sky surveys, diverse electromagnetic wave data (e.g., optical and infrared) and non-electromagnetic wave data (e.g., gravitational waves) have become increasingly accessible. Astronomy has thus entered an unprecedented era of data abundance and complexity. Astronomers have long relied on unimodal data analysis to perceive the universe, but these efforts often provide only limited insights when confronted with the current massive and heterogeneous astronomical data. In this context, multimodal data fusion (MDF), as an emerging method, provides new opportunities to enhance the value of astronomical data and deepening the understanding of the universe by integrating information from different modalities. Recent progress in artificial intelligence (AI), particularly in deep learning (DL), has greatly accelerated the development of multimodal research in astronomy. Therefore, a timely review of this field is essential. This paper begins by discussing the motivation and necessity of astronomical MDF, followed by an overview of astronomical data sources and major data modalities. It then introduces representative DL models commonly used in astronomical multimodal studies, the general fusion process as well as various fusion strategies, emphasizing their characteristics, applicability, advantages, and limitations. Subsequently, the paper surveys existing astronomical multimodal studies and datasets. Finally, the discussion section synthesizes key findings, identifies potential challenges, and suggests promising directions for future research. By offering a structured overview and critical analysis, this review aims to inspire and guide researchers engaged in DL-based MDF in astronomy.&lt;/p&gt;</content:encoded></item><item><title>Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models</title><link>https://arxiv.org/abs/2512.21593v1</link><guid>http://arxiv.org/abs/2512.21593v1</guid><pubDate>Thu, 25 Dec 2025 09:19:10 +0000</pubDate><dc:creator>Takuro Kutsuna</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion models have become a central tool in deep generative modeling, but standard formulations rely on a single network and a single diffusion schedule to transform a simple prior, typically a standard normal distribution, into the target data distribution. As a result, the model must simultaneously represent the global structure of the distribution and its fine-scale local variations, which becomes difficult when these scales are strongly mismatched. This issue arises both in natural images, where coarse manifold-level structure and fine textures coexist, and in low-dimensional distributions with highly concentrated local structure. To address this issue, we propose Residual Prior Diffusion (RPD), a two-stage framework in which a coarse prior model first captures the large-scale structure of the data distribution, and a diffusion model is then trained to represent the residual between the prior and the target data distribution. We formulate RPD as an explicit probabilistic model with a tractable evidence lower bound, whose optimization reduces to the familiar objectives of noise prediction or velocity prediction. We further introduce auxiliary variables that leverage information from the prior model and theoretically analyze how they reduce the difficulty of the prediction problem in RPD. Experiments on synthetic datasets with fine-grained local structure show that standard diffusion models fail to capture local details, whereas RPD accurately captures fine-scale detail while preserving the large-scale structure of the distribution. On natural image generation tasks, RPD achieved generation quality that matched or exceeded that of representative diffusion-based baselines and it maintained strong performance even with a small number of inference steps.
Published: 2025-12-25T09:19:10+00:00
Venue: arXiv
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Takuro Kutsuna&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models have become a central tool in deep generative modeling, but standard formulations rely on a single network and a single diffusion schedule to transform a simple prior, typically a standard normal distribution, into the target data distribution. As a result, the model must simultaneously represent the global structure of the distribution and its fine-scale local variations, which becomes difficult when these scales are strongly mismatched. This issue arises both in natural images, where coarse manifold-level structure and fine textures coexist, and in low-dimensional distributions with highly concentrated local structure. To address this issue, we propose Residual Prior Diffusion (RPD), a two-stage framework in which a coarse prior model first captures the large-scale structure of the data distribution, and a diffusion model is then trained to represent the residual between the prior and the target data distribution. We formulate RPD as an explicit probabilistic model with a tractable evidence lower bound, whose optimization reduces to the familiar objectives of noise prediction or velocity prediction. We further introduce auxiliary variables that leverage information from the prior model and theoretically analyze how they reduce the difficulty of the prediction problem in RPD. Experiments on synthetic datasets with fine-grained local structure show that standard diffusion models fail to capture local details, whereas RPD accurately captures fine-scale detail while preserving the large-scale structure of the distribution. On natural image generation tasks, RPD achieved generation quality that matched or exceeded that of representative diffusion-based baselines and it maintained strong performance even with a small number of inference steps.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical cross-module knowledge transfer based on structural multi-view least squares support vector classification</title><link>https://doi.org/10.1016/j.inffus.2025.104099</link><guid>10.1016/j.inffus.2025.104099</guid><pubDate>Sat, 27 Dec 2025 16:17:55 +0000</pubDate><dc:creator>Siyuan Zhang</dc:creator><dc:creator>Shuangrui Jia</dc:creator><dc:creator>Jianying Feng</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104099</prism:doi><description>Multi-view learning has garnered significant attention in machine learning due to its ability to leverage complementary information from diverse data sources. However, multi-view least squares support vector machines (MvLSSVMs) suffer from two critical limitations. Firstly, their reliance on pairwise view comparisons hinders their ability to capture complex inter-view relationships. Secondly, the high computational costs associated with hyperparameter tuning impede their scalability. To address these challenges, this paper proposes hierarchical transfer-based structural multi-view least squares support vector classification (HT-SMLSSVC). Inspired by the previous work of the multi-view structural large margin classifier (MvSLMC), the proposed HT-SMLSSVC achieves complementarity and consensus principles in each layer through a weighting strategy and clustering, which is used to form structural regularization. This term can enhance within-class cohesion and between-class separability within each view. At the same time, different views provide complementary structural information to one another, thereby enriching classifier diversity and further avoiding reliance on pairwise view-comparison strategies. The difference lies in the adoption of least squares loss in each layer of the model, whereby the solution for the hyperplane is a set of linear equations rather than a standard quadratic programming problem. In addition, hierarchical knowledge transfer is achieved through a deep stacked architecture, which propagates cross-layer predictions to enhance generalization ability. At the same time, efficient learning is achieved through randomized hyperparameter assignment and adaptive validation, eliminating the need for manual tuning and thereby significantly reducing model training time. Extensive experiments on 17 UCI and 45 AWA datasets demonstrate that HT-SMLSSVC outperforms state-of-the-art methods in both computational efficiency and classification accuracy, offering a scalable solution for real-world multi-view tasks.
Published: 2025-12-27T16:17:55+00:00
Venue: Information Fusion
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siyuan Zhang; Shuangrui Jia; Jianying Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104099"&gt;10.1016/j.inffus.2025.104099&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-view learning has garnered significant attention in machine learning due to its ability to leverage complementary information from diverse data sources. However, multi-view least squares support vector machines (MvLSSVMs) suffer from two critical limitations. Firstly, their reliance on pairwise view comparisons hinders their ability to capture complex inter-view relationships. Secondly, the high computational costs associated with hyperparameter tuning impede their scalability. To address these challenges, this paper proposes hierarchical transfer-based structural multi-view least squares support vector classification (HT-SMLSSVC). Inspired by the previous work of the multi-view structural large margin classifier (MvSLMC), the proposed HT-SMLSSVC achieves complementarity and consensus principles in each layer through a weighting strategy and clustering, which is used to form structural regularization. This term can enhance within-class cohesion and between-class separability within each view. At the same time, different views provide complementary structural information to one another, thereby enriching classifier diversity and further avoiding reliance on pairwise view-comparison strategies. The difference lies in the adoption of least squares loss in each layer of the model, whereby the solution for the hyperplane is a set of linear equations rather than a standard quadratic programming problem. In addition, hierarchical knowledge transfer is achieved through a deep stacked architecture, which propagates cross-layer predictions to enhance generalization ability. At the same time, efficient learning is achieved through randomized hyperparameter assignment and adaptive validation, eliminating the need for manual tuning and thereby significantly reducing model training time. Extensive experiments on 17 UCI and 45 AWA datasets demonstrate that HT-SMLSSVC outperforms state-of-the-art methods in both computational efficiency and classification accuracy, offering a scalable solution for real-world multi-view tasks.&lt;/p&gt;</content:encoded></item><item><title>PMM3D: a transformer-based monocular 3D detector with parallel multi-time inquiry and mixup enhancement</title><link>https://doi.org/10.1016/j.eswa.2025.131014</link><guid>10.1016/j.eswa.2025.131014</guid><pubDate>Sat, 27 Dec 2025 16:14:50 +0000</pubDate><dc:creator>Chao Lin</dc:creator><dc:creator>Tongzhou Zhang</dc:creator><dc:creator>Wei Zhou</dc:creator><dc:creator>Yiou Wang</dc:creator><dc:creator>Wei Zhang</dc:creator><dc:creator>Gang Wang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131014</prism:doi><description>Monocular 3D object detection, which aims to infer 3D geometric properties and spatial locations from a single image, is critical for applications such as autonomous driving. However, the inherent depth ambiguity in projecting 3D space from 2D images makes this task particularly challenging. Existing methods often suffer from insufficient interaction between encoded features and object queries during decoding, limiting their ability to model complex 3D relationships. To address these issues, this paper proposes Parallel Multi-time Inquiry and Mixup-enhanced Monocular 3D Detector (PMM3D), a novel framework that enhances feature interaction and data diversity. The core of our method is a Parallel Multi-time Inquiries (PMI) mechanism integrated into the decoder, which allows object queries to interact multiple times in parallel with both visual and depth-aware features within a single decoding layer. This design significantly improves the modeling capacity for 3D structures. In addition, we introduce a conditionally constrained data augmentation strategy, MixDA3D, which synthesizes diverse training samples while maintaining geometric plausibility, thereby improving generalization. Extensive experiments on the KITTI benchmark demonstrate the effectiveness of PMM3D. It achieves competitive performance, especially in moderate and hard scenarios. Ablation studies confirm the complementary contributions of the PMI mechanism and MixDA3D. Moreover, qualitative visualizations reveal the adaptive behavior of the inquiry heads in different scenarios.
Published: 2025-12-27T16:14:50+00:00
Venue: Expert Systems with Applications
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Lin; Tongzhou Zhang; Wei Zhou; Yiou Wang; Wei Zhang; Gang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131014"&gt;10.1016/j.eswa.2025.131014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection, which aims to infer 3D geometric properties and spatial locations from a single image, is critical for applications such as autonomous driving. However, the inherent depth ambiguity in projecting 3D space from 2D images makes this task particularly challenging. Existing methods often suffer from insufficient interaction between encoded features and object queries during decoding, limiting their ability to model complex 3D relationships. To address these issues, this paper proposes Parallel Multi-time Inquiry and Mixup-enhanced Monocular 3D Detector (PMM3D), a novel framework that enhances feature interaction and data diversity. The core of our method is a Parallel Multi-time Inquiries (PMI) mechanism integrated into the decoder, which allows object queries to interact multiple times in parallel with both visual and depth-aware features within a single decoding layer. This design significantly improves the modeling capacity for 3D structures. In addition, we introduce a conditionally constrained data augmentation strategy, MixDA3D, which synthesizes diverse training samples while maintaining geometric plausibility, thereby improving generalization. Extensive experiments on the KITTI benchmark demonstrate the effectiveness of PMM3D. It achieves competitive performance, especially in moderate and hard scenarios. Ablation studies confirm the complementary contributions of the PMI mechanism and MixDA3D. Moreover, qualitative visualizations reveal the adaptive behavior of the inquiry heads in different scenarios.&lt;/p&gt;</content:encoded></item><item><title>Scoping Review of Multimodal Sentiment Analysis and Summarization: State of the Art, Challenges and Future Directions</title><link>https://doi.org/10.1016/j.inffus.2025.104082</link><guid>10.1016/j.inffus.2025.104082</guid><pubDate>Sat, 27 Dec 2025 07:23:47 +0000</pubDate><dc:creator>Magaly Lika Fujimoto</dc:creator><dc:creator>Ricardo Marcondes Marcacini</dc:creator><dc:creator>Solange Oliveira Rezende</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104082</prism:doi><description>In recent decades, advancements in computing power and the widespread availability of multimodal data have significantly redirected research, shifting the primary focus from text based approaches. This paper presents a scoping review focusing on approaches that jointly perform Multimodal Sentiment Analysis and Multimodal Summarization within the same framework. Beyond this, the review comprehensively surveys each domain individually, highlighting state-of-the-art techniques, key methodologies, and commonly used datasets. It also provides key insights into current challenges and proposes future research directions.
Published: 2025-12-27T07:23:47+00:00
Venue: Information Fusion
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Magaly Lika Fujimoto; Ricardo Marcondes Marcacini; Solange Oliveira Rezende&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104082"&gt;10.1016/j.inffus.2025.104082&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;In recent decades, advancements in computing power and the widespread availability of multimodal data have significantly redirected research, shifting the primary focus from text based approaches. This paper presents a scoping review focusing on approaches that jointly perform Multimodal Sentiment Analysis and Multimodal Summarization within the same framework. Beyond this, the review comprehensively surveys each domain individually, highlighting state-of-the-art techniques, key methodologies, and commonly used datasets. It also provides key insights into current challenges and proposes future research directions.&lt;/p&gt;</content:encoded></item><item><title>Synthetic learning for primitive-based building model reconstruction from point clouds</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.012</link><guid>10.1016/j.isprsjprs.2025.12.012</guid><pubDate>Sat, 27 Dec 2025 19:51:14 +0000</pubDate><dc:creator>Zhixin Li</dc:creator><dc:creator>Jie Shan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.012</prism:doi><description>The rapid advancement of digital 3D environments has significantly increased the demand for geometrically accurate and semantically rich parametric building models. However, existing primitive- or model-based building reconstruction approaches often struggle with limited availability of labeled datasets and insufficient reconstruction accuracy. To address these challenges, we propose a novel learning-based method for building reconstruction from point clouds that leverages roof primitives and relies exclusively on synthetic data for supervision. Our approach begins with the generation of a large synthetic dataset comprising 100,000 buildings of varying scales based on a predefined library of 10 roof primitive classes. The synthetic point clouds are created by randomly sampling not only the interiors but also the edges and corners of the roof primitives. Two lightweight transformer-based neural networks are then trained to classify roof primitive classes and estimate their corresponding parameters. Compared to conventional learning-free fitting methods, our learning-based approach achieves higher parameter estimation accuracy and greater robustness when applied to six real-world point cloud datasets collected from drone, airborne, and spaceborne platforms. Notably, the synthetic learning approach reduces primitive parameter estimation errors from approximately 50% to 6% of the point ground spacing — demonstrating a distinctive advantage when trained effectively on synthetic data. Future work may explore generating synthetic data for irregular, complex buildings, expanding the library with additional roof primitive classes, and applying the proposed training strategy to such synthetic datasets.
Published: 2025-12-27T19:51:14+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhixin Li; Jie Shan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.012"&gt;10.1016/j.isprsjprs.2025.12.012&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of digital 3D environments has significantly increased the demand for geometrically accurate and semantically rich parametric building models. However, existing primitive- or model-based building reconstruction approaches often struggle with limited availability of labeled datasets and insufficient reconstruction accuracy. To address these challenges, we propose a novel learning-based method for building reconstruction from point clouds that leverages roof primitives and relies exclusively on synthetic data for supervision. Our approach begins with the generation of a large synthetic dataset comprising 100,000 buildings of varying scales based on a predefined library of 10 roof primitive classes. The synthetic point clouds are created by randomly sampling not only the interiors but also the edges and corners of the roof primitives. Two lightweight transformer-based neural networks are then trained to classify roof primitive classes and estimate their corresponding parameters. Compared to conventional learning-free fitting methods, our learning-based approach achieves higher parameter estimation accuracy and greater robustness when applied to six real-world point cloud datasets collected from drone, airborne, and spaceborne platforms. Notably, the synthetic learning approach reduces primitive parameter estimation errors from approximately 50% to 6% of the point ground spacing — demonstrating a distinctive advantage when trained effectively on synthetic data. Future work may explore generating synthetic data for irregular, complex buildings, expanding the library with additional roof primitive classes, and applying the proposed training strategy to such synthetic datasets.&lt;/p&gt;</content:encoded></item><item><title>ICSD-YOLO: Intelligent Detection for Real-time Industrial Field Safety</title><link>https://doi.org/10.1016/j.eswa.2025.130994</link><guid>10.1016/j.eswa.2025.130994</guid><pubDate>Sat, 27 Dec 2025 00:07:00 +0000</pubDate><dc:creator>Cheng Shi</dc:creator><dc:creator>Yan Chen</dc:creator><dc:creator>Chong Zhang</dc:creator><dc:creator>Dong-Guo Chang</dc:creator><dc:creator>Yi-Jia Chen</dc:creator><dc:creator>Qian Wang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130994</prism:doi><description>Comprehensive and accurate object detection in industrial environments is crucial for ensuring operational safety. However, existing real-time detectors based on ConvNet such as YOLOv series face constraints in early-stage feature extraction capability and lack effective perception mechanisms to deal with occlusion, deformation, and scale variation. Transformer-based detectors strengthen global context modeling through self-attention and achieve better performance on complex benchmarks. However, their high computational cost and large model size limit their applicability, particularly in resource-constrained industrial environments. To address these issues, we propose a family of object detectors, named ICSD-YOLO (Information ConvStem FocusBlock Detector) , a lightweight detection framework that enhances features encode and hierarchical perception. We design a ConvStemBlock to improve low-level feature extraction and enlarge the receptive field, and a FocusBlock to perform multi-level semantic refinement.We implement ICSD-YOLO based on YOLO Series, and evaluate it across five model scales (Nano, Small, Medium, Large, Extra-Large) on the COCO benchmark and a industrial field dataset. Experimental results show that the mAP 50: 95 of our ICSD-YOLO-X rises from 66.9% to 68.1% (+1.2%), and the F1-score increases from 80.8% to 83.0% (+2.2%) compared to the original YOLOv12-X while reducing FLOPs by 41.3% (from 199G to 116.8G), demonstrating better perception under complex conditions and suitability for deployment in safety-critical scenarios.The code is available at https://github.com/PrintSC/code
Published: 2025-12-27T00:07:00+00:00
Venue: Expert Systems with Applications
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheng Shi; Yan Chen; Chong Zhang; Dong-Guo Chang; Yi-Jia Chen; Qian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130994"&gt;10.1016/j.eswa.2025.130994&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Comprehensive and accurate object detection in industrial environments is crucial for ensuring operational safety. However, existing real-time detectors based on ConvNet such as YOLOv series face constraints in early-stage feature extraction capability and lack effective perception mechanisms to deal with occlusion, deformation, and scale variation. Transformer-based detectors strengthen global context modeling through self-attention and achieve better performance on complex benchmarks. However, their high computational cost and large model size limit their applicability, particularly in resource-constrained industrial environments. To address these issues, we propose a family of object detectors, named ICSD-YOLO (Information ConvStem FocusBlock Detector) , a lightweight detection framework that enhances features encode and hierarchical perception. We design a ConvStemBlock to improve low-level feature extraction and enlarge the receptive field, and a FocusBlock to perform multi-level semantic refinement.We implement ICSD-YOLO based on YOLO Series, and evaluate it across five model scales (Nano, Small, Medium, Large, Extra-Large) on the COCO benchmark and a industrial field dataset. Experimental results show that the mAP 50: 95 of our ICSD-YOLO-X rises from 66.9% to 68.1% (+1.2%), and the F1-score increases from 80.8% to 83.0% (+2.2%) compared to the original YOLOv12-X while reducing FLOPs by 41.3% (from 199G to 116.8G), demonstrating better perception under complex conditions and suitability for deployment in safety-critical scenarios.The code is available at https://github.com/PrintSC/code&lt;/p&gt;</content:encoded></item><item><title>Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model</title><link>https://arxiv.org/abs/2512.21540v1</link><guid>http://arxiv.org/abs/2512.21540v1</guid><pubDate>Thu, 25 Dec 2025 07:16:26 +0000</pubDate><dc:creator>Yanhao Li</dc:creator><dc:creator>Lu Ma</dc:creator><dc:creator>Jiaran Zhang</dc:creator><dc:creator>Lexiang Tang</dc:creator><dc:creator>Wentao Zhang</dc:creator><dc:creator>Guibo Luo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.
Published: 2025-12-25T07:16:26+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanhao Li; Lu Ma; Jiaran Zhang; Lexiang Tang; Wentao Zhang; Guibo Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.&lt;/p&gt;</content:encoded></item><item><title>All You Need Is Two Domains: Unified RGB-Wavelet Transformer for Visual Representation Learning</title><link>https://doi.org/10.1016/j.knosys.2025.115239</link><guid>10.1016/j.knosys.2025.115239</guid><pubDate>Sun, 28 Dec 2025 07:00:46 +0000</pubDate><dc:creator>Yu Fu</dc:creator><dc:creator>Weichao Yi</dc:creator><dc:creator>Liquan Dong</dc:creator><dc:creator>Ming Liu</dc:creator><dc:creator>Lingqin Kong</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115239</prism:doi><description>Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .
Published: 2025-12-28T07:00:46+00:00
Venue: Knowledge-Based Systems
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Fu; Weichao Yi; Liquan Dong; Ming Liu; Lingqin Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115239"&gt;10.1016/j.knosys.2025.115239&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .&lt;/p&gt;</content:encoded></item><item><title>DAE-YOLO: Remote Sensing Small Object Detection Method Integrating YOLO and State Space Models</title><link>https://doi.org/10.3390/rs18010109</link><guid>10.3390/rs18010109</guid><pubDate>Sun, 28 Dec 2025 23:54:36 +0000</pubDate><dc:creator>Bing Li</dc:creator><dc:creator>Yongtao Kang</dc:creator><dc:creator>Yao Ding</dc:creator><dc:creator>Shaopeng Li</dc:creator><dc:creator>Zhili Zhang</dc:creator><dc:creator>Decao Ma</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010109</prism:doi><description>Small object detection in remote sensing images provides significant value for urban monitoring, aerospace reconnaissance, and other fields. However, detection accuracy still faces multiple challenges including limited target information, weak feature representation, and complex backgrounds. This research aims to improve the performance of the YOLO11 model for small object detection in remote sensing imagery by addressing key issues in long-distance spatial dependency modeling, multi-scale feature adaptive fusion, and computational efficiency. We constructed a specialized Remote Sensing Airport-Plane Detection (RS-APD) dataset and used the public VisDrone2019 dataset for generalization verification. Based on the YOLO11 architecture, we proposed the DAE-YOLO model with three innovative modules: Dynamic Spatial Sequence Module (DSSM) for enhanced long-distance spatial dependency capture; Adaptive Multi-scale Feature Enhancement (AMFE) for multi-scale feature adaptive receptive field adjustment; and Efficient Dual-level Attention Mechanism (EDAM) to reduce computational complexity while maintaining feature expression capability. Experimental results demonstrate that compared to the baseline YOLO11, our proposed model improved mAP50 and mAP50:95 on the RS-APD dataset by 2.1% and 2.5%, respectively, with APs increasing by 2.8%. This research provides an efficient and reliable small object detection solution for remote sensing applications.
Published: 2025-12-28T23:54:36+00:00
Venue: Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bing Li; Yongtao Kang; Yao Ding; Shaopeng Li; Zhili Zhang; Decao Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010109"&gt;10.3390/rs18010109&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Small object detection in remote sensing images provides significant value for urban monitoring, aerospace reconnaissance, and other fields. However, detection accuracy still faces multiple challenges including limited target information, weak feature representation, and complex backgrounds. This research aims to improve the performance of the YOLO11 model for small object detection in remote sensing imagery by addressing key issues in long-distance spatial dependency modeling, multi-scale feature adaptive fusion, and computational efficiency. We constructed a specialized Remote Sensing Airport-Plane Detection (RS-APD) dataset and used the public VisDrone2019 dataset for generalization verification. Based on the YOLO11 architecture, we proposed the DAE-YOLO model with three innovative modules: Dynamic Spatial Sequence Module (DSSM) for enhanced long-distance spatial dependency capture; Adaptive Multi-scale Feature Enhancement (AMFE) for multi-scale feature adaptive receptive field adjustment; and Efficient Dual-level Attention Mechanism (EDAM) to reduce computational complexity while maintaining feature expression capability. Experimental results demonstrate that compared to the baseline YOLO11, our proposed model improved mAP50 and mAP50:95 on the RS-APD dataset by 2.1% and 2.5%, respectively, with APs increasing by 2.8%. This research provides an efficient and reliable small object detection solution for remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</title><link>https://arxiv.org/abs/2512.22120v1</link><guid>http://arxiv.org/abs/2512.22120v1</guid><pubDate>Fri, 26 Dec 2025 18:59:47 +0000</pubDate><dc:creator>Shuoshuo Zhang</dc:creator><dc:creator>Yizhen Zhang</dc:creator><dc:creator>Jingjing Fu</dc:creator><dc:creator>Lei Song</dc:creator><dc:creator>Jiang Bian</dc:creator><dc:creator>Yujiu Yang</dc:creator><dc:creator>Rui Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.
Published: 2025-12-26T18:59:47+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuoshuo Zhang; Yizhen Zhang; Jingjing Fu; Lei Song; Jiang Bian; Yujiu Yang; Rui Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.&lt;/p&gt;</content:encoded></item><item><title>SAR and Visible Image Fusion via Retinex-Guided SAR Reconstruction</title><link>https://doi.org/10.3390/rs18010111</link><guid>10.3390/rs18010111</guid><pubDate>Sun, 28 Dec 2025 23:54:36 +0000</pubDate><dc:creator>Yuman Yuan</dc:creator><dc:creator>Tianyu Deng</dc:creator><dc:creator>Yi Le</dc:creator><dc:creator>Hongyang Bai</dc:creator><dc:creator>Shuai Guo</dc:creator><dc:creator>Shangjing Sun</dc:creator><dc:creator>Yuanbo Chen</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010111</prism:doi><description>The fusion of synthetic aperture radar (SAR) and visible images offers complementary spatial and spectral information, enabling more reliable and comprehensive scene interpretation. However, SAR speckle noise and the intrinsic modality gap pose significant challenges for existing methods in extracting consistent and complementary features. To address these issues, we propose VGSRF-Net, a Retinex-guided SAR reconstruction-driven fusion network that leverages visible-image priors to refine SAR features. This approach effectively reduces modality discrepancies before fusion, enabling improved multi-modal representation. The cross-modality reconstruction module (CMRM) reconstructs SAR features guided by visible priors, effectively reducing modality discrepancies before fusion and enabling improved multi-modal representation. The multi-modal feature joint representation module (MFJRM) enhances cross-modal complementarity by integrating global contextual interactions and local dynamic convolution, thereby achieving further feature alignment. Finally, the feature enhancement module (FEM) refines multi-scale spatial features and selectively enhances high-frequency details in the frequency domain, improving structural clarity and texture fidelity. Extensive experiments on diverse real-world remote sensing datasets demonstrate that VGSRF-Net surpasses state-of-the-art methods in denoising, structural preservation, and generalization under varying noise and illumination conditions.
Published: 2025-12-28T23:54:36+00:00
Venue: Remote Sensing
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuman Yuan; Tianyu Deng; Yi Le; Hongyang Bai; Shuai Guo; Shangjing Sun; Yuanbo Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010111"&gt;10.3390/rs18010111&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;The fusion of synthetic aperture radar (SAR) and visible images offers complementary spatial and spectral information, enabling more reliable and comprehensive scene interpretation. However, SAR speckle noise and the intrinsic modality gap pose significant challenges for existing methods in extracting consistent and complementary features. To address these issues, we propose VGSRF-Net, a Retinex-guided SAR reconstruction-driven fusion network that leverages visible-image priors to refine SAR features. This approach effectively reduces modality discrepancies before fusion, enabling improved multi-modal representation. The cross-modality reconstruction module (CMRM) reconstructs SAR features guided by visible priors, effectively reducing modality discrepancies before fusion and enabling improved multi-modal representation. The multi-modal feature joint representation module (MFJRM) enhances cross-modal complementarity by integrating global contextual interactions and local dynamic convolution, thereby achieving further feature alignment. Finally, the feature enhancement module (FEM) refines multi-scale spatial features and selectively enhances high-frequency details in the frequency domain, improving structural clarity and texture fidelity. Extensive experiments on diverse real-world remote sensing datasets demonstrate that VGSRF-Net surpasses state-of-the-art methods in denoising, structural preservation, and generalization under varying noise and illumination conditions.&lt;/p&gt;</content:encoded></item><item><title>Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models</title><link>https://arxiv.org/abs/2512.21651v1</link><guid>http://arxiv.org/abs/2512.21651v1</guid><pubDate>Thu, 25 Dec 2025 12:39:36 +0000</pubDate><dc:creator>Dung Anh Hoang</dc:creator><dc:creator>Cuong Pham</dc:creator><dc:creator>Cuong Nguyen</dc:creator><dc:creator>Trung le</dc:creator><dc:creator>Jianfei Cai</dc:creator><dc:creator>Thanh-Toan Do</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) deliver strong performance across a wide range of NLP tasks, but their massive sizes hinder deployment on resource-constrained devices. To reduce their computational and memory burden, various compression techniques have been proposed, including quantization, pruning, and knowledge distillation. Among these, post-training quantization (PTQ) is widely adopted for its efficiency, as it requires no retraining and only a small dataset for calibration, enabling low-cost deployment. Recent advances for post-training quantization have demonstrated that even sub-4-bit methods can maintain most of the original model performance. However, 1-bit quantization that converts floating-point weights to \(\pm\)1, remains particularly challenging, as existing 1-bit PTQ methods often suffer from significant performance degradation compared to the full-precision models. Specifically, most of existing 1-bit PTQ approaches focus on weight alignment, aligning the full-precision model weights with those of the quantized models, rather than directly aligning their outputs. Although the output-matching approach objective is more intuitive and aligns with the quantization goal, naively applying it in 1-bit LLMs often leads to notable performance degradation. In this paper, we investigate why and under what conditions output-matching fails, in the context of 1-bit LLM quantization. Based on our findings, we propose a novel data-aware PTQ approach for 1-bit LLMs that explicitly accounts for activation error accumulation while keeping optimization efficient. Empirical experiments demonstrate that our solution consistently outperforms existing 1-bit PTQ methods with minimal overhead.
Published: 2025-12-25T12:39:36+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dung Anh Hoang; Cuong Pham; Cuong Nguyen; Trung le; Jianfei Cai; Thanh-Toan Do&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) deliver strong performance across a wide range of NLP tasks, but their massive sizes hinder deployment on resource-constrained devices. To reduce their computational and memory burden, various compression techniques have been proposed, including quantization, pruning, and knowledge distillation. Among these, post-training quantization (PTQ) is widely adopted for its efficiency, as it requires no retraining and only a small dataset for calibration, enabling low-cost deployment. Recent advances for post-training quantization have demonstrated that even sub-4-bit methods can maintain most of the original model performance. However, 1-bit quantization that converts floating-point weights to \(\pm\)1, remains particularly challenging, as existing 1-bit PTQ methods often suffer from significant performance degradation compared to the full-precision models. Specifically, most of existing 1-bit PTQ approaches focus on weight alignment, aligning the full-precision model weights with those of the quantized models, rather than directly aligning their outputs. Although the output-matching approach objective is more intuitive and aligns with the quantization goal, naively applying it in 1-bit LLMs often leads to notable performance degradation. In this paper, we investigate why and under what conditions output-matching fails, in the context of 1-bit LLM quantization. Based on our findings, we propose a novel data-aware PTQ approach for 1-bit LLMs that explicitly accounts for activation error accumulation while keeping optimization efficient. Empirical experiments demonstrate that our solution consistently outperforms existing 1-bit PTQ methods with minimal overhead.&lt;/p&gt;</content:encoded></item><item><title>Contourlet Refinement Gate Framework for Thermal Spectrum Distribution Regularized Infrared Image Super-Resolution</title><link>https://doi.org/10.1007/s11263-025-02668-0</link><guid>10.1007/s11263-025-02668-0</guid><pubDate>Sat, 27 Dec 2025 07:49:37 +0000</pubDate><dc:creator>Yang Zou</dc:creator><dc:creator>Zhixin Chen</dc:creator><dc:creator>Zhipeng Zhang</dc:creator><dc:creator>Xingyuan Li</dc:creator><dc:creator>Long Ma</dc:creator><dc:creator>Jinyuan Liu</dc:creator><dc:creator>Peng Wang</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02668-0</prism:doi><description>Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at .
Published: 2025-12-27T07:49:37+00:00
Venue: International Journal of Computer Vision
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Zou; Zhixin Chen; Zhipeng Zhang; Xingyuan Li; Long Ma; Jinyuan Liu; Peng Wang; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02668-0"&gt;10.1007/s11263-025-02668-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at .&lt;/p&gt;</content:encoded></item><item><title>Shadow-DETR: Alleviating Matching Conflicts through Shadow Queries</title><link>https://doi.org/10.1016/j.neunet.2025.108524</link><guid>10.1016/j.neunet.2025.108524</guid><pubDate>Sat, 27 Dec 2025 22:56:36 +0000</pubDate><dc:creator>Yunfei Ma</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Lingfeng Yang</dc:creator><dc:creator>Yifei Su</dc:creator><dc:creator>Yingpeng Li</dc:creator><dc:creator>Wankou Yang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108524</prism:doi><description>Leveraging the end-to-end detection capability enabled by one-to-one matching, DETR has achieved state-of-the-art performance in simplified pipelines. However, the one-to-one matching mechanism also introduces certain limitations, such as slow convergence, which can be attributed to challenges like matching conflicts and limited supervision imposed by the matching process. This paper analyzes and identifies two forms of conflicts that arise from one-to-one matching: opposite optimization directions for similar samples and misalignment in query-object matching between different decoder layers. To mitigate the conflict while maintaining the end-to-end properties, we identify negative samples that closely resemble positive samples as shadow samples and ignore their classification loss during training. To address the issue of limited supervision, we compute the regression loss for these shadow samples, thereby providing additional localization supervision. By addressing these issues, our strategy enhances network training efficiency and improves overall performance under identical training configurations. Furthermore, we propose a loss-balancing strategy to enhance the effectiveness of shadow samples. Additionally, a feature-aware query initialization approach is proposed that offers the benefits of providing distinct features to shadow queries and strengthening the interaction between queries and image features. Experimental results demonstrate that our Shadow-DETR substantially boosts existing methods such as DAB-DETR, Deformable-DETR, and DINO while achieving comparable performance with SOTA methods.
Published: 2025-12-27T22:56:36+00:00
Venue: Neural Networks
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunfei Ma; Jie Li; Lingfeng Yang; Yifei Su; Yingpeng Li; Wankou Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108524"&gt;10.1016/j.neunet.2025.108524&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;Leveraging the end-to-end detection capability enabled by one-to-one matching, DETR has achieved state-of-the-art performance in simplified pipelines. However, the one-to-one matching mechanism also introduces certain limitations, such as slow convergence, which can be attributed to challenges like matching conflicts and limited supervision imposed by the matching process. This paper analyzes and identifies two forms of conflicts that arise from one-to-one matching: opposite optimization directions for similar samples and misalignment in query-object matching between different decoder layers. To mitigate the conflict while maintaining the end-to-end properties, we identify negative samples that closely resemble positive samples as shadow samples and ignore their classification loss during training. To address the issue of limited supervision, we compute the regression loss for these shadow samples, thereby providing additional localization supervision. By addressing these issues, our strategy enhances network training efficiency and improves overall performance under identical training configurations. Furthermore, we propose a loss-balancing strategy to enhance the effectiveness of shadow samples. Additionally, a feature-aware query initialization approach is proposed that offers the benefits of providing distinct features to shadow queries and strengthening the interaction between queries and image features. Experimental results demonstrate that our Shadow-DETR substantially boosts existing methods such as DAB-DETR, Deformable-DETR, and DINO while achieving comparable performance with SOTA methods.&lt;/p&gt;</content:encoded></item><item><title>UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</title><link>https://arxiv.org/abs/2512.21675v1</link><guid>http://arxiv.org/abs/2512.21675v1</guid><pubDate>Thu, 25 Dec 2025 13:35:52 +0000</pubDate><dc:creator>Shuo Cao</dc:creator><dc:creator>Jiayang Li</dc:creator><dc:creator>Xiaohui Li</dc:creator><dc:creator>Yuandong Pu</dc:creator><dc:creator>Kaiwen Zhu</dc:creator><dc:creator>Yuanting Gao</dc:creator><dc:creator>Siqi Luo</dc:creator><dc:creator>Yi Xin</dc:creator><dc:creator>Qi Qin</dc:creator><dc:creator>Yu Zhou</dc:creator><dc:creator>Xiangyu Chen</dc:creator><dc:creator>Wenlong Zhang</dc:creator><dc:creator>Bin Fu</dc:creator><dc:creator>Yu Qiao</dc:creator><dc:creator>Yihao Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.
Published: 2025-12-25T13:35:52+00:00
Venue: arXiv
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuo Cao; Jiayang Li; Xiaohui Li; Yuandong Pu; Kaiwen Zhu; Yuanting Gao; Siqi Luo; Yi Xin; Qi Qin; Yu Zhou; Xiangyu Chen; Wenlong Zhang; Bin Fu; Yu Qiao; Yihao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.&lt;/p&gt;</content:encoded></item><item><title>Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models</title><link>https://arxiv.org/abs/2512.21860v1</link><guid>http://arxiv.org/abs/2512.21860v1</guid><pubDate>Fri, 26 Dec 2025 04:51:23 +0000</pubDate><dc:creator>Masayuki Kawarada</dc:creator><dc:creator>Kosuke Yamada</dc:creator><dc:creator>Antonio Tejero-de-Pablos</dc:creator><dc:creator>Naoto Inoue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM's last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.
Published: 2025-12-26T04:51:23+00:00
Venue: arXiv
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Masayuki Kawarada; Kosuke Yamada; Antonio Tejero-de-Pablos; Naoto Inoue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM&amp;#x27;s last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.&lt;/p&gt;</content:encoded></item><item><title>Towards Long-window Anchoring in Vision-Language Model Distillation</title><link>https://arxiv.org/abs/2512.21576v1</link><guid>http://arxiv.org/abs/2512.21576v1</guid><pubDate>Thu, 25 Dec 2025 08:39:14 +0000</pubDate><dc:creator>Haoyi Zhou</dc:creator><dc:creator>Shuo Li</dc:creator><dc:creator>Tianyu Chen</dc:creator><dc:creator>Qi Song</dc:creator><dc:creator>Chonghan Gao</dc:creator><dc:creator>Jianxin Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While large vision-language models (VLMs) demonstrate strong long-context understanding, their prevalent small branches fail on linguistics-photography alignment for a limited window size. We discover that knowledge distillation improves students' capability as a complement to Rotary Position Embeddings (RoPE) on window sizes (anchored from large models). Building on this insight, we propose LAid, which directly aims at the transfer of long-range attention mechanisms through two complementary components: (1) a progressive distance-weighted attention matching that dynamically emphasizes longer position differences during training, and (2) a learnable RoPE response gain modulation that selectively amplifies position sensitivity where needed. Extensive experiments across multiple model families demonstrate that LAid-distilled models achieve up to 3.2 times longer effective context windows compared to baseline small models, while maintaining or improving performance on standard VL benchmarks. Spectral analysis also suggests that LAid successfully preserves crucial low-frequency attention components that conventional methods fail to transfer. Our work not only provides practical techniques for building more efficient long-context VLMs but also offers theoretical insights into how positional understanding emerges and transfers during distillation.
Published: 2025-12-25T08:39:14+00:00
Venue: arXiv
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoyi Zhou; Shuo Li; Tianyu Chen; Qi Song; Chonghan Gao; Jianxin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;While large vision-language models (VLMs) demonstrate strong long-context understanding, their prevalent small branches fail on linguistics-photography alignment for a limited window size. We discover that knowledge distillation improves students&amp;#x27; capability as a complement to Rotary Position Embeddings (RoPE) on window sizes (anchored from large models). Building on this insight, we propose LAid, which directly aims at the transfer of long-range attention mechanisms through two complementary components: (1) a progressive distance-weighted attention matching that dynamically emphasizes longer position differences during training, and (2) a learnable RoPE response gain modulation that selectively amplifies position sensitivity where needed. Extensive experiments across multiple model families demonstrate that LAid-distilled models achieve up to 3.2 times longer effective context windows compared to baseline small models, while maintaining or improving performance on standard VL benchmarks. Spectral analysis also suggests that LAid successfully preserves crucial low-frequency attention components that conventional methods fail to transfer. Our work not only provides practical techniques for building more efficient long-context VLMs but also offers theoretical insights into how positional understanding emerges and transfers during distillation.&lt;/p&gt;</content:encoded></item><item><title>DBMambaPose: Decoupled Spatial-Temporal Bidirectional State Space Model for Efficient 3D Human Pose Estimation</title><link>https://doi.org/10.1016/j.patcog.2025.112922</link><guid>10.1016/j.patcog.2025.112922</guid><pubDate>Sat, 27 Dec 2025 16:09:06 +0000</pubDate><dc:creator>Xiaoqing Wang</dc:creator><dc:creator>Yong Wang</dc:creator><dc:creator>Xuguang Liu</dc:creator><dc:creator>Hongbo Kang</dc:creator><dc:creator>Wenming Yang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112922</prism:doi><description>Transformer-based methods for 3D human pose estimation (HPE) have achieved notable progress. However, their reliance on self-attention mechanisms results in quadratic computational complexity, posing challenges in balancing accuracy and efficiency. Recently, State Space Models (SSMs) have demonstrated superior performance in vision tasks, offering linear complexity and long-range modeling capabilities. Nevertheless, directly applying Mamba to video-based 3D HPE is suboptimal, as its unidirectional SSM may inadequately capture spatio-temporal contextual information. To address these issues, we propose DBMambaPose , a novel, attention-free, and efficient 3D HPE architecture based on SSMs. At its core is the DBMambaPose Block, which alternately stacks Spatial Disentangled Bidirectional Mamba Block (S-DBMB) and Temporal Disentangled Bidirectional Mamba Block (T-DBMB). The S-DBMB learns intra-frame inter-joint spatial dependencies, while the T-DBMB models inter-frame joint-level motion trajectories. In the DBMambaPose Block, we incorporate a Decoupled Spatial-Temporal Bidirectional Scanning mechanism (DST-BS), which performs frame-ordered spatial bidirectional processing and joint-ordered temporal bidirectional processing, enhancing fine-grained spatio-temporal feature modeling. Furthermore, we present four variants of DBMambaPose to flexibly accommodate accuracy-efficiency trade-off. Extensive experiments on Human3.6M and MPI-INF-3DHP demonstrate that DBMambaPose achieves state-of-the-art performance while reducing computational costs. Code is available at https://github.com/camelliawxq/DBMambaPose .
Published: 2025-12-27T16:09:06+00:00
Venue: Pattern Recognition
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoqing Wang; Yong Wang; Xuguang Liu; Hongbo Kang; Wenming Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112922"&gt;10.1016/j.patcog.2025.112922&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;Transformer-based methods for 3D human pose estimation (HPE) have achieved notable progress. However, their reliance on self-attention mechanisms results in quadratic computational complexity, posing challenges in balancing accuracy and efficiency. Recently, State Space Models (SSMs) have demonstrated superior performance in vision tasks, offering linear complexity and long-range modeling capabilities. Nevertheless, directly applying Mamba to video-based 3D HPE is suboptimal, as its unidirectional SSM may inadequately capture spatio-temporal contextual information. To address these issues, we propose DBMambaPose , a novel, attention-free, and efficient 3D HPE architecture based on SSMs. At its core is the DBMambaPose Block, which alternately stacks Spatial Disentangled Bidirectional Mamba Block (S-DBMB) and Temporal Disentangled Bidirectional Mamba Block (T-DBMB). The S-DBMB learns intra-frame inter-joint spatial dependencies, while the T-DBMB models inter-frame joint-level motion trajectories. In the DBMambaPose Block, we incorporate a Decoupled Spatial-Temporal Bidirectional Scanning mechanism (DST-BS), which performs frame-ordered spatial bidirectional processing and joint-ordered temporal bidirectional processing, enhancing fine-grained spatio-temporal feature modeling. Furthermore, we present four variants of DBMambaPose to flexibly accommodate accuracy-efficiency trade-off. Extensive experiments on Human3.6M and MPI-INF-3DHP demonstrate that DBMambaPose achieves state-of-the-art performance while reducing computational costs. Code is available at https://github.com/camelliawxq/DBMambaPose .&lt;/p&gt;</content:encoded></item><item><title>Compositional Concept Extraction with Multimodal Large Models: A Unified Framework with Thought Chain Optimization</title><link>https://doi.org/10.1016/j.eswa.2025.130925</link><guid>10.1016/j.eswa.2025.130925</guid><pubDate>Sun, 28 Dec 2025 15:06:17 +0000</pubDate><dc:creator>Yuxin Wu</dc:creator><dc:creator>Zichen Song</dc:creator><dc:creator>Sitan Huang</dc:creator><dc:creator>Zhongfeng Kang</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130925</prism:doi><description>Compositional Concept Extraction (CCE) is a pivotal direction in machine learning, aiming to construct higher-level semantic representations by combining fundamental concepts. This paper introduces a novel framework for CCE based on ’thought chain generation and optimization,’ leveraging the capabilities of CLIP and GPT-4o. Specifically, the CLIP model extracts initial concepts from image and text data, generating reasoning paths (thought chains), which are subsequently validated and supplemented by GPT-4o to ensure logical consistency and semantic completeness. Contrastive learning methods are then employed to enhance compositional semantic representations, and the entire extraction process is further refined using PPO-based reinforcement learning, improving the expressiveness of compositional concepts. Experimental results demonstrate that the proposed framework significantly outperforms existing methods in compositional concept extraction tasks across multiple vision and language datasets and achieves superior performance in downstream classification tasks. Our study highlights the potential of large multimodal models in compositional concept extraction and offers a novel approach for generating complex semantic representations.
Published: 2025-12-28T15:06:17+00:00
Venue: Expert Systems with Applications
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxin Wu; Zichen Song; Sitan Huang; Zhongfeng Kang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130925"&gt;10.1016/j.eswa.2025.130925&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Compositional Concept Extraction (CCE) is a pivotal direction in machine learning, aiming to construct higher-level semantic representations by combining fundamental concepts. This paper introduces a novel framework for CCE based on ’thought chain generation and optimization,’ leveraging the capabilities of CLIP and GPT-4o. Specifically, the CLIP model extracts initial concepts from image and text data, generating reasoning paths (thought chains), which are subsequently validated and supplemented by GPT-4o to ensure logical consistency and semantic completeness. Contrastive learning methods are then employed to enhance compositional semantic representations, and the entire extraction process is further refined using PPO-based reinforcement learning, improving the expressiveness of compositional concepts. Experimental results demonstrate that the proposed framework significantly outperforms existing methods in compositional concept extraction tasks across multiple vision and language datasets and achieves superior performance in downstream classification tasks. Our study highlights the potential of large multimodal models in compositional concept extraction and offers a novel approach for generating complex semantic representations.&lt;/p&gt;</content:encoded></item><item><title>Pythia-RAG: Retrieval-Augmented Generation over a Unified Multimodal Knowledge Graph for Enhanced QA</title><link>https://doi.org/10.1016/j.knosys.2025.115200</link><guid>10.1016/j.knosys.2025.115200</guid><pubDate>Sat, 27 Dec 2025 00:06:50 +0000</pubDate><dc:creator>Zafar Ali</dc:creator><dc:creator>Yi Huang</dc:creator><dc:creator>Asad Khan</dc:creator><dc:creator>Guilin Qi</dc:creator><dc:creator>Yuxin Zhang</dc:creator><dc:creator>Junlan Feng</dc:creator><dc:creator>Chao Deng</dc:creator><dc:creator>Pavlos Kefalas</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115200</prism:doi><description>The multimodal question-answering (QA) capabilities of large language models (LLMs) continue to face challenges such as hallucinations, dependence on outdated or incomplete information, and insufficient support for structured reasoning across modalities. Existing methods that incorporate textual knowledge graphs offer partial solutions but often neglect visual context, limiting their ability to perform comprehensive multimodal reasoning. We introduce Pythia-RAG , a novel retrieval-augmented generation framework that builds a unified Multimodal Knowledge Graph (MMKG) from both textual and visual sources. Semantic triplets are extracted from text using GPT-4 and from images using a modified Faster-RCNN enhanced with the Relation Transformer (ReITR) and Convolutional Block Attention Module (CBAM). To broaden the coverage and strengthen the factual grounding of the MMKG, we augment it with external commonsense triplets sourced from ConceptNet. A relevant subgraph is retrieved from the MMKG using the Prize-Collecting Steiner Tree (PCST) algorithm to ensure high relevance and structural cohesion. This subgraph is then processed through two complementary paths: (i) it is converted into a textual format and encoded by an LLM, and (ii) it is structurally encoded using a graph neural network. Meanwhile, the associated image is encoded using a visual encoder. The resulting text, graph, and visual embeddings are fused via self-attention layers to form a unified multimodal representation, which is then used by the LLM to generate a coherent, context-aware answer. Experiments on ScienceQA and MultiModalQA show that Pythia-RAG significantly improves multimodal reasoning performance and reduces hallucinations compared to baseline methods, achieving a relative accuracy gain of 5.4% on ScienceQA and 4.8% on MultiModalQA.
Published: 2025-12-27T00:06:50+00:00
Venue: Knowledge-Based Systems
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zafar Ali; Yi Huang; Asad Khan; Guilin Qi; Yuxin Zhang; Junlan Feng; Chao Deng; Pavlos Kefalas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115200"&gt;10.1016/j.knosys.2025.115200&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;The multimodal question-answering (QA) capabilities of large language models (LLMs) continue to face challenges such as hallucinations, dependence on outdated or incomplete information, and insufficient support for structured reasoning across modalities. Existing methods that incorporate textual knowledge graphs offer partial solutions but often neglect visual context, limiting their ability to perform comprehensive multimodal reasoning. We introduce Pythia-RAG , a novel retrieval-augmented generation framework that builds a unified Multimodal Knowledge Graph (MMKG) from both textual and visual sources. Semantic triplets are extracted from text using GPT-4 and from images using a modified Faster-RCNN enhanced with the Relation Transformer (ReITR) and Convolutional Block Attention Module (CBAM). To broaden the coverage and strengthen the factual grounding of the MMKG, we augment it with external commonsense triplets sourced from ConceptNet. A relevant subgraph is retrieved from the MMKG using the Prize-Collecting Steiner Tree (PCST) algorithm to ensure high relevance and structural cohesion. This subgraph is then processed through two complementary paths: (i) it is converted into a textual format and encoded by an LLM, and (ii) it is structurally encoded using a graph neural network. Meanwhile, the associated image is encoded using a visual encoder. The resulting text, graph, and visual embeddings are fused via self-attention layers to form a unified multimodal representation, which is then used by the LLM to generate a coherent, context-aware answer. Experiments on ScienceQA and MultiModalQA show that Pythia-RAG significantly improves multimodal reasoning performance and reduces hallucinations compared to baseline methods, achieving a relative accuracy gain of 5.4% on ScienceQA and 4.8% on MultiModalQA.&lt;/p&gt;</content:encoded></item><item><title>DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation</title><link>https://arxiv.org/abs/2512.21867v1</link><guid>http://arxiv.org/abs/2512.21867v1</guid><pubDate>Fri, 26 Dec 2025 05:03:47 +0000</pubDate><dc:creator>Divyansh Srivastava</dc:creator><dc:creator>Akshay Mehra</dc:creator><dc:creator>Pranav Maneriker</dc:creator><dc:creator>Debopam Sanyal</dc:creator><dc:creator>Vishnu Raj</dc:creator><dc:creator>Vijay Kamarshi</dc:creator><dc:creator>Fan Du</dc:creator><dc:creator>Joshua Kimball</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Decoder-only autoregressive image generation typically relies on fixed-length tokenization schemes whose token counts grow quadratically with resolution, substantially increasing the computational and memory demands of attention. We present DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into a variable number of patches for efficient image generation. Our work is the first to demonstrate that next-token prediction entropy from a lightweight and unsupervised autoregressive model provides a reliable criterion for merging tokens into larger patches based on information content. DPAR makes minimal modifications to the standard decoder architecture, ensuring compatibility with multimodal generation frameworks and allocating more compute to generation of high-information image regions. Further, we demonstrate that training with dynamically sized patches yields representations that are robust to patch boundaries, allowing DPAR to scale to larger patch sizes at inference. DPAR reduces token count by 1.81x and 2.06x on Imagenet 256 and 384 generation resolution respectively, leading to a reduction of up to 40% FLOPs in training costs. Further, our method exhibits faster convergence and improves FID by up to 27.1% relative to baseline models.
Published: 2025-12-26T05:03:47+00:00
Venue: arXiv
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Divyansh Srivastava; Akshay Mehra; Pranav Maneriker; Debopam Sanyal; Vishnu Raj; Vijay Kamarshi; Fan Du; Joshua Kimball&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Decoder-only autoregressive image generation typically relies on fixed-length tokenization schemes whose token counts grow quadratically with resolution, substantially increasing the computational and memory demands of attention. We present DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into a variable number of patches for efficient image generation. Our work is the first to demonstrate that next-token prediction entropy from a lightweight and unsupervised autoregressive model provides a reliable criterion for merging tokens into larger patches based on information content. DPAR makes minimal modifications to the standard decoder architecture, ensuring compatibility with multimodal generation frameworks and allocating more compute to generation of high-information image regions. Further, we demonstrate that training with dynamically sized patches yields representations that are robust to patch boundaries, allowing DPAR to scale to larger patch sizes at inference. DPAR reduces token count by 1.81x and 2.06x on Imagenet 256 and 384 generation resolution respectively, leading to a reduction of up to 40% FLOPs in training costs. Further, our method exhibits faster convergence and improves FID by up to 27.1% relative to baseline models.&lt;/p&gt;</content:encoded></item><item><title>Enhanced Visual Prompt Meets Low-Light Saliency Detection</title><link>https://doi.org/10.1016/j.patcog.2025.113008</link><guid>10.1016/j.patcog.2025.113008</guid><pubDate>Sat, 27 Dec 2025 07:16:40 +0000</pubDate><dc:creator>Nana Yu</dc:creator><dc:creator>Jie Wang</dc:creator><dc:creator>Yahong Han</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113008</prism:doi><description>The presence of low-light scenes poses significant challenges to salient object detection (SOD), including false positives, false negatives, and missed detections. Existing approaches to low-light SOD can be broadly categorized into two paradigms. The first employs a two-stage framework, where image enhancement precedes saliency detection. The second integrates enhancement and saliency detection within a unified end-to-end framework, typically trained with comprehensive fine-tuning. However, these approaches face two main issues. In the two-stage framework, enhancement and SOD are treated as largely independent tasks, resulting in poor adaptability of enhanced images for SOD. In fully fine-tuned end-to-end frameworks, an inherent optimization conflict exists between enhancement and SOD. To address these issues, we propose Enhancement Visual Prompt (EnVP), which adopts local fine-tuning for low-light SOD. The core idea of EnVP is to fine-tune only the enhancement module rather than performing full fine-tuning. Specifically, the Transformer backbone is frozen, and only the enhancement prompt is fine-tuned. The enhancement level is constrained through illumination estimation and grayscale threshold judgment, allowing the model to gradually adapt to diverse low-light conditions. This approach mitigates the adverse effects of uniform enhancement on SOD performance. Extensive experiments show that EnVP outperforms state-of-the-art fully fine-tuned methods on various low-light SOD datasets. Moreover, on the RGBD-385 and RGBT-621 sub-datasets, EnVP improves the MAE metric by 27% and 35% , respectively.
Published: 2025-12-27T07:16:40+00:00
Venue: Pattern Recognition
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nana Yu; Jie Wang; Yahong Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113008"&gt;10.1016/j.patcog.2025.113008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;The presence of low-light scenes poses significant challenges to salient object detection (SOD), including false positives, false negatives, and missed detections. Existing approaches to low-light SOD can be broadly categorized into two paradigms. The first employs a two-stage framework, where image enhancement precedes saliency detection. The second integrates enhancement and saliency detection within a unified end-to-end framework, typically trained with comprehensive fine-tuning. However, these approaches face two main issues. In the two-stage framework, enhancement and SOD are treated as largely independent tasks, resulting in poor adaptability of enhanced images for SOD. In fully fine-tuned end-to-end frameworks, an inherent optimization conflict exists between enhancement and SOD. To address these issues, we propose Enhancement Visual Prompt (EnVP), which adopts local fine-tuning for low-light SOD. The core idea of EnVP is to fine-tune only the enhancement module rather than performing full fine-tuning. Specifically, the Transformer backbone is frozen, and only the enhancement prompt is fine-tuned. The enhancement level is constrained through illumination estimation and grayscale threshold judgment, allowing the model to gradually adapt to diverse low-light conditions. This approach mitigates the adverse effects of uniform enhancement on SOD performance. Extensive experiments show that EnVP outperforms state-of-the-art fully fine-tuned methods on various low-light SOD datasets. Moreover, on the RGBD-385 and RGBT-621 sub-datasets, EnVP improves the MAE metric by 27% and 35% , respectively.&lt;/p&gt;</content:encoded></item><item><title>Vision Transformers are Circulant Attention Learners</title><link>https://arxiv.org/abs/2512.21542v1</link><guid>http://arxiv.org/abs/2512.21542v1</guid><pubDate>Thu, 25 Dec 2025 07:28:33 +0000</pubDate><dc:creator>Dongchen Han</dc:creator><dc:creator>Tianyu Li</dc:creator><dc:creator>Ziyi Wang</dc:creator><dc:creator>Gao Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The self-attention mechanism has been a key factor in the advancement of vision Transformers. However, its quadratic complexity imposes a heavy computational burden in high-resolution scenarios, restricting the practical application. Previous methods attempt to mitigate this issue by introducing handcrafted patterns such as locality or sparsity, which inevitably compromise model capacity. In this paper, we present a novel attention paradigm termed \textbf{Circulant Attention} by exploiting the inherent efficient pattern of self-attention. Specifically, we first identify that the self-attention matrix in vision Transformers often approximates the Block Circulant matrix with Circulant Blocks (BCCB), a kind of structured matrix whose multiplication with other matrices can be performed in $\mathcal{O}(N\log N)$ time. Leveraging this interesting pattern, we explicitly model the attention map as its nearest BCCB matrix and propose an efficient computation algorithm for fast calculation. The resulting approach closely mirrors vanilla self-attention, differing only in its use of BCCB matrices. Since our design is inspired by the inherent efficient paradigm, it not only delivers $\mathcal{O}(N\log N)$ computation complexity, but also largely maintains the capacity of standard self-attention. Extensive experiments on diverse visual tasks demonstrate the effectiveness of our approach, establishing circulant attention as a promising alternative to self-attention for vision Transformer architectures. Code is available at https://github.com/LeapLabTHU/Circulant-Attention.
Published: 2025-12-25T07:28:33+00:00
Venue: arXiv
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dongchen Han; Tianyu Li; Ziyi Wang; Gao Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;The self-attention mechanism has been a key factor in the advancement of vision Transformers. However, its quadratic complexity imposes a heavy computational burden in high-resolution scenarios, restricting the practical application. Previous methods attempt to mitigate this issue by introducing handcrafted patterns such as locality or sparsity, which inevitably compromise model capacity. In this paper, we present a novel attention paradigm termed \textbf{Circulant Attention} by exploiting the inherent efficient pattern of self-attention. Specifically, we first identify that the self-attention matrix in vision Transformers often approximates the Block Circulant matrix with Circulant Blocks (BCCB), a kind of structured matrix whose multiplication with other matrices can be performed in $\mathcal{O}(N\log N)$ time. Leveraging this interesting pattern, we explicitly model the attention map as its nearest BCCB matrix and propose an efficient computation algorithm for fast calculation. The resulting approach closely mirrors vanilla self-attention, differing only in its use of BCCB matrices. Since our design is inspired by the inherent efficient paradigm, it not only delivers $\mathcal{O}(N\log N)$ computation complexity, but also largely maintains the capacity of standard self-attention. Extensive experiments on diverse visual tasks demonstrate the effectiveness of our approach, establishing circulant attention as a promising alternative to self-attention for vision Transformer architectures. Code is available at https://github.com/LeapLabTHU/Circulant-Attention.&lt;/p&gt;</content:encoded></item><item><title>Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism</title><link>https://arxiv.org/abs/2512.21452v1</link><guid>http://arxiv.org/abs/2512.21452v1</guid><pubDate>Thu, 25 Dec 2025 00:29:58 +0000</pubDate><dc:creator>Haotian Lv</dc:creator><dc:creator>Yuhui Zhang</dc:creator><dc:creator>Jiangbo Dai</dc:creator><dc:creator>Hanli Wu</dc:creator><dc:creator>Jiaji Wang</dc:creator><dc:creator>Dawei Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TGRS.2025.3575293</prism:doi><description>Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework's efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.
Published: 2025-12-25T00:29:58+00:00
Venue: arXiv
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haotian Lv; Yuhui Zhang; Jiangbo Dai; Hanli Wu; Jiaji Wang; Dawei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TGRS.2025.3575293"&gt;10.1109/TGRS.2025.3575293&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework&amp;#x27;s efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.&lt;/p&gt;</content:encoded></item><item><title>CDG-Rec: Contrastive-Regularized diffusion with a Synergistic Dual-view Decoder for recommendation</title><link>https://doi.org/10.1016/j.eswa.2025.131026</link><guid>10.1016/j.eswa.2025.131026</guid><pubDate>Sat, 27 Dec 2025 16:14:44 +0000</pubDate><dc:creator>Jun Geng</dc:creator><dc:creator>Shaohua Tuo</dc:creator><dc:creator>Sibeier Chen</dc:creator><dc:creator>Peiming Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.131026</prism:doi><description>Generative recommender systems often suffer from a critical precision-recall trade-off, where high recall is achieved at the expense of recommendation precision, primarily due to structural flaws like anisotropy in their latent representations. To address this fundamental challenge, we propose CDG-Rec, a novel framework following a “Purify-Refine-Decode” paradigm. The core innovations of CDG-Rec are embodied in three synergistic components: a contrastive regularizer for the Variational Autoencoder (VAE) that mitigates representation anisotropy; a diffusion-based refinement process that enhances representational robustness; and a Synergistic Dual-View Decoder that translates the high-quality representations into precise recommendations by concurrently modeling global and local user preferences. Extensive experiments on three benchmark datasets demonstrate that CDG-Rec significantly outperforms state-of-the-art baseline methods, achieving double-digit relative improvements in precision-oriented metrics such as NDCG@10. Ablation studies further confirm that these components jointly drive this performance leap, validating our core argument: the key to superior generative recommendation lies not only in the underlying quality of the representation space but also in a downstream decoding architecture that can fully leverage it.
Published: 2025-12-27T16:14:44+00:00
Venue: Expert Systems with Applications
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jun Geng; Shaohua Tuo; Sibeier Chen; Peiming Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.131026"&gt;10.1016/j.eswa.2025.131026&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;Generative recommender systems often suffer from a critical precision-recall trade-off, where high recall is achieved at the expense of recommendation precision, primarily due to structural flaws like anisotropy in their latent representations. To address this fundamental challenge, we propose CDG-Rec, a novel framework following a “Purify-Refine-Decode” paradigm. The core innovations of CDG-Rec are embodied in three synergistic components: a contrastive regularizer for the Variational Autoencoder (VAE) that mitigates representation anisotropy; a diffusion-based refinement process that enhances representational robustness; and a Synergistic Dual-View Decoder that translates the high-quality representations into precise recommendations by concurrently modeling global and local user preferences. Extensive experiments on three benchmark datasets demonstrate that CDG-Rec significantly outperforms state-of-the-art baseline methods, achieving double-digit relative improvements in precision-oriented metrics such as NDCG@10. Ablation studies further confirm that these components jointly drive this performance leap, validating our core argument: the key to superior generative recommendation lies not only in the underlying quality of the representation space but also in a downstream decoding architecture that can fully leverage it.&lt;/p&gt;</content:encoded></item><item><title>Electrostatic force regularization for neural structured pruning</title><link>https://doi.org/10.1016/j.ins.2025.123045</link><guid>10.1016/j.ins.2025.123045</guid><pubDate>Sat, 27 Dec 2025 07:15:16 +0000</pubDate><dc:creator>Abdesselam Ferdi</dc:creator><dc:creator>Abdelmalik Taleb-Ahmed</dc:creator><dc:creator>Amir Nakib</dc:creator><dc:creator>Youcef Ferdi</dc:creator><prism:publicationName>Information Sciences</prism:publicationName><prism:doi>10.1016/j.ins.2025.123045</prism:doi><description>The demand for deploying deep convolutional neural networks (DCNNs) on resource-constrained devices for real-time applications remains substantial. However, existing state-of-the-art structured pruning methods often involve intricate implementations, require modifications to the original network architectures, and necessitate an extensive fine-tuning phase. To address these challenges, we introduce a novel approach that integrates the concept of electrostatic force from physics into the training process of DCNNs. The magnitude of this force is directly proportional to the product of the charges of the convolution filter and the source filter, and inversely proportional to the square of the distance between them. We applied the electrostatic-like force to the convolution filters, attracting those with opposite charges toward non-zero weights and repelling similar ones toward zero. Consequently, repelled filters are pruned as their weights approach zero, while attracted filters retain significant parameters that preserve essential information. Unlike conventional methods, our approach is straightforward to implement, does not require any architectural modifications, and simultaneously optimizes weights and ranks filter importance, all without the need for extensive fine-tuning. We validated the efficacy of our method on modern DCNN architectures using the MNIST, CIFAR, and ImageNet datasets, achieving competitive performance compared to existing structured pruning approaches.
Published: 2025-12-27T07:15:16+00:00
Venue: Information Sciences
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Abdesselam Ferdi; Abdelmalik Taleb-Ahmed; Amir Nakib; Youcef Ferdi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Sciences&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.ins.2025.123045"&gt;10.1016/j.ins.2025.123045&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;The demand for deploying deep convolutional neural networks (DCNNs) on resource-constrained devices for real-time applications remains substantial. However, existing state-of-the-art structured pruning methods often involve intricate implementations, require modifications to the original network architectures, and necessitate an extensive fine-tuning phase. To address these challenges, we introduce a novel approach that integrates the concept of electrostatic force from physics into the training process of DCNNs. The magnitude of this force is directly proportional to the product of the charges of the convolution filter and the source filter, and inversely proportional to the square of the distance between them. We applied the electrostatic-like force to the convolution filters, attracting those with opposite charges toward non-zero weights and repelling similar ones toward zero. Consequently, repelled filters are pruned as their weights approach zero, while attracted filters retain significant parameters that preserve essential information. Unlike conventional methods, our approach is straightforward to implement, does not require any architectural modifications, and simultaneously optimizes weights and ranks filter importance, all without the need for extensive fine-tuning. We validated the efficacy of our method on modern DCNN architectures using the MNIST, CIFAR, and ImageNet datasets, achieving competitive performance compared to existing structured pruning approaches.&lt;/p&gt;</content:encoded></item><item><title>Improving Multi-Label Contrastive Learning by Leveraging Label Distribution</title><link>https://doi.org/10.1016/j.patcog.2025.113011</link><guid>10.1016/j.patcog.2025.113011</guid><pubDate>Sat, 27 Dec 2025 16:09:05 +0000</pubDate><dc:creator>Ning Chen</dc:creator><dc:creator>Shen-Huan Lyu</dc:creator><dc:creator>Tian-Shuang Wu</dc:creator><dc:creator>Yanyan Wang</dc:creator><dc:creator>Bin Tang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113011</prism:doi><description>In multi-label learning, leveraging contrastive learning to learn better representations faces a key challenge: selecting positive and negative samples and effectively utilizing label information. Previous studies address the former through differential overlap degrees between positive and negative samples, while existing approaches typically employ logical labels for the latter. However, directly using logical labels fails to fully utilize inter-label information, as they ignore the varying importance among labels. To address this problem, we propose a novel method that improves multi-label contrastive learning through label distribution. Specifically, the framework first leverages contrastive loss to estimate label distributions from logical labels, then integrates label-aware information from these distributions into the loss function. We conduct evaluations on multiple widely-used multi-label datasets, including image and vector datasets, and additionally validate the feasibility of learning latent label distributions from logical labels using contrastive loss on label distribution datasets. The results demonstrate that our method outperforms state-of-the-art methods in six evaluation metrics.
Published: 2025-12-27T16:09:05+00:00
Venue: Pattern Recognition
Score: 0.774 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ning Chen; Shen-Huan Lyu; Tian-Shuang Wu; Yanyan Wang; Bin Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113011"&gt;10.1016/j.patcog.2025.113011&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (consider)&lt;/p&gt;
&lt;p&gt;In multi-label learning, leveraging contrastive learning to learn better representations faces a key challenge: selecting positive and negative samples and effectively utilizing label information. Previous studies address the former through differential overlap degrees between positive and negative samples, while existing approaches typically employ logical labels for the latter. However, directly using logical labels fails to fully utilize inter-label information, as they ignore the varying importance among labels. To address this problem, we propose a novel method that improves multi-label contrastive learning through label distribution. Specifically, the framework first leverages contrastive loss to estimate label distributions from logical labels, then integrates label-aware information from these distributions into the loss function. We conduct evaluations on multiple widely-used multi-label datasets, including image and vector datasets, and additionally validate the feasibility of learning latent label distributions from logical labels using contrastive loss on label distribution datasets. The results demonstrate that our method outperforms state-of-the-art methods in six evaluation metrics.&lt;/p&gt;</content:encoded></item><item><title>Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models</title><link>https://arxiv.org/abs/2512.21778v1</link><guid>http://arxiv.org/abs/2512.21778v1</guid><pubDate>Thu, 25 Dec 2025 20:31:36 +0000</pubDate><dc:creator>Nimrod Berman</dc:creator><dc:creator>Adam Botach</dc:creator><dc:creator>Emanuel Ben-Baruch</dc:creator><dc:creator>Shunit Haviv Hakimi</dc:creator><dc:creator>Asaf Gendler</dc:creator><dc:creator>Ilan Naiman</dc:creator><dc:creator>Erez Yosef</dc:creator><dc:creator>Igor Kviatkovsky</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.
Published: 2025-12-25T20:31:36+00:00
Venue: arXiv
Score: 0.773 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nimrod Berman; Adam Botach; Emanuel Ben-Baruch; Shunit Haviv Hakimi; Asaf Gendler; Ilan Naiman; Erez Yosef; Igor Kviatkovsky&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (consider)&lt;/p&gt;
&lt;p&gt;Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.&lt;/p&gt;</content:encoded></item></channel></rss>