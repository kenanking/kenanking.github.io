<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 10 Jan 2026 02:38:03 +0000</lastBuildDate><item><title>Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.004</link><guid>10.1016/j.isprsjprs.2026.01.004</guid><pubDate>Fri, 09 Jan 2026 09:38:18 +0000</pubDate><dc:creator>Gui Gao</dc:creator><dc:creator>Caiyi Li</dc:creator><dc:creator>Xi Zhang</dc:creator><dc:creator>Bingxiu Yao</dc:creator><dc:creator>Zhen Chen</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.004</prism:doi><description>Ship detection is crucial for both military and civilian applications and is a key use of polarimetric SAR (PolSAR). While convolutional neural networks (CNNs) enhance PolSAR ship detection with powerful feature extraction, existing approaches still face challenges in discriminating targets from clutter, detecting multi-scale objects in complex scenes, and achieving real-time detection. To address these issues, we propose a Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering. First, at the input feature level, a Dual-perception feature of Doppler and Scattering (DDS) is introduced, effectively differentiating ship and clutter pixels to enhance the network’s ship discrimination. Specifically, Doppler characteristics distinguish between moving and stationary targets, while scattering characteristics reveal fundamental differences between targets and clutter. Second, at the network architecture level, a Mamba-CNN hybrid Multi-scale ship detection Network (MCMN) is designed to improve multi-scale ship detection in complex scenarios. It uses a Multi-scale Information Perception Module (MIPM) to adaptively aggregate multi-scale features and a Local-Global Feature Enhancement Module (LGFEM) based on Mamba for long-range context modeling. MCMN remains efficient through feature grouping, pointwise and depthwise convolutions, meeting real-time requirements. Finally, extensive experiments on the GF-3 and SSDD datasets demonstrate the superiority of DDS and MCMN. DDS effectively distinguishes ships from clutter across scenarios. As an input feature, it boosts average F1-score and AP by 4.3% and 4.3%, respectively, over HV intensity, and outperforms other polarization features. MCMN achieves state-of-the-art results, improving AP by 1.2% and 0.8% on the two datasets while reducing parameters by 1.29M, FLOPs by 1.5G, and inference time by 59.2%.
Published: 2026-01-09T09:38:18+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.863 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gui Gao; Caiyi Li; Xi Zhang; Bingxiu Yao; Zhen Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.004"&gt;10.1016/j.isprsjprs.2026.01.004&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.863 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection is crucial for both military and civilian applications and is a key use of polarimetric SAR (PolSAR). While convolutional neural networks (CNNs) enhance PolSAR ship detection with powerful feature extraction, existing approaches still face challenges in discriminating targets from clutter, detecting multi-scale objects in complex scenes, and achieving real-time detection. To address these issues, we propose a Mamba-CNN hybrid Multi-scale ship detection Network driven by a Dual-perception feature of Doppler and Scattering. First, at the input feature level, a Dual-perception feature of Doppler and Scattering (DDS) is introduced, effectively differentiating ship and clutter pixels to enhance the network’s ship discrimination. Specifically, Doppler characteristics distinguish between moving and stationary targets, while scattering characteristics reveal fundamental differences between targets and clutter. Second, at the network architecture level, a Mamba-CNN hybrid Multi-scale ship detection Network (MCMN) is designed to improve multi-scale ship detection in complex scenarios. It uses a Multi-scale Information Perception Module (MIPM) to adaptively aggregate multi-scale features and a Local-Global Feature Enhancement Module (LGFEM) based on Mamba for long-range context modeling. MCMN remains efficient through feature grouping, pointwise and depthwise convolutions, meeting real-time requirements. Finally, extensive experiments on the GF-3 and SSDD datasets demonstrate the superiority of DDS and MCMN. DDS effectively distinguishes ships from clutter across scenarios. As an input feature, it boosts average F1-score and AP by 4.3% and 4.3%, respectively, over HV intensity, and outperforms other polarization features. MCMN achieves state-of-the-art results, improving AP by 1.2% and 0.8% on the two datasets while reducing parameters by 1.29M, FLOPs by 1.5G, and inference time by 59.2%.&lt;/p&gt;</content:encoded></item><item><title>微波与光学遥感图像联合目标检测与识别技术研究进展</title><link>https://doi.org/10.11834/jig.250648</link><guid>10.11834/jig.250648</guid><pubDate>Thu, 08 Jan 2026 03:05:32 +0000</pubDate><dc:creator>Yang Jian</dc:creator><dc:creator>Chen Jie</dc:creator><dc:creator>Xu Huaping</dc:creator><dc:creator>Wang Xiaoliang</dc:creator><dc:creator>You Ya’nan</dc:creator><dc:creator>Feng Xiao</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250648</prism:doi><description>随着对地观测技术的飞速发展，从海量遥感图像中快速准确地检测与识别特定目标，已成为环境监测、灾害评估及国防安全等领域的关键任务。光学图像和微波图像是最常见的遥感图像类型，将二者相结合进行联合目标检测与识别，可以优势互补，有效克服单一类型传感器获取目标信息的局限性，在突破单源遥感性能瓶颈、提升复杂环境下目标解译能力等方面具有重要价值与广阔应用前景。本文综述了微波与光学遥感图像联合目标检测与识别技术的研究进展。首先，概述了两类图像的特点以及联合目标检测与识别的一般处理流程。其次，深入剖析了该领域当前所面临的主要挑战：成像机理与特征表达的差异性、数据集规模与分辨率的不均衡性、数据获取的时空异步性以及复杂背景下的弱小目标检测与识别。在此基础上，重点围绕海洋与陆地两类典型应用环境，分别分析了当前的主流技术。在海洋应用领域，以海上舰船目标检测与识别为核心，讨论了基于特征融合的方法、知识驱动的方法、复杂场景下的方法以及基于尾迹的间接方法。在陆地应用领域，聚焦飞机、车辆、基础设施等关键目标，探讨了基于特征融合、知识迁移与蒸馏和复杂场景下的弱小目标检测与识别技术。此外，本文还梳理了该领域的常用性能评价指标与公开数据集资源，并对未来发展趋势进行了展望。
Published: 2026-01-08T03:05:32+00:00
Venue: Journal of Image and Graphics
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Jian; Chen Jie; Xu Huaping; Wang Xiaoliang; You Ya’nan; Feng Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250648"&gt;10.11834/jig.250648&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;随着对地观测技术的飞速发展，从海量遥感图像中快速准确地检测与识别特定目标，已成为环境监测、灾害评估及国防安全等领域的关键任务。光学图像和微波图像是最常见的遥感图像类型，将二者相结合进行联合目标检测与识别，可以优势互补，有效克服单一类型传感器获取目标信息的局限性，在突破单源遥感性能瓶颈、提升复杂环境下目标解译能力等方面具有重要价值与广阔应用前景。本文综述了微波与光学遥感图像联合目标检测与识别技术的研究进展。首先，概述了两类图像的特点以及联合目标检测与识别的一般处理流程。其次，深入剖析了该领域当前所面临的主要挑战：成像机理与特征表达的差异性、数据集规模与分辨率的不均衡性、数据获取的时空异步性以及复杂背景下的弱小目标检测与识别。在此基础上，重点围绕海洋与陆地两类典型应用环境，分别分析了当前的主流技术。在海洋应用领域，以海上舰船目标检测与识别为核心，讨论了基于特征融合的方法、知识驱动的方法、复杂场景下的方法以及基于尾迹的间接方法。在陆地应用领域，聚焦飞机、车辆、基础设施等关键目标，探讨了基于特征融合、知识迁移与蒸馏和复杂场景下的弱小目标检测与识别技术。此外，本文还梳理了该领域的常用性能评价指标与公开数据集资源，并对未来发展趋势进行了展望。&lt;/p&gt;</content:encoded></item><item><title>Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2601.02837v1</link><guid>http://arxiv.org/abs/2601.02837v1</guid><pubDate>Tue, 06 Jan 2026 09:14:01 +0000</pubDate><dc:creator>Yuteng Liu</dc:creator><dc:creator>Duanni Meng</dc:creator><dc:creator>Maoxun Yuan</dc:creator><dc:creator>Xingxing Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.
Published: 2026-01-06T09:14:01+00:00
Venue: arXiv
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuteng Liu; Duanni Meng; Maoxun Yuan; Xingxing Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.&lt;/p&gt;</content:encoded></item><item><title>多分支感知与跨层语义融合的红外小目标检测</title><link>https://doi.org/10.11834/jig.250448</link><guid>10.11834/jig.250448</guid><pubDate>Thu, 08 Jan 2026 03:05:10 +0000</pubDate><dc:creator>Qian Menghao</dc:creator><dc:creator>Liu Kui</dc:creator><dc:creator>Zhang Fengbo</dc:creator><dc:creator>Su Benyue</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250448</prism:doi><description>目的红外小目标检测在军事和民用等领域具有重要应用价值。然而，由于目标尺度极小且常处于复杂背景之中，如何有效提取边缘等判别性特征仍然是亟待解决的难题。同时，现有基于 U-Net 的检测网络在跨层特征融合过程中存在明显的语义差异，导致浅层细节信息与深层语义特征难以充分结合，从而进一步限制了检测精度的提升。方法基于U-Net结构，提出一种多分支感知与跨层语义融合的红外小目标检测网络（multi-branch perception and cross-layer semantic fusion network，MPCF-Net）。在编码器阶段，为增强边缘等判别性特征的提取，引入了多分支感知融合注意力（multi-branch perception fusion attention module，MPFM）。该模块通过局部分支、全局分支及串行卷积分支实现多尺度特征提取，并结合局部-全局引导注意力（local-global guided attention，LGGA）与全局通道空间注意力（global channel spatial attention，GCSA），分别强化小目标的响应能力与特征表达能力。随后，为缓解跨层特征间的语义差异并建模上下文依赖关系，采用空间-通道交叉Transformer块（spatial-channel cross transformer block，SCTB）替代传统的跳跃连接，从而提升多层特征融合效果。在解码器阶段，虽然深度可分离卷积能够有效降低参数量和计算复杂度，但由于缺乏跨通道特征交互，削弱了小目标的细节特征。为此，在输出端引入轻量梯度门控模块（lightweight gradient gating，LGG），利用Sobel梯度引导的空间注意力进一步强化小目标的边缘与细节特征。结果在SIRST、IRSTD和NUDT-SIRST三个公开红外小目标数据集上的实验表明，MPCF-Net在交并比（intersection over union，IoU）和归一化交并比（normalized intersection over union，nIoU）指标上分别达到80.12% 、66.28%和84.26%，以及78.23%、64.58%和86.48%。同时，该方法在检测概率（probability of detection，Pd）上分别达到99.88%、94.23%和98.21%，虚警率（false alarm，Fa）仅为1.12×10 -6 、4.39×10 -6 和14.57×10 -6 ，展现了更优的检测性能。结论所提方法通过多分支感知和跨层语义融合，有效增强了红外小目标的边缘等判别特征提取能力及上下文建模能力，从而实现了更高精度的红外小目标检测。
Published: 2026-01-08T03:05:10+00:00
Venue: Journal of Image and Graphics
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Menghao; Liu Kui; Zhang Fengbo; Su Benyue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250448"&gt;10.11834/jig.250448&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;目的红外小目标检测在军事和民用等领域具有重要应用价值。然而，由于目标尺度极小且常处于复杂背景之中，如何有效提取边缘等判别性特征仍然是亟待解决的难题。同时，现有基于 U-Net 的检测网络在跨层特征融合过程中存在明显的语义差异，导致浅层细节信息与深层语义特征难以充分结合，从而进一步限制了检测精度的提升。方法基于U-Net结构，提出一种多分支感知与跨层语义融合的红外小目标检测网络（multi-branch perception and cross-layer semantic fusion network，MPCF-Net）。在编码器阶段，为增强边缘等判别性特征的提取，引入了多分支感知融合注意力（multi-branch perception fusion attention module，MPFM）。该模块通过局部分支、全局分支及串行卷积分支实现多尺度特征提取，并结合局部-全局引导注意力（local-global guided attention，LGGA）与全局通道空间注意力（global channel spatial attention，GCSA），分别强化小目标的响应能力与特征表达能力。随后，为缓解跨层特征间的语义差异并建模上下文依赖关系，采用空间-通道交叉Transformer块（spatial-channel cross transformer block，SCTB）替代传统的跳跃连接，从而提升多层特征融合效果。在解码器阶段，虽然深度可分离卷积能够有效降低参数量和计算复杂度，但由于缺乏跨通道特征交互，削弱了小目标的细节特征。为此，在输出端引入轻量梯度门控模块（lightweight gradient gating，LGG），利用Sobel梯度引导的空间注意力进一步强化小目标的边缘与细节特征。结果在SIRST、IRSTD和NUDT-SIRST三个公开红外小目标数据集上的实验表明，MPCF-Net在交并比（intersection over union，IoU）和归一化交并比（normalized intersection over union，nIoU）指标上分别达到80.12% 、66.28%和84.26%，以及78.23%、64.58%和86.48%。同时，该方法在检测概率（probability of detection，Pd）上分别达到99.88%、94.23%和98.21%，虚警率（false alarm，Fa）仅为1.12×10 -6 、4.39×10 -6 和14.57×10 -6 ，展现了更优的检测性能。结论所提方法通过多分支感知和跨层语义融合，有效增强了红外小目标的边缘等判别特征提取能力及上下文建模能力，从而实现了更高精度的红外小目标检测。&lt;/p&gt;</content:encoded></item><item><title>BEVFormer++: Enhancing BEV Fusion with Normalized Embedding and Range Attention for 3D Object Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131131</link><guid>10.1016/j.eswa.2026.131131</guid><pubDate>Thu, 08 Jan 2026 00:22:20 +0000</pubDate><dc:creator>Shazib Qayyum</dc:creator><dc:creator>Xiaoheng Deng</dc:creator><dc:creator>Husnain Mushtaq</dc:creator><dc:creator>Ping Jiang</dc:creator><dc:creator>Shaohua Wan</dc:creator><dc:creator>Irsha Ullah</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131131</prism:doi><description>Accurate 3D object detection remains a critical challenge in autonomous driving due to the sparsity and range-dependent density of LiDAR point clouds. Objects at greater distances often contain limited structural information, making them difficult to detect with conventional range-invariant attention and naïve sampling. Furthermore, existing multimodal fusion approaches struggle with spatial misalignment and inconsistent geometric representation, leading to suboptimal performance in complex driving environments. We propose BEVFormer++, a unified multimodal detection framework that enhances feature representation and fusion in Bird’s Eye View (BEV) space. Our approach introduces three key innovations: (1) a Normalized Positional Embedding (NPE) that encodes scale-invariant geometric cues, improving alignment between LiDAR and camera features; (2) a Diversity Sampling (cloud mining) strategy that selects informative and representative points, enriching structural features and improving small/occluded object detection; and (3) a Range-Aware Attention (RAA) mechanism that adaptively adjusts attention weights across distance bins, mitigating long-range sparsity and improving far-field detection. These modules are integrated into a robust BEV fusion pipeline, ensuring consistent cross-modal reasoning and spatial awareness. Extensive experiments demonstrate the effectiveness of BEVFormer++. On the KITTI dataset, our method achieves 90.1%, 82.0%, 78.3% AP 3 D for Easy/Moderate/Hard cases, significantly outperforming baselines. On the nuScenes benchmark, BEVFormer++ delivers consistent gains in mean AP and NDS, highlighting its robustness across diverse driving scenarios. Together, these results confirm that our framework effectively addresses sparsity, distance variation, and multimodal misalignment, setting a new benchmark for 3D object detection.
Published: 2026-01-08T00:22:20+00:00
Venue: Expert Systems with Applications
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shazib Qayyum; Xiaoheng Deng; Husnain Mushtaq; Ping Jiang; Shaohua Wan; Irsha Ullah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131131"&gt;10.1016/j.eswa.2026.131131&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate 3D object detection remains a critical challenge in autonomous driving due to the sparsity and range-dependent density of LiDAR point clouds. Objects at greater distances often contain limited structural information, making them difficult to detect with conventional range-invariant attention and naïve sampling. Furthermore, existing multimodal fusion approaches struggle with spatial misalignment and inconsistent geometric representation, leading to suboptimal performance in complex driving environments. We propose BEVFormer++, a unified multimodal detection framework that enhances feature representation and fusion in Bird’s Eye View (BEV) space. Our approach introduces three key innovations: (1) a Normalized Positional Embedding (NPE) that encodes scale-invariant geometric cues, improving alignment between LiDAR and camera features; (2) a Diversity Sampling (cloud mining) strategy that selects informative and representative points, enriching structural features and improving small/occluded object detection; and (3) a Range-Aware Attention (RAA) mechanism that adaptively adjusts attention weights across distance bins, mitigating long-range sparsity and improving far-field detection. These modules are integrated into a robust BEV fusion pipeline, ensuring consistent cross-modal reasoning and spatial awareness. Extensive experiments demonstrate the effectiveness of BEVFormer++. On the KITTI dataset, our method achieves 90.1%, 82.0%, 78.3% AP 3 D for Easy/Moderate/Hard cases, significantly outperforming baselines. On the nuScenes benchmark, BEVFormer++ delivers consistent gains in mean AP and NDS, highlighting its robustness across diverse driving scenarios. Together, these results confirm that our framework effectively addresses sparsity, distance variation, and multimodal misalignment, setting a new benchmark for 3D object detection.&lt;/p&gt;</content:encoded></item><item><title>Attentional dual-stream interactive perception network for efficient infrared small aerial target detection</title><link>https://doi.org/10.1016/j.neunet.2026.108563</link><guid>10.1016/j.neunet.2026.108563</guid><pubDate>Thu, 08 Jan 2026 16:52:57 +0000</pubDate><dc:creator>Lihao Zhou</dc:creator><dc:creator>Huawei Wang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108563</prism:doi><description>Drones and other flying objects can be regarded as small targets from a long-distance perspective. Considering the occlusion and interference caused by the external environment, the infrared detection methods are adopted to help identify and manage small aerial targets. However, remote infrared imaging often leads to small target feature detail loss. And the general methods have low detection efficiency, difficult to deeply extract target features. To better address the above problems, we propose an attentional dual-stream interactive perception network (ADIPNet) in this paper. Based on dual-stream U-Net, ADIPNet mainly combines the multi-patch series-parallel attention module (MSPA), edge anchoring module with regret (EAR), context scene perception module (CSP) and dual-stream interaction fusion module (DSIF). MSPA manually constructs the weight of patch regions at multiple scales and then performs the nested self-attention so as to fully mine global target information. EAR unites two types of global features using local mapping and matrix product, which helps accurately capture small target edge. CSP exchanges context information multiple times and conducts mutual complementation of semantic scenarios to enhances the perception of small target features. Finally, DSIF conducts cross attention for high-level encoded features on double U-Nets, further improving the network’s understanding of complex scenario information. The proposed ADIPNet alleviates the insufficient feature extraction of infrared small targets. Compared with other state-of-the-art methods, mIoU respectively reaches 80.52% and 72.54% on two large infrared datasets. It achieves more accurate detection of small aerial targets with low operating cost, possessing potential application prospect in various infrared surveillance systems.
Published: 2026-01-08T16:52:57+00:00
Venue: Neural Networks
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihao Zhou; Huawei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108563"&gt;10.1016/j.neunet.2026.108563&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Drones and other flying objects can be regarded as small targets from a long-distance perspective. Considering the occlusion and interference caused by the external environment, the infrared detection methods are adopted to help identify and manage small aerial targets. However, remote infrared imaging often leads to small target feature detail loss. And the general methods have low detection efficiency, difficult to deeply extract target features. To better address the above problems, we propose an attentional dual-stream interactive perception network (ADIPNet) in this paper. Based on dual-stream U-Net, ADIPNet mainly combines the multi-patch series-parallel attention module (MSPA), edge anchoring module with regret (EAR), context scene perception module (CSP) and dual-stream interaction fusion module (DSIF). MSPA manually constructs the weight of patch regions at multiple scales and then performs the nested self-attention so as to fully mine global target information. EAR unites two types of global features using local mapping and matrix product, which helps accurately capture small target edge. CSP exchanges context information multiple times and conducts mutual complementation of semantic scenarios to enhances the perception of small target features. Finally, DSIF conducts cross attention for high-level encoded features on double U-Nets, further improving the network’s understanding of complex scenario information. The proposed ADIPNet alleviates the insufficient feature extraction of infrared small targets. Compared with other state-of-the-art methods, mIoU respectively reaches 80.52% and 72.54% on two large infrared datasets. It achieves more accurate detection of small aerial targets with low operating cost, possessing potential application prospect in various infrared surveillance systems.&lt;/p&gt;</content:encoded></item><item><title>AgriFM: A multi-source temporal remote sensing foundation model for Agriculture mapping</title><link>https://doi.org/10.1016/j.rse.2026.115234</link><guid>10.1016/j.rse.2026.115234</guid><pubDate>Fri, 09 Jan 2026 11:12:22 +0000</pubDate><dc:creator>Wenyuan Li</dc:creator><dc:creator>Shunlin Liang</dc:creator><dc:creator>Keyan Chen</dc:creator><dc:creator>Yongzhe Chen</dc:creator><dc:creator>Han Ma</dc:creator><dc:creator>Jianglei Xu</dc:creator><dc:creator>Yichuan Ma</dc:creator><dc:creator>Yuxiang Zhang</dc:creator><dc:creator>Shikang Guan</dc:creator><dc:creator>Husheng Fang</dc:creator><dc:creator>Zhenwei Shi</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115234</prism:doi><description>Climate change and population growth intensify the demand for precise agriculture mapping to enhance food security. Such mapping tasks require robust modeling of multi-scale spatiotemporal patterns from fine field textures to landscape context, and from short-term phenology to full growing-season dynamics. Existing methods often process spatial and temporal features separately, limiting their ability to capture essential agricultural dynamics. While transformer-based remote sensing foundation models (RSFMs) offer unified spatiotemporal modeling ability, most of them remain suboptimal: they either use fixed windows that ignore multi-scale crop characteristics or neglect temporal information entirely. To address these gaps, we propose AgriFM, a multi-source, multi-temporal foundation model for agriculture mapping. AgriFM introduces a synchronized spatiotemporal downsampling strategy within a Video Swin Transformer backbone, enabling efficient handling of long and variable-length satellite time series while preserving multi-scale spatial and phenological information. It is pre-trained on a globally representative dataset comprising over 25 million samples from MODIS, Landsat-8/9, and Sentinel-2 with land cover fractions as pre-training supervision. AgriFM further integrates a versatile decoder specifically designed to dynamically fuse multi-source features from different stages of backbone and accommodate varying temporal lengths, thereby supporting consistent and scalable agriculture mapping across diverse satellite sources and task requirements. It supports diverse tasks including agricultural land mapping, field boundary delineation, agricultural land use/land cover mapping, and specific crop mapping (e.g., winter wheat and paddy rice) with different data sources. Comprehensive evaluations show that AgriFM consistently outperforms existing deep learning models and general-purpose RSFMs across multiple agriculture mapping tasks. Codes and models are available at https://github.com/flyakon/AgriFM and https://glass.hku.hk
Published: 2026-01-09T11:12:22+00:00
Venue: Remote Sensing of Environment
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenyuan Li; Shunlin Liang; Keyan Chen; Yongzhe Chen; Han Ma; Jianglei Xu; Yichuan Ma; Yuxiang Zhang; Shikang Guan; Husheng Fang; Zhenwei Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115234"&gt;10.1016/j.rse.2026.115234&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Climate change and population growth intensify the demand for precise agriculture mapping to enhance food security. Such mapping tasks require robust modeling of multi-scale spatiotemporal patterns from fine field textures to landscape context, and from short-term phenology to full growing-season dynamics. Existing methods often process spatial and temporal features separately, limiting their ability to capture essential agricultural dynamics. While transformer-based remote sensing foundation models (RSFMs) offer unified spatiotemporal modeling ability, most of them remain suboptimal: they either use fixed windows that ignore multi-scale crop characteristics or neglect temporal information entirely. To address these gaps, we propose AgriFM, a multi-source, multi-temporal foundation model for agriculture mapping. AgriFM introduces a synchronized spatiotemporal downsampling strategy within a Video Swin Transformer backbone, enabling efficient handling of long and variable-length satellite time series while preserving multi-scale spatial and phenological information. It is pre-trained on a globally representative dataset comprising over 25 million samples from MODIS, Landsat-8/9, and Sentinel-2 with land cover fractions as pre-training supervision. AgriFM further integrates a versatile decoder specifically designed to dynamically fuse multi-source features from different stages of backbone and accommodate varying temporal lengths, thereby supporting consistent and scalable agriculture mapping across diverse satellite sources and task requirements. It supports diverse tasks including agricultural land mapping, field boundary delineation, agricultural land use/land cover mapping, and specific crop mapping (e.g., winter wheat and paddy rice) with different data sources. Comprehensive evaluations show that AgriFM consistently outperforms existing deep learning models and general-purpose RSFMs across multiple agriculture mapping tasks. Codes and models are available at https://github.com/flyakon/AgriFM and https://glass.hku.hk&lt;/p&gt;</content:encoded></item><item><title>Delving into Pre-training for Domain Transfer: A Broad Study of Pre-training for Domain Generalization and Domain Adaptation</title><link>https://doi.org/10.1007/s11263-025-02590-5</link><guid>10.1007/s11263-025-02590-5</guid><pubDate>Fri, 09 Jan 2026 18:22:43 +0000</pubDate><dc:creator>Jungmyung Wi</dc:creator><dc:creator>Youngkyun Jang</dc:creator><dc:creator>Dujin Lee</dc:creator><dc:creator>Myeongseok Nam</dc:creator><dc:creator>Donghyun Kim</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02590-5</prism:doi><description>As deep learning models suffer from domain shifts, domain transfer methods have been developed to learn robust and reliable feature representations on unseen domains. Existing domain transfer methods, such as domain adaptation and domain generalization, focused on developing new adaptation or alignment algorithms, typically utilizing outdated ResNet backbones pre-trained on ImageNet-1K. However, the impact of recent pre-training approaches on domain transfer has not been thoroughly investigated. In this work, we provide a broad study and in-depth analysis of pre-training for domain adaptation and generalization from four distinct perspectives; network architectures, sizes, pre-training objectives, and pre-training datasets. Our extensive experiments cover a variety of domain transfer settings, including domain generalization, unsupervised domain adaptation, source free domain adaptation, and universal domain adaptation. Our study reveals two key findings: (1) state-of-the-art pre-training has a greater impact on performance than advanced generalization or adaptation techniques, (2) domain adaptation baselines tend to overfit to older pre-training backbones, indicating that top-performing methods under previous settings may no longer be optimal with modern pre-training, and (3) these trends are also observed in other tasks, such as object detection and semantic segmentation. Furthermore, we investigate what makes pre-training effective for domain transfer. Interestingly, our findings suggest that the performance gains are largely due to the presence of a significantly higher number of classes in recent pre-training datasets (e.g., ImageNet-22K) that closely resemble those in downstream tasks, rather than solely the result of large-scale data. In addition, we examine potential train/test contamination between web-scale pre-training datasets and downstream benchmarks and find that such data leakage has only a negligible impact on evaluation. We hope this work highlights the importance of pre-training for domain transfer and offers valuable insights for future domain transfer research.
Published: 2026-01-09T18:22:43+00:00
Venue: International Journal of Computer Vision
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jungmyung Wi; Youngkyun Jang; Dujin Lee; Myeongseok Nam; Donghyun Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02590-5"&gt;10.1007/s11263-025-02590-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;As deep learning models suffer from domain shifts, domain transfer methods have been developed to learn robust and reliable feature representations on unseen domains. Existing domain transfer methods, such as domain adaptation and domain generalization, focused on developing new adaptation or alignment algorithms, typically utilizing outdated ResNet backbones pre-trained on ImageNet-1K. However, the impact of recent pre-training approaches on domain transfer has not been thoroughly investigated. In this work, we provide a broad study and in-depth analysis of pre-training for domain adaptation and generalization from four distinct perspectives; network architectures, sizes, pre-training objectives, and pre-training datasets. Our extensive experiments cover a variety of domain transfer settings, including domain generalization, unsupervised domain adaptation, source free domain adaptation, and universal domain adaptation. Our study reveals two key findings: (1) state-of-the-art pre-training has a greater impact on performance than advanced generalization or adaptation techniques, (2) domain adaptation baselines tend to overfit to older pre-training backbones, indicating that top-performing methods under previous settings may no longer be optimal with modern pre-training, and (3) these trends are also observed in other tasks, such as object detection and semantic segmentation. Furthermore, we investigate what makes pre-training effective for domain transfer. Interestingly, our findings suggest that the performance gains are largely due to the presence of a significantly higher number of classes in recent pre-training datasets (e.g., ImageNet-22K) that closely resemble those in downstream tasks, rather than solely the result of large-scale data. In addition, we examine potential train/test contamination between web-scale pre-training datasets and downstream benchmarks and find that such data leakage has only a negligible impact on evaluation. We hope this work highlights the importance of pre-training for domain transfer and offers valuable insights for future domain transfer research.&lt;/p&gt;</content:encoded></item><item><title>D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images</title><link>https://arxiv.org/abs/2601.02747v1</link><guid>http://arxiv.org/abs/2601.02747v1</guid><pubDate>Tue, 06 Jan 2026 06:21:50 +0000</pubDate><dc:creator>Zixiao Wen</dc:creator><dc:creator>Zhen Yang</dc:creator><dc:creator>Xianjie Bao</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Xiantai Xiang</dc:creator><dc:creator>Wenshuai Li</dc:creator><dc:creator>Yuhan Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.
Published: 2026-01-06T06:21:50+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixiao Wen; Zhen Yang; Xianjie Bao; Lei Zhang; Xiantai Xiang; Wenshuai Li; Yuhan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.&lt;/p&gt;</content:encoded></item><item><title>X-CLPA: A Contrastive Learning and Prototypical Alignment-based Crossmodal Remote Sensing Image Retrieval</title><link>https://doi.org/10.1016/j.eswa.2026.131169</link><guid>10.1016/j.eswa.2026.131169</guid><pubDate>Fri, 09 Jan 2026 08:03:00 +0000</pubDate><dc:creator>Aparna H</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:creator>Avik Hati</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131169</prism:doi><description>Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.
Published: 2026-01-09T08:03:00+00:00
Venue: Expert Systems with Applications
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aparna H; Biplab Banerjee; Avik Hati&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131169"&gt;10.1016/j.eswa.2026.131169&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Information retrieval across modalities has become crucial in remote sensing (RS) with ever-growing multimodal satellite data and the requirement of multiple perspectives in RS scene analysis. These techniques are often helpful in disaster management and land use mapping as they can retrieve information from different time periods and geographical locations. However, crossmodal image retrieval comes with its own set of challenges like variation in statistical properties and heterogeneity gap. This paper proposes a novel crossmodal image retrieval algorithm based on contrastive learning and prototypical alignment (X-CLPA) which aims to overcome the above challenges and is capable of retrieving images from multiple modalities like optical, multispectral (MS), synthetic aperture radar (SAR), and panchromatic (PAN). The backbone network is a hybrid ResNet module that can capture salient features, and an attention based context-aware pooling is performed to identify interpixel relationships. A novel combination of objective functions – the crossmodal contrastive loss which helps to distinctively identify classes across modalities, and the crossmodal prototype alignment loss which aligns the modalities in a common feature space – is used. The algorithm is tested on EuroSAT, DSRSID and TUM datasets, and the model is able to achieve 98.69% mAP in the DSRSID dataset, 95.96% in EuroSAT dataset (RGB, MS), and 99.31% in TUM dataset. Extensive experiments were conducted to further test the efficacy of the model, and the results show that X-CLPA exhibits superior overall performance.&lt;/p&gt;</content:encoded></item><item><title>夜间无人机航拍图像目标检测与跟踪方法研究进展</title><link>https://doi.org/10.11834/jig.250459</link><guid>10.11834/jig.250459</guid><pubDate>Thu, 08 Jan 2026 03:05:09 +0000</pubDate><dc:creator>Bi Shifan</dc:creator><dc:creator>Ye Liang</dc:creator><dc:creator>Wang Zhixiang</dc:creator><dc:creator>Zhang Ziyang</dc:creator><dc:creator>Hong Hanyu</dc:creator><dc:creator>Sang Nong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250459</prism:doi><description>视觉目标检测与跟踪技术已在白天场景中取得显著突破，为无人机（unmanned aerial vehicle，UAV）在智能领域的广泛应用提供了强大支撑。然而，这些方法在夜间场景下往往表现不佳，检测与跟踪精度显著下降。夜间作为无人机应用中不可或缺的场景，其复杂性与挑战性凸显了开展针对夜间无人机航拍图像目标检测与跟踪研究的必要性和现实意义。针对夜间无人机目标航拍图像检测与跟踪技术的现状及发展趋势，本文分析了感知能力有限、可视化特征不足、硬件平台资源受限以及复杂成像条件等因素所带来的挑战。从夜间无人机航拍图像目标检测研究出发，综述了夜间图像增强、域适应学习、多模态感知融合和轻量化模型等方法的研究进展。在夜间无人机航拍图像目标跟踪方面，重点综述了基于深度学习的五类范式，包括先增强后跟踪、域自适应、视觉提示学习、课程学习和多模态融合，系统总结了相关方法的优缺点及所应对的挑战。随后，介绍了夜间及全天候无人机航拍图像目标检测与跟踪常用的评价指标与典型数据集，并在构建的夜间无人机车辆目标检测集DroneVehicle-Night上进行性能评估与对比分析；同时，从VisDrone2019的测试集中筛选昼夜样本，对现有检测方法的夜间适应性进行了对比测试；此外，还汇总了包含四类跟踪范式在内的20种算法在夜间无人机航拍图像目标跟踪数据集UAVDark135与NAT2021上的性能评估结果。最后，对夜间无人机航拍图像目标检测与跟踪未来的发展方向进行了展望，为该领域的后续研究提供参考。本文实验所用到的算法、构建的数据集已经汇总至https：//github.com/bsfsf/DroneVehicle-Night和https：//doi.org/10.57760/sciencedb.32435以便后续研究者使用。
Published: 2026-01-08T03:05:09+00:00
Venue: Journal of Image and Graphics
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bi Shifan; Ye Liang; Wang Zhixiang; Zhang Ziyang; Hong Hanyu; Sang Nong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250459"&gt;10.11834/jig.250459&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;视觉目标检测与跟踪技术已在白天场景中取得显著突破，为无人机（unmanned aerial vehicle，UAV）在智能领域的广泛应用提供了强大支撑。然而，这些方法在夜间场景下往往表现不佳，检测与跟踪精度显著下降。夜间作为无人机应用中不可或缺的场景，其复杂性与挑战性凸显了开展针对夜间无人机航拍图像目标检测与跟踪研究的必要性和现实意义。针对夜间无人机目标航拍图像检测与跟踪技术的现状及发展趋势，本文分析了感知能力有限、可视化特征不足、硬件平台资源受限以及复杂成像条件等因素所带来的挑战。从夜间无人机航拍图像目标检测研究出发，综述了夜间图像增强、域适应学习、多模态感知融合和轻量化模型等方法的研究进展。在夜间无人机航拍图像目标跟踪方面，重点综述了基于深度学习的五类范式，包括先增强后跟踪、域自适应、视觉提示学习、课程学习和多模态融合，系统总结了相关方法的优缺点及所应对的挑战。随后，介绍了夜间及全天候无人机航拍图像目标检测与跟踪常用的评价指标与典型数据集，并在构建的夜间无人机车辆目标检测集DroneVehicle-Night上进行性能评估与对比分析；同时，从VisDrone2019的测试集中筛选昼夜样本，对现有检测方法的夜间适应性进行了对比测试；此外，还汇总了包含四类跟踪范式在内的20种算法在夜间无人机航拍图像目标跟踪数据集UAVDark135与NAT2021上的性能评估结果。最后，对夜间无人机航拍图像目标检测与跟踪未来的发展方向进行了展望，为该领域的后续研究提供参考。本文实验所用到的算法、构建的数据集已经汇总至https：//github.com/bsfsf/DroneVehicle-Night和https：//doi.org/10.57760/sciencedb.32435以便后续研究者使用。&lt;/p&gt;</content:encoded></item><item><title>轻量级稀疏置换自注意力图像超分辨率网络</title><link>https://doi.org/10.11834/jig.250519</link><guid>10.11834/jig.250519</guid><pubDate>Thu, 08 Jan 2026 03:05:30 +0000</pubDate><dc:creator>Wu Siqi</dc:creator><dc:creator>Liu Wei</dc:creator><dc:creator>Chen Weidong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250519</prism:doi><description>目的图像超分辨重建是计算机视觉领域中的一个典型低层视觉任务，能够为目标检测、图像分割等高层任务提供更清晰更结构化的输入。基于CNN的图像超分辨率模型注重恢复图像的纹理和边缘信息，而基于Transformer的方法能建模全局上下文信息，但是存在注意力权重冗余问题。针对这两种模型的优缺点，本文设计了一种轻量级图像超分辨率网络。方法首先改进了传统的Transformer，提出了一种稀疏置换自注意力机制，在扩大窗口的同时解决冗余问题。在此基础上，我们基于CNN构建高频信息增强模块加强模型对局部细节信息的重建。在得到两种结构提取的特征后，我们提出一种双分支特征融合模块对全局特征和局部特征进行高效融合。结果本文方法在5个公开数据集上与11种先进超分辨方法进行了对比实验。结果表明，在保证模型轻量化的前提下，稀疏置换自注意力网络（Sparse and Permuted Self-Attention Network，SPSANet）在不同放大倍率和数据集上均取得最优或次优性能。当放大倍率为3时，在Urban100和Manga109数据集上的峰值信噪比（peak signal-to-noise ratio，PSNR）分别较最新的SOTA（state of the art）方法提升了0.15dB和0.25dB。主观视觉效果显示，SPSANet在复杂纹理和细节丰富的场景中重建的图像更加清晰、自然。结论本文提出的轻量级稀疏置换自注意力图像超分辨率网络能够在保持较低参数量与计算复杂度的同时，在多个数据集上取得优异的重建效果，展现出良好的泛化性与应用价值。
Published: 2026-01-08T03:05:30+00:00
Venue: Journal of Image and Graphics
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wu Siqi; Liu Wei; Chen Weidong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250519"&gt;10.11834/jig.250519&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;目的图像超分辨重建是计算机视觉领域中的一个典型低层视觉任务，能够为目标检测、图像分割等高层任务提供更清晰更结构化的输入。基于CNN的图像超分辨率模型注重恢复图像的纹理和边缘信息，而基于Transformer的方法能建模全局上下文信息，但是存在注意力权重冗余问题。针对这两种模型的优缺点，本文设计了一种轻量级图像超分辨率网络。方法首先改进了传统的Transformer，提出了一种稀疏置换自注意力机制，在扩大窗口的同时解决冗余问题。在此基础上，我们基于CNN构建高频信息增强模块加强模型对局部细节信息的重建。在得到两种结构提取的特征后，我们提出一种双分支特征融合模块对全局特征和局部特征进行高效融合。结果本文方法在5个公开数据集上与11种先进超分辨方法进行了对比实验。结果表明，在保证模型轻量化的前提下，稀疏置换自注意力网络（Sparse and Permuted Self-Attention Network，SPSANet）在不同放大倍率和数据集上均取得最优或次优性能。当放大倍率为3时，在Urban100和Manga109数据集上的峰值信噪比（peak signal-to-noise ratio，PSNR）分别较最新的SOTA（state of the art）方法提升了0.15dB和0.25dB。主观视觉效果显示，SPSANet在复杂纹理和细节丰富的场景中重建的图像更加清晰、自然。结论本文提出的轻量级稀疏置换自注意力图像超分辨率网络能够在保持较低参数量与计算复杂度的同时，在多个数据集上取得优异的重建效果，展现出良好的泛化性与应用价值。&lt;/p&gt;</content:encoded></item><item><title>Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection</title><link>https://arxiv.org/abs/2601.04381v1</link><guid>http://arxiv.org/abs/2601.04381v1</guid><pubDate>Wed, 07 Jan 2026 20:41:26 +0000</pubDate><dc:creator>Maxim Clouser</dc:creator><dc:creator>Kia Khezeli</dc:creator><dc:creator>John Kalantari</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.
Published: 2026-01-07T20:41:26+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maxim Clouser; Kia Khezeli; John Kalantari&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.&lt;/p&gt;</content:encoded></item><item><title>SCMT-Net: Spatial Curvature and Motion Temporal Feature Synergy Network for Multi-Frame Infrared Small Target Detection</title><link>https://doi.org/10.3390/rs18020215</link><guid>10.3390/rs18020215</guid><pubDate>Fri, 09 Jan 2026 11:45:33 +0000</pubDate><dc:creator>Ruiqi Yang</dc:creator><dc:creator>Yuan Liu</dc:creator><dc:creator>Ming Zhu</dc:creator><dc:creator>Huiping Zhu</dc:creator><dc:creator>Yuanfu Yuan</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020215</prism:doi><description>Infrared small target (IRST) detection remains a challenging task due to extremely small target sizes, low signal-to-noise ratios (SNR), and complex background clutter. Existing methods often fail to balance reliable detection with low false alarm rates due to limited spatial–temporal modeling. To address this, we propose a multi-frame network that synergistically integrates spatial curvature and temporal motion consistency. Specifically, in the single-frame stage, a Gaussian Curvature Attention (GCA) module is introduced to exploit spatial curvature and geometric saliency, enhancing the discriminability of weak targets. In the multi-frame stage, a Motion-Aware Encoding Block (MAEB) utilizes MotionPool3D to capture temporal motion consistency and extract salient motion regions, while a Temporal Consistency Enhancement Module (TCEM) further refines cross-frame features to effectively suppress noise. Extensive experiments demonstrate that the proposed method achieves advanced overall performance. In particular, under low-SNR conditions, the method improves the detection rate by 0.29% while maintaining a low false alarm rate, providing an effective solution for the stable detection of weak and small targets.
Published: 2026-01-09T11:45:33+00:00
Venue: Remote Sensing
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiqi Yang; Yuan Liu; Ming Zhu; Huiping Zhu; Yuanfu Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020215"&gt;10.3390/rs18020215&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target (IRST) detection remains a challenging task due to extremely small target sizes, low signal-to-noise ratios (SNR), and complex background clutter. Existing methods often fail to balance reliable detection with low false alarm rates due to limited spatial–temporal modeling. To address this, we propose a multi-frame network that synergistically integrates spatial curvature and temporal motion consistency. Specifically, in the single-frame stage, a Gaussian Curvature Attention (GCA) module is introduced to exploit spatial curvature and geometric saliency, enhancing the discriminability of weak targets. In the multi-frame stage, a Motion-Aware Encoding Block (MAEB) utilizes MotionPool3D to capture temporal motion consistency and extract salient motion regions, while a Temporal Consistency Enhancement Module (TCEM) further refines cross-frame features to effectively suppress noise. Extensive experiments demonstrate that the proposed method achieves advanced overall performance. In particular, under low-SNR conditions, the method improves the detection rate by 0.29% while maintaining a low false alarm rate, providing an effective solution for the stable detection of weak and small targets.&lt;/p&gt;</content:encoded></item><item><title>Research on Infrared Small Target Detection Technology Based on DCS-YOLO Algorithm</title><link>https://doi.org/10.1016/j.dsp.2026.105898</link><guid>10.1016/j.dsp.2026.105898</guid><pubDate>Fri, 09 Jan 2026 00:18:13 +0000</pubDate><dc:creator>Meng Yin</dc:creator><dc:creator>Binghe Sun</dc:creator><dc:creator>Rugang Wang</dc:creator><dc:creator>Yuanyuan Wang</dc:creator><dc:creator>Feng Zhou</dc:creator><dc:creator>Xuesheng Bian</dc:creator><prism:publicationName>Digital Signal Processing</prism:publicationName><prism:doi>10.1016/j.dsp.2026.105898</prism:doi><description>To address the challenges of weak features, susceptibility to complex background interference in infrared small targets, and the high computational cost of existing specialized detection models, this paper proposes the Dual-Domain Fusion and Class-Aware Self-supervised YOLO (DCS-YOLO). This framework leverages dual-domain feature fusion and class-aware self-supervised learning for semantic enhancement. During feature extraction, a Class-aware Self-supervised Semantic Fusion Module (CSSFM) utilizes a class-aware self-supervised architecture as a deep semantic guide for generating discriminative semantic features, thereby enhancing the perception of faint target characteristics. Additionally, a Dual-domain Aware Enhancement Module (A2C2f_DDA) is designed, which analyzes the high-frequency components of small targets and employs a spatial-frequency domain feature complementary fusion strategy to sharpen feature capture while suppressing background clutter. For feature upsampling and fusion, a Multi-dimensional Selective Feature Pyramid Network (MSFPN) employs a frequency-domain, spatial, and channel three-dimensional cooperative selection mechanism, integrated with deep semantic information, to enhance feature integration across dimensions and improve detection performance in complex scenes. Furthermore, lightweight components including GSConv, VoVGSCSP, and LSCD-Detect are incorporated to reduce computational complexity and model parameters. Comprehensive evaluations on the IRSTD-1K, RealScene-ISTD, and SIRST-v2 datasets demonstrate the effectiveness of the proposed algorithm, achieving mAP@0.5 scores of 80.7%, 90.2%, and 93.3%, respectively. The results indicate that the algorithm effectively utilizes frequency-domain analysis and semantic enhancement, providing a powerful and efficient solution for infrared small target detection in complex scenarios while maintaining a favorable balance between accuracy and computational cost.
Published: 2026-01-09T00:18:13+00:00
Venue: Digital Signal Processing
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meng Yin; Binghe Sun; Rugang Wang; Yuanyuan Wang; Feng Zhou; Xuesheng Bian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Digital Signal Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.dsp.2026.105898"&gt;10.1016/j.dsp.2026.105898&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;To address the challenges of weak features, susceptibility to complex background interference in infrared small targets, and the high computational cost of existing specialized detection models, this paper proposes the Dual-Domain Fusion and Class-Aware Self-supervised YOLO (DCS-YOLO). This framework leverages dual-domain feature fusion and class-aware self-supervised learning for semantic enhancement. During feature extraction, a Class-aware Self-supervised Semantic Fusion Module (CSSFM) utilizes a class-aware self-supervised architecture as a deep semantic guide for generating discriminative semantic features, thereby enhancing the perception of faint target characteristics. Additionally, a Dual-domain Aware Enhancement Module (A2C2f_DDA) is designed, which analyzes the high-frequency components of small targets and employs a spatial-frequency domain feature complementary fusion strategy to sharpen feature capture while suppressing background clutter. For feature upsampling and fusion, a Multi-dimensional Selective Feature Pyramid Network (MSFPN) employs a frequency-domain, spatial, and channel three-dimensional cooperative selection mechanism, integrated with deep semantic information, to enhance feature integration across dimensions and improve detection performance in complex scenes. Furthermore, lightweight components including GSConv, VoVGSCSP, and LSCD-Detect are incorporated to reduce computational complexity and model parameters. Comprehensive evaluations on the IRSTD-1K, RealScene-ISTD, and SIRST-v2 datasets demonstrate the effectiveness of the proposed algorithm, achieving mAP@0.5 scores of 80.7%, 90.2%, and 93.3%, respectively. The results indicate that the algorithm effectively utilizes frequency-domain analysis and semantic enhancement, providing a powerful and efficient solution for infrared small target detection in complex scenarios while maintaining a favorable balance between accuracy and computational cost.&lt;/p&gt;</content:encoded></item><item><title>Region-based Deep Metric Learning for Tackling Class Overlap in Online Semi-Supervised Data Stream Classification</title><link>https://doi.org/10.1016/j.inffus.2026.104126</link><guid>10.1016/j.inffus.2026.104126</guid><pubDate>Fri, 09 Jan 2026 16:48:04 +0000</pubDate><dc:creator>Zhonglin Wu</dc:creator><dc:creator>Hongliang Wang</dc:creator><dc:creator>Tongze Zhang</dc:creator><dc:creator>Hongyuan Liu</dc:creator><dc:creator>Jinxia Guo</dc:creator><dc:creator>Qinli Yang</dc:creator><dc:creator>Junming Shao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104126</prism:doi><description>Class overlap in data streams presents a significant challenge for real-time classification, particularly when confronted with the high dimensionality and evolving distributions inherent in such streams. Traditional classification methods, typically designed for static datasets, struggle to adapt to the dynamic nature of data streams, where both high-dimensional feature spaces and class imbalance exacerbate the complexity of classifying overlapping regions. In this paper, we propose a novel deep metric learning framework specifically tailored to address the challenges of class overlap in high-dimensional data streams. Our approach introduces two key innovations. First, we develop a multi-anchor sample mining mechanism based on neighborhood rough set theory, which partitions the data into non-overlapping and overlapping regions. By utilizing region-specific triplet-margin losses and hinge embedding loss, we construct a more refined discriminative metric space that significantly enhances the separation of overlapping classes. Furthermore, we introduce a dynamic, density-aware real-time label propagation mechanism with class-imbalance compensation. This component integrates real-time distribution estimation with a nonlinear adaptive threshold controller, enabling dual adaptivity: (1) dynamically re-weighting density contributions via inverse-frequency scaling to mitigate the dominance of majority classes and (2) adjusting threshold boundaries for frequent classes while relaxing propagation criteria for rare classes through nonlinear adjustments. Empirical evaluations on both synthetic and real-world data streams demonstrate that our method not only improves balanced accuracy but also enhances robustness in the presence of class overlap and class imbalance, outperforming state-of-the-art techniques.
Published: 2026-01-09T16:48:04+00:00
Venue: Information Fusion
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhonglin Wu; Hongliang Wang; Tongze Zhang; Hongyuan Liu; Jinxia Guo; Qinli Yang; Junming Shao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104126"&gt;10.1016/j.inffus.2026.104126&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Class overlap in data streams presents a significant challenge for real-time classification, particularly when confronted with the high dimensionality and evolving distributions inherent in such streams. Traditional classification methods, typically designed for static datasets, struggle to adapt to the dynamic nature of data streams, where both high-dimensional feature spaces and class imbalance exacerbate the complexity of classifying overlapping regions. In this paper, we propose a novel deep metric learning framework specifically tailored to address the challenges of class overlap in high-dimensional data streams. Our approach introduces two key innovations. First, we develop a multi-anchor sample mining mechanism based on neighborhood rough set theory, which partitions the data into non-overlapping and overlapping regions. By utilizing region-specific triplet-margin losses and hinge embedding loss, we construct a more refined discriminative metric space that significantly enhances the separation of overlapping classes. Furthermore, we introduce a dynamic, density-aware real-time label propagation mechanism with class-imbalance compensation. This component integrates real-time distribution estimation with a nonlinear adaptive threshold controller, enabling dual adaptivity: (1) dynamically re-weighting density contributions via inverse-frequency scaling to mitigate the dominance of majority classes and (2) adjusting threshold boundaries for frequent classes while relaxing propagation criteria for rare classes through nonlinear adjustments. Empirical evaluations on both synthetic and real-world data streams demonstrate that our method not only improves balanced accuracy but also enhances robustness in the presence of class overlap and class imbalance, outperforming state-of-the-art techniques.&lt;/p&gt;</content:encoded></item><item><title>AFR-CR: An Adaptive Frequency Domain Feature Reconstruction-Based Method for Cloud Removal via SAR-Assisted Remote Sensing Image Fusion</title><link>https://doi.org/10.3390/rs18020201</link><guid>10.3390/rs18020201</guid><pubDate>Thu, 08 Jan 2026 09:01:29 +0000</pubDate><dc:creator>Xiufang Zhou</dc:creator><dc:creator>Qirui Fang</dc:creator><dc:creator>Xunqiang Gong</dc:creator><dc:creator>Shuting Yang</dc:creator><dc:creator>Tieding Lu</dc:creator><dc:creator>Yuting Wan</dc:creator><dc:creator>Ailong Ma</dc:creator><dc:creator>Yanfei Zhong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020201</prism:doi><description>Optical imagery is often contaminated by clouds to varying degrees, which greatly affects the interpretation and analysis of images. Synthetic Aperture Radar (SAR) possesses the characteristic of penetrating clouds and mist, and a common strategy in SAR-assisted cloud removal involves fusing SAR and optical data and leveraging deep learning networks to reconstruct cloud-free optical imagery. However, these methods do not fully consider the characteristics of the frequency domain when processing feature integration, resulting in blurred edges of the generated cloudless optical images. Therefore, an adaptive frequency domain feature reconstruction-based cloud removal method is proposed to solve the problem. The proposed method comprises four key sequential stages. First, shallow features are extracted by fusing optical and SAR images. Second, a Transformer-based encoder captures multi-scale semantic features. Subsequently, the Frequency Domain Decoupling Module (FDDM) is employed. Utilizing a Dynamic Mask Generation mechanism, it explicitly decomposes features into low-frequency structures and high-frequency details, effectively suppressing cloud interference while preserving surface textures. Finally, robust information interaction is facilitated by the Cross-Frequency Reconstruction Module (CFRM) via transposed cross-attention, ensuring precise fusion and reconstruction. Experimental evaluation on the M3R-CR dataset confirms that the proposed approach achieves the best results on all four evaluated metrics, surpassing the performance of the eight other State-of-the-Art methods. It has demonstrated its effectiveness and advanced capabilities in the task of SAR-optical fusion for cloud removal.
Published: 2026-01-08T09:01:29+00:00
Venue: Remote Sensing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiufang Zhou; Qirui Fang; Xunqiang Gong; Shuting Yang; Tieding Lu; Yuting Wan; Ailong Ma; Yanfei Zhong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020201"&gt;10.3390/rs18020201&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Optical imagery is often contaminated by clouds to varying degrees, which greatly affects the interpretation and analysis of images. Synthetic Aperture Radar (SAR) possesses the characteristic of penetrating clouds and mist, and a common strategy in SAR-assisted cloud removal involves fusing SAR and optical data and leveraging deep learning networks to reconstruct cloud-free optical imagery. However, these methods do not fully consider the characteristics of the frequency domain when processing feature integration, resulting in blurred edges of the generated cloudless optical images. Therefore, an adaptive frequency domain feature reconstruction-based cloud removal method is proposed to solve the problem. The proposed method comprises four key sequential stages. First, shallow features are extracted by fusing optical and SAR images. Second, a Transformer-based encoder captures multi-scale semantic features. Subsequently, the Frequency Domain Decoupling Module (FDDM) is employed. Utilizing a Dynamic Mask Generation mechanism, it explicitly decomposes features into low-frequency structures and high-frequency details, effectively suppressing cloud interference while preserving surface textures. Finally, robust information interaction is facilitated by the Cross-Frequency Reconstruction Module (CFRM) via transposed cross-attention, ensuring precise fusion and reconstruction. Experimental evaluation on the M3R-CR dataset confirms that the proposed approach achieves the best results on all four evaluated metrics, surpassing the performance of the eight other State-of-the-Art methods. It has demonstrated its effectiveness and advanced capabilities in the task of SAR-optical fusion for cloud removal.&lt;/p&gt;</content:encoded></item><item><title>AT-adapter: Leveraging attribute knowledge of CLIP for few-shot classification</title><link>https://doi.org/10.1016/j.neucom.2026.132627</link><guid>10.1016/j.neucom.2026.132627</guid><pubDate>Thu, 08 Jan 2026 16:17:22 +0000</pubDate><dc:creator>Yonghyeon Jo</dc:creator><dc:creator>Janghyun Kim</dc:creator><dc:creator>ChanIll Park</dc:creator><dc:creator>Seonghoon Choi</dc:creator><dc:creator>Jin-Woo Lee</dc:creator><dc:creator>Jinsun Park</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132627</prism:doi><description>The CLIP model has introduced a novel approach to training large-scale vision-language models. Recent few-shot classification works have explored various methods to leverage CLIP’s knowledge for learning. However, conventional approaches biasedly leverage CLIP knowledge from the perspective of classes. Consequently, they fall short of acquiring sufficient detailed information in the more intricate and diverse landscape of few-shot learning. To address this issue, we propose an AT-Adapter which consists of categorized attribute adapters exploiting not individual class information but only attribute information. The attribute information is obtained from a large-scale language model (i.e., GPT) which might be ignorant of some classes but still can provide generic attribute knowledge. The AT-Adapter configures the textual features of a small number of attributes as lightweight parameters, enabling the extraction of various features by leveraging CLIP’s attribute knowledge. The proposed method can be seamlessly integrated into existing few-shot classification models and provide attribute-based guidance for improving performance. Experimental results demonstrate that the proposed AT-Adapter achieves state-of-the-art performance on 10 benchmark datasets.
Published: 2026-01-08T16:17:22+00:00
Venue: Neurocomputing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yonghyeon Jo; Janghyun Kim; ChanIll Park; Seonghoon Choi; Jin-Woo Lee; Jinsun Park&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132627"&gt;10.1016/j.neucom.2026.132627&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;The CLIP model has introduced a novel approach to training large-scale vision-language models. Recent few-shot classification works have explored various methods to leverage CLIP’s knowledge for learning. However, conventional approaches biasedly leverage CLIP knowledge from the perspective of classes. Consequently, they fall short of acquiring sufficient detailed information in the more intricate and diverse landscape of few-shot learning. To address this issue, we propose an AT-Adapter which consists of categorized attribute adapters exploiting not individual class information but only attribute information. The attribute information is obtained from a large-scale language model (i.e., GPT) which might be ignorant of some classes but still can provide generic attribute knowledge. The AT-Adapter configures the textual features of a small number of attributes as lightweight parameters, enabling the extraction of various features by leveraging CLIP’s attribute knowledge. The proposed method can be seamlessly integrated into existing few-shot classification models and provide attribute-based guidance for improving performance. Experimental results demonstrate that the proposed AT-Adapter achieves state-of-the-art performance on 10 benchmark datasets.&lt;/p&gt;</content:encoded></item><item><title>AnyDepth: Depth Estimation Made Easy</title><link>https://arxiv.org/abs/2601.02760v1</link><guid>http://arxiv.org/abs/2601.02760v1</guid><pubDate>Tue, 06 Jan 2026 06:51:35 +0000</pubDate><dc:creator>Zeyu Ren</dc:creator><dc:creator>Zeyu Zhang</dc:creator><dc:creator>Wukai Li</dc:creator><dc:creator>Qingxiang Liu</dc:creator><dc:creator>Hao Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.
Published: 2026-01-06T06:51:35+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeyu Ren; Zeyu Zhang; Wukai Li; Qingxiang Liu; Hao Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Tucker Decomposition-based Progressive Model Compression for Convolutional Neural Networks</title><link>https://doi.org/10.1016/j.eswa.2026.131153</link><guid>10.1016/j.eswa.2026.131153</guid><pubDate>Thu, 08 Jan 2026 16:18:53 +0000</pubDate><dc:creator>Yaping He</dc:creator><dc:creator>Hao Wu</dc:creator><dc:creator>Xin Luo</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131153</prism:doi><description>Large-scale convolutional neural networks rely heavily on convolutional operations, where the tremendous parameters pose challenges for model deployment and execution in resource-constrained environments like in an end-device with limited computational power or storage. Low-rank approximation-based approaches are widely-utilized for neural network model compression. Unfortunately, existing methods of this kind fail in bridging the pre-trained weights and approximated results appropriately or selecting the approximation rank selection adaptively, leading to significant performance degradation. To address these vital issues, this paper proposes an Adaptive Tucker Decomposition-based Progressive Model Compression (ATD-PMC) method with the following three-fold ideas: 1) innovatively building a parallel structure for efficient representation of convolutional weight tensors; 2) presenting a degradation mechanism to gradually reduce the dependence on the pre-trained weights, thus enabling progressive compression; and 3) proposing a adaptive rank selection strategy based on 0-1 programming, thereby well-balancing the resultant model’s learning accuracy and compression ratio. Experimental results on five benchmark datasets demonstrate that compared with state-of-the-art compression schemes based on low-rank approximation, the proposed ATD-PMC method compresses a target neural network with the highest compression ratio as well keeps (or even slightly increases) its classification accuracy.
Published: 2026-01-08T16:18:53+00:00
Venue: Expert Systems with Applications
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaping He; Hao Wu; Xin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131153"&gt;10.1016/j.eswa.2026.131153&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale convolutional neural networks rely heavily on convolutional operations, where the tremendous parameters pose challenges for model deployment and execution in resource-constrained environments like in an end-device with limited computational power or storage. Low-rank approximation-based approaches are widely-utilized for neural network model compression. Unfortunately, existing methods of this kind fail in bridging the pre-trained weights and approximated results appropriately or selecting the approximation rank selection adaptively, leading to significant performance degradation. To address these vital issues, this paper proposes an Adaptive Tucker Decomposition-based Progressive Model Compression (ATD-PMC) method with the following three-fold ideas: 1) innovatively building a parallel structure for efficient representation of convolutional weight tensors; 2) presenting a degradation mechanism to gradually reduce the dependence on the pre-trained weights, thus enabling progressive compression; and 3) proposing a adaptive rank selection strategy based on 0-1 programming, thereby well-balancing the resultant model’s learning accuracy and compression ratio. Experimental results on five benchmark datasets demonstrate that compared with state-of-the-art compression schemes based on low-rank approximation, the proposed ATD-PMC method compresses a target neural network with the highest compression ratio as well keeps (or even slightly increases) its classification accuracy.&lt;/p&gt;</content:encoded></item><item><title>SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection</title><link>https://arxiv.org/abs/2601.04968v1</link><guid>http://arxiv.org/abs/2601.04968v1</guid><pubDate>Thu, 08 Jan 2026 14:16:11 +0000</pubDate><dc:creator>Maximilian Pittner</dc:creator><dc:creator>Joel Janai</dc:creator><dc:creator>Mario Faigle</dc:creator><dc:creator>Alexandru Paul Condurache</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.
Published: 2026-01-08T14:16:11+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maximilian Pittner; Joel Janai; Mario Faigle; Alexandru Paul Condurache&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment</title><link>https://arxiv.org/abs/2601.04571v1</link><guid>http://arxiv.org/abs/2601.04571v1</guid><pubDate>Thu, 08 Jan 2026 04:02:49 +0000</pubDate><dc:creator>Delong Zeng</dc:creator><dc:creator>Yuexiang Xie</dc:creator><dc:creator>Yaliang Li</dc:creator><dc:creator>Ying Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.
Published: 2026-01-08T04:02:49+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Delong Zeng; Yuexiang Xie; Yaliang Li; Ying Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.&lt;/p&gt;</content:encoded></item><item><title>Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection</title><link>https://arxiv.org/abs/2601.03617v1</link><guid>http://arxiv.org/abs/2601.03617v1</guid><pubDate>Wed, 07 Jan 2026 05:57:19 +0000</pubDate><dc:creator>Samson Oseiwe Ajadalu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.
Published: 2026-01-07T05:57:19+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Samson Oseiwe Ajadalu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.&lt;/p&gt;</content:encoded></item><item><title>Experimental Comparison of Light-Weight and Deep CNN Models Across Diverse Datasets</title><link>https://arxiv.org/abs/2601.03463v1</link><guid>http://arxiv.org/abs/2601.03463v1</guid><pubDate>Tue, 06 Jan 2026 23:22:22 +0000</pubDate><dc:creator>Md. Hefzul Hossain Papon</dc:creator><dc:creator>Shadman Rabby</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.
Published: 2026-01-06T23:22:22+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Md. Hefzul Hossain Papon; Shadman Rabby&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.&lt;/p&gt;</content:encoded></item><item><title>Mapping melliferous tree species in Kenya via one-class classification with hyperspectral unsupervised domain adaptation</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.028</link><guid>10.1016/j.isprsjprs.2025.12.028</guid><pubDate>Thu, 08 Jan 2026 20:15:41 +0000</pubDate><dc:creator>Zhaozhi Luo</dc:creator><dc:creator>Janne Heiskanen</dc:creator><dc:creator>Ilja Vuorinne</dc:creator><dc:creator>Ian Ocholla</dc:creator><dc:creator>Shiqi Zhang</dc:creator><dc:creator>Saana Järvinen</dc:creator><dc:creator>Xinyu Wang</dc:creator><dc:creator>Yanfei Zhong</dc:creator><dc:creator>Petri Pellikka</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.028</prism:doi><description>The beekeeping sector holds significant potential for livelihood diversification among the agropastoral communities in Kenya. Melliferous tree species play a critical role by providing essential nectar sources for bees. However, limited knowledge of their precise spatial distributions constrains the full development of beekeeping. One-class classification (OCC) offers a practical solution for detecting single target species without requiring extensive labeled data from other classes. Although existing OCC methods perform well in trained domains, the generalization capability to unseen domains remains limited due to domain shift. To address these challenges, this study proposes a hyperspectral unsupervised domain adaptation OCC framework (HyUDA-One) for tree species mapping using airborne hyperspectral imagery and laser scanning data. The spatial–spectral regularized pseudo-positive learning was designed to mitigate domain shift and improve model generalizability. The effectiveness of HyUDA-One was demonstrated by mapping three key melliferous tree species in two savanna landscapes in southern Kenya. The results show that HyUDA-One significantly improves performance in unlabeled domains. The F1-scores of 0.788, 0.845, and 0.768 were achieved for Senegalia mellifera , Vachellia tortilis , and Commiphora africana in the trained domain, respectively. In the untrained domain, the F1-scores of Senegalia mellifera and Vachellia tortilis were 0.756 and 0.884, respectively. The distribution maps revealed the spatial patterns of these melliferous tree species and the nectar source availability, offering an important reference for sustainable beekeeping development in savanna landscapes. Furthermore, the proposed framework can potentially be extended to other mapping applications, such as invasive species detection.
Published: 2026-01-08T20:15:41+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaozhi Luo; Janne Heiskanen; Ilja Vuorinne; Ian Ocholla; Shiqi Zhang; Saana Järvinen; Xinyu Wang; Yanfei Zhong; Petri Pellikka&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.028"&gt;10.1016/j.isprsjprs.2025.12.028&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;The beekeeping sector holds significant potential for livelihood diversification among the agropastoral communities in Kenya. Melliferous tree species play a critical role by providing essential nectar sources for bees. However, limited knowledge of their precise spatial distributions constrains the full development of beekeeping. One-class classification (OCC) offers a practical solution for detecting single target species without requiring extensive labeled data from other classes. Although existing OCC methods perform well in trained domains, the generalization capability to unseen domains remains limited due to domain shift. To address these challenges, this study proposes a hyperspectral unsupervised domain adaptation OCC framework (HyUDA-One) for tree species mapping using airborne hyperspectral imagery and laser scanning data. The spatial–spectral regularized pseudo-positive learning was designed to mitigate domain shift and improve model generalizability. The effectiveness of HyUDA-One was demonstrated by mapping three key melliferous tree species in two savanna landscapes in southern Kenya. The results show that HyUDA-One significantly improves performance in unlabeled domains. The F1-scores of 0.788, 0.845, and 0.768 were achieved for Senegalia mellifera , Vachellia tortilis , and Commiphora africana in the trained domain, respectively. In the untrained domain, the F1-scores of Senegalia mellifera and Vachellia tortilis were 0.756 and 0.884, respectively. The distribution maps revealed the spatial patterns of these melliferous tree species and the nectar source availability, offering an important reference for sustainable beekeeping development in savanna landscapes. Furthermore, the proposed framework can potentially be extended to other mapping applications, such as invasive species detection.&lt;/p&gt;</content:encoded></item><item><title>DGA-Net: Enhancing SAM with Depth Prompting and Graph-Anchor Guidance for Camouflaged Object Detection</title><link>https://arxiv.org/abs/2601.02831v1</link><guid>http://arxiv.org/abs/2601.02831v1</guid><pubDate>Tue, 06 Jan 2026 09:04:23 +0000</pubDate><dc:creator>Yuetong Li</dc:creator><dc:creator>Qing Zhang</dc:creator><dc:creator>Yilin Zhao</dc:creator><dc:creator>Gongyang Li</dc:creator><dc:creator>Zeming Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting" paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.
Published: 2026-01-06T09:04:23+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuetong Li; Qing Zhang; Yilin Zhao; Gongyang Li; Zeming Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;To fully exploit depth cues in Camouflaged Object Detection (COD), we present DGA-Net, a specialized framework that adapts the Segment Anything Model (SAM) via a novel ``depth prompting&amp;quot; paradigm. Distinguished from existing approaches that primarily rely on sparse prompts (e.g., points or boxes), our method introduces a holistic mechanism for constructing and propagating dense depth prompts. Specifically, we propose a Cross-modal Graph Enhancement (CGE) module that synthesizes RGB semantics and depth geometric within a heterogeneous graph to form a unified guidance signal. Furthermore, we design an Anchor-Guided Refinement (AGR) module. To counteract the inherent information decay in feature hierarchies, AGR forges a global anchor and establishes direct non-local pathways to broadcast this guidance from deep to shallow layers, ensuring precise and consistent segmentation. Quantitative and qualitative experimental results demonstrate that our proposed DGA-Net outperforms the state-of-the-art COD methods.&lt;/p&gt;</content:encoded></item><item><title>Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images</title><link>https://arxiv.org/abs/2601.04127v1</link><guid>http://arxiv.org/abs/2601.04127v1</guid><pubDate>Wed, 07 Jan 2026 17:41:11 +0000</pubDate><dc:creator>Leandro Stival</dc:creator><dc:creator>Ricardo da Silva Torres</dc:creator><dc:creator>Helio Pedrini</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on
Published: 2026-01-07T17:41:11+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Leandro Stival; Ricardo da Silva Torres; Helio Pedrini&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on&lt;/p&gt;</content:encoded></item><item><title>From Rays to Projections: Better Inputs for Feed-Forward View Synthesis</title><link>https://arxiv.org/abs/2601.05116v1</link><guid>http://arxiv.org/abs/2601.05116v1</guid><pubDate>Thu, 08 Jan 2026 17:03:44 +0000</pubDate><dc:creator>Zirui Wu</dc:creator><dc:creator>Zeren Jiang</dc:creator><dc:creator>Martin R. Oswald</dc:creator><dc:creator>Jie Song</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.
Published: 2026-01-08T17:03:44+00:00
Venue: arXiv
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zirui Wu; Zeren Jiang; Martin R. Oswald; Jie Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Attention mechanisms in neural networks</title><link>https://arxiv.org/abs/2601.03329v1</link><guid>http://arxiv.org/abs/2601.03329v1</guid><pubDate>Tue, 06 Jan 2026 17:12:10 +0000</pubDate><dc:creator>Hasi Hays</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and multimodal learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive transformers, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision Transformers for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.
Published: 2026-01-06T17:12:10+00:00
Venue: arXiv
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hasi Hays&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and multimodal learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive transformers, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision Transformers for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.&lt;/p&gt;</content:encoded></item><item><title>Noise-Robust Tiny Object Localization with Flows</title><link>https://doi.org/10.1016/j.patcog.2026.113041</link><guid>10.1016/j.patcog.2026.113041</guid><pubDate>Fri, 09 Jan 2026 00:01:17 +0000</pubDate><dc:creator>Huixin Sun</dc:creator><dc:creator>Linlin Yang</dc:creator><dc:creator>Ronyu Chen</dc:creator><dc:creator>Kerui Gu</dc:creator><dc:creator>Baochang Zhang</dc:creator><dc:creator>Angela Yao</dc:creator><dc:creator>Xianbin Cao</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113041</prism:doi><description>Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach’s effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.
Published: 2026-01-09T00:01:17+00:00
Venue: Pattern Recognition
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huixin Sun; Linlin Yang; Ronyu Chen; Kerui Gu; Baochang Zhang; Angela Yao; Xianbin Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113041"&gt;10.1016/j.patcog.2026.113041&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach’s effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.&lt;/p&gt;</content:encoded></item></channel></rss>