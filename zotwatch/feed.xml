<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 09 Jan 2026 02:58:26 +0000</lastBuildDate><item><title>微波与光学遥感图像联合目标检测与识别技术研究进展</title><link>https://doi.org/10.11834/jig.250648</link><guid>10.11834/jig.250648</guid><pubDate>Thu, 08 Jan 2026 03:05:32 +0000</pubDate><dc:creator>Yang Jian</dc:creator><dc:creator>Chen Jie</dc:creator><dc:creator>Xu Huaping</dc:creator><dc:creator>Wang Xiaoliang</dc:creator><dc:creator>You Ya’nan</dc:creator><dc:creator>Feng Xiao</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250648</prism:doi><description>随着对地观测技术的飞速发展，从海量遥感图像中快速准确地检测与识别特定目标，已成为环境监测、灾害评估及国防安全等领域的关键任务。光学图像和微波图像是最常见的遥感图像类型，将二者相结合进行联合目标检测与识别，可以优势互补，有效克服单一类型传感器获取目标信息的局限性，在突破单源遥感性能瓶颈、提升复杂环境下目标解译能力等方面具有重要价值与广阔应用前景。本文综述了微波与光学遥感图像联合目标检测与识别技术的研究进展。首先，概述了两类图像的特点以及联合目标检测与识别的一般处理流程。其次，深入剖析了该领域当前所面临的主要挑战：成像机理与特征表达的差异性、数据集规模与分辨率的不均衡性、数据获取的时空异步性以及复杂背景下的弱小目标检测与识别。在此基础上，重点围绕海洋与陆地两类典型应用环境，分别分析了当前的主流技术。在海洋应用领域，以海上舰船目标检测与识别为核心，讨论了基于特征融合的方法、知识驱动的方法、复杂场景下的方法以及基于尾迹的间接方法。在陆地应用领域，聚焦飞机、车辆、基础设施等关键目标，探讨了基于特征融合、知识迁移与蒸馏和复杂场景下的弱小目标检测与识别技术。此外，本文还梳理了该领域的常用性能评价指标与公开数据集资源，并对未来发展趋势进行了展望。
Published: 2026-01-08T03:05:32+00:00
Venue: Journal of Image and Graphics
Score: 0.826 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Jian; Chen Jie; Xu Huaping; Wang Xiaoliang; You Ya’nan; Feng Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250648"&gt;10.11834/jig.250648&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.826 (must_read)&lt;/p&gt;
&lt;p&gt;随着对地观测技术的飞速发展，从海量遥感图像中快速准确地检测与识别特定目标，已成为环境监测、灾害评估及国防安全等领域的关键任务。光学图像和微波图像是最常见的遥感图像类型，将二者相结合进行联合目标检测与识别，可以优势互补，有效克服单一类型传感器获取目标信息的局限性，在突破单源遥感性能瓶颈、提升复杂环境下目标解译能力等方面具有重要价值与广阔应用前景。本文综述了微波与光学遥感图像联合目标检测与识别技术的研究进展。首先，概述了两类图像的特点以及联合目标检测与识别的一般处理流程。其次，深入剖析了该领域当前所面临的主要挑战：成像机理与特征表达的差异性、数据集规模与分辨率的不均衡性、数据获取的时空异步性以及复杂背景下的弱小目标检测与识别。在此基础上，重点围绕海洋与陆地两类典型应用环境，分别分析了当前的主流技术。在海洋应用领域，以海上舰船目标检测与识别为核心，讨论了基于特征融合的方法、知识驱动的方法、复杂场景下的方法以及基于尾迹的间接方法。在陆地应用领域，聚焦飞机、车辆、基础设施等关键目标，探讨了基于特征融合、知识迁移与蒸馏和复杂场景下的弱小目标检测与识别技术。此外，本文还梳理了该领域的常用性能评价指标与公开数据集资源，并对未来发展趋势进行了展望。&lt;/p&gt;</content:encoded></item><item><title>Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2601.02837v1</link><guid>http://arxiv.org/abs/2601.02837v1</guid><pubDate>Tue, 06 Jan 2026 09:14:01 +0000</pubDate><dc:creator>Yuteng Liu</dc:creator><dc:creator>Duanni Meng</dc:creator><dc:creator>Maoxun Yuan</dc:creator><dc:creator>Xingxing Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.
Published: 2026-01-06T09:14:01+00:00
Venue: arXiv
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuteng Liu; Duanni Meng; Maoxun Yuan; Xingxing Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.&lt;/p&gt;</content:encoded></item><item><title>多分支感知与跨层语义融合的红外小目标检测</title><link>https://doi.org/10.11834/jig.250448</link><guid>10.11834/jig.250448</guid><pubDate>Thu, 08 Jan 2026 03:05:10 +0000</pubDate><dc:creator>Qian Menghao</dc:creator><dc:creator>Liu Kui</dc:creator><dc:creator>Zhang Fengbo</dc:creator><dc:creator>Su Benyue</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250448</prism:doi><description>目的红外小目标检测在军事和民用等领域具有重要应用价值。然而，由于目标尺度极小且常处于复杂背景之中，如何有效提取边缘等判别性特征仍然是亟待解决的难题。同时，现有基于 U-Net 的检测网络在跨层特征融合过程中存在明显的语义差异，导致浅层细节信息与深层语义特征难以充分结合，从而进一步限制了检测精度的提升。方法基于U-Net结构，提出一种多分支感知与跨层语义融合的红外小目标检测网络（multi-branch perception and cross-layer semantic fusion network，MPCF-Net）。在编码器阶段，为增强边缘等判别性特征的提取，引入了多分支感知融合注意力（multi-branch perception fusion attention module，MPFM）。该模块通过局部分支、全局分支及串行卷积分支实现多尺度特征提取，并结合局部-全局引导注意力（local-global guided attention，LGGA）与全局通道空间注意力（global channel spatial attention，GCSA），分别强化小目标的响应能力与特征表达能力。随后，为缓解跨层特征间的语义差异并建模上下文依赖关系，采用空间-通道交叉Transformer块（spatial-channel cross transformer block，SCTB）替代传统的跳跃连接，从而提升多层特征融合效果。在解码器阶段，虽然深度可分离卷积能够有效降低参数量和计算复杂度，但由于缺乏跨通道特征交互，削弱了小目标的细节特征。为此，在输出端引入轻量梯度门控模块（lightweight gradient gating，LGG），利用Sobel梯度引导的空间注意力进一步强化小目标的边缘与细节特征。结果在SIRST、IRSTD和NUDT-SIRST三个公开红外小目标数据集上的实验表明，MPCF-Net在交并比（intersection over union，IoU）和归一化交并比（normalized intersection over union，nIoU）指标上分别达到80.12% 、66.28%和84.26%，以及78.23%、64.58%和86.48%。同时，该方法在检测概率（probability of detection，Pd）上分别达到99.88%、94.23%和98.21%，虚警率（false alarm，Fa）仅为1.12×10 -6 、4.39×10 -6 和14.57×10 -6 ，展现了更优的检测性能。结论所提方法通过多分支感知和跨层语义融合，有效增强了红外小目标的边缘等判别特征提取能力及上下文建模能力，从而实现了更高精度的红外小目标检测。
Published: 2026-01-08T03:05:10+00:00
Venue: Journal of Image and Graphics
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Menghao; Liu Kui; Zhang Fengbo; Su Benyue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250448"&gt;10.11834/jig.250448&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;目的红外小目标检测在军事和民用等领域具有重要应用价值。然而，由于目标尺度极小且常处于复杂背景之中，如何有效提取边缘等判别性特征仍然是亟待解决的难题。同时，现有基于 U-Net 的检测网络在跨层特征融合过程中存在明显的语义差异，导致浅层细节信息与深层语义特征难以充分结合，从而进一步限制了检测精度的提升。方法基于U-Net结构，提出一种多分支感知与跨层语义融合的红外小目标检测网络（multi-branch perception and cross-layer semantic fusion network，MPCF-Net）。在编码器阶段，为增强边缘等判别性特征的提取，引入了多分支感知融合注意力（multi-branch perception fusion attention module，MPFM）。该模块通过局部分支、全局分支及串行卷积分支实现多尺度特征提取，并结合局部-全局引导注意力（local-global guided attention，LGGA）与全局通道空间注意力（global channel spatial attention，GCSA），分别强化小目标的响应能力与特征表达能力。随后，为缓解跨层特征间的语义差异并建模上下文依赖关系，采用空间-通道交叉Transformer块（spatial-channel cross transformer block，SCTB）替代传统的跳跃连接，从而提升多层特征融合效果。在解码器阶段，虽然深度可分离卷积能够有效降低参数量和计算复杂度，但由于缺乏跨通道特征交互，削弱了小目标的细节特征。为此，在输出端引入轻量梯度门控模块（lightweight gradient gating，LGG），利用Sobel梯度引导的空间注意力进一步强化小目标的边缘与细节特征。结果在SIRST、IRSTD和NUDT-SIRST三个公开红外小目标数据集上的实验表明，MPCF-Net在交并比（intersection over union，IoU）和归一化交并比（normalized intersection over union，nIoU）指标上分别达到80.12% 、66.28%和84.26%，以及78.23%、64.58%和86.48%。同时，该方法在检测概率（probability of detection，Pd）上分别达到99.88%、94.23%和98.21%，虚警率（false alarm，Fa）仅为1.12×10 -6 、4.39×10 -6 和14.57×10 -6 ，展现了更优的检测性能。结论所提方法通过多分支感知和跨层语义融合，有效增强了红外小目标的边缘等判别特征提取能力及上下文建模能力，从而实现了更高精度的红外小目标检测。&lt;/p&gt;</content:encoded></item><item><title>BEVFormer++: Enhancing BEV Fusion with Normalized Embedding and Range Attention for 3D Object Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131131</link><guid>10.1016/j.eswa.2026.131131</guid><pubDate>Thu, 08 Jan 2026 00:22:20 +0000</pubDate><dc:creator>Shazib Qayyum</dc:creator><dc:creator>Xiaoheng Deng</dc:creator><dc:creator>Husnain Mushtaq</dc:creator><dc:creator>Ping Jiang</dc:creator><dc:creator>Shaohua Wan</dc:creator><dc:creator>Irsha Ullah</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131131</prism:doi><description>Accurate 3D object detection remains a critical challenge in autonomous driving due to the sparsity and range-dependent density of LiDAR point clouds. Objects at greater distances often contain limited structural information, making them difficult to detect with conventional range-invariant attention and naïve sampling. Furthermore, existing multimodal fusion approaches struggle with spatial misalignment and inconsistent geometric representation, leading to suboptimal performance in complex driving environments. We propose BEVFormer++, a unified multimodal detection framework that enhances feature representation and fusion in Bird’s Eye View (BEV) space. Our approach introduces three key innovations: (1) a Normalized Positional Embedding (NPE) that encodes scale-invariant geometric cues, improving alignment between LiDAR and camera features; (2) a Diversity Sampling (cloud mining) strategy that selects informative and representative points, enriching structural features and improving small/occluded object detection; and (3) a Range-Aware Attention (RAA) mechanism that adaptively adjusts attention weights across distance bins, mitigating long-range sparsity and improving far-field detection. These modules are integrated into a robust BEV fusion pipeline, ensuring consistent cross-modal reasoning and spatial awareness. Extensive experiments demonstrate the effectiveness of BEVFormer++. On the KITTI dataset, our method achieves 90.1%, 82.0%, 78.3% AP 3 D for Easy/Moderate/Hard cases, significantly outperforming baselines. On the nuScenes benchmark, BEVFormer++ delivers consistent gains in mean AP and NDS, highlighting its robustness across diverse driving scenarios. Together, these results confirm that our framework effectively addresses sparsity, distance variation, and multimodal misalignment, setting a new benchmark for 3D object detection.
Published: 2026-01-08T00:22:20+00:00
Venue: Expert Systems with Applications
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shazib Qayyum; Xiaoheng Deng; Husnain Mushtaq; Ping Jiang; Shaohua Wan; Irsha Ullah&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131131"&gt;10.1016/j.eswa.2026.131131&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate 3D object detection remains a critical challenge in autonomous driving due to the sparsity and range-dependent density of LiDAR point clouds. Objects at greater distances often contain limited structural information, making them difficult to detect with conventional range-invariant attention and naïve sampling. Furthermore, existing multimodal fusion approaches struggle with spatial misalignment and inconsistent geometric representation, leading to suboptimal performance in complex driving environments. We propose BEVFormer++, a unified multimodal detection framework that enhances feature representation and fusion in Bird’s Eye View (BEV) space. Our approach introduces three key innovations: (1) a Normalized Positional Embedding (NPE) that encodes scale-invariant geometric cues, improving alignment between LiDAR and camera features; (2) a Diversity Sampling (cloud mining) strategy that selects informative and representative points, enriching structural features and improving small/occluded object detection; and (3) a Range-Aware Attention (RAA) mechanism that adaptively adjusts attention weights across distance bins, mitigating long-range sparsity and improving far-field detection. These modules are integrated into a robust BEV fusion pipeline, ensuring consistent cross-modal reasoning and spatial awareness. Extensive experiments demonstrate the effectiveness of BEVFormer++. On the KITTI dataset, our method achieves 90.1%, 82.0%, 78.3% AP 3 D for Easy/Moderate/Hard cases, significantly outperforming baselines. On the nuScenes benchmark, BEVFormer++ delivers consistent gains in mean AP and NDS, highlighting its robustness across diverse driving scenarios. Together, these results confirm that our framework effectively addresses sparsity, distance variation, and multimodal misalignment, setting a new benchmark for 3D object detection.&lt;/p&gt;</content:encoded></item><item><title>Attentional dual-stream interactive perception network for efficient infrared small aerial target detection</title><link>https://doi.org/10.1016/j.neunet.2026.108563</link><guid>10.1016/j.neunet.2026.108563</guid><pubDate>Thu, 08 Jan 2026 16:52:57 +0000</pubDate><dc:creator>Lihao Zhou</dc:creator><dc:creator>Huawei Wang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108563</prism:doi><description>Drones and other flying objects can be regarded as small targets from a long-distance perspective. Considering the occlusion and interference caused by the external environment, the infrared detection methods are adopted to help identify and manage small aerial targets. However, remote infrared imaging often leads to small target feature detail loss. And the general methods have low detection efficiency, difficult to deeply extract target features. To better address the above problems, we propose an attentional dual-stream interactive perception network (ADIPNet) in this paper. Based on dual-stream U-Net, ADIPNet mainly combines the multi-patch series-parallel attention module (MSPA), edge anchoring module with regret (EAR), context scene perception module (CSP) and dual-stream interaction fusion module (DSIF). MSPA manually constructs the weight of patch regions at multiple scales and then performs the nested self-attention so as to fully mine global target information. EAR unites two types of global features using local mapping and matrix product, which helps accurately capture small target edge. CSP exchanges context information multiple times and conducts mutual complementation of semantic scenarios to enhances the perception of small target features. Finally, DSIF conducts cross attention for high-level encoded features on double U-Nets, further improving the network’s understanding of complex scenario information. The proposed ADIPNet alleviates the insufficient feature extraction of infrared small targets. Compared with other state-of-the-art methods, mIoU respectively reaches 80.52% and 72.54% on two large infrared datasets. It achieves more accurate detection of small aerial targets with low operating cost, possessing potential application prospect in various infrared surveillance systems.
Published: 2026-01-08T16:52:57+00:00
Venue: Neural Networks
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihao Zhou; Huawei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108563"&gt;10.1016/j.neunet.2026.108563&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Drones and other flying objects can be regarded as small targets from a long-distance perspective. Considering the occlusion and interference caused by the external environment, the infrared detection methods are adopted to help identify and manage small aerial targets. However, remote infrared imaging often leads to small target feature detail loss. And the general methods have low detection efficiency, difficult to deeply extract target features. To better address the above problems, we propose an attentional dual-stream interactive perception network (ADIPNet) in this paper. Based on dual-stream U-Net, ADIPNet mainly combines the multi-patch series-parallel attention module (MSPA), edge anchoring module with regret (EAR), context scene perception module (CSP) and dual-stream interaction fusion module (DSIF). MSPA manually constructs the weight of patch regions at multiple scales and then performs the nested self-attention so as to fully mine global target information. EAR unites two types of global features using local mapping and matrix product, which helps accurately capture small target edge. CSP exchanges context information multiple times and conducts mutual complementation of semantic scenarios to enhances the perception of small target features. Finally, DSIF conducts cross attention for high-level encoded features on double U-Nets, further improving the network’s understanding of complex scenario information. The proposed ADIPNet alleviates the insufficient feature extraction of infrared small targets. Compared with other state-of-the-art methods, mIoU respectively reaches 80.52% and 72.54% on two large infrared datasets. It achieves more accurate detection of small aerial targets with low operating cost, possessing potential application prospect in various infrared surveillance systems.&lt;/p&gt;</content:encoded></item><item><title>Mapping land uses following tropical deforestation with location-aware deep learning</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.007</link><guid>10.1016/j.isprsjprs.2025.12.007</guid><pubDate>Wed, 07 Jan 2026 17:20:32 +0000</pubDate><dc:creator>Jan Pišl</dc:creator><dc:creator>Gencer Sumbul</dc:creator><dc:creator>Gaston Lenczner</dc:creator><dc:creator>Camilo Zamora</dc:creator><dc:creator>Martin Herold</dc:creator><dc:creator>Jan Dirk Wegner</dc:creator><dc:creator>Devis Tuia</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.007</prism:doi><description>The rates of tropical deforestation remain alarmingly high. To enable effective, targeted policy responses, detailed data on its driving forces is needed—each deforestation event needs to be attributed to an agricultural commodity or another land use. Remote sensing allows us to monitor land use conversion following deforestation, providing a proxy of drivers. However, recognizing individual commodities is challenging due to spectral similarities, the limited spatial resolution of free satellite imagery, and limited labeled data. To tackle these challenges, we propose a deep learning, multi-modal approach for the recognition of post-deforestation land uses from a time series of Sentinel-2 images, geographic coordinates, and country-level statistics of deforestation drivers. To integrate the modalities, we design a Transformer-based model with modality-specific encoders. The approach reaches 87% accuracy, an improvement of 10% over the image-only baseline, with little increase in data volume, computations, and model size. It works well in low-data regimes, and can be easily extended to include other modalities. Overall, this work contributes towards detailed, repeatable, and scalable mapping of deforestation landscapes, providing necessary data for the design and implementation of targeted interventions to protect tropical forests.
Published: 2026-01-07T17:20:32+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jan Pišl; Gencer Sumbul; Gaston Lenczner; Camilo Zamora; Martin Herold; Jan Dirk Wegner; Devis Tuia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.007"&gt;10.1016/j.isprsjprs.2025.12.007&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;The rates of tropical deforestation remain alarmingly high. To enable effective, targeted policy responses, detailed data on its driving forces is needed—each deforestation event needs to be attributed to an agricultural commodity or another land use. Remote sensing allows us to monitor land use conversion following deforestation, providing a proxy of drivers. However, recognizing individual commodities is challenging due to spectral similarities, the limited spatial resolution of free satellite imagery, and limited labeled data. To tackle these challenges, we propose a deep learning, multi-modal approach for the recognition of post-deforestation land uses from a time series of Sentinel-2 images, geographic coordinates, and country-level statistics of deforestation drivers. To integrate the modalities, we design a Transformer-based model with modality-specific encoders. The approach reaches 87% accuracy, an improvement of 10% over the image-only baseline, with little increase in data volume, computations, and model size. It works well in low-data regimes, and can be easily extended to include other modalities. Overall, this work contributes towards detailed, repeatable, and scalable mapping of deforestation landscapes, providing necessary data for the design and implementation of targeted interventions to protect tropical forests.&lt;/p&gt;</content:encoded></item><item><title>D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images</title><link>https://arxiv.org/abs/2601.02747v1</link><guid>http://arxiv.org/abs/2601.02747v1</guid><pubDate>Tue, 06 Jan 2026 06:21:50 +0000</pubDate><dc:creator>Zixiao Wen</dc:creator><dc:creator>Zhen Yang</dc:creator><dc:creator>Xianjie Bao</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Xiantai Xiang</dc:creator><dc:creator>Wenshuai Li</dc:creator><dc:creator>Yuhan Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.
Published: 2026-01-06T06:21:50+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixiao Wen; Zhen Yang; Xianjie Bao; Lei Zhang; Xiantai Xiang; Wenshuai Li; Yuhan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.&lt;/p&gt;</content:encoded></item><item><title>夜间无人机航拍图像目标检测与跟踪方法研究进展</title><link>https://doi.org/10.11834/jig.250459</link><guid>10.11834/jig.250459</guid><pubDate>Thu, 08 Jan 2026 03:05:09 +0000</pubDate><dc:creator>Bi Shifan</dc:creator><dc:creator>Ye Liang</dc:creator><dc:creator>Wang Zhixiang</dc:creator><dc:creator>Zhang Ziyang</dc:creator><dc:creator>Hong Hanyu</dc:creator><dc:creator>Sang Nong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250459</prism:doi><description>视觉目标检测与跟踪技术已在白天场景中取得显著突破，为无人机（unmanned aerial vehicle，UAV）在智能领域的广泛应用提供了强大支撑。然而，这些方法在夜间场景下往往表现不佳，检测与跟踪精度显著下降。夜间作为无人机应用中不可或缺的场景，其复杂性与挑战性凸显了开展针对夜间无人机航拍图像目标检测与跟踪研究的必要性和现实意义。针对夜间无人机目标航拍图像检测与跟踪技术的现状及发展趋势，本文分析了感知能力有限、可视化特征不足、硬件平台资源受限以及复杂成像条件等因素所带来的挑战。从夜间无人机航拍图像目标检测研究出发，综述了夜间图像增强、域适应学习、多模态感知融合和轻量化模型等方法的研究进展。在夜间无人机航拍图像目标跟踪方面，重点综述了基于深度学习的五类范式，包括先增强后跟踪、域自适应、视觉提示学习、课程学习和多模态融合，系统总结了相关方法的优缺点及所应对的挑战。随后，介绍了夜间及全天候无人机航拍图像目标检测与跟踪常用的评价指标与典型数据集，并在构建的夜间无人机车辆目标检测集DroneVehicle-Night上进行性能评估与对比分析；同时，从VisDrone2019的测试集中筛选昼夜样本，对现有检测方法的夜间适应性进行了对比测试；此外，还汇总了包含四类跟踪范式在内的20种算法在夜间无人机航拍图像目标跟踪数据集UAVDark135与NAT2021上的性能评估结果。最后，对夜间无人机航拍图像目标检测与跟踪未来的发展方向进行了展望，为该领域的后续研究提供参考。本文实验所用到的算法、构建的数据集已经汇总至https：//github.com/bsfsf/DroneVehicle-Night和https：//doi.org/10.57760/sciencedb.32435以便后续研究者使用。
Published: 2026-01-08T03:05:09+00:00
Venue: Journal of Image and Graphics
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bi Shifan; Ye Liang; Wang Zhixiang; Zhang Ziyang; Hong Hanyu; Sang Nong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250459"&gt;10.11834/jig.250459&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;视觉目标检测与跟踪技术已在白天场景中取得显著突破，为无人机（unmanned aerial vehicle，UAV）在智能领域的广泛应用提供了强大支撑。然而，这些方法在夜间场景下往往表现不佳，检测与跟踪精度显著下降。夜间作为无人机应用中不可或缺的场景，其复杂性与挑战性凸显了开展针对夜间无人机航拍图像目标检测与跟踪研究的必要性和现实意义。针对夜间无人机目标航拍图像检测与跟踪技术的现状及发展趋势，本文分析了感知能力有限、可视化特征不足、硬件平台资源受限以及复杂成像条件等因素所带来的挑战。从夜间无人机航拍图像目标检测研究出发，综述了夜间图像增强、域适应学习、多模态感知融合和轻量化模型等方法的研究进展。在夜间无人机航拍图像目标跟踪方面，重点综述了基于深度学习的五类范式，包括先增强后跟踪、域自适应、视觉提示学习、课程学习和多模态融合，系统总结了相关方法的优缺点及所应对的挑战。随后，介绍了夜间及全天候无人机航拍图像目标检测与跟踪常用的评价指标与典型数据集，并在构建的夜间无人机车辆目标检测集DroneVehicle-Night上进行性能评估与对比分析；同时，从VisDrone2019的测试集中筛选昼夜样本，对现有检测方法的夜间适应性进行了对比测试；此外，还汇总了包含四类跟踪范式在内的20种算法在夜间无人机航拍图像目标跟踪数据集UAVDark135与NAT2021上的性能评估结果。最后，对夜间无人机航拍图像目标检测与跟踪未来的发展方向进行了展望，为该领域的后续研究提供参考。本文实验所用到的算法、构建的数据集已经汇总至https：//github.com/bsfsf/DroneVehicle-Night和https：//doi.org/10.57760/sciencedb.32435以便后续研究者使用。&lt;/p&gt;</content:encoded></item><item><title>Physics-Driven SAR Target Detection: A Review and Perspective</title><link>https://doi.org/10.3390/rs18020200</link><guid>10.3390/rs18020200</guid><pubDate>Wed, 07 Jan 2026 12:34:21 +0000</pubDate><dc:creator>Xinyi Li</dc:creator><dc:creator>Lei Liu</dc:creator><dc:creator>Gang Wan</dc:creator><dc:creator>Fengjie Zheng</dc:creator><dc:creator>Shihao Guo</dc:creator><dc:creator>Guangde Sun</dc:creator><dc:creator>Ziyan Wang</dc:creator><dc:creator>Xiaoxuan Liu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020200</prism:doi><description>Synthetic Aperture Radar (SAR) is highly valuable for target detection due to its all-weather, day-night operational capability and certain ground penetration potential. However, traditional SAR target detection methods often directly adapt algorithms designed for optical imagery, simplistically treating SAR data as grayscale images. This approach overlooks SAR’s unique physical nature, failing to account for key factors such as backscatter variations from different polarizations, target representation changes across resolutions, and detection threshold shifts due to clutter background heterogeneity. Consequently, these limitations lead to insufficient cross-polarization adaptability, feature masking, and degraded recognition accuracy due to clutter interference. To address these challenges, this paper systematically reviews recent research advances in SAR target detection, focusing on physical constraints including polarization characteristics, scattering mechanisms, signal-domain properties, and resolution effects. Finally, it outlines promising research directions to guide future developments in physics-aware SAR target detection.
Published: 2026-01-07T12:34:21+00:00
Venue: Remote Sensing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyi Li; Lei Liu; Gang Wan; Fengjie Zheng; Shihao Guo; Guangde Sun; Ziyan Wang; Xiaoxuan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020200"&gt;10.3390/rs18020200&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is highly valuable for target detection due to its all-weather, day-night operational capability and certain ground penetration potential. However, traditional SAR target detection methods often directly adapt algorithms designed for optical imagery, simplistically treating SAR data as grayscale images. This approach overlooks SAR’s unique physical nature, failing to account for key factors such as backscatter variations from different polarizations, target representation changes across resolutions, and detection threshold shifts due to clutter background heterogeneity. Consequently, these limitations lead to insufficient cross-polarization adaptability, feature masking, and degraded recognition accuracy due to clutter interference. To address these challenges, this paper systematically reviews recent research advances in SAR target detection, focusing on physical constraints including polarization characteristics, scattering mechanisms, signal-domain properties, and resolution effects. Finally, it outlines promising research directions to guide future developments in physics-aware SAR target detection.&lt;/p&gt;</content:encoded></item><item><title>轻量级稀疏置换自注意力图像超分辨率网络</title><link>https://doi.org/10.11834/jig.250519</link><guid>10.11834/jig.250519</guid><pubDate>Thu, 08 Jan 2026 03:05:30 +0000</pubDate><dc:creator>Wu Siqi</dc:creator><dc:creator>Liu Wei</dc:creator><dc:creator>Chen Weidong</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250519</prism:doi><description>目的图像超分辨重建是计算机视觉领域中的一个典型低层视觉任务，能够为目标检测、图像分割等高层任务提供更清晰更结构化的输入。基于CNN的图像超分辨率模型注重恢复图像的纹理和边缘信息，而基于Transformer的方法能建模全局上下文信息，但是存在注意力权重冗余问题。针对这两种模型的优缺点，本文设计了一种轻量级图像超分辨率网络。方法首先改进了传统的Transformer，提出了一种稀疏置换自注意力机制，在扩大窗口的同时解决冗余问题。在此基础上，我们基于CNN构建高频信息增强模块加强模型对局部细节信息的重建。在得到两种结构提取的特征后，我们提出一种双分支特征融合模块对全局特征和局部特征进行高效融合。结果本文方法在5个公开数据集上与11种先进超分辨方法进行了对比实验。结果表明，在保证模型轻量化的前提下，稀疏置换自注意力网络（Sparse and Permuted Self-Attention Network，SPSANet）在不同放大倍率和数据集上均取得最优或次优性能。当放大倍率为3时，在Urban100和Manga109数据集上的峰值信噪比（peak signal-to-noise ratio，PSNR）分别较最新的SOTA（state of the art）方法提升了0.15dB和0.25dB。主观视觉效果显示，SPSANet在复杂纹理和细节丰富的场景中重建的图像更加清晰、自然。结论本文提出的轻量级稀疏置换自注意力图像超分辨率网络能够在保持较低参数量与计算复杂度的同时，在多个数据集上取得优异的重建效果，展现出良好的泛化性与应用价值。
Published: 2026-01-08T03:05:30+00:00
Venue: Journal of Image and Graphics
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wu Siqi; Liu Wei; Chen Weidong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250519"&gt;10.11834/jig.250519&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;目的图像超分辨重建是计算机视觉领域中的一个典型低层视觉任务，能够为目标检测、图像分割等高层任务提供更清晰更结构化的输入。基于CNN的图像超分辨率模型注重恢复图像的纹理和边缘信息，而基于Transformer的方法能建模全局上下文信息，但是存在注意力权重冗余问题。针对这两种模型的优缺点，本文设计了一种轻量级图像超分辨率网络。方法首先改进了传统的Transformer，提出了一种稀疏置换自注意力机制，在扩大窗口的同时解决冗余问题。在此基础上，我们基于CNN构建高频信息增强模块加强模型对局部细节信息的重建。在得到两种结构提取的特征后，我们提出一种双分支特征融合模块对全局特征和局部特征进行高效融合。结果本文方法在5个公开数据集上与11种先进超分辨方法进行了对比实验。结果表明，在保证模型轻量化的前提下，稀疏置换自注意力网络（Sparse and Permuted Self-Attention Network，SPSANet）在不同放大倍率和数据集上均取得最优或次优性能。当放大倍率为3时，在Urban100和Manga109数据集上的峰值信噪比（peak signal-to-noise ratio，PSNR）分别较最新的SOTA（state of the art）方法提升了0.15dB和0.25dB。主观视觉效果显示，SPSANet在复杂纹理和细节丰富的场景中重建的图像更加清晰、自然。结论本文提出的轻量级稀疏置换自注意力图像超分辨率网络能够在保持较低参数量与计算复杂度的同时，在多个数据集上取得优异的重建效果，展现出良好的泛化性与应用价值。&lt;/p&gt;</content:encoded></item><item><title>Progressive uncertainty-guided network for binary segmentation in high-resolution remote sensing imagery</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.010</link><guid>10.1016/j.isprsjprs.2026.01.010</guid><pubDate>Wed, 07 Jan 2026 08:11:12 +0000</pubDate><dc:creator>Jiepan Li</dc:creator><dc:creator>Wei He</dc:creator><dc:creator>Ting Hu</dc:creator><dc:creator>Minghao Tang</dc:creator><dc:creator>Liangpei Zhang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.010</prism:doi><description>Binary semantic segmentation in remote sensing (RS) imagery faces persistent challenges due to complex object appearances, ambiguous boundaries, and high similarity between foreground and background, all of which introduce significant uncertainty into the prediction process. Existing approaches often treat uncertainty as either a global attribute or a pixel-level estimate, overlooking the critical role of spatial and contextual interactions. To address these limitations, we propose the Progressive Uncertainty-Guided Segmentation Network (PUGNet) , a unified framework that explicitly models uncertainty in a context-aware manner. PUGNet decomposes uncertainty into three distinct components: foreground uncertainty , background uncertainty , and contextual uncertainty . This tripartite modeling enables more precise handling of local ambiguities and global inconsistencies. We adopt a coarse-to-fine decoding strategy that progressively refines features through two specialized modules. The Dynamic Uncertainty-Aware Module enhances regions of high foreground and background uncertainty using Gaussian-based modeling and contrastive learning. The Entropy-Driven Refinement Module quantifies contextual uncertainty via entropy and facilitates adaptive refinement through multi-scale context aggregation. Extensive experiments on ten public benchmark datasets, covering both single-temporal ( e.g. , building and cropland extraction) and bi-temporal ( e.g. , building change detection) binary segmentation tasks, demonstrate that PUGNet consistently achieves superior segmentation accuracy and uncertainty reduction, establishing a new state of the art in RS binary segmentation. The full implementation of the proposed framework and all experimental results can be accessed at https://github.com/Henryjiepanli/PU_RS .
Published: 2026-01-07T08:11:12+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiepan Li; Wei He; Ting Hu; Minghao Tang; Liangpei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.010"&gt;10.1016/j.isprsjprs.2026.01.010&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Binary semantic segmentation in remote sensing (RS) imagery faces persistent challenges due to complex object appearances, ambiguous boundaries, and high similarity between foreground and background, all of which introduce significant uncertainty into the prediction process. Existing approaches often treat uncertainty as either a global attribute or a pixel-level estimate, overlooking the critical role of spatial and contextual interactions. To address these limitations, we propose the Progressive Uncertainty-Guided Segmentation Network (PUGNet) , a unified framework that explicitly models uncertainty in a context-aware manner. PUGNet decomposes uncertainty into three distinct components: foreground uncertainty , background uncertainty , and contextual uncertainty . This tripartite modeling enables more precise handling of local ambiguities and global inconsistencies. We adopt a coarse-to-fine decoding strategy that progressively refines features through two specialized modules. The Dynamic Uncertainty-Aware Module enhances regions of high foreground and background uncertainty using Gaussian-based modeling and contrastive learning. The Entropy-Driven Refinement Module quantifies contextual uncertainty via entropy and facilitates adaptive refinement through multi-scale context aggregation. Extensive experiments on ten public benchmark datasets, covering both single-temporal ( e.g. , building and cropland extraction) and bi-temporal ( e.g. , building change detection) binary segmentation tasks, demonstrate that PUGNet consistently achieves superior segmentation accuracy and uncertainty reduction, establishing a new state of the art in RS binary segmentation. The full implementation of the proposed framework and all experimental results can be accessed at https://github.com/Henryjiepanli/PU_RS .&lt;/p&gt;</content:encoded></item><item><title>Few-Shot LoRA Adaptation of a Flow-Matching Foundation Model for Cross-Spectral Object Detection</title><link>https://arxiv.org/abs/2601.04381v1</link><guid>http://arxiv.org/abs/2601.04381v1</guid><pubDate>Wed, 07 Jan 2026 20:41:26 +0000</pubDate><dc:creator>Maxim Clouser</dc:creator><dc:creator>Kia Khezeli</dc:creator><dc:creator>John Kalantari</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.
Published: 2026-01-07T20:41:26+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maxim Clouser; Kia Khezeli; John Kalantari&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Foundation models for vision are predominantly trained on RGB data, while many safety-critical applications rely on non-visible modalities such as infrared (IR) and synthetic aperture radar (SAR). We study whether a single flow-matching foundation model pre-trained primarily on RGB images can be repurposed as a cross-spectral translator using only a few co-measured examples, and whether the resulting synthetic data can enhance downstream detection. Starting from FLUX.1 Kontext, we insert low-rank adaptation (LoRA) modules and fine-tune them on just 100 paired images per domain for two settings: RGB to IR on the KAIST dataset and RGB to SAR on the M4-SAR dataset. The adapted model translates RGB images into pixel-aligned IR/SAR, enabling us to reuse existing bounding boxes and train object detection models purely in the target modality. Across a grid of LoRA hyperparameters, we find that LPIPS computed on only 50 held-out pairs is a strong proxy for downstream performance: lower LPIPS consistently predicts higher mAP for YOLOv11n on both IR and SAR, and for DETR on KAIST IR test data. Using the best LPIPS-selected LoRA adapter, synthetic IR from external RGB datasets (LLVIP, FLIR ADAS) improves KAIST IR pedestrian detection, and synthetic SAR significantly boosts infrastructure detection on M4-SAR when combined with limited real SAR. Our results suggest that few-shot LoRA adaptation of flow-matching foundation models is a promising path toward foundation-style support for non-visible modalities.&lt;/p&gt;</content:encoded></item><item><title>SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection</title><link>https://arxiv.org/abs/2601.02249v1</link><guid>http://arxiv.org/abs/2601.02249v1</guid><pubDate>Mon, 05 Jan 2026 16:31:41 +0000</pubDate><dc:creator>Xiantai Xiang</dc:creator><dc:creator>Guangyao Zhou</dc:creator><dc:creator>Zixiao Wen</dc:creator><dc:creator>Wenshuai Li</dc:creator><dc:creator>Ben Niu</dc:creator><dc:creator>Feng Wang</dc:creator><dc:creator>Lijia Huang</dc:creator><dc:creator>Qiantong Wang</dc:creator><dc:creator>Yuhan Liu</dc:creator><dc:creator>Zongxu Pan</dc:creator><dc:creator>Yuxin Hu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.
Published: 2026-01-05T16:31:41+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiantai Xiang; Guangyao Zhou; Zixiao Wen; Wenshuai Li; Ben Niu; Feng Wang; Lijia Huang; Qiantong Wang; Yuhan Liu; Zongxu Pan; Yuxin Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.&lt;/p&gt;</content:encoded></item><item><title>Local–global collaborative feature learning with level-wise decoding for infrared small target detection</title><link>https://doi.org/10.1016/j.cviu.2026.104636</link><guid>10.1016/j.cviu.2026.104636</guid><pubDate>Wed, 07 Jan 2026 08:12:49 +0000</pubDate><dc:creator>Weiwei Duan</dc:creator><dc:creator>Luping Ji</dc:creator><dc:creator>Shengjia Chen</dc:creator><dc:creator>Jianghong Huang</dc:creator><prism:publicationName>Computer Vision and Image Understanding</prism:publicationName><prism:doi>10.1016/j.cviu.2026.104636</prism:doi><description>Due to the low signal-to-noise ratio and weak visual contrast, infrared small targets are often submerged in the background. Therefore, it is crucial to preserve target information while extracting distinctive features that distinguish them from the background. However, existing methods generally rely on convolutions and transformers in isolation, which limits their ability to capture robust target features in complex scenes. To address this issue, we propose a new local–global feature collaborative learning (LGFC) framework. It could adequately integrate the local spatial features with the global context of targets in a unified manner. Specifically, we develop an enhanced Gaussian-mask Vision Transformer group with Global Gaussian Attention and Local Window Attention to extract refined global features. The local coarse features obtained from the convolution encoder are then coordinated with the refined global features through Local–Global Collaborating . Moreover, to avoid feature loss during decoding, we propose a level-wise decoding strategy with Cross-layer Feature Interaction to to mitigate information loss in deep networks. Additionally, we introduce a Coarse-to-Fine Refinement post-processing mechanism to improve the precision of target contours. The extensive experiments on three public datasets (NUAA-SIRST, IRSTD-1K and SIRST-AUG) demonstrate the superiority and generalization ability of our proposed LGFC framework for infrared small target detection, outperforming state-of-the-art methods by approximately 2.3% in F1-score on each dataset.
Published: 2026-01-07T08:12:49+00:00
Venue: Computer Vision and Image Understanding
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiwei Duan; Luping Ji; Shengjia Chen; Jianghong Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Computer Vision and Image Understanding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.cviu.2026.104636"&gt;10.1016/j.cviu.2026.104636&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the low signal-to-noise ratio and weak visual contrast, infrared small targets are often submerged in the background. Therefore, it is crucial to preserve target information while extracting distinctive features that distinguish them from the background. However, existing methods generally rely on convolutions and transformers in isolation, which limits their ability to capture robust target features in complex scenes. To address this issue, we propose a new local–global feature collaborative learning (LGFC) framework. It could adequately integrate the local spatial features with the global context of targets in a unified manner. Specifically, we develop an enhanced Gaussian-mask Vision Transformer group with Global Gaussian Attention and Local Window Attention to extract refined global features. The local coarse features obtained from the convolution encoder are then coordinated with the refined global features through Local–Global Collaborating . Moreover, to avoid feature loss during decoding, we propose a level-wise decoding strategy with Cross-layer Feature Interaction to to mitigate information loss in deep networks. Additionally, we introduce a Coarse-to-Fine Refinement post-processing mechanism to improve the precision of target contours. The extensive experiments on three public datasets (NUAA-SIRST, IRSTD-1K and SIRST-AUG) demonstrate the superiority and generalization ability of our proposed LGFC framework for infrared small target detection, outperforming state-of-the-art methods by approximately 2.3% in F1-score on each dataset.&lt;/p&gt;</content:encoded></item><item><title>Integrating linear and circular polarization features for PolSAR land cover classification with deep learning</title><link>https://doi.org/10.1016/j.jag.2026.105090</link><guid>10.1016/j.jag.2026.105090</guid><pubDate>Wed, 07 Jan 2026 10:50:16 +0000</pubDate><dc:creator>Shuaiying Zhang</dc:creator><dc:creator>Zhen Dong</dc:creator><dc:creator>Huadong Lin</dc:creator><dc:creator>Zhendong Zhang</dc:creator><dc:creator>Jinran Wu</dc:creator><dc:creator>Sinong Quan</dc:creator><dc:creator>Wentao An</dc:creator><dc:creator>Tong Li</dc:creator><dc:creator>Rajiv Pandey</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105090</prism:doi><description>Existing deep learning methods for ecological monitoring using polarimetric synthetic aperture radar (PolSAR) imagery primarily rely on coherency ( T " role="presentation"&gt; T T ) or covariance ( C " role="presentation"&gt; C C ) matrices derived from the linear polarization basis, often overlooking scattering information inherent in alternative polarization representations. To address this limitation, this study proposes a novel classification framework that explicitly incorporates a circular polarization basis into the PolSAR deep learning workflow. A circular coherency matrix ( Cir " role="presentation"&gt; Cir Cir ), analogous to the conventional T " role="presentation"&gt; T T matrix, was first derived through polarization basis transformation. Subsequently, a multi-basis input scheme was introduced to fuse linear and circular polarization features to enhance feature representation and information utilization. The proposed framework was validated on two benchmark datasets using multiple deep learning models, achieving state-of-the-art classification accuracies of 97.70% and 98.58%.Compared with standard linear-basis approaches, the proposed scheme yielded accuracy improvements of 2.86% over the T " role="presentation"&gt; T T -matrix-based method and 2.26% over the C " role="presentation"&gt; C C -matrix-based method. In addition, the incorporation of circular polarization features significantly enhanced physical interpretability, particularly for structurally complex targets such as forests and buildings. Overall, the findings provide an effective technical pathway for intelligent land cover classification and broader ecological monitoring. The source code and datasets are available at https://github.com/zhangssy/Circular-Polarization-Basis-Implementation .
Published: 2026-01-07T10:50:16+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shuaiying Zhang; Zhen Dong; Huadong Lin; Zhendong Zhang; Jinran Wu; Sinong Quan; Wentao An; Tong Li; Rajiv Pandey&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105090"&gt;10.1016/j.jag.2026.105090&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Existing deep learning methods for ecological monitoring using polarimetric synthetic aperture radar (PolSAR) imagery primarily rely on coherency ( T &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; T T ) or covariance ( C &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; C C ) matrices derived from the linear polarization basis, often overlooking scattering information inherent in alternative polarization representations. To address this limitation, this study proposes a novel classification framework that explicitly incorporates a circular polarization basis into the PolSAR deep learning workflow. A circular coherency matrix ( Cir &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; Cir Cir ), analogous to the conventional T &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; T T matrix, was first derived through polarization basis transformation. Subsequently, a multi-basis input scheme was introduced to fuse linear and circular polarization features to enhance feature representation and information utilization. The proposed framework was validated on two benchmark datasets using multiple deep learning models, achieving state-of-the-art classification accuracies of 97.70% and 98.58%.Compared with standard linear-basis approaches, the proposed scheme yielded accuracy improvements of 2.86% over the T &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; T T -matrix-based method and 2.26% over the C &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; C C -matrix-based method. In addition, the incorporation of circular polarization features significantly enhanced physical interpretability, particularly for structurally complex targets such as forests and buildings. Overall, the findings provide an effective technical pathway for intelligent land cover classification and broader ecological monitoring. The source code and datasets are available at https://github.com/zhangssy/Circular-Polarization-Basis-Implementation .&lt;/p&gt;</content:encoded></item><item><title>MSdiff: multi-scale diffusion model for image deblurring</title><link>https://doi.org/10.1016/j.eswa.2026.131152</link><guid>10.1016/j.eswa.2026.131152</guid><pubDate>Wed, 07 Jan 2026 08:10:51 +0000</pubDate><dc:creator>Zhaohan Wang</dc:creator><dc:creator>Chengjun Chen</dc:creator><dc:creator>Chenggang Dai</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131152</prism:doi><description>Diffusion-based approaches have brought notable breakthroughs in the field of image deblurring. However, they typically suffer from several drawbacks, such as requiring a large number of iterative steps, long sampling times, and high computational costs. Moreover, existing diffusion models-based deblurring approaches generally struggle with strong and nonlinear regional blurs. For instance, these approaches tend to generate over-smoothed results and additional undesired artifacts, which result in suboptimal performance on distortion-oriented evaluation metrics like PSNR. To overcome these limitations, this study proposes a Multi-Scale Diffusion Model (MSdiff) . Specifically, both blurred images and corresponding ground-truth are mapped into a latent space, where a conditional diffusion model is employed to generate multi-scale prior features. These priors are subsequently integrated with the intermediate features of the blurred image and fed into a regression-based Swin Transformer U-Net architecture to achieve superior deblurring accuracy. Furthermore, to better integrate the prior features with the intermediate representations of the blurred image, this study proposes a Dual-Channel Integration Module (DCIM) , which is capable of improving the performance of extracting multi-level information from the prior information. A series of experiments confirm that MSdiff not only accelerates the diffusion models-based iterative sampling but also improves the generalization capability to complex blur scenarios. Moreover, MSdiff achieves superior performance compared with current state-of-the-art approaches for both simulated and real-world deblurring tasks.
Published: 2026-01-07T08:10:51+00:00
Venue: Expert Systems with Applications
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaohan Wang; Chengjun Chen; Chenggang Dai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131152"&gt;10.1016/j.eswa.2026.131152&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion-based approaches have brought notable breakthroughs in the field of image deblurring. However, they typically suffer from several drawbacks, such as requiring a large number of iterative steps, long sampling times, and high computational costs. Moreover, existing diffusion models-based deblurring approaches generally struggle with strong and nonlinear regional blurs. For instance, these approaches tend to generate over-smoothed results and additional undesired artifacts, which result in suboptimal performance on distortion-oriented evaluation metrics like PSNR. To overcome these limitations, this study proposes a Multi-Scale Diffusion Model (MSdiff) . Specifically, both blurred images and corresponding ground-truth are mapped into a latent space, where a conditional diffusion model is employed to generate multi-scale prior features. These priors are subsequently integrated with the intermediate features of the blurred image and fed into a regression-based Swin Transformer U-Net architecture to achieve superior deblurring accuracy. Furthermore, to better integrate the prior features with the intermediate representations of the blurred image, this study proposes a Dual-Channel Integration Module (DCIM) , which is capable of improving the performance of extracting multi-level information from the prior information. A series of experiments confirm that MSdiff not only accelerates the diffusion models-based iterative sampling but also improves the generalization capability to complex blur scenarios. Moreover, MSdiff achieves superior performance compared with current state-of-the-art approaches for both simulated and real-world deblurring tasks.&lt;/p&gt;</content:encoded></item><item><title>AFR-CR: An Adaptive Frequency Domain Feature Reconstruction-Based Method for Cloud Removal via SAR-Assisted Remote Sensing Image Fusion</title><link>https://doi.org/10.3390/rs18020201</link><guid>10.3390/rs18020201</guid><pubDate>Thu, 08 Jan 2026 09:01:29 +0000</pubDate><dc:creator>Xiufang Zhou</dc:creator><dc:creator>Qirui Fang</dc:creator><dc:creator>Xunqiang Gong</dc:creator><dc:creator>Shuting Yang</dc:creator><dc:creator>Tieding Lu</dc:creator><dc:creator>Yuting Wan</dc:creator><dc:creator>Ailong Ma</dc:creator><dc:creator>Yanfei Zhong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020201</prism:doi><description>Optical imagery is often contaminated by clouds to varying degrees, which greatly affects the interpretation and analysis of images. Synthetic Aperture Radar (SAR) possesses the characteristic of penetrating clouds and mist, and a common strategy in SAR-assisted cloud removal involves fusing SAR and optical data and leveraging deep learning networks to reconstruct cloud-free optical imagery. However, these methods do not fully consider the characteristics of the frequency domain when processing feature integration, resulting in blurred edges of the generated cloudless optical images. Therefore, an adaptive frequency domain feature reconstruction-based cloud removal method is proposed to solve the problem. The proposed method comprises four key sequential stages. First, shallow features are extracted by fusing optical and SAR images. Second, a Transformer-based encoder captures multi-scale semantic features. Subsequently, the Frequency Domain Decoupling Module (FDDM) is employed. Utilizing a Dynamic Mask Generation mechanism, it explicitly decomposes features into low-frequency structures and high-frequency details, effectively suppressing cloud interference while preserving surface textures. Finally, robust information interaction is facilitated by the Cross-Frequency Reconstruction Module (CFRM) via transposed cross-attention, ensuring precise fusion and reconstruction. Experimental evaluation on the M3R-CR dataset confirms that the proposed approach achieves the best results on all four evaluated metrics, surpassing the performance of the eight other State-of-the-Art methods. It has demonstrated its effectiveness and advanced capabilities in the task of SAR-optical fusion for cloud removal.
Published: 2026-01-08T09:01:29+00:00
Venue: Remote Sensing
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiufang Zhou; Qirui Fang; Xunqiang Gong; Shuting Yang; Tieding Lu; Yuting Wan; Ailong Ma; Yanfei Zhong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020201"&gt;10.3390/rs18020201&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Optical imagery is often contaminated by clouds to varying degrees, which greatly affects the interpretation and analysis of images. Synthetic Aperture Radar (SAR) possesses the characteristic of penetrating clouds and mist, and a common strategy in SAR-assisted cloud removal involves fusing SAR and optical data and leveraging deep learning networks to reconstruct cloud-free optical imagery. However, these methods do not fully consider the characteristics of the frequency domain when processing feature integration, resulting in blurred edges of the generated cloudless optical images. Therefore, an adaptive frequency domain feature reconstruction-based cloud removal method is proposed to solve the problem. The proposed method comprises four key sequential stages. First, shallow features are extracted by fusing optical and SAR images. Second, a Transformer-based encoder captures multi-scale semantic features. Subsequently, the Frequency Domain Decoupling Module (FDDM) is employed. Utilizing a Dynamic Mask Generation mechanism, it explicitly decomposes features into low-frequency structures and high-frequency details, effectively suppressing cloud interference while preserving surface textures. Finally, robust information interaction is facilitated by the Cross-Frequency Reconstruction Module (CFRM) via transposed cross-attention, ensuring precise fusion and reconstruction. Experimental evaluation on the M3R-CR dataset confirms that the proposed approach achieves the best results on all four evaluated metrics, surpassing the performance of the eight other State-of-the-Art methods. It has demonstrated its effectiveness and advanced capabilities in the task of SAR-optical fusion for cloud removal.&lt;/p&gt;</content:encoded></item><item><title>Liquid: Language Models are Scalable and Unified Multi-Modal Generators</title><link>https://doi.org/10.1007/s11263-025-02639-5</link><guid>10.1007/s11263-025-02639-5</guid><pubDate>Wed, 07 Jan 2026 03:43:51 +0000</pubDate><dc:creator>Junfeng Wu</dc:creator><dc:creator>Yi Jiang</dc:creator><dc:creator>Chuofan Ma</dc:creator><dc:creator>Yuliang Liu</dc:creator><dc:creator>Hengshuang Zhao</dc:creator><dc:creator>Zehuan Yuan</dc:creator><dc:creator>Song Bai</dc:creator><dc:creator>Xiang Bai</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02639-5</prism:doi><description>We present Liquid, a versatile and native auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using any existing large language models (LLMs), eliminating the need for external pretrained visual modules such as CLIP and diffusion models. For the first time, Liquid reveals that the power-law scaling laws of unified multimodal models align with those observed in language models, and it discovers that the trade-offs between visual and language tasks diminish as model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We demonstrate that existing LLMs can serve as strong foundations for Liquid, saving training costs by 100times while surpassing Chameleon in multimodal capabilities. Compared to previous unified multimodal models, Liquid maintains on-par language performance comparable to mainstream LLMs like Llama2, preserving its potential as a foundational model. Building on this foundation, Liquid outperforms visual generation models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. The code and models are available at https://github.com/FoundationVision/Liquid .
Published: 2026-01-07T03:43:51+00:00
Venue: International Journal of Computer Vision
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junfeng Wu; Yi Jiang; Chuofan Ma; Yuliang Liu; Hengshuang Zhao; Zehuan Yuan; Song Bai; Xiang Bai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02639-5"&gt;10.1007/s11263-025-02639-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;We present Liquid, a versatile and native auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using any existing large language models (LLMs), eliminating the need for external pretrained visual modules such as CLIP and diffusion models. For the first time, Liquid reveals that the power-law scaling laws of unified multimodal models align with those observed in language models, and it discovers that the trade-offs between visual and language tasks diminish as model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We demonstrate that existing LLMs can serve as strong foundations for Liquid, saving training costs by 100times while surpassing Chameleon in multimodal capabilities. Compared to previous unified multimodal models, Liquid maintains on-par language performance comparable to mainstream LLMs like Llama2, preserving its potential as a foundational model. Building on this foundation, Liquid outperforms visual generation models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. The code and models are available at https://github.com/FoundationVision/Liquid .&lt;/p&gt;</content:encoded></item><item><title>AT-adapter: Leveraging attribute knowledge of CLIP for few-shot classification</title><link>https://doi.org/10.1016/j.neucom.2026.132627</link><guid>10.1016/j.neucom.2026.132627</guid><pubDate>Thu, 08 Jan 2026 16:17:22 +0000</pubDate><dc:creator>Yonghyeon Jo</dc:creator><dc:creator>Janghyun Kim</dc:creator><dc:creator>ChanIll Park</dc:creator><dc:creator>Seonghoon Choi</dc:creator><dc:creator>Jin-Woo Lee</dc:creator><dc:creator>Jinsun Park</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132627</prism:doi><description>The CLIP model has introduced a novel approach to training large-scale vision-language models. Recent few-shot classification works have explored various methods to leverage CLIP’s knowledge for learning. However, conventional approaches biasedly leverage CLIP knowledge from the perspective of classes. Consequently, they fall short of acquiring sufficient detailed information in the more intricate and diverse landscape of few-shot learning. To address this issue, we propose an AT-Adapter which consists of categorized attribute adapters exploiting not individual class information but only attribute information. The attribute information is obtained from a large-scale language model (i.e., GPT) which might be ignorant of some classes but still can provide generic attribute knowledge. The AT-Adapter configures the textual features of a small number of attributes as lightweight parameters, enabling the extraction of various features by leveraging CLIP’s attribute knowledge. The proposed method can be seamlessly integrated into existing few-shot classification models and provide attribute-based guidance for improving performance. Experimental results demonstrate that the proposed AT-Adapter achieves state-of-the-art performance on 10 benchmark datasets.
Published: 2026-01-08T16:17:22+00:00
Venue: Neurocomputing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yonghyeon Jo; Janghyun Kim; ChanIll Park; Seonghoon Choi; Jin-Woo Lee; Jinsun Park&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132627"&gt;10.1016/j.neucom.2026.132627&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;The CLIP model has introduced a novel approach to training large-scale vision-language models. Recent few-shot classification works have explored various methods to leverage CLIP’s knowledge for learning. However, conventional approaches biasedly leverage CLIP knowledge from the perspective of classes. Consequently, they fall short of acquiring sufficient detailed information in the more intricate and diverse landscape of few-shot learning. To address this issue, we propose an AT-Adapter which consists of categorized attribute adapters exploiting not individual class information but only attribute information. The attribute information is obtained from a large-scale language model (i.e., GPT) which might be ignorant of some classes but still can provide generic attribute knowledge. The AT-Adapter configures the textual features of a small number of attributes as lightweight parameters, enabling the extraction of various features by leveraging CLIP’s attribute knowledge. The proposed method can be seamlessly integrated into existing few-shot classification models and provide attribute-based guidance for improving performance. Experimental results demonstrate that the proposed AT-Adapter achieves state-of-the-art performance on 10 benchmark datasets.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Tucker Decomposition-based Progressive Model Compression for Convolutional Neural Networks</title><link>https://doi.org/10.1016/j.eswa.2026.131153</link><guid>10.1016/j.eswa.2026.131153</guid><pubDate>Thu, 08 Jan 2026 16:18:53 +0000</pubDate><dc:creator>Yaping He</dc:creator><dc:creator>Hao Wu</dc:creator><dc:creator>Xin Luo</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131153</prism:doi><description>Large-scale convolutional neural networks rely heavily on convolutional operations, where the tremendous parameters pose challenges for model deployment and execution in resource-constrained environments like in an end-device with limited computational power or storage. Low-rank approximation-based approaches are widely-utilized for neural network model compression. Unfortunately, existing methods of this kind fail in bridging the pre-trained weights and approximated results appropriately or selecting the approximation rank selection adaptively, leading to significant performance degradation. To address these vital issues, this paper proposes an Adaptive Tucker Decomposition-based Progressive Model Compression (ATD-PMC) method with the following three-fold ideas: 1) innovatively building a parallel structure for efficient representation of convolutional weight tensors; 2) presenting a degradation mechanism to gradually reduce the dependence on the pre-trained weights, thus enabling progressive compression; and 3) proposing a adaptive rank selection strategy based on 0-1 programming, thereby well-balancing the resultant model’s learning accuracy and compression ratio. Experimental results on five benchmark datasets demonstrate that compared with state-of-the-art compression schemes based on low-rank approximation, the proposed ATD-PMC method compresses a target neural network with the highest compression ratio as well keeps (or even slightly increases) its classification accuracy.
Published: 2026-01-08T16:18:53+00:00
Venue: Expert Systems with Applications
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaping He; Hao Wu; Xin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131153"&gt;10.1016/j.eswa.2026.131153&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale convolutional neural networks rely heavily on convolutional operations, where the tremendous parameters pose challenges for model deployment and execution in resource-constrained environments like in an end-device with limited computational power or storage. Low-rank approximation-based approaches are widely-utilized for neural network model compression. Unfortunately, existing methods of this kind fail in bridging the pre-trained weights and approximated results appropriately or selecting the approximation rank selection adaptively, leading to significant performance degradation. To address these vital issues, this paper proposes an Adaptive Tucker Decomposition-based Progressive Model Compression (ATD-PMC) method with the following three-fold ideas: 1) innovatively building a parallel structure for efficient representation of convolutional weight tensors; 2) presenting a degradation mechanism to gradually reduce the dependence on the pre-trained weights, thus enabling progressive compression; and 3) proposing a adaptive rank selection strategy based on 0-1 programming, thereby well-balancing the resultant model’s learning accuracy and compression ratio. Experimental results on five benchmark datasets demonstrate that compared with state-of-the-art compression schemes based on low-rank approximation, the proposed ATD-PMC method compresses a target neural network with the highest compression ratio as well keeps (or even slightly increases) its classification accuracy.&lt;/p&gt;</content:encoded></item><item><title>RDG-GS: Relative Depth Guidance with Gaussian Splatting for Real-time Sparse-View 3D Rendering</title><link>https://doi.org/10.1007/s11263-025-02594-1</link><guid>10.1007/s11263-025-02594-1</guid><pubDate>Wed, 07 Jan 2026 03:35:29 +0000</pubDate><dc:creator>Chenlu Zhan</dc:creator><dc:creator>Yufei Zhang</dc:creator><dc:creator>Yu Lin</dc:creator><dc:creator>Gaoang Wang</dc:creator><dc:creator>Hongwei Wang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02594-1</prism:doi><description>Efficiently synthesizing novel views from sparse inputs while maintaining accuracy remains a critical challenge in 3D reconstruction. While advanced techniques like radiance fields and 3D Gaussian Splatting achieve rendering quality and impressive efficiency with dense view inputs, they suffer from significant geometric reconstruction errors when applied to sparse input views. Moreover, although recent methods leveraging monocular depth estimation to enhance geometric learning, their dependence on single-view estimated depth often leads to view inconsistency issues across different viewpoints. Consequently, this reliance on absolute depth can introduce inaccuracies in geometric information, ultimately compromising the quality of scene reconstruction with Gaussian splats. In this paper, we present RDG-GS, a novel sparse-view 3D rendering framework with Relative Depth Guidance based on 3D Gaussian Splatting. The core innovation lies in utilizing relative depth guidance to refine the Gaussian field, steering it towards view-consistent spatial geometric representations, thereby enabling the reconstruction of accurate geometric structures and capturing intricate textures. First, we devise refined depth priors to rectify the coarse estimated depth and insert global and fine-grained scene information into regular Gaussians. Building on this, to address spatial geometric inaccuracies from absolute depth, we propose relative depth guidance by optimizing the similarity between spatially correlated patches of depth and images. Additionally, we also directly deal with the sparse areas challenging to converge by the adaptive sampling for quick densification. Across extensive experiments on Mip-NeRF360, LLFF, DTU, and Blender, RDG-GS demonstrates state-of-the-art rendering quality and efficiency, making a significant advancement for real-world applications.
Published: 2026-01-07T03:35:29+00:00
Venue: International Journal of Computer Vision
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenlu Zhan; Yufei Zhang; Yu Lin; Gaoang Wang; Hongwei Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02594-1"&gt;10.1007/s11263-025-02594-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Efficiently synthesizing novel views from sparse inputs while maintaining accuracy remains a critical challenge in 3D reconstruction. While advanced techniques like radiance fields and 3D Gaussian Splatting achieve rendering quality and impressive efficiency with dense view inputs, they suffer from significant geometric reconstruction errors when applied to sparse input views. Moreover, although recent methods leveraging monocular depth estimation to enhance geometric learning, their dependence on single-view estimated depth often leads to view inconsistency issues across different viewpoints. Consequently, this reliance on absolute depth can introduce inaccuracies in geometric information, ultimately compromising the quality of scene reconstruction with Gaussian splats. In this paper, we present RDG-GS, a novel sparse-view 3D rendering framework with Relative Depth Guidance based on 3D Gaussian Splatting. The core innovation lies in utilizing relative depth guidance to refine the Gaussian field, steering it towards view-consistent spatial geometric representations, thereby enabling the reconstruction of accurate geometric structures and capturing intricate textures. First, we devise refined depth priors to rectify the coarse estimated depth and insert global and fine-grained scene information into regular Gaussians. Building on this, to address spatial geometric inaccuracies from absolute depth, we propose relative depth guidance by optimizing the similarity between spatially correlated patches of depth and images. Additionally, we also directly deal with the sparse areas challenging to converge by the adaptive sampling for quick densification. Across extensive experiments on Mip-NeRF360, LLFF, DTU, and Blender, RDG-GS demonstrates state-of-the-art rendering quality and efficiency, making a significant advancement for real-world applications.&lt;/p&gt;</content:encoded></item><item><title>AnyDepth: Depth Estimation Made Easy</title><link>https://arxiv.org/abs/2601.02760v1</link><guid>http://arxiv.org/abs/2601.02760v1</guid><pubDate>Tue, 06 Jan 2026 06:51:35 +0000</pubDate><dc:creator>Zeyu Ren</dc:creator><dc:creator>Zeyu Zhang</dc:creator><dc:creator>Wukai Li</dc:creator><dc:creator>Qingxiang Liu</dc:creator><dc:creator>Hao Tang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.
Published: 2026-01-06T06:51:35+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeyu Ren; Zeyu Zhang; Wukai Li; Qingxiang Liu; Hao Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular depth estimation aims to recover the depth information of 3D scenes from 2D images. Recent work has made significant progress, but its reliance on large-scale datasets and complex decoders has limited its efficiency and generalization ability. In this paper, we propose a lightweight and data-centric framework for zero-shot monocular depth estimation. We first adopt DINOv3 as the visual encoder to obtain high-quality dense features. Secondly, to address the inherent drawbacks of the complex structure of the DPT, we design the Simple Depth Transformer (SDT), a compact transformer-based decoder. Compared to the DPT, it uses a single-path feature fusion and upsampling process to reduce the computational overhead of cross-scale feature fusion, achieving higher accuracy while reducing the number of parameters by approximately 85%-89%. Furthermore, we propose a quality-based filtering strategy to filter out harmful samples, thereby reducing dataset size while improving overall training quality. Extensive experiments on five benchmarks demonstrate that our framework surpasses the DPT in accuracy. This work highlights the importance of balancing model design and data quality for achieving efficient and generalizable zero-shot depth estimation. Code: https://github.com/AIGeeksGroup/AnyDepth. Website: https://aigeeksgroup.github.io/AnyDepth.&lt;/p&gt;</content:encoded></item><item><title>SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection</title><link>https://arxiv.org/abs/2601.04968v1</link><guid>http://arxiv.org/abs/2601.04968v1</guid><pubDate>Thu, 08 Jan 2026 14:16:11 +0000</pubDate><dc:creator>Maximilian Pittner</dc:creator><dc:creator>Joel Janai</dc:creator><dc:creator>Mario Faigle</dc:creator><dc:creator>Alexandru Paul Condurache</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.
Published: 2026-01-08T14:16:11+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maximilian Pittner; Joel Janai; Mario Faigle; Alexandru Paul Condurache&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.&lt;/p&gt;</content:encoded></item><item><title>Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems</title><link>https://arxiv.org/abs/2601.01696v1</link><guid>http://arxiv.org/abs/2601.01696v1</guid><pubDate>Mon, 05 Jan 2026 00:06:06 +0000</pubDate><dc:creator>Yian Liu</dc:creator><dc:creator>Xiong Wang</dc:creator><dc:creator>Ping Xu</dc:creator><dc:creator>Lei Zhu</dc:creator><dc:creator>Ming Yan</dc:creator><dc:creator>Linyun Xue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.
Published: 2026-01-05T00:06:06+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yian Liu; Xiong Wang; Ping Xu; Lei Zhu; Ming Yan; Linyun Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Multimodal Retrieval via Complementary Information Extraction and Alignment</title><link>https://arxiv.org/abs/2601.04571v1</link><guid>http://arxiv.org/abs/2601.04571v1</guid><pubDate>Thu, 08 Jan 2026 04:02:49 +0000</pubDate><dc:creator>Delong Zeng</dc:creator><dc:creator>Yuexiang Xie</dc:creator><dc:creator>Yaliang Li</dc:creator><dc:creator>Ying Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.
Published: 2026-01-08T04:02:49+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Delong Zeng; Yuexiang Xie; Yaliang Li; Ying Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in multimodal retrieval focus on capturing information in multimodal data that is similar to their paired texts, but often ignores the complementary information contained in multimodal data. In this study, we propose CIEA, a novel multimodal retrieval approach that employs Complementary Information Extraction and Alignment, which transforms both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.&lt;/p&gt;</content:encoded></item><item><title>Experimental Comparison of Light-Weight and Deep CNN Models Across Diverse Datasets</title><link>https://arxiv.org/abs/2601.03463v1</link><guid>http://arxiv.org/abs/2601.03463v1</guid><pubDate>Tue, 06 Jan 2026 23:22:22 +0000</pubDate><dc:creator>Md. Hefzul Hossain Papon</dc:creator><dc:creator>Shadman Rabby</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.
Published: 2026-01-06T23:22:22+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Md. Hefzul Hossain Papon; Shadman Rabby&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Our results reveal that a well-regularized shallow architecture can serve as a highly competitive baseline across heterogeneous domains - from smart-city surveillance to agricultural variety classification - without requiring large GPUs or specialized pre-trained models. This work establishes a unified, reproducible benchmark for multiple Bangladeshi vision datasets and highlights the practical value of lightweight CNNs for real-world deployment in low-resource settings.&lt;/p&gt;</content:encoded></item><item><title>Systematic Evaluation of Depth Backbones and Semantic Cues for Monocular Pseudo-LiDAR 3D Detection</title><link>https://arxiv.org/abs/2601.03617v1</link><guid>http://arxiv.org/abs/2601.03617v1</guid><pubDate>Wed, 07 Jan 2026 05:57:19 +0000</pubDate><dc:creator>Samson Oseiwe Ajadalu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.
Published: 2026-01-07T05:57:19+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Samson Oseiwe Ajadalu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Monocular 3D object detection offers a low-cost alternative to LiDAR, yet remains less accurate due to the difficulty of estimating metric depth from a single image. We systematically evaluate how depth backbones and feature engineering affect a monocular Pseudo-LiDAR pipeline on the KITTI validation split. Specifically, we compare NeWCRFs (supervised metric depth) against Depth Anything V2 Metric-Outdoor (Base) under an identical pseudo-LiDAR generation and PointRCNN detection protocol. NeWCRFs yields stronger downstream 3D detection, achieving 10.50\% AP$_{3D}$ at IoU$=0.7$ on the Moderate split using grayscale intensity (Exp~2). We further test point-cloud augmentations using appearance cues (grayscale intensity) and semantic cues (instance segmentation confidence). Contrary to the expectation that semantics would substantially close the gap, these features provide only marginal gains, and mask-based sampling can degrade performance by removing contextual geometry. Finally, we report a depth-accuracy-versus-distance diagnostic using ground-truth 2D boxes (including Ped/Cyc), highlighting that coarse depth correctness does not fully predict strict 3D IoU. Overall, under an off-the-shelf LiDAR detector, depth-backbone choice and geometric fidelity dominate performance, outweighing secondary feature injection.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach</title><link>https://arxiv.org/abs/2601.02016v1</link><guid>http://arxiv.org/abs/2601.02016v1</guid><pubDate>Mon, 05 Jan 2026 11:24:34 +0000</pubDate><dc:creator>Matthias Bartolo</dc:creator><dc:creator>Dylan Seychell</dc:creator><dc:creator>Gabriel Hili</dc:creator><dc:creator>Matthew Montebello</dc:creator><dc:creator>Carl James Debono</dc:creator><dc:creator>Saviour Formosa</dc:creator><dc:creator>Konstantinos Makantasis</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.
Published: 2026-01-05T11:24:34+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Matthias Bartolo; Dylan Seychell; Gabriel Hili; Matthew Montebello; Carl James Debono; Saviour Formosa; Konstantinos Makantasis&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.&lt;/p&gt;</content:encoded></item><item><title>Tokenized EEG Signals with Large Language Models for Epilepsy Detection via Multimodal Information Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104128</link><guid>10.1016/j.inffus.2026.104128</guid><pubDate>Wed, 07 Jan 2026 16:14:52 +0000</pubDate><dc:creator>Xingchi Chen</dc:creator><dc:creator>Fushen Xie</dc:creator><dc:creator>Fa Zhu</dc:creator><dc:creator>Shuanglong Zhang</dc:creator><dc:creator>Xiaoyang Lu</dc:creator><dc:creator>Qing Li</dc:creator><dc:creator>Rong Chen</dc:creator><dc:creator>Dazhou Li</dc:creator><dc:creator>David Camacho</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104128</prism:doi><description>The detection of epileptic seizures using multi-sensor EEG signals is a challenging task due to the inherent complexity of the signals, the variability in sensor configurations, and the difficulty in distinguishing the weak inter-class difference. To address these challenges, we propose a novel multimodal information fusion framework that integrates a large language model (LLM) and a multimodal EEG feature tokenization method for enhanced epilepsy detection. This paper adopts a multimodal feature extraction (MFE) method to effectively generate multimodal feature representations from EEG signals and extract different feature representations of EEG signals from different signal domains. In addition, we design a multimodal EEG feature tokenization method to tokenize EEG signal features and fuse the semantic information, solving the problem of fusing epileptic EEG features with semantic information in prompt words. We use the powerful reasoning and pattern recognition capabilities of pre-trained LLMs to accurately and robustly detect epileptic events. The proposed method is evaluated on a public dataset. Extensive experimental results show that the proposed method outperforms the current comparative methods in multiple performance indicators.
Published: 2026-01-07T16:14:52+00:00
Venue: Information Fusion
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xingchi Chen; Fushen Xie; Fa Zhu; Shuanglong Zhang; Xiaoyang Lu; Qing Li; Rong Chen; Dazhou Li; David Camacho&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104128"&gt;10.1016/j.inffus.2026.104128&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;The detection of epileptic seizures using multi-sensor EEG signals is a challenging task due to the inherent complexity of the signals, the variability in sensor configurations, and the difficulty in distinguishing the weak inter-class difference. To address these challenges, we propose a novel multimodal information fusion framework that integrates a large language model (LLM) and a multimodal EEG feature tokenization method for enhanced epilepsy detection. This paper adopts a multimodal feature extraction (MFE) method to effectively generate multimodal feature representations from EEG signals and extract different feature representations of EEG signals from different signal domains. In addition, we design a multimodal EEG feature tokenization method to tokenize EEG signal features and fuse the semantic information, solving the problem of fusing epileptic EEG features with semantic information in prompt words. We use the powerful reasoning and pattern recognition capabilities of pre-trained LLMs to accurately and robustly detect epileptic events. The proposed method is evaluated on a public dataset. Extensive experimental results show that the proposed method outperforms the current comparative methods in multiple performance indicators.&lt;/p&gt;</content:encoded></item><item><title>Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery</title><link>https://arxiv.org/abs/2601.01781v1</link><guid>http://arxiv.org/abs/2601.01781v1</guid><pubDate>Mon, 05 Jan 2026 04:28:49 +0000</pubDate><dc:creator>Lakshay Sharma</dc:creator><dc:creator>Alex Marin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.
Published: 2026-01-05T04:28:49+00:00
Venue: arXiv
Score: 0.772 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lakshay Sharma; Alex Marin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.&lt;/p&gt;</content:encoded></item></channel></rss>