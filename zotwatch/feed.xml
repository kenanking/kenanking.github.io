<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 29 Nov 2025 16:27:04 +0000</lastBuildDate><item><title>DSTransNet: Dynamic Feature Selection Network with Feature Enhancement and Multi-Attention for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638454</link><guid>10.1109/tgrs.2025.3638454</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><dc:creator>Ruimin Huang</dc:creator><dc:creator>Jun Huang</dc:creator><dc:creator>Yong Ma</dc:creator><dc:creator>Fan Fan</dc:creator><dc:creator>Yiming Zhu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3638454</prism:doi><description>Infrared small target detection (IRSTD) has significantly benefited from UNet-based neural models in recent years. However, current methodologies face challenges in achieving optimal compromise between missed detections and false alarms. To overcome this limitation, we rethink the role of each structural component within UNet-based architectures applied for IRSTD. Accordingly, we conceptualize the UNet’s encoder as specializing in feature extraction, the skip connections in feature selection, and the decoder in fusion-based reconstruction. Building upon these conceptualizations, we propose the DSTransNet. Within the feature extraction stage, the edge shape receptive field (ESR) module enhances edge and shape feature extraction and expands the receptive field via multiple convolutional branches, thereby reducing missed detections. At the feature selection stage, the reliable dynamic selection filtering (RDSF) module employs dynamic feature selection, leveraging encoder-based self-attention and decoder-based cross-attention of the Transformer to suppress background features resembling small targets and mitigate false alarms. During the feature fusion-based reconstruction stage, the cross-attention of spaces and channels (CSCE) module emphasizes small target features via spatial and channel cross-attention, reconstructing more accurate multi-scale detection masks. Extensive experiments on the SIRST, NUDT-SIRST, and SIRST-Aug datasets demonstrate that the proposed DSTransNet method outperforms state-of-the-art IRSTD approaches. The code is available at https://github.com/RuiminHuang/DSTransNet.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.856 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruimin Huang; Jun Huang; Yong Ma; Fan Fan; Yiming Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3638454"&gt;10.1109/tgrs.2025.3638454&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.856 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) has significantly benefited from UNet-based neural models in recent years. However, current methodologies face challenges in achieving optimal compromise between missed detections and false alarms. To overcome this limitation, we rethink the role of each structural component within UNet-based architectures applied for IRSTD. Accordingly, we conceptualize the UNet’s encoder as specializing in feature extraction, the skip connections in feature selection, and the decoder in fusion-based reconstruction. Building upon these conceptualizations, we propose the DSTransNet. Within the feature extraction stage, the edge shape receptive field (ESR) module enhances edge and shape feature extraction and expands the receptive field via multiple convolutional branches, thereby reducing missed detections. At the feature selection stage, the reliable dynamic selection filtering (RDSF) module employs dynamic feature selection, leveraging encoder-based self-attention and decoder-based cross-attention of the Transformer to suppress background features resembling small targets and mitigate false alarms. During the feature fusion-based reconstruction stage, the cross-attention of spaces and channels (CSCE) module emphasizes small target features via spatial and channel cross-attention, reconstructing more accurate multi-scale detection masks. Extensive experiments on the SIRST, NUDT-SIRST, and SIRST-Aug datasets demonstrate that the proposed DSTransNet method outperforms state-of-the-art IRSTD approaches. The code is available at https://github.com/RuiminHuang/DSTransNet.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Attention Mechanisms and Transformer Networks for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638738</link><guid>10.1109/tgrs.2025.3638738</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><dc:creator>Libo Cao</dc:creator><dc:creator>Yunxiu Yang</dc:creator><dc:creator>Yapei Zhu</dc:creator><dc:creator>Xiaoguang Shao</dc:creator><dc:creator>Qin Shu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3638738</prism:doi><description>Infrared small target detection plays a vital role in applications such as military surveillance and space observation. Because infrared small targets exhibit weak and indistinct features, they are often submerged within cluttered backgrounds. Capturing long-range dependencies and extracting discriminative differences between targets and backgrounds are key to improving detection accuracy. However, existing attention mechanisms and transformer network architectures have limitations, which impair the ability to explore context and capture long-distance deep dependencies. In addition, the existing methods rarely consider cross-scale feature fusion. To this end, we propose a novel network specifically for infrared small target detection called RAM-TransNet. Firstly, the whole network adopts a U-Net similar multi-attention nested pure transformer structure to learn and extract longer-distance and deeper target features. Secondly, we develop a new contextual transformer block with a dual attention structure. This contextual transformer block allows us to capture dynamic and static contextual information by making the most of the contextual information between input keys in 2D feature maps. As a result, this enhances visual features’ exploration and capture capacity. In addition, we have created a new multi-hierarchical cross-scale interaction module to aid different transformer layer features in performing multi-scale information fusion and enhancing feature perception. Finally, We evaluated our proposed method using comprehensive evaluation metrics on three public datasets. Extensive experimental results demonstrate that the proposed method is highly effective and significantly outperforms state-of-the-art methods. Moreover, the noise immunity experiment indicates that our proposed method has better noise tolerance.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.851 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Libo Cao; Yunxiu Yang; Yapei Zhu; Xiaoguang Shao; Qin Shu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3638738"&gt;10.1109/tgrs.2025.3638738&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.851 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection plays a vital role in applications such as military surveillance and space observation. Because infrared small targets exhibit weak and indistinct features, they are often submerged within cluttered backgrounds. Capturing long-range dependencies and extracting discriminative differences between targets and backgrounds are key to improving detection accuracy. However, existing attention mechanisms and transformer network architectures have limitations, which impair the ability to explore context and capture long-distance deep dependencies. In addition, the existing methods rarely consider cross-scale feature fusion. To this end, we propose a novel network specifically for infrared small target detection called RAM-TransNet. Firstly, the whole network adopts a U-Net similar multi-attention nested pure transformer structure to learn and extract longer-distance and deeper target features. Secondly, we develop a new contextual transformer block with a dual attention structure. This contextual transformer block allows us to capture dynamic and static contextual information by making the most of the contextual information between input keys in 2D feature maps. As a result, this enhances visual features’ exploration and capture capacity. In addition, we have created a new multi-hierarchical cross-scale interaction module to aid different transformer layer features in performing multi-scale information fusion and enhancing feature perception. Finally, We evaluated our proposed method using comprehensive evaluation metrics on three public datasets. Extensive experimental results demonstrate that the proposed method is highly effective and significantly outperforms state-of-the-art methods. Moreover, the noise immunity experiment indicates that our proposed method has better noise tolerance.&lt;/p&gt;</content:encoded></item><item><title>InterpIoU: Robust bounding box regression loss within an interpolation-based IoU framework</title><link>https://doi.org/10.1016/j.neucom.2025.132230</link><guid>10.1016/j.neucom.2025.132230</guid><pubDate>Fri, 28 Nov 2025 07:48:13 +0000</pubDate><dc:creator>Haoyuan Liu</dc:creator><dc:creator>Hiroshi Watanabe</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132230</prism:doi><description>Bounding box regression (BBR) is central to object detection, where regression loss plays a key role in precise localization. Existing IoU-based losses often rely on handcrafted geometric penalties to provide gradients in non-overlapping cases and improve localization. However, these geometric penalties are inherently sensitive to box geometry, producing unstable gradients in extreme cases and a subtle misalignment with the IoU objective, which harms small objects detection and yields undesired converge behaviors such as bounding box enlargement. To address these limitations, we introduce InterpIoU, an interpolation-based IoU optimization framework that rethinks BBR beyond handcrafted penalties. By bridging predictions and ground truth with interpolated boxes, InterpIoU supplies meaningful gradients in non-overlapping cases while ensuring consistent alignment with the BBR objective. Crucially, our findings challenge the convention of using geometric penalties, demonstrating they are often unnecessary and suboptimal. Building on InterpIoU, we propose Dynamic InterpIoU, which adjusts interpolation coefficients based on IoU values, adapting to diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC demonstrate that our methods consistently outperform state-of-the-art IoU-based losses across detection frameworks, including YOLOv8 and DINO, with notable improvements for small object detection.
Published: 2025-11-28T07:48:13+00:00
Venue: Neurocomputing
Score: 0.833 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoyuan Liu; Hiroshi Watanabe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132230"&gt;10.1016/j.neucom.2025.132230&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.833 (must_read)&lt;/p&gt;
&lt;p&gt;Bounding box regression (BBR) is central to object detection, where regression loss plays a key role in precise localization. Existing IoU-based losses often rely on handcrafted geometric penalties to provide gradients in non-overlapping cases and improve localization. However, these geometric penalties are inherently sensitive to box geometry, producing unstable gradients in extreme cases and a subtle misalignment with the IoU objective, which harms small objects detection and yields undesired converge behaviors such as bounding box enlargement. To address these limitations, we introduce InterpIoU, an interpolation-based IoU optimization framework that rethinks BBR beyond handcrafted penalties. By bridging predictions and ground truth with interpolated boxes, InterpIoU supplies meaningful gradients in non-overlapping cases while ensuring consistent alignment with the BBR objective. Crucially, our findings challenge the convention of using geometric penalties, demonstrating they are often unnecessary and suboptimal. Building on InterpIoU, we propose Dynamic InterpIoU, which adjusts interpolation coefficients based on IoU values, adapting to diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC demonstrate that our methods consistently outperform state-of-the-art IoU-based losses across detection frameworks, including YOLOv8 and DINO, with notable improvements for small object detection.&lt;/p&gt;</content:encoded></item><item><title>StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections</title><link>https://arxiv.org/abs/2511.20418v1</link><guid>http://arxiv.org/abs/2511.20418v1</guid><pubDate>Tue, 25 Nov 2025 15:42:33 +0000</pubDate><dc:creator>Matvei Shelukhan</dc:creator><dc:creator>Timur Mamedov</dc:creator><dc:creator>Karina Kvanchiani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.
Published: 2025-11-25T15:42:33+00:00
Venue: arXiv
Score: 0.832 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Matvei Shelukhan; Timur Mamedov; Karina Kvanchiani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.832 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.&lt;/p&gt;</content:encoded></item><item><title>PixelDiT: Pixel Diffusion Transformers for Image Generation</title><link>https://arxiv.org/abs/2511.20645v1</link><guid>http://arxiv.org/abs/2511.20645v1</guid><pubDate>Tue, 25 Nov 2025 18:59:25 +0000</pubDate><dc:creator>Yongsheng Yu</dc:creator><dc:creator>Wei Xiong</dc:creator><dc:creator>Weili Nie</dc:creator><dc:creator>Yichen Sheng</dc:creator><dc:creator>Shiqiu Liu</dc:creator><dc:creator>Jiebo Luo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.
Published: 2025-11-25T18:59:25+00:00
Venue: arXiv
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongsheng Yu; Wei Xiong; Weili Nie; Yichen Sheng; Shiqiu Liu; Jiebo Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.&lt;/p&gt;</content:encoded></item><item><title>S4DR-Net: Self-Supervised Spatial-Spectral Distance Reconstruction Network for Multispectral Point Cloud Classification</title><link>https://doi.org/10.1109/tgrs.2025.3638606</link><guid>10.1109/tgrs.2025.3638606</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><dc:creator>Qingwang Wang</dc:creator><dc:creator>Jianling Kuang</dc:creator><dc:creator>Tao Shen</dc:creator><dc:creator>Yanfeng Gu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3638606</prism:doi><description>Multispectral LiDAR point clouds are valuable in remote sensing for their spatial-spectral consistency, yet their high acquisition and annotation costs pose significant challenges. To mitigate this, self-supervised learning has emerged as a promising solution, reducing reliance on annotated data while improving model generalization. However, existing self-supervised frameworks for point clouds often overlook the complexity of ground object distribution in large-scale remote sensing scenarios and fail to leverage the spectral information inherent in multispectral point clouds. In this paper, we introduce the Self-Supervised Spatial-Spectral Distance Reconstruction Network (S4DR-Net), a novel self-supervised pre-training network designed for multispectral point cloud classification. Serving as the key component of the network, the Spatial-Spectral Distance Prediction module (S-SDP) effectively addresses these limitations by reconstructing the distance relationships between voxel blocks in three-dimensional Euclidean as well as spectral spaces. By jointly considering spatial and spectral distances, S-SDP enables the network to learn a unified representation that captures the intrinsic spatial-spectral consistency of multispectral point clouds. This design allows S4DR-Net to generate low-dimensional feature representations in a self-supervised manner, without reliance on manual annotations. We conducted experiments and evaluated on two real-world multispectral point cloud datasets. The results demonstrate that S4DR-Net consistently outperforms existing self-supervised pre-training methods, achieving superior accuracy and generalization compared with current state-of-the-art approaches. The code will be released at https://github.com/KustTeamWQW/S4DR-Net.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.824 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingwang Wang; Jianling Kuang; Tao Shen; Yanfeng Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3638606"&gt;10.1109/tgrs.2025.3638606&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.824 (must_read)&lt;/p&gt;
&lt;p&gt;Multispectral LiDAR point clouds are valuable in remote sensing for their spatial-spectral consistency, yet their high acquisition and annotation costs pose significant challenges. To mitigate this, self-supervised learning has emerged as a promising solution, reducing reliance on annotated data while improving model generalization. However, existing self-supervised frameworks for point clouds often overlook the complexity of ground object distribution in large-scale remote sensing scenarios and fail to leverage the spectral information inherent in multispectral point clouds. In this paper, we introduce the Self-Supervised Spatial-Spectral Distance Reconstruction Network (S4DR-Net), a novel self-supervised pre-training network designed for multispectral point cloud classification. Serving as the key component of the network, the Spatial-Spectral Distance Prediction module (S-SDP) effectively addresses these limitations by reconstructing the distance relationships between voxel blocks in three-dimensional Euclidean as well as spectral spaces. By jointly considering spatial and spectral distances, S-SDP enables the network to learn a unified representation that captures the intrinsic spatial-spectral consistency of multispectral point clouds. This design allows S4DR-Net to generate low-dimensional feature representations in a self-supervised manner, without reliance on manual annotations. We conducted experiments and evaluated on two real-world multispectral point cloud datasets. The results demonstrate that S4DR-Net consistently outperforms existing self-supervised pre-training methods, achieving superior accuracy and generalization compared with current state-of-the-art approaches. The code will be released at https://github.com/KustTeamWQW/S4DR-Net.&lt;/p&gt;</content:encoded></item><item><title>Lightweight Local–Global Dual-Path Feature Fusion Network for Infrared Small Target Image Super-Resolution and Enhancement</title><link>https://doi.org/10.1109/tgrs.2025.3638791</link><guid>10.1109/tgrs.2025.3638791</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><dc:creator>Haoran Jia</dc:creator><dc:creator>Xin Wang</dc:creator><dc:creator>Songyue Yang</dc:creator><dc:creator>Tongtai Cao</dc:creator><dc:creator>Yue Liu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3638791</prism:doi><description>Infrared imaging is widely used in remote sensing and military target recognition due to its strong resistance to interference in complex environments. However, imaging mechanisms and hardware limitations cause infrared images to have low-resolution, sparse textures, and significant background noise, which severely restrict the detection of small targets such as low-altitude drones and weak thermal emitters. To overcome these limitations, we propose a lightweight Local–Global Dual-Path Feature Fusion Network (LDFF-Net) that enhances the resolution and quality of infrared images, providing high-quality inputs for subsequent detection tasks. The network includes a Small Target Feature Recognition Module (STFRM) composed of three key components. The Enhanced High Frequency Perception Module (EHFPM) strengthens high-frequency details of small targets while suppressing noise, enabling robust local feature extraction. The State-Space Model (SSM) captures long-range dependencies with linear complexity and models semantic relationships between targets and background to compensate for the limited receptive field of local features. The Adaptive Feature Fusion Unit (AFFU) combines local and global features adaptively to improve the saliency of small targets. During training, we introduce a realistic degradation process based on visible-light images to generate training samples that include complex degradation patterns and noise, which enhances the model’s robustness and generalization. Evaluation on the ARCHIVE and SIRST datasets demonstrates that LDFF-Net outperforms existing state-of-the-art methods across eight widely used full-reference and no-reference metrics, including PSNR, LPIPS, FID, and NIQE. This result confirms the model’s effectiveness in enhancing both the super-resolution and detection performance of infrared small target images. The code and pretrained model weights are publicly available at https://github.com/98Hao/LDFF-Net.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.823 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoran Jia; Xin Wang; Songyue Yang; Tongtai Cao; Yue Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3638791"&gt;10.1109/tgrs.2025.3638791&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.823 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared imaging is widely used in remote sensing and military target recognition due to its strong resistance to interference in complex environments. However, imaging mechanisms and hardware limitations cause infrared images to have low-resolution, sparse textures, and significant background noise, which severely restrict the detection of small targets such as low-altitude drones and weak thermal emitters. To overcome these limitations, we propose a lightweight Local–Global Dual-Path Feature Fusion Network (LDFF-Net) that enhances the resolution and quality of infrared images, providing high-quality inputs for subsequent detection tasks. The network includes a Small Target Feature Recognition Module (STFRM) composed of three key components. The Enhanced High Frequency Perception Module (EHFPM) strengthens high-frequency details of small targets while suppressing noise, enabling robust local feature extraction. The State-Space Model (SSM) captures long-range dependencies with linear complexity and models semantic relationships between targets and background to compensate for the limited receptive field of local features. The Adaptive Feature Fusion Unit (AFFU) combines local and global features adaptively to improve the saliency of small targets. During training, we introduce a realistic degradation process based on visible-light images to generate training samples that include complex degradation patterns and noise, which enhances the model’s robustness and generalization. Evaluation on the ARCHIVE and SIRST datasets demonstrates that LDFF-Net outperforms existing state-of-the-art methods across eight widely used full-reference and no-reference metrics, including PSNR, LPIPS, FID, and NIQE. This result confirms the model’s effectiveness in enhancing both the super-resolution and detection performance of infrared small target images. The code and pretrained model weights are publicly available at https://github.com/98Hao/LDFF-Net.&lt;/p&gt;</content:encoded></item><item><title>IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection</title><link>https://arxiv.org/abs/2511.20319v1</link><guid>http://arxiv.org/abs/2511.20319v1</guid><pubDate>Tue, 25 Nov 2025 13:53:54 +0000</pubDate><dc:creator>Xuelin Qian</dc:creator><dc:creator>Jiaming Lu</dc:creator><dc:creator>Zixuan Wang</dc:creator><dc:creator>Wenxuan Wang</dc:creator><dc:creator>Zhongling Huang</dc:creator><dc:creator>Dingwen Zhang</dc:creator><dc:creator>Junwei Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.
Published: 2025-11-25T13:53:54+00:00
Venue: arXiv
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuelin Qian; Jiaming Lu; Zixuan Wang; Wenxuan Wang; Zhongling Huang; Dingwen Zhang; Junwei Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>What neuroscience can tell AI about learning in continuously changing environments</title><link>https://doi.org/10.1038/s42256-025-01146-z</link><guid>10.1038/s42256-025-01146-z</guid><pubDate>Fri, 28 Nov 2025 10:02:42 +0000</pubDate><dc:creator>Daniel Durstewitz</dc:creator><dc:creator>Bruno Averbeck</dc:creator><dc:creator>Georgia Koppe</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01146-z</prism:doi><description>Modern artificial intelligence (AI) models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task and then deployed with fixed parameters. Their training is costly, slow and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioural policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal’s behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioural tasks with shifting rules, reward probabilities or outcomes. We outline an agenda for how the links between neuroscience and AI could be tightened, thus supporting the transfer of ideas and findings between both areas and contributing to the evolving field of NeuroAI. Durstewitz et al. explore what artificial intelligence can learn from the brain’s ability to adjust quickly to changing environments. By linking neuroscience studies of flexible behaviour with advances in continual and in-context learning, this Perspective outlines ways to strengthen the exchange of ideas between the two fields and advance NeuroAI.
Published: 2025-11-28T10:02:42+00:00
Venue: Nature Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daniel Durstewitz; Bruno Averbeck; Georgia Koppe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01146-z"&gt;10.1038/s42256-025-01146-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Modern artificial intelligence (AI) models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task and then deployed with fixed parameters. Their training is costly, slow and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioural policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal’s behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioural tasks with shifting rules, reward probabilities or outcomes. We outline an agenda for how the links between neuroscience and AI could be tightened, thus supporting the transfer of ideas and findings between both areas and contributing to the evolving field of NeuroAI. Durstewitz et al. explore what artificial intelligence can learn from the brain’s ability to adjust quickly to changing environments. By linking neuroscience studies of flexible behaviour with advances in continual and in-context learning, this Perspective outlines ways to strengthen the exchange of ideas between the two fields and advance NeuroAI.&lt;/p&gt;</content:encoded></item><item><title>GLANet: Global-Local Adaptive Network for Efficient Rotated Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638781</link><guid>10.1109/tgrs.2025.3638781</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><dc:creator>Jie Xu</dc:creator><dc:creator>Liwei Deng</dc:creator><dc:creator>Tian Zhou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3638781</prism:doi><description>Rotated object detection plays a crucial role in various visual perception tasks such as aerial photography, remote sensing imagery, and low-altitude unmanned aerial vehicle (UAV) imagery. However, targets in both high-altitude remote sensing images and low-altitude UAV images often exhibit significant scale variations, diverse orientations, and dense spatial distributions, posing formidable challenges to detection algorithms in terms of accuracy and real-time performance. To address these issues, this paper proposes a Global-Local Adaptive Network for Efficient Rotated Object Detection (GLANet), designed to enhance detection precision and efficiency in complex scenarios. GLANet incorporates a lightweight backbone network, Revisiting Mobile CNN From ViT Perspective (RepViT), which balances inference efficiency with an improved capability to represent directional structural features of objects. During feature fusion, we introduce the Geometry-Enhanced Attention guided Rotated Feature Pyramid Network (GEAR-FPN), which jointly models global semantic context and local detailed features, thereby strengthening detection performance for small-scale and densely packed targets. In the detection head, we present a Dynamic Lightweight Geometric-Aware Head (DLGA-Head) alongside a Dynamic Lightweight Global Attention (Dynamic LWGA) mechanism to strengthen the representation of target orientation and boundary information. The effectiveness of the proposed method is validated on both the DOTA and CODrone datasets. GLANet achieves an mAP of 78.12% on DOTA with competitive, near-state-of-the-art accuracy and significantly higher computational efficiency than other top-performing models. Specifically, it contains only 8.64M parameters and 35.69 GFLOPs, ensuring real-time inference while maintaining high precision. On the CODrone dataset, it further delivers improved detection performance while maintaining superior efficiency compared with existing approaches.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Xu; Liwei Deng; Tian Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3638781"&gt;10.1109/tgrs.2025.3638781&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Rotated object detection plays a crucial role in various visual perception tasks such as aerial photography, remote sensing imagery, and low-altitude unmanned aerial vehicle (UAV) imagery. However, targets in both high-altitude remote sensing images and low-altitude UAV images often exhibit significant scale variations, diverse orientations, and dense spatial distributions, posing formidable challenges to detection algorithms in terms of accuracy and real-time performance. To address these issues, this paper proposes a Global-Local Adaptive Network for Efficient Rotated Object Detection (GLANet), designed to enhance detection precision and efficiency in complex scenarios. GLANet incorporates a lightweight backbone network, Revisiting Mobile CNN From ViT Perspective (RepViT), which balances inference efficiency with an improved capability to represent directional structural features of objects. During feature fusion, we introduce the Geometry-Enhanced Attention guided Rotated Feature Pyramid Network (GEAR-FPN), which jointly models global semantic context and local detailed features, thereby strengthening detection performance for small-scale and densely packed targets. In the detection head, we present a Dynamic Lightweight Geometric-Aware Head (DLGA-Head) alongside a Dynamic Lightweight Global Attention (Dynamic LWGA) mechanism to strengthen the representation of target orientation and boundary information. The effectiveness of the proposed method is validated on both the DOTA and CODrone datasets. GLANet achieves an mAP of 78.12% on DOTA with competitive, near-state-of-the-art accuracy and significantly higher computational efficiency than other top-performing models. Specifically, it contains only 8.64M parameters and 35.69 GFLOPs, ensuring real-time inference while maintaining high precision. On the CODrone dataset, it further delivers improved detection performance while maintaining superior efficiency compared with existing approaches.&lt;/p&gt;</content:encoded></item><item><title>From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting</title><link>https://arxiv.org/abs/2511.21215v1</link><guid>http://arxiv.org/abs/2511.21215v1</guid><pubDate>Wed, 26 Nov 2025 09:44:51 +0000</pubDate><dc:creator>Umang Agarwal</dc:creator><dc:creator>Rudraksh Sangore</dc:creator><dc:creator>Sumit Laddha</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.
Published: 2025-11-26T09:44:51+00:00
Venue: arXiv
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Umang Agarwal; Rudraksh Sangore; Sumit Laddha&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&amp;lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.&lt;/p&gt;</content:encoded></item><item><title>Dual-Pathway Feature Separation and Gated Fusion Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638739</link><guid>10.1109/tgrs.2025.3638739</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><dc:creator>Xiaoyang Yuan</dc:creator><dc:creator>Yan Zhang</dc:creator><dc:creator>Chunling Yang</dc:creator><dc:creator>Jiankai Zhu</dc:creator><dc:creator>Hanwen Zhang</dc:creator><dc:creator>Aishi Zhou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3638739</prism:doi><description>Infrared Small Target Detection (IRSTD) plays a vital role in Infrared Search and Tracking (IRST), enabling intelligent systems to accurately detect dim and small targets within cluttered thermal environments. However, most existing deep learning approaches for IRSTD employ a unified-pathway architecture that conflates saliency and edge information within a shared representation space. This limitation causes feature entanglement, hindering the network’s capacity to accurately separate and represent global saliency and fine-grained edge contours. To overcome these challenges, we propose LoveNet, a dual-pathway network architecture that explicitly separates feature learning into two specialized branches. The first is a multi-scale saliency learning branch designed to extract comprehensive structural and contrast information, capturing the global context of targets. The second is a fixed-scale edge learning branch aimed at preserving spatial details and enhancing the precision of edge contour delineation. To integrate the heterogeneous features extracted by two branches, a gated feature fusion mechanism is proposed to adaptively combine saliency and edge representations based on their spatial and semantic relevance. Furthermore, to provide robust and comprehensive supervision, a hybrid supervision strategy is designed to guide the learning process of hierarchical feature representations. Experiments on the NUDT-SIRST, IRSTD-1k, and SIRST datasets demonstrate that LoveNet consistently achieves the best segmentation performance compared to the state-of-the-art methods, while maintaining a lightweight structure suitable for real-time applications.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoyang Yuan; Yan Zhang; Chunling Yang; Jiankai Zhu; Hanwen Zhang; Aishi Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3638739"&gt;10.1109/tgrs.2025.3638739&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared Small Target Detection (IRSTD) plays a vital role in Infrared Search and Tracking (IRST), enabling intelligent systems to accurately detect dim and small targets within cluttered thermal environments. However, most existing deep learning approaches for IRSTD employ a unified-pathway architecture that conflates saliency and edge information within a shared representation space. This limitation causes feature entanglement, hindering the network’s capacity to accurately separate and represent global saliency and fine-grained edge contours. To overcome these challenges, we propose LoveNet, a dual-pathway network architecture that explicitly separates feature learning into two specialized branches. The first is a multi-scale saliency learning branch designed to extract comprehensive structural and contrast information, capturing the global context of targets. The second is a fixed-scale edge learning branch aimed at preserving spatial details and enhancing the precision of edge contour delineation. To integrate the heterogeneous features extracted by two branches, a gated feature fusion mechanism is proposed to adaptively combine saliency and edge representations based on their spatial and semantic relevance. Furthermore, to provide robust and comprehensive supervision, a hybrid supervision strategy is designed to guide the learning process of hierarchical feature representations. Experiments on the NUDT-SIRST, IRSTD-1k, and SIRST datasets demonstrate that LoveNet consistently achieves the best segmentation performance compared to the state-of-the-art methods, while maintaining a lightweight structure suitable for real-time applications.&lt;/p&gt;</content:encoded></item><item><title>Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models</title><link>https://arxiv.org/abs/2511.19822v1</link><guid>http://arxiv.org/abs/2511.19822v1</guid><pubDate>Tue, 25 Nov 2025 01:24:41 +0000</pubDate><dc:creator>Wentao Hu</dc:creator><dc:creator>Mingkuan Zhao</dc:creator><dc:creator>Shuangyong Song</dc:creator><dc:creator>Xiaoyan Zhu</dc:creator><dc:creator>Xin Lai</dc:creator><dc:creator>Jiayin Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.
Published: 2025-11-25T01:24:41+00:00
Venue: arXiv
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wentao Hu; Mingkuan Zhao; Shuangyong Song; Xiaoyan Zhu; Xin Lai; Jiayin Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select&amp;quot; process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model&amp;#x27;s capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.&lt;/p&gt;</content:encoded></item><item><title>SAM-MI: A Mask-Injected Framework for Enhancing Open-Vocabulary Semantic Segmentation with SAM</title><link>https://arxiv.org/abs/2511.20027v1</link><guid>http://arxiv.org/abs/2511.20027v1</guid><pubDate>Tue, 25 Nov 2025 07:52:07 +0000</pubDate><dc:creator>Lin Chen</dc:creator><dc:creator>Yingjian Zhu</dc:creator><dc:creator>Qi Yang</dc:creator><dc:creator>Xin Niu</dc:creator><dc:creator>Kun Ding</dc:creator><dc:creator>Shiming Xiang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1007/s11633-025-1615-8</prism:doi><description>Open-vocabulary semantic segmentation (OVSS) aims to segment and recognize objects universally. Trained on extensive high-quality segmentation data, the segment anything model (SAM) has demonstrated remarkable universal segmentation capabilities, offering valuable support for OVSS. Although previous methods have made progress in leveraging SAM for OVSS, there are still some challenges: (1) SAM's tendency to over-segment and (2) hard combinations between fixed masks and labels. This paper introduces a novel mask-injected framework, SAM-MI, which effectively integrates SAM with OVSS models to address these challenges. Initially, SAM-MI employs a Text-guided Sparse Point Prompter to sample sparse prompts for SAM instead of previous dense grid-like prompts, thus significantly accelerating the mask generation process. The framework then introduces Shallow Mask Aggregation (SMAgg) to merge partial masks to mitigate the SAM's over-segmentation issue. Finally, Decoupled Mask Injection (DMI) incorporates SAM-generated masks for guidance at low-frequency and high-frequency separately, rather than directly combining them with labels. Extensive experiments on multiple benchmarks validate the superiority of SAM-MI. Notably, the proposed method achieves a 16.7% relative improvement in mIoU over Grounded-SAM on the MESS benchmark, along with a 1.6$\times$ speedup. We hope SAM-MI can serve as an alternative methodology to effectively equip the OVSS model with SAM.
Published: 2025-11-25T07:52:07+00:00
Venue: arXiv
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lin Chen; Yingjian Zhu; Qi Yang; Xin Niu; Kun Ding; Shiming Xiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11633-025-1615-8"&gt;10.1007/s11633-025-1615-8&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Open-vocabulary semantic segmentation (OVSS) aims to segment and recognize objects universally. Trained on extensive high-quality segmentation data, the segment anything model (SAM) has demonstrated remarkable universal segmentation capabilities, offering valuable support for OVSS. Although previous methods have made progress in leveraging SAM for OVSS, there are still some challenges: (1) SAM&amp;#x27;s tendency to over-segment and (2) hard combinations between fixed masks and labels. This paper introduces a novel mask-injected framework, SAM-MI, which effectively integrates SAM with OVSS models to address these challenges. Initially, SAM-MI employs a Text-guided Sparse Point Prompter to sample sparse prompts for SAM instead of previous dense grid-like prompts, thus significantly accelerating the mask generation process. The framework then introduces Shallow Mask Aggregation (SMAgg) to merge partial masks to mitigate the SAM&amp;#x27;s over-segmentation issue. Finally, Decoupled Mask Injection (DMI) incorporates SAM-generated masks for guidance at low-frequency and high-frequency separately, rather than directly combining them with labels. Extensive experiments on multiple benchmarks validate the superiority of SAM-MI. Notably, the proposed method achieves a 16.7% relative improvement in mIoU over Grounded-SAM on the MESS benchmark, along with a 1.6$\times$ speedup. We hope SAM-MI can serve as an alternative methodology to effectively equip the OVSS model with SAM.&lt;/p&gt;</content:encoded></item><item><title>VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction</title><link>https://arxiv.org/abs/2511.19971v1</link><guid>http://arxiv.org/abs/2511.19971v1</guid><pubDate>Tue, 25 Nov 2025 06:30:22 +0000</pubDate><dc:creator>Yu Hu</dc:creator><dc:creator>Chong Cheng</dc:creator><dc:creator>Sicheng Yu</dc:creator><dc:creator>Xiaoyang Guo</dc:creator><dc:creator>Hao Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.
Published: 2025-11-25T06:30:22+00:00
Venue: arXiv
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Hu; Chong Cheng; Sicheng Yu; Xiaoyang Guo; Hao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT&amp;#x27;s global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT&amp;#x27;s early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.&lt;/p&gt;</content:encoded></item><item><title>DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs</title><link>https://arxiv.org/abs/2511.20468v1</link><guid>http://arxiv.org/abs/2511.20468v1</guid><pubDate>Tue, 25 Nov 2025 16:33:42 +0000</pubDate><dc:creator>Yuanhao Li</dc:creator><dc:creator>Mingshan Liu</dc:creator><dc:creator>Hongbo Wang</dc:creator><dc:creator>Yiding Zhang</dc:creator><dc:creator>Yifei Ma</dc:creator><dc:creator>Wei Tan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed
Published: 2025-11-25T16:33:42+00:00
Venue: arXiv
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanhao Li; Mingshan Liu; Hongbo Wang; Yiding Zhang; Yifei Ma; Wei Tan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other&amp;#x27;s outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed&lt;/p&gt;</content:encoded></item><item><title>ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images</title><link>https://arxiv.org/abs/2511.21606v1</link><guid>http://arxiv.org/abs/2511.21606v1</guid><pubDate>Wed, 26 Nov 2025 17:26:00 +0000</pubDate><dc:creator>M. Naseer Subhani</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.
Published: 2025-11-26T17:26:00+00:00
Venue: arXiv
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; M. Naseer Subhani&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM&amp;#x27;s segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>Momentum-Enhanced Dual-Prototype Learning Framework for Robust Few-Shot Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tgrs.2025.3638757</link><guid>10.1109/tgrs.2025.3638757</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><dc:creator>Hanchi Liu</dc:creator><dc:creator>Jinrong He</dc:creator><dc:creator>Xiangqing Zhang</dc:creator><dc:creator>Zhaokui Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3638757</prism:doi><description>Prototypical network-based few-shot learning (FSL) has demonstrated promising performance for hyperspectral image (HSI) classification tasks under scarce sample conditions. However, existing prototype-based FSL methods suffer from data distribution variations among randomly sampled tasks, leading to unstable class prototype representations and weak cross-task generalization with limited samples. To address this issue, we propose a momentum-enhanced dual-prototype learning (MEDPL) framework for robust few-shot HSI classification. Firstly, a momentum-updated prototype mechanism constructs an iteratively optimized prototype memory bank. It obtains accumulated prototypes by exponentially decaying weighted fusion of historical and current prototypes, significantly suppressing noise from randomly sampled data and class center shifts caused by distribution bias. Simultaneously, a class-conditioned perturbation-augmentation strategy is introduced. It generates adaptive noise perturbations for support set features based on learnable covariance matrices to obtain enhanced prototypes, thereby improving the generalization representation capability of class prototypes across tasks. Secondly, a dual-prototype metric learning framework is designed, jointly utilizing accumulated prototypes and enhanced prototypes to synergistically enhance the model’s classification stability and cross-task generalization, thus significantly improving the robustness of few-shot classification. Experimental results demonstrate that MEDPL outperforms other few-shot hyperspectral image classification methods. Our source code is available at https://github.com/hejinrong/MEDPL.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanchi Liu; Jinrong He; Xiangqing Zhang; Zhaokui Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3638757"&gt;10.1109/tgrs.2025.3638757&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Prototypical network-based few-shot learning (FSL) has demonstrated promising performance for hyperspectral image (HSI) classification tasks under scarce sample conditions. However, existing prototype-based FSL methods suffer from data distribution variations among randomly sampled tasks, leading to unstable class prototype representations and weak cross-task generalization with limited samples. To address this issue, we propose a momentum-enhanced dual-prototype learning (MEDPL) framework for robust few-shot HSI classification. Firstly, a momentum-updated prototype mechanism constructs an iteratively optimized prototype memory bank. It obtains accumulated prototypes by exponentially decaying weighted fusion of historical and current prototypes, significantly suppressing noise from randomly sampled data and class center shifts caused by distribution bias. Simultaneously, a class-conditioned perturbation-augmentation strategy is introduced. It generates adaptive noise perturbations for support set features based on learnable covariance matrices to obtain enhanced prototypes, thereby improving the generalization representation capability of class prototypes across tasks. Secondly, a dual-prototype metric learning framework is designed, jointly utilizing accumulated prototypes and enhanced prototypes to synergistically enhance the model’s classification stability and cross-task generalization, thus significantly improving the robustness of few-shot classification. Experimental results demonstrate that MEDPL outperforms other few-shot hyperspectral image classification methods. Our source code is available at https://github.com/hejinrong/MEDPL.&lt;/p&gt;</content:encoded></item><item><title>V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence</title><link>https://arxiv.org/abs/2511.20886v1</link><guid>http://arxiv.org/abs/2511.20886v1</guid><pubDate>Tue, 25 Nov 2025 22:06:30 +0000</pubDate><dc:creator>Jiancheng Pan</dc:creator><dc:creator>Runze Wang</dc:creator><dc:creator>Tianwen Qian</dc:creator><dc:creator>Mohammad Mahdi</dc:creator><dc:creator>Yanwei Fu</dc:creator><dc:creator>Xiangyang Xue</dc:creator><dc:creator>Xiaomeng Huang</dc:creator><dc:creator>Luc Van Gool</dc:creator><dc:creator>Danda Pani Paudel</dc:creator><dc:creator>Yuqian Fu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).
Published: 2025-11-25T22:06:30+00:00
Venue: arXiv
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiancheng Pan; Runze Wang; Tianwen Qian; Mohammad Mahdi; Yanwei Fu; Xiangyang Xue; Xiaomeng Huang; Luc Van Gool; Danda Pani Paudel; Yuqian Fu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).&lt;/p&gt;</content:encoded></item><item><title>Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits</title><link>https://arxiv.org/abs/2511.20273v1</link><guid>http://arxiv.org/abs/2511.20273v1</guid><pubDate>Tue, 25 Nov 2025 12:59:15 +0000</pubDate><dc:creator>Areeb Ahmad</dc:creator><dc:creator>Abhinav Joshi</dc:creator><dc:creator>Ashutosh Modi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.
Published: 2025-11-25T12:59:15+00:00
Venue: arXiv
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Areeb Ahmad; Abhinav Joshi; Ashutosh Modi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.&lt;/p&gt;</content:encoded></item><item><title>RDAM: Domain Adaptation under Small and Class-Imbalanced Samples</title><link>https://doi.org/10.1016/j.knosys.2025.114909</link><guid>10.1016/j.knosys.2025.114909</guid><pubDate>Fri, 28 Nov 2025 16:05:05 +0000</pubDate><dc:creator>Youquan Fu</dc:creator><dc:creator>Song Huang</dc:creator><dc:creator>Zhixi Feng</dc:creator><dc:creator>Yue Ma</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.114909</prism:doi><description>Domain Adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain by aligning their feature distributions. Although notable advancements have been achieved, domain adaptation remains challenging for scenarios involving small sample size and class-imbalanced datasets, where limited and imbalanced data hinder effective domain alignment. To address this issue, we propose RDAM (Feature Regeneration Domain Adaptation with Manifold Maintenance Loss), a novel framework that enhances cross domain generalization under data scarcity and class imbalance. Our key idea is to perform feature regeneration in the source domain to balance feature quantities, thereby constructing a more comprehensive and inclusive source representation whose spatial distribution effectively covers that of the target domain. To further preserve local geometric structures and improve alignment of ambiguous or boundary samples, we introduce the manifold maintenance loss, which enforces consistency in neighborhood relationships across domains. We evaluate RDAM on four time series datasets and four image domain adaptation benchmarks. Extensive experiments show that our method achieves superior accuracy and robustness across diverse modalities and imbalance settings.
Published: 2025-11-28T16:05:05+00:00
Venue: Knowledge-Based Systems
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Youquan Fu; Song Huang; Zhixi Feng; Yue Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.114909"&gt;10.1016/j.knosys.2025.114909&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Domain Adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain by aligning their feature distributions. Although notable advancements have been achieved, domain adaptation remains challenging for scenarios involving small sample size and class-imbalanced datasets, where limited and imbalanced data hinder effective domain alignment. To address this issue, we propose RDAM (Feature Regeneration Domain Adaptation with Manifold Maintenance Loss), a novel framework that enhances cross domain generalization under data scarcity and class imbalance. Our key idea is to perform feature regeneration in the source domain to balance feature quantities, thereby constructing a more comprehensive and inclusive source representation whose spatial distribution effectively covers that of the target domain. To further preserve local geometric structures and improve alignment of ambiguous or boundary samples, we introduce the manifold maintenance loss, which enforces consistency in neighborhood relationships across domains. We evaluate RDAM on four time series datasets and four image domain adaptation benchmarks. Extensive experiments show that our method achieves superior accuracy and robustness across diverse modalities and imbalance settings.&lt;/p&gt;</content:encoded></item><item><title>Spatially-Aware Adaptive Diffusion: Unifying Low-Resolution Image Fusion and Super-Resolution</title><link>https://doi.org/10.1109/tcsvt.2025.3638425</link><guid>10.1109/tcsvt.2025.3638425</guid><pubDate>Fri, 28 Nov 2025 18:44:43 +0000</pubDate><dc:creator>Jiajia Fu</dc:creator><dc:creator>Zhenni Yu</dc:creator><dc:creator>Haosheng Chen</dc:creator><dc:creator>Songlin Du</dc:creator><dc:creator>Changcai Yang</dc:creator><dc:creator>Lianghua He</dc:creator><dc:creator>Guobao Xiao</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3638425</prism:doi><description>Low-resolution visible-infrared image fusion and super-resolution (LRVIF) are critical for enhancing image quality in low-resolution scenarios, yet limited information in the input images often constrains performance. To address these challenges, we propose SaDiff, a spatially-aware adaptive diffusion model that introduces diffusion processes into LRVIF for the first time, representing a major breakthrough in the field. Leveraging the generative capabilities of diffusion models, our approach unifies and enhances image fusion and super-resolution within a cohesive framework. A key component of SaDiff is the Spatial Residual Adaptation Block, which extends the diffusion process by dynamically adapting feature representations to spatial variations in the local regions of the input images. This module maximally preserves crucial information from the input images, such as texture details and contrast, while effectively suppressing noise, ensuring robust and context-aware feature refinement. Then we further propose Direct Diffusion Synthesis, a novel mechanism that utilizes noise predictions during diffusion to generate fused images, enabling joint training of the fusion and super-resolution networks. Additionally, a Cross-Feature Fusion Module integrates texture and contrast details, producing super-resolution fused images with improved clarity and structural integrity. Extensive experiments show that SaDiff achieves state-of-the-art performance, offering a robust and unified solution to infrared-visible image fusion and super-resolution. The code for the proposed method will be made available at https://github.com/guobaoxiao/SaDiff.
Published: 2025-11-28T18:44:43+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiajia Fu; Zhenni Yu; Haosheng Chen; Songlin Du; Changcai Yang; Lianghua He; Guobao Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3638425"&gt;10.1109/tcsvt.2025.3638425&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Low-resolution visible-infrared image fusion and super-resolution (LRVIF) are critical for enhancing image quality in low-resolution scenarios, yet limited information in the input images often constrains performance. To address these challenges, we propose SaDiff, a spatially-aware adaptive diffusion model that introduces diffusion processes into LRVIF for the first time, representing a major breakthrough in the field. Leveraging the generative capabilities of diffusion models, our approach unifies and enhances image fusion and super-resolution within a cohesive framework. A key component of SaDiff is the Spatial Residual Adaptation Block, which extends the diffusion process by dynamically adapting feature representations to spatial variations in the local regions of the input images. This module maximally preserves crucial information from the input images, such as texture details and contrast, while effectively suppressing noise, ensuring robust and context-aware feature refinement. Then we further propose Direct Diffusion Synthesis, a novel mechanism that utilizes noise predictions during diffusion to generate fused images, enabling joint training of the fusion and super-resolution networks. Additionally, a Cross-Feature Fusion Module integrates texture and contrast details, producing super-resolution fused images with improved clarity and structural integrity. Extensive experiments show that SaDiff achieves state-of-the-art performance, offering a robust and unified solution to infrared-visible image fusion and super-resolution. The code for the proposed method will be made available at https://github.com/guobaoxiao/SaDiff.&lt;/p&gt;</content:encoded></item><item><title>HTTM: Head-wise Temporal Token Merging for Faster VGGT</title><link>https://arxiv.org/abs/2511.21317v1</link><guid>http://arxiv.org/abs/2511.21317v1</guid><pubDate>Wed, 26 Nov 2025 12:04:03 +0000</pubDate><dc:creator>Weitian Wang</dc:creator><dc:creator>Lukas Meiner</dc:creator><dc:creator>Rai Shubham</dc:creator><dc:creator>Cecilia De La Parra</dc:creator><dc:creator>Akash Kumar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.
Published: 2025-11-26T12:04:03+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weitian Wang; Lukas Meiner; Rai Shubham; Cecilia De La Parra; Akash Kumar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers&amp;#x27; output, which hinders the model&amp;#x27;s representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.&lt;/p&gt;</content:encoded></item><item><title>AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend</title><link>https://arxiv.org/abs/2511.20343v1</link><guid>http://arxiv.org/abs/2511.20343v1</guid><pubDate>Tue, 25 Nov 2025 14:23:04 +0000</pubDate><dc:creator>Hengyi Wang</dc:creator><dc:creator>Lourdes Agapito</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.
Published: 2025-11-25T14:23:04+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hengyi Wang; Lourdes Agapito&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Adam Simplified: Bias Correction Debunked</title><link>https://arxiv.org/abs/2511.20516v2</link><guid>http://arxiv.org/abs/2511.20516v2</guid><pubDate>Tue, 25 Nov 2025 17:20:40 +0000</pubDate><dc:creator>Sam Laing</dc:creator><dc:creator>Antonio Orvieto</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.
Published: 2025-11-25T17:20:40+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sam Laing; Antonio Orvieto&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.&lt;/p&gt;</content:encoded></item><item><title>Sawtooth Sampling for Time Series Denoising Diffusion Implicit Models</title><link>https://arxiv.org/abs/2511.21320v1</link><guid>http://arxiv.org/abs/2511.21320v1</guid><pubDate>Wed, 26 Nov 2025 12:05:44 +0000</pubDate><dc:creator>Heiko Oppel</dc:creator><dc:creator>Andreas Spilz</dc:creator><dc:creator>Michael Munz</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.
Published: 2025-11-26T12:05:44+00:00
Venue: arXiv
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Heiko Oppel; Andreas Spilz; Michael Munz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Denoising Diffusion Probabilistic Models (DDPMs) can generate synthetic timeseries data to help improve the performance of a classifier, but their sampling process is computationally expensive. We address this by combining implicit diffusion models with a novel Sawtooth Sampler that accelerates the reverse process and can be applied to any pretrained diffusion model. Our approach achieves a 30 times speed-up over the standard baseline while also enhancing the quality of the generated sequences for classification tasks.&lt;/p&gt;</content:encoded></item><item><title>ERDDCI: Exact Reversible Diffusion via Dual-Chain Inversion for High-Quality Image Editing</title><link>https://doi.org/10.1109/tcsvt.2025.3638406</link><guid>10.1109/tcsvt.2025.3638406</guid><pubDate>Fri, 28 Nov 2025 18:44:43 +0000</pubDate><dc:creator>Jimin Dai</dc:creator><dc:creator>Yingzhen Zhang</dc:creator><dc:creator>Shuo Chen</dc:creator><dc:creator>Jian Yang</dc:creator><dc:creator>Lei Luo</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3638406</prism:doi><description>Diffusion models (DMs) have been successfully applied to real image editing. These models typically invert images into latent noise vectors during the inversion process, and then edit them during the inference process. However, DMs often rely on the local linearization assumption, which assumes that the noise injected during the inversion process approximates the noise removed during the inference process. While DMs efficiently generate images under this assumption, it also accumulates errors during the diffusion process due to the assumption, ultimately negatively impacting the quality of real image reconstruction and editing. To address this issue, we propose a novel ERDDCI (Exact Reversible Diffusion via Dual-Chain Inversion). ERDDCI uses the new Dual-Chain Inversion (DCI) for joint inference to derive an exact reversible diffusion process. Using DCI, our method avoids the cumbersome optimization process in existing inversion approaches and achieves high-quality image editing. Additionally, to accommodate image operations under high guidance scales, we introduce a dynamic control strategy that enables more refined image reconstruction and editing. Our experiments demonstrate that ERDDCI significantly outperforms state-of-the-art methods in a 50-step diffusion process. It achieves rapid and precise image reconstruction with SSIM of 0.999 and LPIPS of 0.001, and delivers competitive results in image editing. The source code is available at: https://github.com/daii-y/ERDDCI.
Published: 2025-11-28T18:44:43+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jimin Dai; Yingzhen Zhang; Shuo Chen; Jian Yang; Lei Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3638406"&gt;10.1109/tcsvt.2025.3638406&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models (DMs) have been successfully applied to real image editing. These models typically invert images into latent noise vectors during the inversion process, and then edit them during the inference process. However, DMs often rely on the local linearization assumption, which assumes that the noise injected during the inversion process approximates the noise removed during the inference process. While DMs efficiently generate images under this assumption, it also accumulates errors during the diffusion process due to the assumption, ultimately negatively impacting the quality of real image reconstruction and editing. To address this issue, we propose a novel ERDDCI (Exact Reversible Diffusion via Dual-Chain Inversion). ERDDCI uses the new Dual-Chain Inversion (DCI) for joint inference to derive an exact reversible diffusion process. Using DCI, our method avoids the cumbersome optimization process in existing inversion approaches and achieves high-quality image editing. Additionally, to accommodate image operations under high guidance scales, we introduce a dynamic control strategy that enables more refined image reconstruction and editing. Our experiments demonstrate that ERDDCI significantly outperforms state-of-the-art methods in a 50-step diffusion process. It achieves rapid and precise image reconstruction with SSIM of 0.999 and LPIPS of 0.001, and delivers competitive results in image editing. The source code is available at: https://github.com/daii-y/ERDDCI.&lt;/p&gt;</content:encoded></item><item><title>Escaping the Verifier: Learning to Reason via Demonstrations</title><link>https://arxiv.org/abs/2511.21667v1</link><guid>http://arxiv.org/abs/2511.21667v1</guid><pubDate>Wed, 26 Nov 2025 18:42:52 +0000</pubDate><dc:creator>Locke Cai</dc:creator><dc:creator>Ivan Provilkov</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
Published: 2025-11-26T18:42:52+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Locke Cai; Ivan Provilkov&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.&lt;/p&gt;</content:encoded></item><item><title>Seeing without Pixels: Perception from Camera Trajectories</title><link>https://arxiv.org/abs/2511.21681v1</link><guid>http://arxiv.org/abs/2511.21681v1</guid><pubDate>Wed, 26 Nov 2025 18:57:01 +0000</pubDate><dc:creator>Zihui Xue</dc:creator><dc:creator>Kristen Grauman</dc:creator><dc:creator>Dima Damen</dc:creator><dc:creator>Andrew Zisserman</dc:creator><dc:creator>Tengda Han</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.
Published: 2025-11-26T18:57:01+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihui Xue; Kristen Grauman; Dima Damen; Andrew Zisserman; Tengda Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Can one perceive a video&amp;#x27;s content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, &amp;quot;how you move&amp;quot; can indeed reveal &amp;quot;what you are doing&amp;quot; (egocentric) or &amp;quot;observing&amp;quot; (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.&lt;/p&gt;</content:encoded></item><item><title>OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection</title><link>https://arxiv.org/abs/2511.21064v1</link><guid>http://arxiv.org/abs/2511.21064v1</guid><pubDate>Wed, 26 Nov 2025 05:08:26 +0000</pubDate><dc:creator>Chujie Wang</dc:creator><dc:creator>Jianyu Lu</dc:creator><dc:creator>Zhiyuan Luo</dc:creator><dc:creator>Xi Chen</dc:creator><dc:creator>Chu He</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.
Published: 2025-11-26T05:08:26+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chujie Wang; Jianyu Lu; Zhiyuan Luo; Xi Chen; Chu He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD&amp;#x27;s lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent&amp;#x27;s state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.&lt;/p&gt;</content:encoded></item></channel></rss>