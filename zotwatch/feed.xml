<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 01 Feb 2026 03:48:04 +0000</lastBuildDate><item><title>CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation</title><link>https://doi.org/10.1109/tip.2026.3657240</link><guid>10.1109/tip.2026.3657240</guid><pubDate>Fri, 30 Jan 2026 21:05:59 +0000</pubDate><dc:creator>Shilong Zou</dc:creator><dc:creator>Yuhang Huang</dc:creator><dc:creator>Renjiao Yi</dc:creator><dc:creator>Chenyang Zhu</dc:creator><dc:creator>Kai Xu</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657240</prism:doi><description>We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GAN-based methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusion-based studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an end-to- end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB↔RGB and diverse cross-modality translation tasks including RGB↔Edge, RGB↔Semantics and RGB↔Depth, showcasing better generative performances than the state of the arts. Especially, our method achieves the best FID score in widely-adopted tasks and outperforms the second-best method with an improved FID of 19.61 and 19.67 on Dog→Cat and Dog→Wil...
Published: 2026-01-30T21:05:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shilong Zou; Yuhang Huang; Renjiao Yi; Chenyang Zhu; Kai Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657240"&gt;10.1109/tip.2026.3657240&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GAN-based methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusion-based studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an end-to- end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB↔RGB and diverse cross-modality translation tasks including RGB↔Edge, RGB↔Semantics and RGB↔Depth, showcasing better generative performances than the state of the arts. Especially, our method achieves the best FID score in widely-adopted tasks and outperforms the second-best method with an improved FID of 19.61 and 19.67 on Dog→Cat and Dog→Wil...&lt;/p&gt;</content:encoded></item><item><title>KFIA-Net: a knowledge fusion and imbalance-aware network for multi-category SAR ship detection</title><link>https://doi.org/10.1016/j.jag.2026.105127</link><guid>10.1016/j.jag.2026.105127</guid><pubDate>Fri, 30 Jan 2026 17:36:20 +0000</pubDate><dc:creator>Zhongzhen Sun</dc:creator><dc:creator>Xianghui Zhang</dc:creator><dc:creator>Xiangguang Leng</dc:creator><dc:creator>Xueqi Wu</dc:creator><dc:creator>Boli Xiong</dc:creator><dc:creator>Kefeng Ji</dc:creator><dc:creator>Gangyao Kuang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105127</prism:doi><description>Multi-category synthetic aperture radar (SAR) ship detection is limited by heterogeneity in imaging mechanisms and severe class imbalance, yielding accurate localization but frequent misclassification. To address this issue, this paper proposes a Knowledge Fusion and Imbalance-Aware Network (KFIA-Net). Specifically, we first propose a Domain Knowledge Feature Extraction (DKFE) to extract and encode knowledge tokens from four priors. Second, a Knowledge Cross-Attention Fusion (KCAF) module is designed to perform interpretable and sparsely selectable channel modulation using cross-attention and FiLM decoding. Thirdly, we further design an Imbalance-Aware Loss Function (IALF) that combines prior calibration, minority category margin expansion, and knowledge-consistency weighting to reduce loss bias. Finally, systematic experiments and comparisons are conducted on three datasets: SRSDD-v1.0, FAIR-CSAR-v1.0, and NUDT-SARship-v1.0. Our KFIA-Net achieves mAP 50 scores of 64.29%, 37.99%, and 78.26%, and mAP 75 scores of 34.96%, 19.70%, and 66.36%, respectively. These results demonstrate knowledge injection simultaneously improves class accuracy and sustains robust localization. Furthermore, KFIA-Net requires only 11.47 M parameters and 66.79G FLOPs, achieving an inference speed of 47.21 FPS on a 1024 × 1024 input, achieving a good trade-off between accuracy and efficiency.
Published: 2026-01-30T17:36:20+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhongzhen Sun; Xianghui Zhang; Xiangguang Leng; Xueqi Wu; Boli Xiong; Kefeng Ji; Gangyao Kuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105127"&gt;10.1016/j.jag.2026.105127&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-category synthetic aperture radar (SAR) ship detection is limited by heterogeneity in imaging mechanisms and severe class imbalance, yielding accurate localization but frequent misclassification. To address this issue, this paper proposes a Knowledge Fusion and Imbalance-Aware Network (KFIA-Net). Specifically, we first propose a Domain Knowledge Feature Extraction (DKFE) to extract and encode knowledge tokens from four priors. Second, a Knowledge Cross-Attention Fusion (KCAF) module is designed to perform interpretable and sparsely selectable channel modulation using cross-attention and FiLM decoding. Thirdly, we further design an Imbalance-Aware Loss Function (IALF) that combines prior calibration, minority category margin expansion, and knowledge-consistency weighting to reduce loss bias. Finally, systematic experiments and comparisons are conducted on three datasets: SRSDD-v1.0, FAIR-CSAR-v1.0, and NUDT-SARship-v1.0. Our KFIA-Net achieves mAP 50 scores of 64.29%, 37.99%, and 78.26%, and mAP 75 scores of 34.96%, 19.70%, and 66.36%, respectively. These results demonstrate knowledge injection simultaneously improves class accuracy and sustains robust localization. Furthermore, KFIA-Net requires only 11.47 M parameters and 66.79G FLOPs, achieving an inference speed of 47.21 FPS on a 1024 × 1024 input, achieving a good trade-off between accuracy and efficiency.&lt;/p&gt;</content:encoded></item><item><title>Disentangle to Fuse: Towards Content Preservation and Cross-Modality Consistency for Multi-Modality Image Fusion</title><link>https://doi.org/10.1109/tip.2026.3657183</link><guid>10.1109/tip.2026.3657183</guid><pubDate>Fri, 30 Jan 2026 21:05:59 +0000</pubDate><dc:creator>Xinran Qin</dc:creator><dc:creator>Yuning Cui</dc:creator><dc:creator>Shangquan Sun</dc:creator><dc:creator>Ruoyu Chen</dc:creator><dc:creator>Wenqi Ren</dc:creator><dc:creator>Alois Knoll</dc:creator><dc:creator>Xiaochun Cao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3657183</prism:doi><description>Multi-modal image fusion (MMIF) aims to integrate complementary information from heterogeneous sensor modalities. However, substantial cross-modality discrepancies hinder joint scene representation and lead to semantic degradation in the fused output. To address this limitation, we propose C2MFuse, a novel framework designed to preserve content while ensuring cross-modality consistency. To the best of our knowledge, this is the first MMIF approach to explicitly disentangle style and content representations across modalities for image fusion. C2MFuse introduces a content-preserving style normalization mechanism that suppresses modality-specific variations while maintaining the underlying scene structure. The normalized features are then progressively aggregated to enhance fine-grained details and improve content completeness. In light of the lack of ground truth and the inherent ambiguity of the fused distribution, we further align the fused representation with a well-defined source modality, thereby enhancing semantic consistency and reducing distributional uncertainty. Additionally, we introduce an adaptive consistency loss with learnable transformation, which provides dynamic, modality-aware supervision by enforcing global consistency across heterogeneous inputs. Extensive experiments on five datasets across three representative MMIF tasks demonstrate that C2MFuse achieves efficient and high-quality fusion, surpasses existing methods, and generalizes effectively to downstream visual applications.
Published: 2026-01-30T21:05:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinran Qin; Yuning Cui; Shangquan Sun; Ruoyu Chen; Wenqi Ren; Alois Knoll; Xiaochun Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3657183"&gt;10.1109/tip.2026.3657183&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal image fusion (MMIF) aims to integrate complementary information from heterogeneous sensor modalities. However, substantial cross-modality discrepancies hinder joint scene representation and lead to semantic degradation in the fused output. To address this limitation, we propose C2MFuse, a novel framework designed to preserve content while ensuring cross-modality consistency. To the best of our knowledge, this is the first MMIF approach to explicitly disentangle style and content representations across modalities for image fusion. C2MFuse introduces a content-preserving style normalization mechanism that suppresses modality-specific variations while maintaining the underlying scene structure. The normalized features are then progressively aggregated to enhance fine-grained details and improve content completeness. In light of the lack of ground truth and the inherent ambiguity of the fused distribution, we further align the fused representation with a well-defined source modality, thereby enhancing semantic consistency and reducing distributional uncertainty. Additionally, we introduce an adaptive consistency loss with learnable transformation, which provides dynamic, modality-aware supervision by enforcing global consistency across heterogeneous inputs. Extensive experiments on five datasets across three representative MMIF tasks demonstrate that C2MFuse achieves efficient and high-quality fusion, surpasses existing methods, and generalizes effectively to downstream visual applications.&lt;/p&gt;</content:encoded></item><item><title>RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse Gradient Descent</title><link>https://doi.org/10.1109/tpami.2026.3651958</link><guid>10.1109/tpami.2026.3651958</guid><pubDate>Fri, 30 Jan 2026 21:03:08 +0000</pubDate><dc:creator>Yijie Deng</dc:creator><dc:creator>Lei Han</dc:creator><dc:creator>Tianpeng Lin</dc:creator><dc:creator>Lin Li</dc:creator><dc:creator>Jinzhi Zhang</dc:creator><dc:creator>Lu Fang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3651958</prism:doi><description>With the rise of Extended Reality (XR) technology, there is a growing need for real-time light field reconstruction from sparse view inputs. Existing methods can be classified into offline techniques, which can generate high-quality novel views but at the cost of long inference/training time, and online methods, which either lack generalizability or produce unsatisfactory results. However, we have observed that the intrinsic sparse manifold of Multi-plane Images (MPI) enables a significant acceleration of light field reconstruction while maintaining rendering quality. Based on this insight, we introduce RealLiFe, a novel light field optimization method, which leverages the proposed Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light fields from sparse input images in real time. Technically, the coarse MPI of a scene is first generated using a 3D CNN, and it is further optimized leveraging only the scene content aligned sparse MPI gradients in a few iterations. Extensive experiments demonstrate that our method achieves comparable visual quality while being 100x faster on average than state-of-the-art offline methods and delivers better performance (about 2 dB higher in PSNR) compared to other online approaches.
Published: 2026-01-30T21:03:08+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yijie Deng; Lei Han; Tianpeng Lin; Lin Li; Jinzhi Zhang; Lu Fang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3651958"&gt;10.1109/tpami.2026.3651958&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;With the rise of Extended Reality (XR) technology, there is a growing need for real-time light field reconstruction from sparse view inputs. Existing methods can be classified into offline techniques, which can generate high-quality novel views but at the cost of long inference/training time, and online methods, which either lack generalizability or produce unsatisfactory results. However, we have observed that the intrinsic sparse manifold of Multi-plane Images (MPI) enables a significant acceleration of light field reconstruction while maintaining rendering quality. Based on this insight, we introduce RealLiFe, a novel light field optimization method, which leverages the proposed Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light fields from sparse input images in real time. Technically, the coarse MPI of a scene is first generated using a 3D CNN, and it is further optimized leveraging only the scene content aligned sparse MPI gradients in a few iterations. Extensive experiments demonstrate that our method achieves comparable visual quality while being 100x faster on average than state-of-the-art offline methods and delivers better performance (about 2 dB higher in PSNR) compared to other online approaches.&lt;/p&gt;</content:encoded></item><item><title>Diffusion Model-Based Data Augmentation for Land Cover Segmentation in Pol-SAR Imagery</title><link>https://doi.org/10.1016/j.patcog.2026.113171</link><guid>10.1016/j.patcog.2026.113171</guid><pubDate>Fri, 30 Jan 2026 00:19:12 +0000</pubDate><dc:creator>Keunhoon Choi</dc:creator><dc:creator>Sunok Kim</dc:creator><dc:creator>Kwanghoon Sohn</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113171</prism:doi><description>Polarimetric Synthetic Aperture Radar (Pol-SAR) provides representations encapsulating physical texture information of land surfaces, useful for land cover segmentation. However, Pol-SAR images and precise segmentation maps are difficult to obtain, limiting public access to large datasets and hindering deep learning methods from achieving optimal performance. To address this, we propose two methods. First, we transform the channel axis to polar coordinates to better exploit surface information in Pol-SAR data. This allows deep learning models to directly learn polarization angles, which improves segmentation performance and resolves the channel imbalance problem in diffusion models. Second, we introduce a diffusion model-based data augmentation framework to generate Pol-SAR imagery with paired land cover maps. By representing land cover maps in a 2-channel format using the Gaussian distribution’s symmetry, we reduce GPU memory compared to one-hot encoding. We also propose a Guided Sampling strategy to generate paired Pol-SAR images when only land cover maps are available. Experimental results validate the effectiveness of our methods on the Pol-SAR dataset.
Published: 2026-01-30T00:19:12+00:00
Venue: Pattern Recognition
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keunhoon Choi; Sunok Kim; Kwanghoon Sohn&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113171"&gt;10.1016/j.patcog.2026.113171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Polarimetric Synthetic Aperture Radar (Pol-SAR) provides representations encapsulating physical texture information of land surfaces, useful for land cover segmentation. However, Pol-SAR images and precise segmentation maps are difficult to obtain, limiting public access to large datasets and hindering deep learning methods from achieving optimal performance. To address this, we propose two methods. First, we transform the channel axis to polar coordinates to better exploit surface information in Pol-SAR data. This allows deep learning models to directly learn polarization angles, which improves segmentation performance and resolves the channel imbalance problem in diffusion models. Second, we introduce a diffusion model-based data augmentation framework to generate Pol-SAR imagery with paired land cover maps. By representing land cover maps in a 2-channel format using the Gaussian distribution’s symmetry, we reduce GPU memory compared to one-hot encoding. We also propose a Guided Sampling strategy to generate paired Pol-SAR images when only land cover maps are available. Experimental results validate the effectiveness of our methods on the Pol-SAR dataset.&lt;/p&gt;</content:encoded></item><item><title>NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models</title><link>https://doi.org/10.1109/tpami.2026.3659598</link><guid>10.1109/tpami.2026.3659598</guid><pubDate>Fri, 30 Jan 2026 21:03:08 +0000</pubDate><dc:creator>Jiaming Zhang</dc:creator><dc:creator>Xin Wang</dc:creator><dc:creator>Xingjun Ma</dc:creator><dc:creator>Lingyu Qiu</dc:creator><dc:creator>Yu-Gang Jiang</dc:creator><dc:creator>Jitao Sang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3659598</prism:doi><description>Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning). As a significant extension, NAP-Tuning first establishes a comprehensive multi-modal (text and visual) and multi-layer prompting framework. The core of this framework is a targeted structural augmentation for feature-level purification, implemented through our Neural Augmentor approach. This framework implements feature purification by incorporating TokenRefiners-lightweight neural modules that learn to reconstruct purified features via residual connections-to directly address distortions in the feature space. This structural intervention is what enables the multi-modal and multi-layer system to effectively perform modality-specific and layer-specific feature rectification. Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 32.3% on ViT-B16 and 31.3% on ViT-B32 architectures while maintaining competitive clean accuracy. This work highlights the efficacy of internal feature-level intervention in prompt tuning for adversarial robustness, moving beyond input-side alignment approaches to create an adaptive defense mechanism that can identify and rectify adversarial perturbations across ...
Published: 2026-01-30T21:03:08+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaming Zhang; Xin Wang; Xingjun Ma; Lingyu Qiu; Yu-Gang Jiang; Jitao Sang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3659598"&gt;10.1109/tpami.2026.3659598&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning). As a significant extension, NAP-Tuning first establishes a comprehensive multi-modal (text and visual) and multi-layer prompting framework. The core of this framework is a targeted structural augmentation for feature-level purification, implemented through our Neural Augmentor approach. This framework implements feature purification by incorporating TokenRefiners-lightweight neural modules that learn to reconstruct purified features via residual connections-to directly address distortions in the feature space. This structural intervention is what enables the multi-modal and multi-layer system to effectively perform modality-specific and layer-specific feature rectification. Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 32.3% on ViT-B16 and 31.3% on ViT-B32 architectures while maintaining competitive clean accuracy. This work highlights the efficacy of internal feature-level intervention in prompt tuning for adversarial robustness, moving beyond input-side alignment approaches to create an adaptive defense mechanism that can identify and rectify adversarial perturbations across ...&lt;/p&gt;</content:encoded></item><item><title>Representation Learning for Tabular Data: A Comprehensive Survey</title><link>https://doi.org/10.1109/tpami.2026.3657217</link><guid>10.1109/tpami.2026.3657217</guid><pubDate>Fri, 30 Jan 2026 21:03:08 +0000</pubDate><dc:creator>Jun-Peng Jiang</dc:creator><dc:creator>Si-Yang Liu</dc:creator><dc:creator>Hao-Run Cai</dc:creator><dc:creator>Qi-Le Zhou</dc:creator><dc:creator>Han-Jia Ye</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3657217</prism:doi><description>Tabular data, structured as rows and columns, is among the most prevalent data types in machine learning classification and regression applications. Models for learning from tabular data have continuously evolved, with Deep Neural Networks (DNNs) recently demonstrating promising results through their capability of representation learning. In this survey, we systematically introduce the field of tabular representation learning, covering the background, challenges, and benchmarks, along with the pros and cons of using DNNs. We organize existing methods into three main categories according to their generalization capabilities: specialized, transferable, and general models. Specialized models focus on tasks where training and evaluation occur within the same data distribution. We introduce a hierarchical taxonomy for specialized models based on the key aspects of tabular data—features, samples, and objectives—and delve into detailed strategies for obtaining high-quality feature- and sample-level representations. Transferable models are pre-trained on one or more datasets and subsequently fine-tuned on downstream tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources, or even cross-modalities such as vision and language. General models, also known as tabular foundation models, extend this concept further, allowing direct application to downstream tasks without additional fine-tuning. We group these general models based on the strategies used to adapt across heterogeneous datasets. Additionally, we explore ensemble methods, which integrate the strengths of multiple tabular models. Finally, we discuss representative extensions of tabular learning, including open-environment tabular machine learning, multimodal learning with tabular data, and tabular understanding tasks. More information can be found in the following repository: https://github.com/LAMDA-Tabular/Tabular-Survey.
Published: 2026-01-30T21:03:08+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jun-Peng Jiang; Si-Yang Liu; Hao-Run Cai; Qi-Le Zhou; Han-Jia Ye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3657217"&gt;10.1109/tpami.2026.3657217&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Tabular data, structured as rows and columns, is among the most prevalent data types in machine learning classification and regression applications. Models for learning from tabular data have continuously evolved, with Deep Neural Networks (DNNs) recently demonstrating promising results through their capability of representation learning. In this survey, we systematically introduce the field of tabular representation learning, covering the background, challenges, and benchmarks, along with the pros and cons of using DNNs. We organize existing methods into three main categories according to their generalization capabilities: specialized, transferable, and general models. Specialized models focus on tasks where training and evaluation occur within the same data distribution. We introduce a hierarchical taxonomy for specialized models based on the key aspects of tabular data—features, samples, and objectives—and delve into detailed strategies for obtaining high-quality feature- and sample-level representations. Transferable models are pre-trained on one or more datasets and subsequently fine-tuned on downstream tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources, or even cross-modalities such as vision and language. General models, also known as tabular foundation models, extend this concept further, allowing direct application to downstream tasks without additional fine-tuning. We group these general models based on the strategies used to adapt across heterogeneous datasets. Additionally, we explore ensemble methods, which integrate the strengths of multiple tabular models. Finally, we discuss representative extensions of tabular learning, including open-environment tabular machine learning, multimodal learning with tabular data, and tabular understanding tasks. More information can be found in the following repository: https://github.com/LAMDA-Tabular/Tabular-Survey.&lt;/p&gt;</content:encoded></item><item><title>Multi-object tracking of vehicles and anomalous states in remote sensing videos: Joint learning of historical trajectory guidance and ID prediction</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.038</link><guid>10.1016/j.isprsjprs.2026.01.038</guid><pubDate>Sat, 31 Jan 2026 14:09:30 +0000</pubDate><dc:creator>Bin Wang</dc:creator><dc:creator>Yuan Zhou</dc:creator><dc:creator>Haigang Sui</dc:creator><dc:creator>Guorui Ma</dc:creator><dc:creator>Peng Cheng</dc:creator><dc:creator>Di Wang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.038</prism:doi><description>Research on multi-object tracking (MOT) of vehicles based on remote sensing video data has achieved breakthrough progress. However, MOT of vehicles in complex scenarios and their anomalous states after being subjected to strong deformation interference remains a huge challenge. This is of great significance for military defense, traffic flow management, vehicle damage assessment, etc. To address this problem, this study proposes an end-to-end MOT method that integrates a joint learning paradigm of historical trajectory guidance and identity (ID) prediction, aiming to bridge the gap between vehicle detection and continuous tracking after anomalous states occurrence. The proposed network framework primarily consists of a Frame Feature Aggregation Module (FFAM) that enhances spatial consistency of objects across consecutive video frames, a Historical Tracklets Flow Encoder (HTFE) that employs Mamba blocks to guide object embedding within potential motion flows based on historical frames, and a Semantic-Consistent Clustering Module (SCM) constructed via sparse attention computation to capture global semantic information. The discriminative features extracted by these modules are fused by a Dual-branch Modulation Fusion Unit (DMFU) to maximize the performance of the model. This study also constructs a new dataset for MOT of vehicles and anomalous states in videos, termed the VAS-MOT dataset. Extensive validation experiments conducted on this dataset demonstrate that the method achieves the highest level of performance, with HOTA and MOTA reaching 68.2% and 71.5%, respectively. Additional validation on the open-source dataset IRTS-AG confirms the strong robustness of the proposed method, showing excellent performance in long-term tracking of small vehicles in infrared videos under complex scenarios, where HOTA and MOTA reached 70.9% and 91.6%, respectively. The proposed method provides valuable insights for capturing moving objects and their anomalous states, laying a foundation for further damage assessment.
Published: 2026-01-31T14:09:30+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bin Wang; Yuan Zhou; Haigang Sui; Guorui Ma; Peng Cheng; Di Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.038"&gt;10.1016/j.isprsjprs.2026.01.038&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Research on multi-object tracking (MOT) of vehicles based on remote sensing video data has achieved breakthrough progress. However, MOT of vehicles in complex scenarios and their anomalous states after being subjected to strong deformation interference remains a huge challenge. This is of great significance for military defense, traffic flow management, vehicle damage assessment, etc. To address this problem, this study proposes an end-to-end MOT method that integrates a joint learning paradigm of historical trajectory guidance and identity (ID) prediction, aiming to bridge the gap between vehicle detection and continuous tracking after anomalous states occurrence. The proposed network framework primarily consists of a Frame Feature Aggregation Module (FFAM) that enhances spatial consistency of objects across consecutive video frames, a Historical Tracklets Flow Encoder (HTFE) that employs Mamba blocks to guide object embedding within potential motion flows based on historical frames, and a Semantic-Consistent Clustering Module (SCM) constructed via sparse attention computation to capture global semantic information. The discriminative features extracted by these modules are fused by a Dual-branch Modulation Fusion Unit (DMFU) to maximize the performance of the model. This study also constructs a new dataset for MOT of vehicles and anomalous states in videos, termed the VAS-MOT dataset. Extensive validation experiments conducted on this dataset demonstrate that the method achieves the highest level of performance, with HOTA and MOTA reaching 68.2% and 71.5%, respectively. Additional validation on the open-source dataset IRTS-AG confirms the strong robustness of the proposed method, showing excellent performance in long-term tracking of small vehicles in infrared videos under complex scenarios, where HOTA and MOTA reached 70.9% and 91.6%, respectively. The proposed method provides valuable insights for capturing moving objects and their anomalous states, laying a foundation for further damage assessment.&lt;/p&gt;</content:encoded></item><item><title>Urban Neural Surface Reconstruction from Constrained Sparse Aerial Imagery with 3D SAR Fusion</title><link>https://arxiv.org/abs/2601.22045v1</link><guid>http://arxiv.org/abs/2601.22045v1</guid><pubDate>Thu, 29 Jan 2026 17:47:07 +0000</pubDate><dc:creator>Da Li</dc:creator><dc:creator>Chen Yao</dc:creator><dc:creator>Tong Mao</dc:creator><dc:creator>Jiacheng Bao</dc:creator><dc:creator>Houjun Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.
Published: 2026-01-29T17:47:07+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Da Li; Chen Yao; Tong Mao; Jiacheng Bao; Houjun Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Neural surface reconstruction (NSR) has recently shown strong potential for urban 3D reconstruction from multi-view aerial imagery. However, existing NSR methods often suffer from geometric ambiguity and instability, particularly under sparse-view conditions. This issue is critical in large-scale urban remote sensing, where aerial image acquisition is limited by flight paths, terrain, and cost. To address this challenge, we present the first urban NSR framework that fuses 3D synthetic aperture radar (SAR) point clouds with aerial imagery for high-fidelity reconstruction under constrained, sparse-view settings. 3D SAR can efficiently capture large-scale geometry even from a single side-looking flight path, providing robust priors that complement photometric cues from images. Our framework integrates radar-derived spatial constraints into an SDF-based NSR backbone, guiding structure-aware ray selection and adaptive sampling for stable and efficient optimization. We also construct the first benchmark dataset with co-registered 3D SAR point clouds and aerial imagery, facilitating systematic evaluation of cross-modal 3D reconstruction. Extensive experiments show that incorporating 3D SAR markedly enhances reconstruction accuracy, completeness, and robustness compared with single-modality baselines under highly sparse and oblique-view conditions, highlighting a viable route toward scalable high-fidelity urban reconstruction with advanced airborne and spaceborne optical-SAR sensing.&lt;/p&gt;</content:encoded></item><item><title>Lane Detection for Autonomous Driving: A Comprehensive Review</title><link>https://doi.org/10.1016/j.neucom.2026.132864</link><guid>10.1016/j.neucom.2026.132864</guid><pubDate>Sat, 31 Jan 2026 07:30:02 +0000</pubDate><dc:creator>Hongrui Kou</dc:creator><dc:creator>Ziyu Wang</dc:creator><dc:creator>Zhouhang Lv</dc:creator><dc:creator>Cheng Wang</dc:creator><dc:creator>Zixuan Guo</dc:creator><dc:creator>Yuxin Zhang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132864</prism:doi><description>Lane Detection plays a fundamental and critical role in autonomous driving systems, which can provide accurate road structure information for vehicles and lay a visual foundation for downstream trajectory prediction and planning control. Despite its significance, few papers survey existing lane detection algorithms, leading to unclear research gaps and technical challenges. To this end, this paper reviews lane detection comprehensively, ranging from datasets, loss functions and evaluation metrics to 2D and more advanced 3D lane detection, with the aim of presenting a clear and complete technical chain for developing lane detection algorithms. Specifically, the paper proposes a taxonomy for lane detection and analyzes the technical principles, advantages, and limitations of each category. Benchmark experiments are introduced to reveal the trade-off relationships between complexity and performance. Finally, we identify seven promising research directions that address current limitations in the field, charting a path toward safer, more efficient, and more reliable autonomous driving systems.
Published: 2026-01-31T07:30:02+00:00
Venue: Neurocomputing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongrui Kou; Ziyu Wang; Zhouhang Lv; Cheng Wang; Zixuan Guo; Yuxin Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132864"&gt;10.1016/j.neucom.2026.132864&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Lane Detection plays a fundamental and critical role in autonomous driving systems, which can provide accurate road structure information for vehicles and lay a visual foundation for downstream trajectory prediction and planning control. Despite its significance, few papers survey existing lane detection algorithms, leading to unclear research gaps and technical challenges. To this end, this paper reviews lane detection comprehensively, ranging from datasets, loss functions and evaluation metrics to 2D and more advanced 3D lane detection, with the aim of presenting a clear and complete technical chain for developing lane detection algorithms. Specifically, the paper proposes a taxonomy for lane detection and analyzes the technical principles, advantages, and limitations of each category. Benchmark experiments are introduced to reveal the trade-off relationships between complexity and performance. Finally, we identify seven promising research directions that address current limitations in the field, charting a path toward safer, more efficient, and more reliable autonomous driving systems.&lt;/p&gt;</content:encoded></item><item><title>Distributionally Robust Classification for Multi-source Unsupervised Domain Adaptation</title><link>https://arxiv.org/abs/2601.21315v1</link><guid>http://arxiv.org/abs/2601.21315v1</guid><pubDate>Thu, 29 Jan 2026 06:23:14 +0000</pubDate><dc:creator>Seonghwi Kim</dc:creator><dc:creator>Sung Ho Jo</dc:creator><dc:creator>Wooseok Ha</dc:creator><dc:creator>Minwoo Chae</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.
Published: 2026-01-29T06:23:14+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seonghwi Kim; Sung Ho Jo; Wooseok Ha; Minwoo Chae&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation (UDA) is a statistical learning problem when the distribution of training (source) data is different from that of test (target) data. In this setting, one has access to labeled data only from the source domain and unlabeled data from the target domain. The central objective is to leverage the source data and the unlabeled target data to build models that generalize to the target domain. Despite its potential, existing UDA approaches often struggle in practice, particularly in scenarios where the target domain offers only limited unlabeled data or spurious correlations dominate the source domain. To address these challenges, we propose a novel distributionally robust learning framework that models uncertainty in both the covariate distribution and the conditional label distribution. Our approach is motivated by the multi-source domain adaptation setting but is also directly applicable to the single-source scenario, making it versatile in practice. We develop an efficient learning algorithm that can be seamlessly integrated with existing UDA methods. Extensive experiments under various distribution shift scenarios show that our method consistently outperforms strong baselines, especially when target data are extremely scarce.&lt;/p&gt;</content:encoded></item><item><title>Practical Video Object Detection via Feature Selection and Aggregation</title><link>https://doi.org/10.1007/s11263-025-02700-3</link><guid>10.1007/s11263-025-02700-3</guid><pubDate>Fri, 30 Jan 2026 05:07:41 +0000</pubDate><dc:creator>Yuheng Shi</dc:creator><dc:creator>Tong Zhang</dc:creator><dc:creator>Xiaojie Guo</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02700-3</prism:doi><description>Compared with still image object detection, video object detection (VOD) needs to particularly concern the high across-frame variation in object appearance, and the diverse deterioration in some frames. In principle, the detection in a certain frame of a video can benefit from information in other frames. Thus, how to effectively aggregate features across different frames is key to the target problem. Most of contemporary aggregation methods are tailored for two-stage detectors, suffering from high computational costs due to the dual-stage nature. On the other hand, although one-stage detectors have made continuous progress in handling static images, their applicability to VOD lacks sufficient exploration. To tackle the above issues, this study invents a very simple yet potent strategy of feature selection and aggregation, gaining significant accuracy at marginal computational expense. Concretely, for cutting the massive computation and memory consumption from the dense prediction characteristic of one-stage object detectors, we first condense candidate features from dense prediction maps. Then, the relationship between a target frame and its reference frames is evaluated to guide the aggregation. Comprehensive experiments and ablation studies are conducted to validate the efficacy of our design, and showcase its advantage over other cutting-edge VOD methods in both effectiveness and efficiency. Notably, our model reaches a new record performance, i.e., 93.0% AP50 at over 30 FPS on the ImageNet VID dataset on a single 3090 GPU, making it a compelling option for large-scale or real-time applications. The implementation is simple, and accessible at https://github.com/YuHengsss/YOLOV .
Published: 2026-01-30T05:07:41+00:00
Venue: International Journal of Computer Vision
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuheng Shi; Tong Zhang; Xiaojie Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02700-3"&gt;10.1007/s11263-025-02700-3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Compared with still image object detection, video object detection (VOD) needs to particularly concern the high across-frame variation in object appearance, and the diverse deterioration in some frames. In principle, the detection in a certain frame of a video can benefit from information in other frames. Thus, how to effectively aggregate features across different frames is key to the target problem. Most of contemporary aggregation methods are tailored for two-stage detectors, suffering from high computational costs due to the dual-stage nature. On the other hand, although one-stage detectors have made continuous progress in handling static images, their applicability to VOD lacks sufficient exploration. To tackle the above issues, this study invents a very simple yet potent strategy of feature selection and aggregation, gaining significant accuracy at marginal computational expense. Concretely, for cutting the massive computation and memory consumption from the dense prediction characteristic of one-stage object detectors, we first condense candidate features from dense prediction maps. Then, the relationship between a target frame and its reference frames is evaluated to guide the aggregation. Comprehensive experiments and ablation studies are conducted to validate the efficacy of our design, and showcase its advantage over other cutting-edge VOD methods in both effectiveness and efficiency. Notably, our model reaches a new record performance, i.e., 93.0% AP50 at over 30 FPS on the ImageNet VID dataset on a single 3090 GPU, making it a compelling option for large-scale or real-time applications. The implementation is simple, and accessible at https://github.com/YuHengsss/YOLOV .&lt;/p&gt;</content:encoded></item><item><title>A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency</title><link>https://arxiv.org/abs/2601.20284v1</link><guid>http://arxiv.org/abs/2601.20284v1</guid><pubDate>Wed, 28 Jan 2026 05:59:20 +0000</pubDate><dc:creator>Debopom Sutradhar</dc:creator><dc:creator>Md. Abdur Rahman</dc:creator><dc:creator>Mohaimenul Azam Khan Raiaan</dc:creator><dc:creator>Reem E. Mohamed</dc:creator><dc:creator>Sami Azam</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.
Published: 2026-01-28T05:59:20+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Debopom Sutradhar; Md. Abdur Rahman; Mohaimenul Azam Khan Raiaan; Reem E. Mohamed; Sami Azam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.&lt;/p&gt;</content:encoded></item><item><title>SSS-DPM: Diffusion Probabilistic Model for Speckle Noise Reduction in Side-Scan Sonar Images</title><link>https://doi.org/10.1109/tgrs.2026.3656240</link><guid>10.1109/tgrs.2026.3656240</guid><pubDate>Fri, 30 Jan 2026 21:03:13 +0000</pubDate><dc:creator>Qi Wang</dc:creator><dc:creator>Bo He</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3656240</prism:doi><description>The side-scan sonar (SSS) mounted on autonomous underwater vehicles (AUVs) enables efficient and precise seabed mapping as well as underwater exploration. However, SSS images are often impaired by seabed reverberation, environmental interference, and self-noise, with speckle noise from sub-resolution scatterers being a particularly significant issue. This signal-dependent multiplicative noise not only reduces image quality but also complicates feature extraction. To address these challenges, we introduce a diffusion probabilistic model (SSS-DPM) specifically designed to reduce speckle noise in SSS images. SSS-DPM employs a dual-branch forward diffusion process to accurately characterize the noise distribution in these images. During the reverse diffusion phase, it leverages a noise predictor based on Kullback-Leibler (KL) divergence to effectively suppress noise. Moreover, the model incorporates a multi-scale feature extraction network, allowing it to tackle complex noise patterns while preserving critical structural details. Experimental results demonstrate that SSS-DPM consistently surpasses state-of-the-art methods in peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and visual information fidelity (VIF) across diverse real-world datasets. Even in cases of severe degradation, SSS-DPM exhibits exceptional robustness and reliability.
Published: 2026-01-30T21:03:13+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qi Wang; Bo He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3656240"&gt;10.1109/tgrs.2026.3656240&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;The side-scan sonar (SSS) mounted on autonomous underwater vehicles (AUVs) enables efficient and precise seabed mapping as well as underwater exploration. However, SSS images are often impaired by seabed reverberation, environmental interference, and self-noise, with speckle noise from sub-resolution scatterers being a particularly significant issue. This signal-dependent multiplicative noise not only reduces image quality but also complicates feature extraction. To address these challenges, we introduce a diffusion probabilistic model (SSS-DPM) specifically designed to reduce speckle noise in SSS images. SSS-DPM employs a dual-branch forward diffusion process to accurately characterize the noise distribution in these images. During the reverse diffusion phase, it leverages a noise predictor based on Kullback-Leibler (KL) divergence to effectively suppress noise. Moreover, the model incorporates a multi-scale feature extraction network, allowing it to tackle complex noise patterns while preserving critical structural details. Experimental results demonstrate that SSS-DPM consistently surpasses state-of-the-art methods in peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), and visual information fidelity (VIF) across diverse real-world datasets. Even in cases of severe degradation, SSS-DPM exhibits exceptional robustness and reliability.&lt;/p&gt;</content:encoded></item><item><title>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</title><link>https://arxiv.org/abs/2601.22054v1</link><guid>http://arxiv.org/abs/2601.22054v1</guid><pubDate>Thu, 29 Jan 2026 17:52:41 +0000</pubDate><dc:creator>Baorui Ma</dc:creator><dc:creator>Jiahui Yang</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Xuancheng Zhang</dc:creator><dc:creator>Jianxun Cui</dc:creator><dc:creator>Hao Li</dc:creator><dc:creator>Yan Xie</dc:creator><dc:creator>Wei Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.
Published: 2026-01-29T17:52:41+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Baorui Ma; Jiahui Yang; Donglin Di; Xuancheng Zhang; Jianxun Cui; Hao Li; Yan Xie; Wei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Scaling has powered recent advances in vision foundation models, yet extending this paradigm to metric depth estimation remains challenging due to heterogeneous sensor noise, camera-dependent biases, and metric ambiguity in noisy cross-source 3D data. We introduce Metric Anything, a simple and scalable pretraining framework that learns metric depth from noisy, diverse 3D sources without manually engineered prompts, camera-specific modeling, or task-specific architectures. Central to our approach is the Sparse Metric Prompt, created by randomly masking depth maps, which serves as a universal interface that decouples spatial reasoning from sensor and camera biases. Using about 20M image-depth pairs spanning reconstructed, captured, and rendered 3D data across 10000 camera models, we demonstrate-for the first time-a clear scaling trend in the metric depth track. The pretrained model excels at prompt-driven tasks such as depth completion, super-resolution and Radar-camera fusion, while its distilled prompt-free student achieves state-of-the-art results on monocular depth estimation, camera intrinsics recovery, single/multi-view metric 3D reconstruction, and VLA planning. We also show that using pretrained ViT of Metric Anything as a visual encoder significantly boosts Multimodal Large Language Model capabilities in spatial intelligence. These results show that metric depth estimation can benefit from the same scaling laws that drive modern foundation models, establishing a new path toward scalable and efficient real-world metric perception. We open-source MetricAnything at http://metric-anything.github.io/metric-anything-io/ to support community research.&lt;/p&gt;</content:encoded></item><item><title>Soft Quantization: Model Compression Via Weight Coupling</title><link>https://arxiv.org/abs/2601.21219v1</link><guid>http://arxiv.org/abs/2601.21219v1</guid><pubDate>Thu, 29 Jan 2026 03:34:06 +0000</pubDate><dc:creator>Daniel T. Bernstein</dc:creator><dc:creator>Luca Di Carlo</dc:creator><dc:creator>David Schwab</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model's weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our "soft quantization'' scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.
Published: 2026-01-29T03:34:06+00:00
Venue: arXiv
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Daniel T. Bernstein; Luca Di Carlo; David Schwab&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;We show that introducing short-range attractive couplings between the weights of a neural network during training provides a novel avenue for model quantization. These couplings rapidly induce the discretization of a model&amp;#x27;s weight distribution, and they do so in a mixed-precision manner despite only relying on two additional hyperparameters. We demonstrate that, within an appropriate range of hyperparameters, our &amp;quot;soft quantization&amp;#x27;&amp;#x27; scheme outperforms histogram-equalized post-training quantization on ResNet-20/CIFAR-10. Soft quantization provides both a new pipeline for the flexible compression of machine learning models and a new tool for investigating the trade-off between compression and generalization in high-dimensional loss landscapes.&lt;/p&gt;</content:encoded></item><item><title>Quad-pol reconstruction of dual-pol SAR data via a physically constrained diffusion model for building damage assessment</title><link>https://doi.org/10.1016/j.jag.2026.105132</link><guid>10.1016/j.jag.2026.105132</guid><pubDate>Fri, 30 Jan 2026 10:02:57 +0000</pubDate><dc:creator>Zihuan Guo</dc:creator><dc:creator>Hong Zhang</dc:creator><dc:creator>Xiao-Ming Li</dc:creator><dc:creator>Yukun Fan</dc:creator><dc:creator>Haoxuan Duan</dc:creator><dc:creator>Qiming Zeng</dc:creator><dc:creator>Ji Ge</dc:creator><dc:creator>Chao Wang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105132</prism:doi><description>Quad-polarimetric (quad-pol) synthetic aperture radar (SAR) data provides crucial polarimetric information for post-disaster building damage assessment. However, most current spaceborne SAR platforms prioritize dual-polarization (dual-pol) mode, which ensures high temporal and spatial data availability but limits damage analysis accuracy due to the absence of some polarimetric information. Existing methods for reconstructing dual-pol to quad-pol SAR data often fail to ensure that the reconstructed data meets fundamental physical properties, while traditional building damage detection methods still struggle to accurately capture complex depolarization effects. To address these challenges, this paper proposes a diffusion model-based method for reconstructing dual-pol data to quad-pol data, applied to post-earthquake building damage analysis. The method introduces a Positive Semi-definite Constraint Module and a Plug-and-Play SVD Parameter Fine-tuning Module to ensure the physical validity and accuracy of the reconstructed data. Additionally, a Stokes vector-based Degree of Polarization frequency analysis method is proposed to enhance the description of depolarization information. A multi-dimensional polarimetric feature combination is constructed for grid-level building damage assessment. Experiments on Gaofen-3, ALOS-2/PALSAR-2, and Sentinel-1 data show that the proposed method performs optimally in complex scenarios, with all pixels meeting the positive semi-definite constraint. Compared to the original dual-pol SAR data, building damage assessment using the reconstructed quad-pol SAR data resulted in an F1 score improvement of 16.3% and 8.4% for detecting moderately and severely damaged buildings, respectively. This research provides crucial technical support for fully harnessing the potential of dual-pol SAR data in building damage assessment.
Published: 2026-01-30T10:02:57+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zihuan Guo; Hong Zhang; Xiao-Ming Li; Yukun Fan; Haoxuan Duan; Qiming Zeng; Ji Ge; Chao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105132"&gt;10.1016/j.jag.2026.105132&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;Quad-polarimetric (quad-pol) synthetic aperture radar (SAR) data provides crucial polarimetric information for post-disaster building damage assessment. However, most current spaceborne SAR platforms prioritize dual-polarization (dual-pol) mode, which ensures high temporal and spatial data availability but limits damage analysis accuracy due to the absence of some polarimetric information. Existing methods for reconstructing dual-pol to quad-pol SAR data often fail to ensure that the reconstructed data meets fundamental physical properties, while traditional building damage detection methods still struggle to accurately capture complex depolarization effects. To address these challenges, this paper proposes a diffusion model-based method for reconstructing dual-pol data to quad-pol data, applied to post-earthquake building damage analysis. The method introduces a Positive Semi-definite Constraint Module and a Plug-and-Play SVD Parameter Fine-tuning Module to ensure the physical validity and accuracy of the reconstructed data. Additionally, a Stokes vector-based Degree of Polarization frequency analysis method is proposed to enhance the description of depolarization information. A multi-dimensional polarimetric feature combination is constructed for grid-level building damage assessment. Experiments on Gaofen-3, ALOS-2/PALSAR-2, and Sentinel-1 data show that the proposed method performs optimally in complex scenarios, with all pixels meeting the positive semi-definite constraint. Compared to the original dual-pol SAR data, building damage assessment using the reconstructed quad-pol SAR data resulted in an F1 score improvement of 16.3% and 8.4% for detecting moderately and severely damaged buildings, respectively. This research provides crucial technical support for fully harnessing the potential of dual-pol SAR data in building damage assessment.&lt;/p&gt;</content:encoded></item><item><title>A Two-Stage Learning Network for PVINS Modeling and Fusion Estimation in Challenging Environments</title><link>https://doi.org/10.1016/j.inffus.2026.104192</link><guid>10.1016/j.inffus.2026.104192</guid><pubDate>Fri, 30 Jan 2026 16:28:59 +0000</pubDate><dc:creator>Xuanyu Wu</dc:creator><dc:creator>Jiankai Yin</dc:creator><dc:creator>Jian Yang</dc:creator><dc:creator>Xin Liu</dc:creator><dc:creator>Wenshuo Li</dc:creator><dc:creator>Lei Guo</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104192</prism:doi><description>In the polarization-based visual-inertial navigation system (PVINS), information from polarization sensor (PS) and visual–inertial navigation system (VINS) is fused to enable position and attitude estimation, thereby offering an effective solution for autonomous navigation in global navigation satellite system (GNSS)-denied environments. However, under challenging conditions such as complex weather, the state-space model of PVINS becomes susceptible to uncertain model error, limiting the accuracy and adaptability of the system. To address this issue, we propose a tightly coupled PVINS integration scheme based on a two-stage learning network, which consists of model error compensation and adaptive Kalman gain learning. In the first stage, a deep neural network with a shared-weight architecture is designed to learn and compensate for the state-space model error, thereby reducing network complexity and enabling more precise system modeling. In the second stage, to improve fusion accuracy of PVINS, a Kalman gain learning network (KGLN)-based intelligent fusion method is proposed. This approach enables the adaptive learning of Kalman gains, circumventing the dependency of the system on knowledge of the noise statistics. Finally, the performance of the system is verified through the semi-physical simulation and flight test. The experimental results confirm that the proposed method outperforms conventional PVINS in terms of both position and heading estimation.
Published: 2026-01-30T16:28:59+00:00
Venue: Information Fusion
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuanyu Wu; Jiankai Yin; Jian Yang; Xin Liu; Wenshuo Li; Lei Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104192"&gt;10.1016/j.inffus.2026.104192&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;In the polarization-based visual-inertial navigation system (PVINS), information from polarization sensor (PS) and visual–inertial navigation system (VINS) is fused to enable position and attitude estimation, thereby offering an effective solution for autonomous navigation in global navigation satellite system (GNSS)-denied environments. However, under challenging conditions such as complex weather, the state-space model of PVINS becomes susceptible to uncertain model error, limiting the accuracy and adaptability of the system. To address this issue, we propose a tightly coupled PVINS integration scheme based on a two-stage learning network, which consists of model error compensation and adaptive Kalman gain learning. In the first stage, a deep neural network with a shared-weight architecture is designed to learn and compensate for the state-space model error, thereby reducing network complexity and enabling more precise system modeling. In the second stage, to improve fusion accuracy of PVINS, a Kalman gain learning network (KGLN)-based intelligent fusion method is proposed. This approach enables the adaptive learning of Kalman gains, circumventing the dependency of the system on knowledge of the noise statistics. Finally, the performance of the system is verified through the semi-physical simulation and flight test. The experimental results confirm that the proposed method outperforms conventional PVINS in terms of both position and heading estimation.&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Multimodal Graph Completion Networks with Multi-level Contrastiveness for Modality-missing Conversation Understanding</title><link>https://doi.org/10.1016/j.inffus.2026.104197</link><guid>10.1016/j.inffus.2026.104197</guid><pubDate>Fri, 30 Jan 2026 00:38:35 +0000</pubDate><dc:creator>Sichao Fu</dc:creator><dc:creator>Songren Peng</dc:creator><dc:creator>Bin Zou</dc:creator><dc:creator>Xiao-Yuan Jing</dc:creator><dc:creator>Wei Yu</dc:creator><dc:creator>Qinmu Peng</dc:creator><dc:creator>Xinge You</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104197</prism:doi><description>Multimodal conversation understanding has received increasing research interest in recent years, which aims to integrate multimodal conversation information to improve the accuracy of computer understanding of user intentions. However, the existing multimodal conversation understanding methods often suffer from a conversation modality missing challenge, which seriously damages their superior performance. Recently emerged imputation-based incomplete multimodal learning (I 2 ML) provides an effective solution, which aims to reconstruct the missing modality features under the supervision of a downstream task. Such reliance on labels causes both the bias of the reconstructed modality features and the limitation of their scope of application. Besides, these proposed I 2 ML methods independently consider the missing modality features reconstruction process between different utterances, which further leads to a specific utterance over-reliance (model sub-optimal) issue. To address the above-mentioned issues, a more general unsupervised I 2 ML is proposed to effectively improve the performance of the modality-missing conversation understanding (M 2 CU) task, termed unsupervised multimodal graph completion networks (UMGCN). Specifically, to improve the accuracy of each reconstructed modality feature, an effective missing modality recovery module is designed to enhance the information interaction process between different utterances for generating robust missing modality recovery features. Then, a multi-level graph contrastive loss on the cross-structure and cross-view level is proposed to learn utterance-general conversation representations by maximizing the mutual information between the same conversation representations across different structures and views. Finally, the learned utterance-general conversation representations can be applied to arbitrary M 2 CU tasks. Extensive experiments on four datasets, seven missing rates and two M 2 CU tasks show that our proposed UMGCN outperforms the existing incomplete multimodal learning methods.
Published: 2026-01-30T00:38:35+00:00
Venue: Information Fusion
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sichao Fu; Songren Peng; Bin Zou; Xiao-Yuan Jing; Wei Yu; Qinmu Peng; Xinge You&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104197"&gt;10.1016/j.inffus.2026.104197&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal conversation understanding has received increasing research interest in recent years, which aims to integrate multimodal conversation information to improve the accuracy of computer understanding of user intentions. However, the existing multimodal conversation understanding methods often suffer from a conversation modality missing challenge, which seriously damages their superior performance. Recently emerged imputation-based incomplete multimodal learning (I 2 ML) provides an effective solution, which aims to reconstruct the missing modality features under the supervision of a downstream task. Such reliance on labels causes both the bias of the reconstructed modality features and the limitation of their scope of application. Besides, these proposed I 2 ML methods independently consider the missing modality features reconstruction process between different utterances, which further leads to a specific utterance over-reliance (model sub-optimal) issue. To address the above-mentioned issues, a more general unsupervised I 2 ML is proposed to effectively improve the performance of the modality-missing conversation understanding (M 2 CU) task, termed unsupervised multimodal graph completion networks (UMGCN). Specifically, to improve the accuracy of each reconstructed modality feature, an effective missing modality recovery module is designed to enhance the information interaction process between different utterances for generating robust missing modality recovery features. Then, a multi-level graph contrastive loss on the cross-structure and cross-view level is proposed to learn utterance-general conversation representations by maximizing the mutual information between the same conversation representations across different structures and views. Finally, the learned utterance-general conversation representations can be applied to arbitrary M 2 CU tasks. Extensive experiments on four datasets, seven missing rates and two M 2 CU tasks show that our proposed UMGCN outperforms the existing incomplete multimodal learning methods.&lt;/p&gt;</content:encoded></item><item><title>Improved prediction of winter wheat yield at regional scale with limited ground samples by unmanned aerial vehicle and satellite synergy</title><link>https://doi.org/10.1016/j.rse.2026.115271</link><guid>10.1016/j.rse.2026.115271</guid><pubDate>Sat, 31 Jan 2026 10:59:02 +0000</pubDate><dc:creator>Yuan Xiong</dc:creator><dc:creator>Gaoxiang Yang</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Weiguo Yu</dc:creator><dc:creator>Yapeng Wu</dc:creator><dc:creator>Jun Lu</dc:creator><dc:creator>Chongya Jiang</dc:creator><dc:creator>Xia Yao</dc:creator><dc:creator>Yan Zhu</dc:creator><dc:creator>Weixing Cao</dc:creator><dc:creator>Tao Cheng</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2026.115271</prism:doi><description>Rapid, accurate, and large-scale in-season prediction of winter wheat yield is essential for enhancing food security and guiding agricultural policies. Traditional data-driven methods with satellite imagery face challenges in large-scale prediction of winter wheat yield because of the limited ground sampling data available for model training. Although unmanned aerial vehicle (UAV) images have been integrated with satellite imagery for generating reference data in monitoring vegetation dynamics, the UAV and satellite synergy has not yet been investigated for cross-scale sample augmentation and information fusion in large-scale prediction of winter wheat yield. To address these issues, this study proposed a novel framework integrating ground, UAV, and satellite data with data-driven algorithms to improve regional-scale yield prediction without the need of adding field measured yield samples. The potential contributions of UAV data to yield sample augmentation were examined for compensating the lack of ground samples and improving regional-scale wheat yield prediction. Subsequently, an optimal yield prediction strategy was developed through augmented sample quality and spatial variability analysis with cross-scale information fusion. The proposed framework was evaluated with extensive field-level yield measurements over three consecutive seasons of winter wheat across Jiangsu Province, China.
Published: 2026-01-31T10:59:02+00:00
Venue: Remote Sensing of Environment
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuan Xiong; Gaoxiang Yang; Lei Zhang; Weiguo Yu; Yapeng Wu; Jun Lu; Chongya Jiang; Xia Yao; Yan Zhu; Weixing Cao; Tao Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2026.115271"&gt;10.1016/j.rse.2026.115271&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Rapid, accurate, and large-scale in-season prediction of winter wheat yield is essential for enhancing food security and guiding agricultural policies. Traditional data-driven methods with satellite imagery face challenges in large-scale prediction of winter wheat yield because of the limited ground sampling data available for model training. Although unmanned aerial vehicle (UAV) images have been integrated with satellite imagery for generating reference data in monitoring vegetation dynamics, the UAV and satellite synergy has not yet been investigated for cross-scale sample augmentation and information fusion in large-scale prediction of winter wheat yield. To address these issues, this study proposed a novel framework integrating ground, UAV, and satellite data with data-driven algorithms to improve regional-scale yield prediction without the need of adding field measured yield samples. The potential contributions of UAV data to yield sample augmentation were examined for compensating the lack of ground samples and improving regional-scale wheat yield prediction. Subsequently, an optimal yield prediction strategy was developed through augmented sample quality and spatial variability analysis with cross-scale information fusion. The proposed framework was evaluated with extensive field-level yield measurements over three consecutive seasons of winter wheat across Jiangsu Province, China.&lt;/p&gt;</content:encoded></item><item><title>Learning Dynamic Representations via An Optimally-Weighted Maximum Mean Discrepancy Optimization Framework for Continual Learning</title><link>https://doi.org/10.1016/j.knosys.2026.115419</link><guid>10.1016/j.knosys.2026.115419</guid><pubDate>Fri, 30 Jan 2026 07:46:53 +0000</pubDate><dc:creator>Kaihui Huang</dc:creator><dc:creator>Runqing Wu</dc:creator><dc:creator>Jinhui Sheng</dc:creator><dc:creator>Hanyi Zhang</dc:creator><dc:creator>Ling Ge</dc:creator><dc:creator>Jinyu Guo</dc:creator><dc:creator>Fei Ye</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115419</prism:doi><description>Continual learning has emerged as a pivotal area of research, primarily due to its advantageous characteristic that allows models to persistently acquire and retain information. However, catastrophic forgetting can severely impair model performance. In this study, we address network forgetting by introducing a novel framework termed Optimally-Weighted Maximum Mean Discrepancy (OWMMD), which imposes penalties on representation alterations via a Multi-Level Feature Matching Mechanism (MLFMM). Furthermore, we propose an Adaptive Regularization Optimization (ARO) strategy to refine the adaptive weight vectors, which autonomously assess the significance of each feature layer throughout the optimization process, The proposed ARO approach can relieve the over-regularization problem and promote the future task learning. We conduct a comprehensive series of experiments, benchmarking our proposed method against several established baselines. The empirical findings indicate that our approach achieves state-of-the-art performance.
Published: 2026-01-30T07:46:53+00:00
Venue: Knowledge-Based Systems
Score: 0.773 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kaihui Huang; Runqing Wu; Jinhui Sheng; Hanyi Zhang; Ling Ge; Jinyu Guo; Fei Ye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115419"&gt;10.1016/j.knosys.2026.115419&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (must_read)&lt;/p&gt;
&lt;p&gt;Continual learning has emerged as a pivotal area of research, primarily due to its advantageous characteristic that allows models to persistently acquire and retain information. However, catastrophic forgetting can severely impair model performance. In this study, we address network forgetting by introducing a novel framework termed Optimally-Weighted Maximum Mean Discrepancy (OWMMD), which imposes penalties on representation alterations via a Multi-Level Feature Matching Mechanism (MLFMM). Furthermore, we propose an Adaptive Regularization Optimization (ARO) strategy to refine the adaptive weight vectors, which autonomously assess the significance of each feature layer throughout the optimization process, The proposed ARO approach can relieve the over-regularization problem and promote the future task learning. We conduct a comprehensive series of experiments, benchmarking our proposed method against several established baselines. The empirical findings indicate that our approach achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Diff-GDAformer: A Diffusion-Guided Dynamic Attention Transformer for Image Inpainting</title><link>https://doi.org/10.1016/j.knosys.2026.115443</link><guid>10.1016/j.knosys.2026.115443</guid><pubDate>Sat, 31 Jan 2026 07:30:05 +0000</pubDate><dc:creator>Hao Wu</dc:creator><dc:creator>Shuzhen Xu</dc:creator><dc:creator>Cuicui Lv</dc:creator><dc:creator>Yuanwei Bi</dc:creator><dc:creator>Zhizhong Liu</dc:creator><dc:creator>Shuo Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115443</prism:doi><description>Diffusion model (DM) has shown great promise in image inpainting by modeling complex data distributions and generating high-quality reconstructions. However, current diffusion-based methods often face challenges such as excessive iterative steps and limited adaptability to both local and global features, resulting in high computational costs and suboptimal restoration quality. To address these issues, we propose Diff-GDAformer, a novel image inpainting framework that combines diffusion-based prior feature generation with guided dynamic attention Transformer (GDAformer) for robust and efficient restoration. In our approach, the DM iteratively refines Gaussian noise in a compressed latent space to generate high-quality prior features, which guide the restoration process. These prior features are injected into GDAformer, which innovatively adopts a dynamic recursive local attention (DRLA) module. DRLA makes use of two complementary attention mechanisms: guided local self-attention (GL-SA) and guided recursive-generalized self-attention (GRG-SA). GL-SA enhances the model’s ability to capture fine-grained local details, while GRG-SA focuses on aggregating global contextual information efficiently. To bridge the gap between local and global features, we introduce the hybrid feature integration (HFI) module, which effectively fuses features from different attention layers, enabling a more comprehensive understanding of image contexts. The two-stage training strategy combines GDAformer with DM optimization, ensuring that the extracted prior features are accurate and seamlessly integrated into the restoration pipeline. Extensive experiments demonstrate that Diff-GDAformer achieves state-of-the-art performance on standard benchmarks, delivering superior visual quality and computational efficiency compared to existing methods.
Published: 2026-01-31T07:30:05+00:00
Venue: Knowledge-Based Systems
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wu; Shuzhen Xu; Cuicui Lv; Yuanwei Bi; Zhizhong Liu; Shuo Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115443"&gt;10.1016/j.knosys.2026.115443&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion model (DM) has shown great promise in image inpainting by modeling complex data distributions and generating high-quality reconstructions. However, current diffusion-based methods often face challenges such as excessive iterative steps and limited adaptability to both local and global features, resulting in high computational costs and suboptimal restoration quality. To address these issues, we propose Diff-GDAformer, a novel image inpainting framework that combines diffusion-based prior feature generation with guided dynamic attention Transformer (GDAformer) for robust and efficient restoration. In our approach, the DM iteratively refines Gaussian noise in a compressed latent space to generate high-quality prior features, which guide the restoration process. These prior features are injected into GDAformer, which innovatively adopts a dynamic recursive local attention (DRLA) module. DRLA makes use of two complementary attention mechanisms: guided local self-attention (GL-SA) and guided recursive-generalized self-attention (GRG-SA). GL-SA enhances the model’s ability to capture fine-grained local details, while GRG-SA focuses on aggregating global contextual information efficiently. To bridge the gap between local and global features, we introduce the hybrid feature integration (HFI) module, which effectively fuses features from different attention layers, enabling a more comprehensive understanding of image contexts. The two-stage training strategy combines GDAformer with DM optimization, ensuring that the extracted prior features are accurate and seamlessly integrated into the restoration pipeline. Extensive experiments demonstrate that Diff-GDAformer achieves state-of-the-art performance on standard benchmarks, delivering superior visual quality and computational efficiency compared to existing methods.&lt;/p&gt;</content:encoded></item><item><title>Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction</title><link>https://arxiv.org/abs/2601.20720v1</link><guid>http://arxiv.org/abs/2601.20720v1</guid><pubDate>Wed, 28 Jan 2026 15:53:32 +0000</pubDate><dc:creator>Matej Halinkovic</dc:creator><dc:creator>Nina Masarykova</dc:creator><dc:creator>Alexey Vinel</dc:creator><dc:creator>Marek Galinski</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.
Published: 2026-01-28T15:53:32+00:00
Venue: arXiv
Score: 0.770 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Matej Halinkovic; Nina Masarykova; Alexey Vinel; Marek Galinski&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (must_read)&lt;/p&gt;
&lt;p&gt;End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.&lt;/p&gt;</content:encoded></item><item><title>BLO-Inst: Bi-Level Optimization Based Alignment of YOLO and SAM for Robust Instance Segmentation</title><link>https://arxiv.org/abs/2601.22061v1</link><guid>http://arxiv.org/abs/2601.22061v1</guid><pubDate>Thu, 29 Jan 2026 17:58:55 +0000</pubDate><dc:creator>Li Zhang</dc:creator><dc:creator>Pengtao Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.
Published: 2026-01-29T17:58:55+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Zhang; Pengtao Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model has revolutionized image segmentation with its zero-shot capabilities, yet its reliance on manual prompts hinders fully automated deployment. While integrating object detectors as prompt generators offers a pathway to automation, existing pipelines suffer from two fundamental limitations: objective mismatch, where detectors optimized for geometric localization do not correspond to the optimal prompting context required by SAM, and alignment overfitting in standard joint training, where the detector simply memorizes specific prompt adjustments for training samples rather than learning a generalizable policy. To bridge this gap, we introduce BLO-Inst, a unified framework that aligns detection and segmentation objectives by bi-level optimization. We formulate the alignment as a nested optimization problem over disjoint data splits. In the lower level, the SAM is fine-tuned to maximize segmentation fidelity given the current detection proposals on a subset ($D_1$). In the upper level, the detector is updated to generate bounding boxes that explicitly minimize the validation loss of the fine-tuned SAM on a separate subset ($D_2$). This effectively transforms the detector into a segmentation-aware prompt generator, optimizing the bounding boxes not just for localization accuracy, but for downstream mask quality. Extensive experiments demonstrate that BLO-Inst achieves superior performance, outperforming standard baselines on tasks in general and biomedical domains.&lt;/p&gt;</content:encoded></item><item><title>OCSA-FN: A Fusion Network with Orthogonality-Constrained Spatial Attention for Hyperspectral and Land Surface Temperature Data Classification</title><link>https://doi.org/10.1109/tgrs.2026.3659827</link><guid>10.1109/tgrs.2026.3659827</guid><pubDate>Fri, 30 Jan 2026 21:03:13 +0000</pubDate><dc:creator>Enyu Zhao</dc:creator><dc:creator>Yongfang Su</dc:creator><dc:creator>Nianxin Qu</dc:creator><dc:creator>Yulei Wang</dc:creator><dc:creator>Yongguang Zhao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3659827</prism:doi><description>With the continuous advancement of remote sensing (RS) technology, collaborative image land cover classification using multi-source RS data has gradually emerged as a prominent research focus. However, one of the core challenges during the feature fusion process of multi-source RS data is to achieve efficient feature interaction while minimizing redundancy and enhancing the representational capacity of features. To address this issue, this paper proposes a fusion network with orthogonality-constrained spatial attention (OCSA-FN) for hyperspectral image (HSI) and land surface temperature (LST) data classification. First, OCSA-FN employs a dual-branch convolutional neural network (DB-CNN) module, where one branch utilizes a cross spatial-spectral CNN (CSS-CNN) to extract spectral features from HSI, while the other branch incorporates a learnable Sobel CNN (LS-CNN) to adaptively extract temperature features from single-channel LST data. Next, OCSA-FN introduces a dual-pooling residual channel attention (DRCA) module that leverages pooling-based interaction and residual connect to perform channel-wise weighting on deep features. Subsequently, OCSA-FN presents an adaptive orthogonal feature fusion (AOFF) module designed to construct two sets of mutually orthogonal spatial basis vectors by imposing orthogonal constraints, this effectively reduces the feature redundancy. Meanwhile, an adaptive spatial attention mechanism dynamically adjusts the fusion weights of the features between HSI and LST, facilitating efficient complementary fusion of multi-source features. Finally, the weighted fused features are utilized for the classification. Extensive experimental results demonstrate that OCSA-FN outperforms state-of-the-art existing methods in terms of classification accuracy while reducing both the parameter count and computational complexity within the model.
Published: 2026-01-30T21:03:13+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Enyu Zhao; Yongfang Su; Nianxin Qu; Yulei Wang; Yongguang Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3659827"&gt;10.1109/tgrs.2026.3659827&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;With the continuous advancement of remote sensing (RS) technology, collaborative image land cover classification using multi-source RS data has gradually emerged as a prominent research focus. However, one of the core challenges during the feature fusion process of multi-source RS data is to achieve efficient feature interaction while minimizing redundancy and enhancing the representational capacity of features. To address this issue, this paper proposes a fusion network with orthogonality-constrained spatial attention (OCSA-FN) for hyperspectral image (HSI) and land surface temperature (LST) data classification. First, OCSA-FN employs a dual-branch convolutional neural network (DB-CNN) module, where one branch utilizes a cross spatial-spectral CNN (CSS-CNN) to extract spectral features from HSI, while the other branch incorporates a learnable Sobel CNN (LS-CNN) to adaptively extract temperature features from single-channel LST data. Next, OCSA-FN introduces a dual-pooling residual channel attention (DRCA) module that leverages pooling-based interaction and residual connect to perform channel-wise weighting on deep features. Subsequently, OCSA-FN presents an adaptive orthogonal feature fusion (AOFF) module designed to construct two sets of mutually orthogonal spatial basis vectors by imposing orthogonal constraints, this effectively reduces the feature redundancy. Meanwhile, an adaptive spatial attention mechanism dynamically adjusts the fusion weights of the features between HSI and LST, facilitating efficient complementary fusion of multi-source features. Finally, the weighted fused features are utilized for the classification. Extensive experimental results demonstrate that OCSA-FN outperforms state-of-the-art existing methods in terms of classification accuracy while reducing both the parameter count and computational complexity within the model.&lt;/p&gt;</content:encoded></item><item><title>A dual pixel-level and subpatch-level network with cross-temporal super resolution for change detection across spatial resolutions</title><link>https://doi.org/10.1016/j.jag.2026.105134</link><guid>10.1016/j.jag.2026.105134</guid><pubDate>Fri, 30 Jan 2026 20:21:28 +0000</pubDate><dc:creator>Dawei Wen</dc:creator><dc:creator>Yunlong Zhang</dc:creator><dc:creator>Binqiang Zhang</dc:creator><dc:creator>Deng Chen</dc:creator><dc:creator>Xiaofeng Pan</dc:creator><dc:creator>Xin Huang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105134</prism:doi><description>Remote sensing image change detection is critical for monitoring earth surface dynamics. Although deep learning has significantly improved change detection performance, traditional and existing deep super-resolution techniques for cross-resolution change detection often assume bi-temporal images share the same resolution, at least in the training phase. They also suffer from limitations including dependency on expensive high-resolution paired training data, suboptimal performance transfer from super-resolution to change detection accuracy, and heavy reliance on extensive pixel-level annotations. To address these limitations, we propose a novel dual pixel-level and subpatch-level network with cross-temporal super resolution (DPSNet) for change detection across spatial resolutions. Our method DPSNet, comprises two core components: 1) a Reference Image-Guided Generative Adversarial Network (RefIGM GAN) for cross-temporal super resolution; and 2) a Semi-supervised Dual-Path Network (SDNet) for pixel-level and subpatch-level change detection. A resource-efficient alternating optimization strategy is employed between RefIGM GAN and SDNet, creating a virtuous cycle in which super-resolution improves detection accuracy, and detection results optimize super-resolution reconstruction. Experiments were conducted on three datasets, i.e., CDD, SYSU, and HTCD, each characterized by distinct resolution variations. The CDD and SYSU datasets include bi-temporal images with 4 × and 8 × resolution differences, respectively, while the HTCD dataset contains both satellite and UAV imagery with inherent resolution disparities. The results demonstrate that by integrating reference-guided super resolution and semi-supervised learning, effective cross-resolution change detection can be achieved with only limited high-resolution data and pixel-level labels, showing great practical significance in scenarios where solely low-resolution historical images are available. Our source code will be released at https://github.com/Flandre7155/DPSNet .
Published: 2026-01-30T20:21:28+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.768 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dawei Wen; Yunlong Zhang; Binqiang Zhang; Deng Chen; Xiaofeng Pan; Xin Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105134"&gt;10.1016/j.jag.2026.105134&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing image change detection is critical for monitoring earth surface dynamics. Although deep learning has significantly improved change detection performance, traditional and existing deep super-resolution techniques for cross-resolution change detection often assume bi-temporal images share the same resolution, at least in the training phase. They also suffer from limitations including dependency on expensive high-resolution paired training data, suboptimal performance transfer from super-resolution to change detection accuracy, and heavy reliance on extensive pixel-level annotations. To address these limitations, we propose a novel dual pixel-level and subpatch-level network with cross-temporal super resolution (DPSNet) for change detection across spatial resolutions. Our method DPSNet, comprises two core components: 1) a Reference Image-Guided Generative Adversarial Network (RefIGM GAN) for cross-temporal super resolution; and 2) a Semi-supervised Dual-Path Network (SDNet) for pixel-level and subpatch-level change detection. A resource-efficient alternating optimization strategy is employed between RefIGM GAN and SDNet, creating a virtuous cycle in which super-resolution improves detection accuracy, and detection results optimize super-resolution reconstruction. Experiments were conducted on three datasets, i.e., CDD, SYSU, and HTCD, each characterized by distinct resolution variations. The CDD and SYSU datasets include bi-temporal images with 4 × and 8 × resolution differences, respectively, while the HTCD dataset contains both satellite and UAV imagery with inherent resolution disparities. The results demonstrate that by integrating reference-guided super resolution and semi-supervised learning, effective cross-resolution change detection can be achieved with only limited high-resolution data and pixel-level labels, showing great practical significance in scenarios where solely low-resolution historical images are available. Our source code will be released at https://github.com/Flandre7155/DPSNet .&lt;/p&gt;</content:encoded></item><item><title>Set-CVGL: A new perspective on cross-view geo-localization with unordered ground-view image sets</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.037</link><guid>10.1016/j.isprsjprs.2026.01.037</guid><pubDate>Fri, 30 Jan 2026 10:57:03 +0000</pubDate><dc:creator>Qiong Wu</dc:creator><dc:creator>Panwang Xia</dc:creator><dc:creator>Lei Yu</dc:creator><dc:creator>Yi Liu</dc:creator><dc:creator>Mingtao Xiong</dc:creator><dc:creator>Liheng Zhong</dc:creator><dc:creator>Jingdong Chen</dc:creator><dc:creator>Ming Yang</dc:creator><dc:creator>Yongjun Zhang</dc:creator><dc:creator>Yi Wan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.037</prism:doi><description>Cross-view geo-localization (CVGL) has been widely applied in fields such as robotic navigation and geographic information coupling. Existing approaches primarily use single images or fixed-view image sequences as queries, which limits perspective diversity. In contrast, when humans determine their location visually, they typically move around to gather multiple perspectives. This behavior suggests that integrating diverse visual cues can improve geo-localization reliability. Therefore, we propose a novel task: Cross-View Image Set Geo-Localization (Set-CVGL), which gathers multiple images with diverse perspectives as a query set for localization. To support this task, we introduce SetVL-480K, a benchmark comprising 480,000 ground images captured worldwide and their corresponding satellite images, with each satellite image corresponds to an average of 40 ground images from varied perspectives and locations. Furthermore, we propose FlexGeo, a flexible method designed for Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo includes two key modules: the Similarity-guided Feature Fuser (SFF), which adaptively fuses image features without prior content dependency, and the Individual-level Attributes Learner (IAL), leveraging geo-attributes of each image for comprehensive scene perception. FlexGeo consistently outperforms existing methods on SetVL-480K and four public datasets (VIGOR, University-1652, SeqGeo, and KITTI-CVL), achieving a 2.34 &amp;#xD7; " role="presentation"&gt; × × improvement in localization accuracy on SetVL-480K. The codes and dataset will be available at https://github.com/Mabel0403/Set-CVGL .
Published: 2026-01-30T10:57:03+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.767 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiong Wu; Panwang Xia; Lei Yu; Yi Liu; Mingtao Xiong; Liheng Zhong; Jingdong Chen; Ming Yang; Yongjun Zhang; Yi Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.037"&gt;10.1016/j.isprsjprs.2026.01.037&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-view geo-localization (CVGL) has been widely applied in fields such as robotic navigation and geographic information coupling. Existing approaches primarily use single images or fixed-view image sequences as queries, which limits perspective diversity. In contrast, when humans determine their location visually, they typically move around to gather multiple perspectives. This behavior suggests that integrating diverse visual cues can improve geo-localization reliability. Therefore, we propose a novel task: Cross-View Image Set Geo-Localization (Set-CVGL), which gathers multiple images with diverse perspectives as a query set for localization. To support this task, we introduce SetVL-480K, a benchmark comprising 480,000 ground images captured worldwide and their corresponding satellite images, with each satellite image corresponds to an average of 40 ground images from varied perspectives and locations. Furthermore, we propose FlexGeo, a flexible method designed for Set-CVGL that can also adapt to single-image and image-sequence inputs. FlexGeo includes two key modules: the Similarity-guided Feature Fuser (SFF), which adaptively fuses image features without prior content dependency, and the Individual-level Attributes Learner (IAL), leveraging geo-attributes of each image for comprehensive scene perception. FlexGeo consistently outperforms existing methods on SetVL-480K and four public datasets (VIGOR, University-1652, SeqGeo, and KITTI-CVL), achieving a 2.34 &amp;amp;#xD7; &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; × × improvement in localization accuracy on SetVL-480K. The codes and dataset will be available at https://github.com/Mabel0403/Set-CVGL .&lt;/p&gt;</content:encoded></item><item><title>TranSAC: An unsupervised transferability metric based on task speciality and domain commonality</title><link>https://doi.org/10.1016/j.patcog.2026.113137</link><guid>10.1016/j.patcog.2026.113137</guid><pubDate>Fri, 30 Jan 2026 00:18:53 +0000</pubDate><dc:creator>Qianshan Zhan</dc:creator><dc:creator>Xiao-Jun Zeng</dc:creator><dc:creator>Qian Wang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113137</prism:doi><description>In transfer learning, one fundamental problem is transferability estimation, where a metric measures transfer performance without training. Existing metrics face two issues: 1) requiring target domain labels, and 2) only focusing on task speciality but ignoring equally important domain commonality. To overcome these limitations, we propose TranSAC, a Tran sferability metric based on task S peciality A nd domain C ommonality, capturing the separation between classes and the similarity between domains. Its main advantages are: 1) unsupervised, 2) fine-tuning free, and 3) applicable to source-dependent and source-free transfer scenarios. To achieve this, we investigate the upper and lower bounds of transfer performance based on fixed representations extracted from the pre-trained model. Theoretical results reveal that unsupervised transfer performance is characterized by entropy-based quantities, naturally reflecting task specificity and domain commonality. These insights motivate the design of TranSAC, which integrates both factors to enhance transferability. Extensive experiments are performed across 12 target datasets with 36 pre-trained models, including supervised CNNs, self-supervised CNNs, and ViTs. Results demonstrate the importance of domain commonality and task speciality, allowing TranSAC as superior to state-of-the-art metrics for pre-trained model ranking, target domain ranking, and source domain ranking.
Published: 2026-01-30T00:18:53+00:00
Venue: Pattern Recognition
Score: 0.767 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qianshan Zhan; Xiao-Jun Zeng; Qian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113137"&gt;10.1016/j.patcog.2026.113137&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (must_read)&lt;/p&gt;
&lt;p&gt;In transfer learning, one fundamental problem is transferability estimation, where a metric measures transfer performance without training. Existing metrics face two issues: 1) requiring target domain labels, and 2) only focusing on task speciality but ignoring equally important domain commonality. To overcome these limitations, we propose TranSAC, a Tran sferability metric based on task S peciality A nd domain C ommonality, capturing the separation between classes and the similarity between domains. Its main advantages are: 1) unsupervised, 2) fine-tuning free, and 3) applicable to source-dependent and source-free transfer scenarios. To achieve this, we investigate the upper and lower bounds of transfer performance based on fixed representations extracted from the pre-trained model. Theoretical results reveal that unsupervised transfer performance is characterized by entropy-based quantities, naturally reflecting task specificity and domain commonality. These insights motivate the design of TranSAC, which integrates both factors to enhance transferability. Extensive experiments are performed across 12 target datasets with 36 pre-trained models, including supervised CNNs, self-supervised CNNs, and ViTs. Results demonstrate the importance of domain commonality and task speciality, allowing TranSAC as superior to state-of-the-art metrics for pre-trained model ranking, target domain ranking, and source domain ranking.&lt;/p&gt;</content:encoded></item><item><title>Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models</title><link>https://arxiv.org/abs/2601.20419v1</link><guid>http://arxiv.org/abs/2601.20419v1</guid><pubDate>Wed, 28 Jan 2026 09:24:14 +0000</pubDate><dc:creator>Yuhao Sun</dc:creator><dc:creator>Chengyi Cai</dc:creator><dc:creator>Jiacheng Zhang</dc:creator><dc:creator>Zesheng Ye</dc:creator><dc:creator>Xingliang Yuan</dc:creator><dc:creator>Feng Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.
Published: 2026-01-28T09:24:14+00:00
Venue: arXiv
Score: 0.767 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhao Sun; Chengyi Cai; Jiacheng Zhang; Zesheng Ye; Xingliang Yuan; Feng Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (must_read)&lt;/p&gt;
&lt;p&gt;Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.&lt;/p&gt;</content:encoded></item><item><title>Hypersolid: Emergent Vision Representations via Short-Range Repulsion</title><link>https://arxiv.org/abs/2601.21255v1</link><guid>http://arxiv.org/abs/2601.21255v1</guid><pubDate>Thu, 29 Jan 2026 04:25:43 +0000</pubDate><dc:creator>Esteban Rodríguez-Betancourt</dc:creator><dc:creator>Edgar Casasola-Murillo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.
Published: 2026-01-29T04:25:43+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Esteban Rodríguez-Betancourt; Edgar Casasola-Murillo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;A recurring challenge in self-supervised learning is preventing representation collapse. Existing solutions typically rely on global regularization, such as maximizing distances, decorrelating dimensions or enforcing certain distributions. We instead reinterpret representation learning as a discrete packing problem, where preserving information simplifies to maintaining injectivity. We operationalize this in Hypersolid, a method using short-range hard-ball repulsion to prevent local collisions. This constraint results in a high-separation geometric regime that preserves augmentation diversity, excelling on fine-grained and low-resolution classification tasks.&lt;/p&gt;</content:encoded></item></channel></rss>