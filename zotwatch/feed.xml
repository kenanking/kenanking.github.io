<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 29 Nov 2025 04:26:53 +0000</lastBuildDate><item><title>DSTransNet: Dynamic Feature Selection Network with Feature Enhancement and Multi-Attention for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638454</link><guid>10.1109/tgrs.2025.3638454</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Infrared small target detection (IRSTD) has significantly benefited from UNet-based neural models in recent years. However, current methodologies face challenges in achieving optimal compromise between missed detections and false alarms. To overcome this limitation, we rethink the role of each structural component within UNet-based architectures applied for IRSTD. Accordingly, we conceptualize the UNet’s encoder as specializing in feature extraction, the skip connections in feature selection, and the decoder in fusion-based reconstruction. Building upon these conceptualizations, we propose the DSTransNet. Within the feature extraction stage, the edge shape receptive field (ESR) module enhances edge and shape feature extraction and expands the receptive field via multiple convolutional branches, thereby reducing missed detections. At the feature selection stage, the reliable dynamic selection filtering (RDSF) module employs dynamic feature selection, leveraging encoder-based self-attention and decoder-based cross-attention of the Transformer to suppress background features resembling small targets and mitigate false alarms. During the feature fusion-based reconstruction stage, the cross-attention of spaces and channels (CSCE) module emphasizes small target features via spatial and channel cross-attention, reconstructing more accurate multi-scale detection masks. Extensive experiments on the SIRST, NUDT-SIRST, and SIRST-Aug datasets demonstrate that the proposed DSTransNet method outperforms state-of-the-art IRSTD approaches. The code is available at https://github.com/RuiminHuang/DSTransNet.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.856 (must_read)</description></item><item><title>Understanding, Accelerating, and Improving MeanFlow Training</title><link>https://arxiv.org/abs/2511.19065v1</link><guid>http://arxiv.org/abs/2511.19065v1</guid><pubDate>Mon, 24 Nov 2025 12:59:27 +0000</pubDate><category>arXiv</category><description>MeanFlow promises high-quality generative modeling in few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics remain unclear. We analyze the interaction between the two velocities and find: (i) well-established instantaneous velocity is a prerequisite for learning average velocity; (ii) learning of instantaneous velocity benefits from average velocity when the temporal gap is small, but degrades as the gap increases; and (iii) task-affinity analysis indicates that smooth learning of large-gap average velocities, essential for one-step generation, depends on the prior formation of accurate instantaneous and small-gap average velocities. Guided by these observations, we design an effective training scheme that accelerates the formation of instantaneous velocity, then shifts emphasis from short- to long-interval average velocity. Our enhanced MeanFlow training yields faster convergence and significantly better few-step generation: With the same DiT-XL backbone, our method reaches an impressive FID of 2.87 on 1-NFE ImageNet 256x256, compared to 3.43 for the conventional MeanFlow baseline. Alternatively, our method matches the performance of the MeanFlow baseline with 2.5x shorter training time, or with a smaller DiT-L backbone.
Published: 2025-11-24T12:59:27+00:00
Venue: arXiv
Score: 0.853 (must_read)</description></item><item><title>Revisiting Attention Mechanisms and Transformer Networks for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638738</link><guid>10.1109/tgrs.2025.3638738</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Infrared small target detection plays a vital role in applications such as military surveillance and space observation. Because infrared small targets exhibit weak and indistinct features, they are often submerged within cluttered backgrounds. Capturing long-range dependencies and extracting discriminative differences between targets and backgrounds are key to improving detection accuracy. However, existing attention mechanisms and transformer network architectures have limitations, which impair the ability to explore context and capture long-distance deep dependencies. In addition, the existing methods rarely consider cross-scale feature fusion. To this end, we propose a novel network specifically for infrared small target detection called RAM-TransNet. Firstly, the whole network adopts a U-Net similar multi-attention nested pure transformer structure to learn and extract longer-distance and deeper target features. Secondly, we develop a new contextual transformer block with a dual attention structure. This contextual transformer block allows us to capture dynamic and static contextual information by making the most of the contextual information between input keys in 2D feature maps. As a result, this enhances visual features’ exploration and capture capacity. In addition, we have created a new multi-hierarchical cross-scale interaction module to aid different transformer layer features in performing multi-scale information fusion and enhancing feature perception. Finally, We evaluated our proposed method using comprehensive evaluation metrics on three public datasets. Extensive experimental results demonstrate that the proposed method is highly effective and significantly outperforms state-of-the-art methods. Moreover, the noise immunity experiment indicates that our proposed method has better noise tolerance.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.851 (must_read)</description></item><item><title>InterpIoU: Robust bounding box regression loss within an interpolation-based IoU framework</title><link>https://doi.org/10.1016/j.neucom.2025.132230</link><guid>10.1016/j.neucom.2025.132230</guid><pubDate>Fri, 28 Nov 2025 07:48:13 +0000</pubDate><category>Neurocomputing</category><description>Bounding box regression (BBR) is central to object detection, where regression loss plays a key role in precise localization. Existing IoU-based losses often rely on handcrafted geometric penalties to provide gradients in non-overlapping cases and improve localization. However, these geometric penalties are inherently sensitive to box geometry, producing unstable gradients in extreme cases and a subtle misalignment with the IoU objective, which harms small objects detection and yields undesired converge behaviors such as bounding box enlargement. To address these limitations, we introduce InterpIoU, an interpolation-based IoU optimization framework that rethinks BBR beyond handcrafted penalties. By bridging predictions and ground truth with interpolated boxes, InterpIoU supplies meaningful gradients in non-overlapping cases while ensuring consistent alignment with the BBR objective. Crucially, our findings challenge the convention of using geometric penalties, demonstrating they are often unnecessary and suboptimal. Building on InterpIoU, we propose Dynamic InterpIoU, which adjusts interpolation coefficients based on IoU values, adapting to diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC demonstrate that our methods consistently outperform state-of-the-art IoU-based losses across detection frameworks, including YOLOv8 and DINO, with notable improvements for small object detection.
Published: 2025-11-28T07:48:13+00:00
Venue: Neurocomputing
Score: 0.833 (must_read)</description></item><item><title>StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections</title><link>https://arxiv.org/abs/2511.20418v1</link><guid>http://arxiv.org/abs/2511.20418v1</guid><pubDate>Tue, 25 Nov 2025 15:42:33 +0000</pubDate><category>arXiv</category><description>Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.
Published: 2025-11-25T15:42:33+00:00
Venue: arXiv
Score: 0.832 (must_read)</description></item><item><title>Granular Computing-driven SAM: From Coarse-to-Fine Guidance for Prompt-Free Segmentation</title><link>https://arxiv.org/abs/2511.19062v1</link><guid>http://arxiv.org/abs/2511.19062v1</guid><pubDate>Mon, 24 Nov 2025 12:55:02 +0000</pubDate><category>arXiv</category><description>Prompt-free image segmentation aims to generate accurate masks without manual guidance. Typical pre-trained models, notably Segmentation Anything Model (SAM), generate prompts directly at a single granularity level. However, this approach has two limitations: (1) Localizability, lacking mechanisms for autonomous region localization; (2) Scalability, limited fine-grained modeling at high resolution. To address these challenges, we introduce Granular Computing-driven SAM (Grc-SAM), a coarse-to-fine framework motivated by Granular Computing (GrC). First, the coarse stage adaptively extracts high-response regions from features to achieve precise foreground localization and reduce reliance on external prompts. Second, the fine stage applies finer patch partitioning with sparse local swin-style attention to enhance detail modeling and enable high-resolution segmentation. Third, refined masks are encoded as latent prompt embeddings for the SAM decoder, replacing handcrafted prompts with an automated reasoning process. By integrating multi-granularity attention, Grc-SAM bridges granular computing with vision transformers. Extensive experimental results demonstrate Grc-SAM outperforms baseline methods in both accuracy and scalability. It offers a unique granular computational perspective for prompt-free segmentation.
Published: 2025-11-24T12:55:02+00:00
Venue: arXiv
Score: 0.825 (must_read)</description></item><item><title>PixelDiT: Pixel Diffusion Transformers for Image Generation</title><link>https://arxiv.org/abs/2511.20645v1</link><guid>http://arxiv.org/abs/2511.20645v1</guid><pubDate>Tue, 25 Nov 2025 18:59:25 +0000</pubDate><category>arXiv</category><description>Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.
Published: 2025-11-25T18:59:25+00:00
Venue: arXiv
Score: 0.824 (must_read)</description></item><item><title>S4DR-Net: Self-Supervised Spatial-Spectral Distance Reconstruction Network for Multispectral Point Cloud Classification</title><link>https://doi.org/10.1109/tgrs.2025.3638606</link><guid>10.1109/tgrs.2025.3638606</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Multispectral LiDAR point clouds are valuable in remote sensing for their spatial-spectral consistency, yet their high acquisition and annotation costs pose significant challenges. To mitigate this, self-supervised learning has emerged as a promising solution, reducing reliance on annotated data while improving model generalization. However, existing self-supervised frameworks for point clouds often overlook the complexity of ground object distribution in large-scale remote sensing scenarios and fail to leverage the spectral information inherent in multispectral point clouds. In this paper, we introduce the Self-Supervised Spatial-Spectral Distance Reconstruction Network (S4DR-Net), a novel self-supervised pre-training network designed for multispectral point cloud classification. Serving as the key component of the network, the Spatial-Spectral Distance Prediction module (S-SDP) effectively addresses these limitations by reconstructing the distance relationships between voxel blocks in three-dimensional Euclidean as well as spectral spaces. By jointly considering spatial and spectral distances, S-SDP enables the network to learn a unified representation that captures the intrinsic spatial-spectral consistency of multispectral point clouds. This design allows S4DR-Net to generate low-dimensional feature representations in a self-supervised manner, without reliance on manual annotations. We conducted experiments and evaluated on two real-world multispectral point cloud datasets. The results demonstrate that S4DR-Net consistently outperforms existing self-supervised pre-training methods, achieving superior accuracy and generalization compared with current state-of-the-art approaches. The code will be released at https://github.com/KustTeamWQW/S4DR-Net.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.824 (must_read)</description></item><item><title>Rethinking Vision Transformer Depth via Structural Reparameterization</title><link>https://arxiv.org/abs/2511.19718v1</link><guid>http://arxiv.org/abs/2511.19718v1</guid><pubDate>Mon, 24 Nov 2025 21:28:55 +0000</pubDate><category>arXiv</category><description>The computational overhead of Vision Transformers in practice stems fundamentally from their deep architectures, yet existing acceleration strategies have primarily targeted algorithmic-level optimizations such as token pruning and attention speedup. This leaves an underexplored research question: can we reduce the number of stacked transformer layers while maintaining comparable representational capacity? To answer this, we propose a branch-based structural reparameterization technique that operates during the training phase. Our approach leverages parallel branches within transformer blocks that can be systematically consolidated into streamlined single-path models suitable for inference deployment. The consolidation mechanism works by gradually merging branches at the entry points of nonlinear components, enabling both feed-forward networks (FFN) and multi-head self-attention (MHSA) modules to undergo exact mathematical reparameterization without inducing approximation errors at test time. When applied to ViT-Tiny, the framework successfully reduces the original 12-layer architecture to 6, 4, or as few as 3 layers while maintaining classification accuracy on ImageNet-1K. The resulting compressed models achieve inference speedups of up to 37% on mobile CPU platforms. Our findings suggest that the conventional wisdom favoring extremely deep transformer stacks may be unnecessarily restrictive, and point toward new opportunities for constructing efficient vision transformers.
Published: 2025-11-24T21:28:55+00:00
Venue: arXiv
Score: 0.823 (must_read)</description></item><item><title>Lightweight Local–Global Dual-Path Feature Fusion Network for Infrared Small Target Image Super-Resolution and Enhancement</title><link>https://doi.org/10.1109/tgrs.2025.3638791</link><guid>10.1109/tgrs.2025.3638791</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Infrared imaging is widely used in remote sensing and military target recognition due to its strong resistance to interference in complex environments. However, imaging mechanisms and hardware limitations cause infrared images to have low-resolution, sparse textures, and significant background noise, which severely restrict the detection of small targets such as low-altitude drones and weak thermal emitters. To overcome these limitations, we propose a lightweight Local–Global Dual-Path Feature Fusion Network (LDFF-Net) that enhances the resolution and quality of infrared images, providing high-quality inputs for subsequent detection tasks. The network includes a Small Target Feature Recognition Module (STFRM) composed of three key components. The Enhanced High Frequency Perception Module (EHFPM) strengthens high-frequency details of small targets while suppressing noise, enabling robust local feature extraction. The State-Space Model (SSM) captures long-range dependencies with linear complexity and models semantic relationships between targets and background to compensate for the limited receptive field of local features. The Adaptive Feature Fusion Unit (AFFU) combines local and global features adaptively to improve the saliency of small targets. During training, we introduce a realistic degradation process based on visible-light images to generate training samples that include complex degradation patterns and noise, which enhances the model’s robustness and generalization. Evaluation on the ARCHIVE and SIRST datasets demonstrates that LDFF-Net outperforms existing state-of-the-art methods across eight widely used full-reference and no-reference metrics, including PSNR, LPIPS, FID, and NIQE. This result confirms the model’s effectiveness in enhancing both the super-resolution and detection performance of infrared small target images. The code and pretrained model weights are publicly available at https://github.com/98Hao/LDFF-Net.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.823 (must_read)</description></item><item><title>IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection</title><link>https://arxiv.org/abs/2511.20319v1</link><guid>http://arxiv.org/abs/2511.20319v1</guid><pubDate>Tue, 25 Nov 2025 13:53:54 +0000</pubDate><category>arXiv</category><description>Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.
Published: 2025-11-25T13:53:54+00:00
Venue: arXiv
Score: 0.820 (must_read)</description></item><item><title>What neuroscience can tell AI about learning in continuously changing environments</title><link>https://doi.org/10.1038/s42256-025-01146-z</link><guid>10.1038/s42256-025-01146-z</guid><pubDate>Fri, 28 Nov 2025 10:02:42 +0000</pubDate><category>Nature Machine Intelligence</category><description>Modern artificial intelligence (AI) models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task and then deployed with fixed parameters. Their training is costly, slow and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioural policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal’s behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioural tasks with shifting rules, reward probabilities or outcomes. We outline an agenda for how the links between neuroscience and AI could be tightened, thus supporting the transfer of ideas and findings between both areas and contributing to the evolving field of NeuroAI. Durstewitz et al. explore what artificial intelligence can learn from the brain’s ability to adjust quickly to changing environments. By linking neuroscience studies of flexible behaviour with advances in continual and in-context learning, this Perspective outlines ways to strengthen the exchange of ideas between the two fields and advance NeuroAI.
Published: 2025-11-28T10:02:42+00:00
Venue: Nature Machine Intelligence
Score: 0.819 (must_read)</description></item><item><title>GLANet: Global-Local Adaptive Network for Efficient Rotated Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638781</link><guid>10.1109/tgrs.2025.3638781</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Rotated object detection plays a crucial role in various visual perception tasks such as aerial photography, remote sensing imagery, and low-altitude unmanned aerial vehicle (UAV) imagery. However, targets in both high-altitude remote sensing images and low-altitude UAV images often exhibit significant scale variations, diverse orientations, and dense spatial distributions, posing formidable challenges to detection algorithms in terms of accuracy and real-time performance. To address these issues, this paper proposes a Global-Local Adaptive Network for Efficient Rotated Object Detection (GLANet), designed to enhance detection precision and efficiency in complex scenarios. GLANet incorporates a lightweight backbone network, Revisiting Mobile CNN From ViT Perspective (RepViT), which balances inference efficiency with an improved capability to represent directional structural features of objects. During feature fusion, we introduce the Geometry-Enhanced Attention guided Rotated Feature Pyramid Network (GEAR-FPN), which jointly models global semantic context and local detailed features, thereby strengthening detection performance for small-scale and densely packed targets. In the detection head, we present a Dynamic Lightweight Geometric-Aware Head (DLGA-Head) alongside a Dynamic Lightweight Global Attention (Dynamic LWGA) mechanism to strengthen the representation of target orientation and boundary information. The effectiveness of the proposed method is validated on both the DOTA and CODrone datasets. GLANet achieves an mAP of 78.12% on DOTA with competitive, near-state-of-the-art accuracy and significantly higher computational efficiency than other top-performing models. Specifically, it contains only 8.64M parameters and 35.69 GFLOPs, ensuring real-time inference while maintaining high precision. On the CODrone dataset, it further delivers improved detection performance while maintaining superior efficiency compared with existing approaches.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.817 (must_read)</description></item><item><title>From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting</title><link>https://arxiv.org/abs/2511.21215v1</link><guid>http://arxiv.org/abs/2511.21215v1</guid><pubDate>Wed, 26 Nov 2025 09:44:51 +0000</pubDate><category>arXiv</category><description>We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (&lt;1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.
Published: 2025-11-26T09:44:51+00:00
Venue: arXiv
Score: 0.817 (must_read)</description></item><item><title>Dual-Pathway Feature Separation and Gated Fusion Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638739</link><guid>10.1109/tgrs.2025.3638739</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Infrared Small Target Detection (IRSTD) plays a vital role in Infrared Search and Tracking (IRST), enabling intelligent systems to accurately detect dim and small targets within cluttered thermal environments. However, most existing deep learning approaches for IRSTD employ a unified-pathway architecture that conflates saliency and edge information within a shared representation space. This limitation causes feature entanglement, hindering the network’s capacity to accurately separate and represent global saliency and fine-grained edge contours. To overcome these challenges, we propose LoveNet, a dual-pathway network architecture that explicitly separates feature learning into two specialized branches. The first is a multi-scale saliency learning branch designed to extract comprehensive structural and contrast information, capturing the global context of targets. The second is a fixed-scale edge learning branch aimed at preserving spatial details and enhancing the precision of edge contour delineation. To integrate the heterogeneous features extracted by two branches, a gated feature fusion mechanism is proposed to adaptively combine saliency and edge representations based on their spatial and semantic relevance. Furthermore, to provide robust and comprehensive supervision, a hybrid supervision strategy is designed to guide the learning process of hierarchical feature representations. Experiments on the NUDT-SIRST, IRSTD-1k, and SIRST datasets demonstrate that LoveNet consistently achieves the best segmentation performance compared to the state-of-the-art methods, while maintaining a lightweight structure suitable for real-time applications.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.814 (must_read)</description></item><item><title>Mosaic Pruning: A Hierarchical Framework for Generalizable Pruning of Mixture-of-Experts Models</title><link>https://arxiv.org/abs/2511.19822v1</link><guid>http://arxiv.org/abs/2511.19822v1</guid><pubDate>Tue, 25 Nov 2025 01:24:41 +0000</pubDate><category>arXiv</category><description>Sparse Mixture-of-Experts (SMoE) architectures have enabled a new frontier in scaling Large Language Models (LLMs), offering superior performance by activating only a fraction of their total parameters during inference. However, their practical deployment is severely hampered by substantial static memory overhead, as all experts must be loaded into memory. Existing post-training pruning methods, while reducing model size, often derive their pruning criteria from a single, general-purpose corpus. This leads to a critical limitation: a catastrophic performance degradation when the pruned model is applied to other domains, necessitating a costly re-pruning for each new domain. To address this generalization gap, we introduce Mosaic Pruning (MoP). The core idea of MoP is to construct a functionally comprehensive set of experts through a structured ``cluster-then-select" process. This process leverages a similarity metric that captures expert performance across different task domains to functionally cluster the experts, and subsequently selects the most representative expert from each cluster based on our proposed Activation Variability Score. Unlike methods that optimize for a single corpus, our proposed Mosaic Pruning ensures that the pruned model retains a functionally complementary set of experts, much like the tiles of a mosaic that together form a complete picture of the original model's capabilities, enabling it to handle diverse downstream tasks.Extensive experiments on various MoE models demonstrate the superiority of our approach. MoP significantly outperforms prior work, achieving a 7.24\% gain on general tasks and 8.92\% on specialized tasks like math reasoning and code generation.
Published: 2025-11-25T01:24:41+00:00
Venue: arXiv
Score: 0.813 (must_read)</description></item><item><title>SAM-MI: A Mask-Injected Framework for Enhancing Open-Vocabulary Semantic Segmentation with SAM</title><link>https://arxiv.org/abs/2511.20027v1</link><guid>http://arxiv.org/abs/2511.20027v1</guid><pubDate>Tue, 25 Nov 2025 07:52:07 +0000</pubDate><category>arXiv</category><description>Open-vocabulary semantic segmentation (OVSS) aims to segment and recognize objects universally. Trained on extensive high-quality segmentation data, the segment anything model (SAM) has demonstrated remarkable universal segmentation capabilities, offering valuable support for OVSS. Although previous methods have made progress in leveraging SAM for OVSS, there are still some challenges: (1) SAM's tendency to over-segment and (2) hard combinations between fixed masks and labels. This paper introduces a novel mask-injected framework, SAM-MI, which effectively integrates SAM with OVSS models to address these challenges. Initially, SAM-MI employs a Text-guided Sparse Point Prompter to sample sparse prompts for SAM instead of previous dense grid-like prompts, thus significantly accelerating the mask generation process. The framework then introduces Shallow Mask Aggregation (SMAgg) to merge partial masks to mitigate the SAM's over-segmentation issue. Finally, Decoupled Mask Injection (DMI) incorporates SAM-generated masks for guidance at low-frequency and high-frequency separately, rather than directly combining them with labels. Extensive experiments on multiple benchmarks validate the superiority of SAM-MI. Notably, the proposed method achieves a 16.7% relative improvement in mIoU over Grounded-SAM on the MESS benchmark, along with a 1.6$\times$ speedup. We hope SAM-MI can serve as an alternative methodology to effectively equip the OVSS model with SAM.
Published: 2025-11-25T07:52:07+00:00
Venue: arXiv
Score: 0.812 (must_read)</description></item><item><title>DetAny4D: Detect Anything 4D Temporally in a Streaming RGB Video</title><link>https://arxiv.org/abs/2511.18814v1</link><guid>http://arxiv.org/abs/2511.18814v1</guid><pubDate>Mon, 24 Nov 2025 06:42:17 +0000</pubDate><category>arXiv</category><description>Reliable 4D object detection, which refers to 3D object detection in streaming video, is crucial for perceiving and understanding the real world. Existing open-set 4D object detection methods typically make predictions on a frame-by-frame basis without modeling temporal consistency, or rely on complex multi-stage pipelines that are prone to error propagation across cascaded stages. Progress in this area has been hindered by the lack of large-scale datasets that capture continuous reliable 3D bounding box (b-box) annotations. To overcome these challenges, we first introduce DA4D, a large-scale 4D detection dataset containing over 280k sequences with high-quality b-box annotations collected under diverse conditions. Building on DA4D, we propose DetAny4D, an open-set end-to-end framework that predicts 3D b-boxes directly from sequential inputs. DetAny4D fuses multi-modal features from pre-trained foundational models and designs a geometry-aware spatiotemporal decoder to effectively capture both spatial and temporal dynamics. Furthermore, it adopts a multi-task learning architecture coupled with a dedicated training strategy to maintain global consistency across sequences of varying lengths. Extensive experiments show that DetAny4D achieves competitive detection accuracy and significantly improves temporal stability, effectively addressing long-standing issues of jitter and inconsistency in 4D object detection. Data and code will be released upon acceptance.
Published: 2025-11-24T06:42:17+00:00
Venue: arXiv
Score: 0.811 (must_read)</description></item><item><title>DynaMix: Generalizable Person Re-identification via Dynamic Relabeling and Mixed Data Sampling</title><link>https://arxiv.org/abs/2511.19067v1</link><guid>http://arxiv.org/abs/2511.19067v1</guid><pubDate>Mon, 24 Nov 2025 13:01:32 +0000</pubDate><category>arXiv</category><description>Generalizable person re-identification (Re-ID) aims to recognize individuals across unseen cameras and environments. While existing methods rely heavily on limited labeled multi-camera data, we propose DynaMix, a novel method that effectively combines manually labeled multi-camera and large-scale pseudo-labeled single-camera data. Unlike prior works, DynaMix dynamically adapts to the structure and noise of the training data through three core components: (1) a Relabeling Module that refines pseudo-labels of single-camera identities on-the-fly; (2) an Efficient Centroids Module that maintains robust identity representations under a large identity space; and (3) a Data Sampling Module that carefully composes mixed data mini-batches to balance learning complexity and intra-batch diversity. All components are specifically designed to operate efficiently at scale, enabling effective training on millions of images and hundreds of thousands of identities. Extensive experiments demonstrate that DynaMix consistently outperforms state-of-the-art methods in generalizable person Re-ID.
Published: 2025-11-24T13:01:32+00:00
Venue: arXiv
Score: 0.806 (must_read)</description></item><item><title>HunyuanVideo 1.5 Technical Report</title><link>https://arxiv.org/abs/2511.18870v2</link><guid>http://arxiv.org/abs/2511.18870v2</guid><pubDate>Mon, 24 Nov 2025 08:22:07 +0000</pubDate><category>arXiv</category><description>We present HunyuanVideo 1.5, a lightweight yet powerful open-source video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture featuring selective and sliding tile attention (SSTA), enhanced bilingual understanding through glyph-aware text encoding, progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions. Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source video generation models. By releasing the code and model weights, we provide the community with a high-performance foundation that lowers the barrier to video creation and research, making advanced video generation accessible to a broader audience. All open-source assets are publicly available at https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.
Published: 2025-11-24T08:22:07+00:00
Venue: arXiv
Score: 0.806 (must_read)</description></item><item><title>DiP: Taming Diffusion Models in Pixel Space</title><link>https://arxiv.org/abs/2511.18822v1</link><guid>http://arxiv.org/abs/2511.18822v1</guid><pubDate>Mon, 24 Nov 2025 06:55:49 +0000</pubDate><category>arXiv</category><description>Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.90 FID score on ImageNet 256$\times$256.
Published: 2025-11-24T06:55:49+00:00
Venue: arXiv
Score: 0.805 (must_read)</description></item><item><title>VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction</title><link>https://arxiv.org/abs/2511.19971v1</link><guid>http://arxiv.org/abs/2511.19971v1</guid><pubDate>Tue, 25 Nov 2025 06:30:22 +0000</pubDate><category>arXiv</category><description>Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.
Published: 2025-11-25T06:30:22+00:00
Venue: arXiv
Score: 0.802 (must_read)</description></item><item><title>DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs</title><link>https://arxiv.org/abs/2511.20468v1</link><guid>http://arxiv.org/abs/2511.20468v1</guid><pubDate>Tue, 25 Nov 2025 16:33:42 +0000</pubDate><category>arXiv</category><description>Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed
Published: 2025-11-25T16:33:42+00:00
Venue: arXiv
Score: 0.802 (must_read)</description></item><item><title>ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images</title><link>https://arxiv.org/abs/2511.21606v1</link><guid>http://arxiv.org/abs/2511.21606v1</guid><pubDate>Wed, 26 Nov 2025 17:26:00 +0000</pubDate><category>arXiv</category><description>Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.
Published: 2025-11-26T17:26:00+00:00
Venue: arXiv
Score: 0.802 (must_read)</description></item><item><title>Momentum-Enhanced Dual-Prototype Learning Framework for Robust Few-Shot Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tgrs.2025.3638757</link><guid>10.1109/tgrs.2025.3638757</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Prototypical network-based few-shot learning (FSL) has demonstrated promising performance for hyperspectral image (HSI) classification tasks under scarce sample conditions. However, existing prototype-based FSL methods suffer from data distribution variations among randomly sampled tasks, leading to unstable class prototype representations and weak cross-task generalization with limited samples. To address this issue, we propose a momentum-enhanced dual-prototype learning (MEDPL) framework for robust few-shot HSI classification. Firstly, a momentum-updated prototype mechanism constructs an iteratively optimized prototype memory bank. It obtains accumulated prototypes by exponentially decaying weighted fusion of historical and current prototypes, significantly suppressing noise from randomly sampled data and class center shifts caused by distribution bias. Simultaneously, a class-conditioned perturbation-augmentation strategy is introduced. It generates adaptive noise perturbations for support set features based on learnable covariance matrices to obtain enhanced prototypes, thereby improving the generalization representation capability of class prototypes across tasks. Secondly, a dual-prototype metric learning framework is designed, jointly utilizing accumulated prototypes and enhanced prototypes to synergistically enhance the model’s classification stability and cross-task generalization, thus significantly improving the robustness of few-shot classification. Experimental results demonstrate that MEDPL outperforms other few-shot hyperspectral image classification methods. Our source code is available at https://github.com/hejinrong/MEDPL.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.801 (must_read)</description></item><item><title>Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?</title><link>https://arxiv.org/abs/2511.19200v2</link><guid>http://arxiv.org/abs/2511.19200v2</guid><pubDate>Mon, 24 Nov 2025 15:09:32 +0000</pubDate><category>arXiv</category><description>Recent advances in computer vision have yielded models with strong performance on recognition benchmarks; however, significant gaps remain in comparison to human perception. One subtle ability is to judge whether an image looks like a given object without being an instance of that object. We study whether vision-language models such as CLIP capture this distinction. We curated a dataset named RoLA (Real or Lookalike) of real and lookalike exemplars (e.g., toys, statues, drawings, pareidolia) across multiple categories, and first evaluate a prompt-based baseline with paired "real"/"lookalike" prompts. We then estimate a direction in CLIP's embedding space that moves representations between real and lookalike. Applying this direction to image and text embeddings improves discrimination in cross-modal retrieval on Conceptual12M, and also enhances captions produced by a CLIP prefix captioner.
Published: 2025-11-24T15:09:32+00:00
Venue: arXiv
Score: 0.801 (must_read)</description></item><item><title>Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds</title><link>https://arxiv.org/abs/2511.19664v1</link><guid>http://arxiv.org/abs/2511.19664v1</guid><pubDate>Mon, 24 Nov 2025 19:58:45 +0000</pubDate><category>arXiv</category><description>We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.
Published: 2025-11-24T19:58:45+00:00
Venue: arXiv
Score: 0.800 (must_read)</description></item><item><title>V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence</title><link>https://arxiv.org/abs/2511.20886v1</link><guid>http://arxiv.org/abs/2511.20886v1</guid><pubDate>Tue, 25 Nov 2025 22:06:30 +0000</pubDate><category>arXiv</category><description>Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).
Published: 2025-11-25T22:06:30+00:00
Venue: arXiv
Score: 0.800 (must_read)</description></item><item><title>Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits</title><link>https://arxiv.org/abs/2511.20273v1</link><guid>http://arxiv.org/abs/2511.20273v1</guid><pubDate>Tue, 25 Nov 2025 12:59:15 +0000</pubDate><category>arXiv</category><description>Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.
Published: 2025-11-25T12:59:15+00:00
Venue: arXiv
Score: 0.800 (must_read)</description></item><item><title>Dual-Granularity Semantic Prompting for Language Guidance Infrared Small Target Detection</title><link>https://arxiv.org/abs/2511.19306v1</link><guid>http://arxiv.org/abs/2511.19306v1</guid><pubDate>Mon, 24 Nov 2025 16:58:23 +0000</pubDate><category>arXiv</category><description>Infrared small target detection remains challenging due to limited feature representation and severe background interference, resulting in sub-optimal performance. While recent CLIP-inspired methods attempt to leverage textual guidance for detection, they are hindered by inaccurate text descriptions and reliance on manual annotations. To overcome these limitations, we propose DGSPNet, an end-to-end language prompt-driven framework. Our approach integrates dual-granularity semantic prompts: coarse-grained textual priors (e.g., 'infrared image', 'small target') and fine-grained personalized semantic descriptions derived through visual-to-textual mapping within the image space. This design not only facilitates learning fine-grained semantic information but also can inherently leverage language prompts during inference without relying on any annotation requirements. By fully leveraging the precision and conciseness of text descriptions, we further introduce a text-guide channel attention (TGCA) mechanism and text-guide spatial attention (TGSA) mechanism that enhances the model's sensitivity to potential targets across both low- and high-level feature spaces. Extensive experiments demonstrate that our method significantly improves detection accuracy and achieves state-of-the-art performance on three benchmark datasets.
Published: 2025-11-24T16:58:23+00:00
Venue: arXiv
Score: 0.800 (must_read)</description></item></channel></rss>