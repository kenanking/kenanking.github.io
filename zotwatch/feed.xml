<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 17 Dec 2025 02:35:42 +0000</lastBuildDate><item><title>Vision-Language Models Empowered Nighttime Object Detection with Consistency Sampler and Hallucination Feature Generator</title><link>https://doi.org/10.1109/tip.2025.3641316</link><guid>10.1109/tip.2025.3641316</guid><pubDate>Mon, 15 Dec 2025 18:40:43 +0000</pubDate><dc:creator>Lihuo He</dc:creator><dc:creator>Junjie Ke</dc:creator><dc:creator>Zhenghao Wang</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Kai Zhou</dc:creator><dc:creator>Qi Wang</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3641316</prism:doi><description>Current object detectors often suffer performance degradation when applied to cross-domain scenarios, particularly under challenging visual conditions such as nighttime scenes. This is primarily due to the I3 problems: Inadequate sampling of instance-level features, Indistinguishable feature representation across domains and Inaccurate generation for identical category participation. To address these challenges, we propose a domain-adaptive detection framework that enables robust generalization across different visual domains without introducing any additional inference overhead. The framework comprises three key components. Specifically, the centerness–category consistency sampler alleviates inadequate sampling by selecting representative instance-level features, while the paired centerness consistency loss enforces alignment between classification and localization. Second, VLM-based orthogonality enhancement leverages frozen vision–language encoders with an orthogonal projection loss to improve cross-domain feature distinguishability. Third, hallucination feature generator synthesizes robust instance-level features for missing categories, ensuring balanced category participation across domains. Extensive experiments on multiple datasets covering various domain adaptation and generalization settings demonstrate that our method consistently outperforms state-of-the-art detectors, achieving up to 5.5 mAP improvement, with particularly strong gains in nighttime adaptation.
Published: 2025-12-15T18:40:43+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.838 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihuo He; Junjie Ke; Zhenghao Wang; Jie Li; Kai Zhou; Qi Wang; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3641316"&gt;10.1109/tip.2025.3641316&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.838 (must_read)&lt;/p&gt;
&lt;p&gt;Current object detectors often suffer performance degradation when applied to cross-domain scenarios, particularly under challenging visual conditions such as nighttime scenes. This is primarily due to the I3 problems: Inadequate sampling of instance-level features, Indistinguishable feature representation across domains and Inaccurate generation for identical category participation. To address these challenges, we propose a domain-adaptive detection framework that enables robust generalization across different visual domains without introducing any additional inference overhead. The framework comprises three key components. Specifically, the centerness–category consistency sampler alleviates inadequate sampling by selecting representative instance-level features, while the paired centerness consistency loss enforces alignment between classification and localization. Second, VLM-based orthogonality enhancement leverages frozen vision–language encoders with an orthogonal projection loss to improve cross-domain feature distinguishability. Third, hallucination feature generator synthesizes robust instance-level features for missing categories, ensuring balanced category participation across domains. Extensive experiments on multiple datasets covering various domain adaptation and generalization settings demonstrate that our method consistently outperforms state-of-the-art detectors, achieving up to 5.5 mAP improvement, with particularly strong gains in nighttime adaptation.&lt;/p&gt;</content:encoded></item><item><title>PLPFusion: Plane-Line-Pixel Fully Sparse Fusion for Robust Multi-Modal 3D Object Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3644414</link><guid>10.1109/tcsvt.2025.3644414</guid><pubDate>Mon, 15 Dec 2025 18:40:24 +0000</pubDate><dc:creator>Jingfu Hou</dc:creator><dc:creator>Hong Song</dc:creator><dc:creator>Jinfu Li</dc:creator><dc:creator>Yucong Lin</dc:creator><dc:creator>Tianyu Huang</dc:creator><dc:creator>Jugang He</dc:creator><dc:creator>Xiuwei He</dc:creator><dc:creator>Jian Yang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3644414</prism:doi><description>Fully sparse fusion makes an excellent balance between efficiency and accuracy in multi-modal 3D object detection. However, most existing methods focus on foreground objects while overlooking background context. This oversight compromises detection robustness, especially for occluded or small-sized objects, leading to suboptimal detection performance. To address this limitation, we propose a novel fully sparse fusion framework (PLPFusion), which introduces a hierarchical Plane-Line-Pixel representation to progressively model the object-context relationships. PLPFusion comprises three key modules: the Plane Enhancement Module (PEM), the Line Alignment Module (LAM) and the Pixel-Level Aggregation Module (PLAM). Firstly, PEM utilizes geometric cues from LiDAR feature planes to generate spatially-aware object queries. Secondly, LAM further refines these queries with geometric priors for semantic awareness. Lastly, PLAM aggregates pixel-level context to enhance discriminative completeness by leveraging the semantically-aware object queries. On the nuScenes benchmark, PLPFusion achieves 71.9% mAP and 74.0% NDS, outperforming the baseline method FUTR3D by +2.5% mAP and +1.9% NDS, respectively. On the KITTI benchmark, it achieves 72.68% BEV mAP and 67.39% 3D mAP. These results confirm its robustness and effectiveness in diverse multi-modal 3D scenarios. The code of PLPFusion is available on the https://github.com/Text357/PLPFusion.
Published: 2025-12-15T18:40:24+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.822 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingfu Hou; Hong Song; Jinfu Li; Yucong Lin; Tianyu Huang; Jugang He; Xiuwei He; Jian Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3644414"&gt;10.1109/tcsvt.2025.3644414&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.822 (must_read)&lt;/p&gt;
&lt;p&gt;Fully sparse fusion makes an excellent balance between efficiency and accuracy in multi-modal 3D object detection. However, most existing methods focus on foreground objects while overlooking background context. This oversight compromises detection robustness, especially for occluded or small-sized objects, leading to suboptimal detection performance. To address this limitation, we propose a novel fully sparse fusion framework (PLPFusion), which introduces a hierarchical Plane-Line-Pixel representation to progressively model the object-context relationships. PLPFusion comprises three key modules: the Plane Enhancement Module (PEM), the Line Alignment Module (LAM) and the Pixel-Level Aggregation Module (PLAM). Firstly, PEM utilizes geometric cues from LiDAR feature planes to generate spatially-aware object queries. Secondly, LAM further refines these queries with geometric priors for semantic awareness. Lastly, PLAM aggregates pixel-level context to enhance discriminative completeness by leveraging the semantically-aware object queries. On the nuScenes benchmark, PLPFusion achieves 71.9% mAP and 74.0% NDS, outperforming the baseline method FUTR3D by +2.5% mAP and +1.9% NDS, respectively. On the KITTI benchmark, it achieves 72.68% BEV mAP and 67.39% 3D mAP. These results confirm its robustness and effectiveness in diverse multi-modal 3D scenarios. The code of PLPFusion is available on the https://github.com/Text357/PLPFusion.&lt;/p&gt;</content:encoded></item><item><title>Reconstruct Multi-Scale Features for Lightweight Small Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3644176</link><guid>10.1109/tgrs.2025.3644176</guid><pubDate>Mon, 15 Dec 2025 18:38:25 +0000</pubDate><dc:creator>Yuancheng Huang</dc:creator><dc:creator>Renwei Qin</dc:creator><dc:creator>Guoliang Zhao</dc:creator><dc:creator>Hong Ji</dc:creator><dc:creator>Xiangtao Zheng</dc:creator><dc:creator>Yanfei Zhong</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3644176</prism:doi><description>Most small objects are missed when object detection algorithms are transferred from natural images to remote sensing images. Constructing multi-scale features has been proven to be an effective approach for detecting small objects. However, existing methods for multi-scale features have two limitations: insufficient discriminative capability and sparse semantic-spatial information, which fail to fully leverage the potential of multi-scale features. To overcome these limitations, we propose the Multiscale Feature Reconstruction Network (MRN), which introduces three novel modules during feature extraction, fusion, and enhancement: the Composite Multi-scale Feature Extraction Module (CEM), Interlayer Feature Joint Module (IJM), and Spatial-Semantic Information Cross Module (SSM). First, CEM utilizes a multi-branch structure to aggregate scale information. Dilated convolution and asymmetric convolution are extensively used in the branches, which expand the receptive field and capture information of rectangular instances, respectively. Second, the IJM leverages a gating mechanism to achieve pixel-level feature enhancement for feature maps at different hierarchical depths. Finally, the SSM alleviates high-level semantic feature information imbalance through dual-branch information interaction. Furthermore, to utilize the limited computational resources, we propose a lightweight version called MRN_Lite. We evaluate MRN and MRN_Lite on three existing public datasets: AI-TOD, VEDAI, and VisDrone2019. Extensive experiments demonstrate the effectiveness of our method. In comparison experiments, both versions of the model outperform the state of the art (SOTA). And MRN_Lite has less than 50% of the FLOPs and parameters of MRN, which has comparable performance to the original version.
Published: 2025-12-15T18:38:25+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuancheng Huang; Renwei Qin; Guoliang Zhao; Hong Ji; Xiangtao Zheng; Yanfei Zhong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3644176"&gt;10.1109/tgrs.2025.3644176&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Most small objects are missed when object detection algorithms are transferred from natural images to remote sensing images. Constructing multi-scale features has been proven to be an effective approach for detecting small objects. However, existing methods for multi-scale features have two limitations: insufficient discriminative capability and sparse semantic-spatial information, which fail to fully leverage the potential of multi-scale features. To overcome these limitations, we propose the Multiscale Feature Reconstruction Network (MRN), which introduces three novel modules during feature extraction, fusion, and enhancement: the Composite Multi-scale Feature Extraction Module (CEM), Interlayer Feature Joint Module (IJM), and Spatial-Semantic Information Cross Module (SSM). First, CEM utilizes a multi-branch structure to aggregate scale information. Dilated convolution and asymmetric convolution are extensively used in the branches, which expand the receptive field and capture information of rectangular instances, respectively. Second, the IJM leverages a gating mechanism to achieve pixel-level feature enhancement for feature maps at different hierarchical depths. Finally, the SSM alleviates high-level semantic feature information imbalance through dual-branch information interaction. Furthermore, to utilize the limited computational resources, we propose a lightweight version called MRN_Lite. We evaluate MRN and MRN_Lite on three existing public datasets: AI-TOD, VEDAI, and VisDrone2019. Extensive experiments demonstrate the effectiveness of our method. In comparison experiments, both versions of the model outperform the state of the art (SOTA). And MRN_Lite has less than 50% of the FLOPs and parameters of MRN, which has comparable performance to the original version.&lt;/p&gt;</content:encoded></item><item><title>S2FEINet: A Spatial-Spectral Feature Extraction and Interactive Network for Fusing Hyperspectral and Multispectral Images</title><link>https://doi.org/10.1016/j.inffus.2025.104066</link><guid>10.1016/j.inffus.2025.104066</guid><pubDate>Tue, 16 Dec 2025 07:50:31 +0000</pubDate><dc:creator>Yong Zhang</dc:creator><dc:creator>Dayun Wu</dc:creator><dc:creator>Wenlong Ke</dc:creator><dc:creator>Wenzhe Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104066</prism:doi><description>Hyperspectral images (HSIs) provide rich spectral information valuable for numerous applications, but their limited spatial resolution often restricts practical utility. To address this challenge, fusion with high-resolution multispectral images (MSIs) has become an effective strategy for spatial enhancement. While recent deep learning-based fusion methods have shown promising results, several critical limitations persist. Convolutional neural networks (CNNs) are constrained by local receptive fields in capturing global dependencies, while conventional attention mechanisms tend to underutilize local features. Moreover, most existing approaches prioritize spatial features over spectral features, resulting in inadequate spectral reconstruction and loss of spectral details in the fused image. Additionally, many existing methods exhibit insufficient capability in enhancing spatial details and refining spectral information. To overcome these limitations, we propose a novel structured integration framework with an explicit interaction stage, termed the Spatial-Spectral Feature Extraction and Interaction Network (S 2 FEINet). Our architecture incorporates two dedicated modules: the Spectral Feature Extraction (SpeFE) module that captures long-range dependencies in low-resolution HSIs while learning inter-band correlations, and the Spatial Feature Extraction (SpaFE) module that extracts enhanced spatial features from high-resolution MSIs. These modules interact through a cross-domain fusion mechanism to achieve balanced spatial-spectral enhancement. Comprehensive experiments on four benchmark datasets and one real-world dataset demonstrate that S 2 FEINet outperforms ten state-of-the-art methods across multiple evaluation metrics. Specifically, compared to the second-ranked method, S 2 FEINet achieves improvements in mean peak signal-to-noise ratio (MPSNR) by 0.8082 dB, 0.1107 dB, 0.4310 dB, 0.1884 dB, and 0.2238 dB, respectively, across the datasets. The code is available at https://github.com/lab-807/SSFEINet .
Published: 2025-12-16T07:50:31+00:00
Venue: Information Fusion
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yong Zhang; Dayun Wu; Wenlong Ke; Wenzhe Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104066"&gt;10.1016/j.inffus.2025.104066&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral images (HSIs) provide rich spectral information valuable for numerous applications, but their limited spatial resolution often restricts practical utility. To address this challenge, fusion with high-resolution multispectral images (MSIs) has become an effective strategy for spatial enhancement. While recent deep learning-based fusion methods have shown promising results, several critical limitations persist. Convolutional neural networks (CNNs) are constrained by local receptive fields in capturing global dependencies, while conventional attention mechanisms tend to underutilize local features. Moreover, most existing approaches prioritize spatial features over spectral features, resulting in inadequate spectral reconstruction and loss of spectral details in the fused image. Additionally, many existing methods exhibit insufficient capability in enhancing spatial details and refining spectral information. To overcome these limitations, we propose a novel structured integration framework with an explicit interaction stage, termed the Spatial-Spectral Feature Extraction and Interaction Network (S 2 FEINet). Our architecture incorporates two dedicated modules: the Spectral Feature Extraction (SpeFE) module that captures long-range dependencies in low-resolution HSIs while learning inter-band correlations, and the Spatial Feature Extraction (SpaFE) module that extracts enhanced spatial features from high-resolution MSIs. These modules interact through a cross-domain fusion mechanism to achieve balanced spatial-spectral enhancement. Comprehensive experiments on four benchmark datasets and one real-world dataset demonstrate that S 2 FEINet outperforms ten state-of-the-art methods across multiple evaluation metrics. Specifically, compared to the second-ranked method, S 2 FEINet achieves improvements in mean peak signal-to-noise ratio (MPSNR) by 0.8082 dB, 0.1107 dB, 0.4310 dB, 0.1884 dB, and 0.2238 dB, respectively, across the datasets. The code is available at https://github.com/lab-807/SSFEINet .&lt;/p&gt;</content:encoded></item><item><title>MESA: Effective Matching Redundancy Reduction by Semantic Area Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3644296</link><guid>10.1109/tpami.2025.3644296</guid><pubDate>Mon, 15 Dec 2025 18:38:22 +0000</pubDate><dc:creator>Yesheng Zhang</dc:creator><dc:creator>Shuhan Shen</dc:creator><dc:creator>Xu Zhao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644296</prism:doi><description>Matching redundancy, which refers to fine-grained feature comparison between irrelevant image areas, is a prevalent limitation in current feature matching approaches. It leads to unnecessary and error-prone computations, ultimately diminishing matching accuracy. To reduce matching redundancy, we propose MESA and DMESA, both leveraging advanced image understanding of Segment Anything Model (SAM) to establish semantic area matches prior to point matching. These informative area matches, then, can undergo effective internal feature comparison, facilitating precise inside-area point matching. Specifically, MESA adopts a sparse matching framework, while DMESA applies a dense one. Both of them first obtain candidate areas from SAM results through a novel Area Graph (AG). In MESA, matching the candidates is formulated as a graph energy minimization and solved by graphical models derived from AG. In contrast, DMESA performs area matching by generating dense matching distributions on the entire image, aiming at enhancing efficiency. The distributions are produced from off-the-shelf patch matching, modeled as the Gaussian Mixture Model, and refined via the Expectation Maximization. With less repetitive computation, DMESA showcases an area matching speed improvement of nearly five times compared to MESA, while maintaining competitive accuracy. Our methods are extensively evaluated on four different tasks across six datasets, encompassing both indoor and outdoor scenes. The results suggest that our method achieves notable accuracy improvements for nine baselines of point matching in most cases. Furthermore, our methods exhibit promise generalization and improved robustness against image resolution. Code is available at github.com/Easonyesheng/A2PM-MESA.
Published: 2025-12-15T18:38:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yesheng Zhang; Shuhan Shen; Xu Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644296"&gt;10.1109/tpami.2025.3644296&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Matching redundancy, which refers to fine-grained feature comparison between irrelevant image areas, is a prevalent limitation in current feature matching approaches. It leads to unnecessary and error-prone computations, ultimately diminishing matching accuracy. To reduce matching redundancy, we propose MESA and DMESA, both leveraging advanced image understanding of Segment Anything Model (SAM) to establish semantic area matches prior to point matching. These informative area matches, then, can undergo effective internal feature comparison, facilitating precise inside-area point matching. Specifically, MESA adopts a sparse matching framework, while DMESA applies a dense one. Both of them first obtain candidate areas from SAM results through a novel Area Graph (AG). In MESA, matching the candidates is formulated as a graph energy minimization and solved by graphical models derived from AG. In contrast, DMESA performs area matching by generating dense matching distributions on the entire image, aiming at enhancing efficiency. The distributions are produced from off-the-shelf patch matching, modeled as the Gaussian Mixture Model, and refined via the Expectation Maximization. With less repetitive computation, DMESA showcases an area matching speed improvement of nearly five times compared to MESA, while maintaining competitive accuracy. Our methods are extensively evaluated on four different tasks across six datasets, encompassing both indoor and outdoor scenes. The results suggest that our method achieves notable accuracy improvements for nine baselines of point matching in most cases. Furthermore, our methods exhibit promise generalization and improved robustness against image resolution. Code is available at github.com/Easonyesheng/A2PM-MESA.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Maritime Search and Rescue: Incremental Unsupervised Domain Adaptation with Synthetic Data and Pseudo-labeling</title><link>https://doi.org/10.1016/j.eswa.2025.130864</link><guid>10.1016/j.eswa.2025.130864</guid><pubDate>Tue, 16 Dec 2025 16:52:36 +0000</pubDate><dc:creator>Juan P. Martinez-Esteso</dc:creator><dc:creator>Francisco J. Castellanos</dc:creator><dc:creator>Antonio Javier Gallego</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130864</prism:doi><description>Maritime search and rescue operations are critical for saving lives in emergencies, where time is a decisive factor since delays can drastically reduce the chances of survival for those in distress. These missions are particularly challenging due to the inherent complexity of the maritime environment, marked by changing weather, dynamic sea states, and limited visibility. Developing reliable machine learning systems for this task typically requires large amounts of labeled data that capture all possible operating conditions. However, collecting and annotating such data is costly and often unfeasible in real-world maritime scenarios. To address this limitation, we propose a domain adaptation strategy for a segmentation-based detection model that estimates a probability map indicating the presence of human bodies at sea. The method enables unsupervised learning to adapt from a labeled synthetic domain to a real, unlabeled domain by employing a Domain-Adversarial Neural Network that aligns feature representations across domains, and an iterative pseudo-labeling process that selects high-confidence predictions on the target data to progressively refine the model. By leveraging synthetic data—automatically generated and labeled—our approach adapts effectively to real-world conditions without requiring manual annotation. Experimental results show that our method outperforms several state-of-the-art detectors while maintaining a lightweight architecture. Moreover, it generalizes well under diverse and adverse environmental conditions, including fog, rain, and low-light scenes, demonstrating its robustness and suitability for real-world deployment in critical rescue operations.
Published: 2025-12-16T16:52:36+00:00
Venue: Expert Systems with Applications
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Juan P. Martinez-Esteso; Francisco J. Castellanos; Antonio Javier Gallego&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130864"&gt;10.1016/j.eswa.2025.130864&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Maritime search and rescue operations are critical for saving lives in emergencies, where time is a decisive factor since delays can drastically reduce the chances of survival for those in distress. These missions are particularly challenging due to the inherent complexity of the maritime environment, marked by changing weather, dynamic sea states, and limited visibility. Developing reliable machine learning systems for this task typically requires large amounts of labeled data that capture all possible operating conditions. However, collecting and annotating such data is costly and often unfeasible in real-world maritime scenarios. To address this limitation, we propose a domain adaptation strategy for a segmentation-based detection model that estimates a probability map indicating the presence of human bodies at sea. The method enables unsupervised learning to adapt from a labeled synthetic domain to a real, unlabeled domain by employing a Domain-Adversarial Neural Network that aligns feature representations across domains, and an iterative pseudo-labeling process that selects high-confidence predictions on the target data to progressively refine the model. By leveraging synthetic data—automatically generated and labeled—our approach adapts effectively to real-world conditions without requiring manual annotation. Experimental results show that our method outperforms several state-of-the-art detectors while maintaining a lightweight architecture. Moreover, it generalizes well under diverse and adverse environmental conditions, including fog, rain, and low-light scenes, demonstrating its robustness and suitability for real-world deployment in critical rescue operations.&lt;/p&gt;</content:encoded></item><item><title>Fine-Grained Image Captioning by Ranking Diffusion Transformer</title><link>https://doi.org/10.1109/tip.2025.3641303</link><guid>10.1109/tip.2025.3641303</guid><pubDate>Mon, 15 Dec 2025 18:40:43 +0000</pubDate><dc:creator>Jun Wan</dc:creator><dc:creator>Min Gan</dc:creator><dc:creator>Lefei Zhang</dc:creator><dc:creator>Jie Zhou</dc:creator><dc:creator>Jun Liu</dc:creator><dc:creator>Bo Du</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3641303</prism:doi><description>The CLIP visual feature-based image captioning models have developed rapidly and achieved remarkable results. However, existing models still struggle to produce descriptive and discriminative captions because they insufficiently exploit fine-grained visual cues and fail to model complex vision–language alignment. To address these limitations, we propose a Ranking Diffusion Transformer (RDT), which integrates a Ranking Visual Encoder (RVE) and a Ranking Loss (RL) for fine-grained image captioning. The RVE introduces a novel ranking attention mechanism that effectively mines diverse and discriminative visual information from CLIP features. Meanwhile, the RL leverages the ranking of generated caption quality as a global semantic supervisory signal, thereby enhancing the diffusion process and strengthening vision–language semantic alignment. We show that by collaborating RVE and RL via the novel RDT—and by gradually adding and removing noise in the diffusion process—more discriminative visual features are learned and precisely aligned with the language features. Experimental results on popular benchmark datasets demonstrate that our proposed RDT surpasses existing state-of-the-art image captioning models in the literature. The code is publicly available at: https://github.com/junwan2014/RDT.
Published: 2025-12-15T18:40:43+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jun Wan; Min Gan; Lefei Zhang; Jie Zhou; Jun Liu; Bo Du; C. L. Philip Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3641303"&gt;10.1109/tip.2025.3641303&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;The CLIP visual feature-based image captioning models have developed rapidly and achieved remarkable results. However, existing models still struggle to produce descriptive and discriminative captions because they insufficiently exploit fine-grained visual cues and fail to model complex vision–language alignment. To address these limitations, we propose a Ranking Diffusion Transformer (RDT), which integrates a Ranking Visual Encoder (RVE) and a Ranking Loss (RL) for fine-grained image captioning. The RVE introduces a novel ranking attention mechanism that effectively mines diverse and discriminative visual information from CLIP features. Meanwhile, the RL leverages the ranking of generated caption quality as a global semantic supervisory signal, thereby enhancing the diffusion process and strengthening vision–language semantic alignment. We show that by collaborating RVE and RL via the novel RDT—and by gradually adding and removing noise in the diffusion process—more discriminative visual features are learned and precisely aligned with the language features. Experimental results on popular benchmark datasets demonstrate that our proposed RDT surpasses existing state-of-the-art image captioning models in the literature. The code is publicly available at: https://github.com/junwan2014/RDT.&lt;/p&gt;</content:encoded></item><item><title>Development and evolution of YOLO in object detection: A survey</title><link>https://doi.org/10.1016/j.neucom.2025.132436</link><guid>10.1016/j.neucom.2025.132436</guid><pubDate>Mon, 15 Dec 2025 17:05:47 +0000</pubDate><dc:creator>Ying Tian</dc:creator><dc:creator>Wenbo Xu</dc:creator><dc:creator>Bo Yang</dc:creator><dc:creator>Xinglong Yang</dc:creator><dc:creator>Hongliang Guo</dc:creator><dc:creator>Gaige Wang</dc:creator><dc:creator>Helong Yu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132436</prism:doi><description>As a classic problem in computer vision, object detection has become one of the essential challenges that researchers continue to explore. The emergence of You Only Look Once (YOLO) has transformed object detection from two-stage to single-stage detection, enhancing real-time performance. By transforming the object detection task into a regression problem, the detection speed and efficiency have also been significantly improved. This article elaborates on the development history of YOLO object detection algorithm in the past decade, with a focus on the technological evolution, evaluation indicators, dataset selection, and variant improvement from 2016–2025. We have systematically reviewed the technological innovations and major contributions from YOLOv1 to YOLOv13, including the anchor box mechanism, multi-scale prediction, attention module, lightweight design, and anchor-free architecture. Meanwhile, the frequency of use of evaluation metrics for object detection, containing Frames Per Second (FPS), Giga Floating-Point Operations Per Second (GFLOPs), Precision (P), Recall (R), Receiver Operating Characteristic (ROC), Intersection over Union (IoU), F1-score, PR curve, Average Precision (AP), and Mean Average Precision (mAP), was analyzed using statistical literature methods. YOLO algorithm was analyzed for its proportion of utilization in object detection, image classification, and semantic segmentation on various datasets through commonly used datasets, PASCAL VOC, MS COCO, and ImageNet. Finally, the article summarizes the technological innovation and future development trends of the YOLO series, providing a reference for researchers.
Published: 2025-12-15T17:05:47+00:00
Venue: Neurocomputing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ying Tian; Wenbo Xu; Bo Yang; Xinglong Yang; Hongliang Guo; Gaige Wang; Helong Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132436"&gt;10.1016/j.neucom.2025.132436&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;As a classic problem in computer vision, object detection has become one of the essential challenges that researchers continue to explore. The emergence of You Only Look Once (YOLO) has transformed object detection from two-stage to single-stage detection, enhancing real-time performance. By transforming the object detection task into a regression problem, the detection speed and efficiency have also been significantly improved. This article elaborates on the development history of YOLO object detection algorithm in the past decade, with a focus on the technological evolution, evaluation indicators, dataset selection, and variant improvement from 2016–2025. We have systematically reviewed the technological innovations and major contributions from YOLOv1 to YOLOv13, including the anchor box mechanism, multi-scale prediction, attention module, lightweight design, and anchor-free architecture. Meanwhile, the frequency of use of evaluation metrics for object detection, containing Frames Per Second (FPS), Giga Floating-Point Operations Per Second (GFLOPs), Precision (P), Recall (R), Receiver Operating Characteristic (ROC), Intersection over Union (IoU), F1-score, PR curve, Average Precision (AP), and Mean Average Precision (mAP), was analyzed using statistical literature methods. YOLO algorithm was analyzed for its proportion of utilization in object detection, image classification, and semantic segmentation on various datasets through commonly used datasets, PASCAL VOC, MS COCO, and ImageNet. Finally, the article summarizes the technological innovation and future development trends of the YOLO series, providing a reference for researchers.&lt;/p&gt;</content:encoded></item><item><title>Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models</title><link>https://doi.org/10.1109/tpami.2025.3644016</link><guid>10.1109/tpami.2025.3644016</guid><pubDate>Mon, 15 Dec 2025 18:38:22 +0000</pubDate><dc:creator>Zhongqi Wang</dc:creator><dc:creator>Jie Zhang</dc:creator><dc:creator>Shiguang Shan</dc:creator><dc:creator>Xilin Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644016</prism:doi><description>Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the \lt \lt EOS \gt \gt token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens' attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across six representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.27% and an AUC of 86.27%. The code is available at https://github.com/Robin-WZQ/DAA.
Published: 2025-12-15T18:38:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhongqi Wang; Jie Zhang; Shiguang Shan; Xilin Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644016"&gt;10.1109/tpami.2025.3644016&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the \lt \lt EOS \gt \gt token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens&amp;#x27; attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across six representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.27% and an AUC of 86.27%. The code is available at https://github.com/Robin-WZQ/DAA.&lt;/p&gt;</content:encoded></item><item><title>Multi-Scale Local-Global Fusion for Camouflaged Object Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3644658</link><guid>10.1109/tcsvt.2025.3644658</guid><pubDate>Mon, 15 Dec 2025 18:40:24 +0000</pubDate><dc:creator>Boran Yang</dc:creator><dc:creator>Min Zhang</dc:creator><dc:creator>Yong Wang</dc:creator><dc:creator>Duoqian Miao</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3644658</prism:doi><description>Camouflaged Object Detection (COD) is a formidable computer vision challenge due to the striking resemblance between camouflaged objects and their surroundings. Despite progress in existing methods, they still face significant limitations, particularly in addressing the issues of fuzzy boundaries and the inadequate fusion of local and global features. To address these challenges, we present a multi-scale COD network named Multi-Scale Local-Global Fusion (MSLGF). MSLGF incorporates a Multi-Scale Fusion Module (MSFM), which skillfully integrates feature maps at multiple scales to produce high-fidelity edge features. Additionally, to refine the detection process, a Local-Global Feature Fusion Module (LGFFM) combines the local edge details with global semantic information of camouflaged targets, significantly enhancing the accuracy of COD. Experimental results show that MSLGF achieves remarkable performance across 3 benchmark datasets, i.e., Camouflaged Object Dataset (CAMO), Camouflaged Object Dataset with 10,000 Images (COD10K), and NC4K. Specifically, MSLGF attains a structure-measure from 0.879 to 0.894 and a weighted F-measure between 0.817 and 0.856. The source code is publicly available at https://github.com/tc-fro/MSLGF.
Published: 2025-12-15T18:40:24+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Boran Yang; Min Zhang; Yong Wang; Duoqian Miao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3644658"&gt;10.1109/tcsvt.2025.3644658&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Camouflaged Object Detection (COD) is a formidable computer vision challenge due to the striking resemblance between camouflaged objects and their surroundings. Despite progress in existing methods, they still face significant limitations, particularly in addressing the issues of fuzzy boundaries and the inadequate fusion of local and global features. To address these challenges, we present a multi-scale COD network named Multi-Scale Local-Global Fusion (MSLGF). MSLGF incorporates a Multi-Scale Fusion Module (MSFM), which skillfully integrates feature maps at multiple scales to produce high-fidelity edge features. Additionally, to refine the detection process, a Local-Global Feature Fusion Module (LGFFM) combines the local edge details with global semantic information of camouflaged targets, significantly enhancing the accuracy of COD. Experimental results show that MSLGF achieves remarkable performance across 3 benchmark datasets, i.e., Camouflaged Object Dataset (CAMO), Camouflaged Object Dataset with 10,000 Images (COD10K), and NC4K. Specifically, MSLGF attains a structure-measure from 0.879 to 0.894 and a weighted F-measure between 0.817 and 0.856. The source code is publicly available at https://github.com/tc-fro/MSLGF.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Scale Fusion via Uncertainty Estimation for Visual Grounding in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3644749</link><guid>10.1109/tgrs.2025.3644749</guid><pubDate>Mon, 15 Dec 2025 18:38:25 +0000</pubDate><dc:creator>Zhipeng Zhang</dc:creator><dc:creator>Yang Zou</dc:creator><dc:creator>Ji Wang</dc:creator><dc:creator>Peng Wang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3644749</prism:doi><description>Visual grounding in remote sensing (RSVG) aims to locate specific objects in remote sensing images based on referring expressions. While recent methods have achieved promising results by enhancing cross-modal fusion between vision and language, a key challenge remains: significant variation in object scales within a single RS image. Large-scale objects often dominate the representation space, leading to smaller ones being underrepresented or overlooked. To better understand and validate this issue, we analyze typical failure cases and then collect a challenging evaluation set, RSVG-MS, which specifically reflects multi-scale discrepancies. Motivated by these insights, we propose ASF-UE, an Adaptive Scale Fusion framework guided by Uncertainty Estimation. ASF-UE introduces a Scale-Sensitive Attention (SSA) module to extract scale-aware visual features across encoder layers under textual guidance. Additionally, a lightweight T-block is designed for spatial alignment, and a spatial uncertainty estimation (SUE) mechanism is integrated to adaptively score and fuse multi-scale features. Extensive experiments on two standard benchmarks, OPT-RSVG and DIOR-RSVG, demonstrate the effectiveness of our method. More importantly, ASF-UE achieves significant improvements on the challenging RSVG-MS dataset, highlighting its strength in handling scale variation in remote sensing visual grounding.
Published: 2025-12-15T18:38:25+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhipeng Zhang; Yang Zou; Ji Wang; Peng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3644749"&gt;10.1109/tgrs.2025.3644749&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Visual grounding in remote sensing (RSVG) aims to locate specific objects in remote sensing images based on referring expressions. While recent methods have achieved promising results by enhancing cross-modal fusion between vision and language, a key challenge remains: significant variation in object scales within a single RS image. Large-scale objects often dominate the representation space, leading to smaller ones being underrepresented or overlooked. To better understand and validate this issue, we analyze typical failure cases and then collect a challenging evaluation set, RSVG-MS, which specifically reflects multi-scale discrepancies. Motivated by these insights, we propose ASF-UE, an Adaptive Scale Fusion framework guided by Uncertainty Estimation. ASF-UE introduces a Scale-Sensitive Attention (SSA) module to extract scale-aware visual features across encoder layers under textual guidance. Additionally, a lightweight T-block is designed for spatial alignment, and a spatial uncertainty estimation (SUE) mechanism is integrated to adaptively score and fuse multi-scale features. Extensive experiments on two standard benchmarks, OPT-RSVG and DIOR-RSVG, demonstrate the effectiveness of our method. More importantly, ASF-UE achieves significant improvements on the challenging RSVG-MS dataset, highlighting its strength in handling scale variation in remote sensing visual grounding.&lt;/p&gt;</content:encoded></item><item><title>Route-DETR: Pairwise Query Routing in Transformers for Object Detection</title><link>https://arxiv.org/abs/2512.13876v1</link><guid>http://arxiv.org/abs/2512.13876v1</guid><pubDate>Mon, 15 Dec 2025 20:26:58 +0000</pubDate><dc:creator>Ye Zhang</dc:creator><dc:creator>Qi Chen</dc:creator><dc:creator>Wenyou Huang</dc:creator><dc:creator>Rui Liu</dc:creator><dc:creator>Zhengjian Kang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.
Published: 2025-12-15T20:26:58+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ye Zhang; Qi Chen; Wenyou Huang; Rui Liu; Zhengjian Kang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.&lt;/p&gt;</content:encoded></item><item><title>Multimodal Fusion with Vision-Language-Action Models for Robotic Manipulation: A Systematic Review</title><link>https://doi.org/10.1016/j.inffus.2025.104062</link><guid>10.1016/j.inffus.2025.104062</guid><pubDate>Tue, 16 Dec 2025 16:16:13 +0000</pubDate><dc:creator>Muhayy Ud Din</dc:creator><dc:creator>Waseem Akram</dc:creator><dc:creator>Lyes Saad Saoud</dc:creator><dc:creator>Jan Rosell</dc:creator><dc:creator>Irfan Hussain</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104062</prism:doi><description>Vision Language Action (VLA) models represent a new frontier in robotics by unifying perception, reasoning, and control within a single multimodal learning framework. By integrating visual, linguistic, and action modalities, they enable multimodal fusion systems designed for instruction-driven manipulation and generalist autonomy. This systematic review synthesizes the state of the art in VLA research with an emphasis on architectures, algorithms, and applications relevant to robotic manipulation. We examine 102 models, 26 foundational datasets, and 12 simulation platforms, categorizing them according to their fusion strategies and integration mechanisms. Foundational datasets are evaluated using a novel criterion based on task complexity, modality richness, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We further introduce a structured taxonomy of fusion hierarchies and encoder-decoder families, together with a two-dimensional dataset characterization framework and a meta-analytic benchmarking protocol that quantitatively links design variables to empirical performance across benchmarks. Our analysis shows that hierarchical and late fusion architectures achieve the highest manipulation success and generalization, confirming the benefit of multi-level cross-modal integration. Diffusion-based decoders demonstrate superior cross-domain transfer and robustness compared to autoregressive heads. Dataset analysis highlights a persistent lack of benchmarks that combine high-complexity, multimodal, and long-horizon tasks, while existing simulators offer limited multimodal synchronization and real-to-sim consistency. To address these gaps, we propose the VLA Fusion Evaluation Benchmark to quantify fusion efficiency and alignment. Drawing on both academic and industrial advances, the review outlines future research directions in adaptive and modular fusion architectures, computational resource optimization, and the deployment of interpretable, resource-efficient robotic systems. We further propose a forward-looking agentic VLA paradigm where LLM planners integrate VLA skills as verifiable tools within a closed feedback loop for adaptive and self-improving robotic control. This work provides both a conceptual foundation and a quantitative roadmap for advancing embodied intelligence through multimodal information fusion across robotic domains.
Published: 2025-12-16T16:16:13+00:00
Venue: Information Fusion
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Muhayy Ud Din; Waseem Akram; Lyes Saad Saoud; Jan Rosell; Irfan Hussain&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104062"&gt;10.1016/j.inffus.2025.104062&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Language Action (VLA) models represent a new frontier in robotics by unifying perception, reasoning, and control within a single multimodal learning framework. By integrating visual, linguistic, and action modalities, they enable multimodal fusion systems designed for instruction-driven manipulation and generalist autonomy. This systematic review synthesizes the state of the art in VLA research with an emphasis on architectures, algorithms, and applications relevant to robotic manipulation. We examine 102 models, 26 foundational datasets, and 12 simulation platforms, categorizing them according to their fusion strategies and integration mechanisms. Foundational datasets are evaluated using a novel criterion based on task complexity, modality richness, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We further introduce a structured taxonomy of fusion hierarchies and encoder-decoder families, together with a two-dimensional dataset characterization framework and a meta-analytic benchmarking protocol that quantitatively links design variables to empirical performance across benchmarks. Our analysis shows that hierarchical and late fusion architectures achieve the highest manipulation success and generalization, confirming the benefit of multi-level cross-modal integration. Diffusion-based decoders demonstrate superior cross-domain transfer and robustness compared to autoregressive heads. Dataset analysis highlights a persistent lack of benchmarks that combine high-complexity, multimodal, and long-horizon tasks, while existing simulators offer limited multimodal synchronization and real-to-sim consistency. To address these gaps, we propose the VLA Fusion Evaluation Benchmark to quantify fusion efficiency and alignment. Drawing on both academic and industrial advances, the review outlines future research directions in adaptive and modular fusion architectures, computational resource optimization, and the deployment of interpretable, resource-efficient robotic systems. We further propose a forward-looking agentic VLA paradigm where LLM planners integrate VLA skills as verifiable tools within a closed feedback loop for adaptive and self-improving robotic control. This work provides both a conceptual foundation and a quantitative roadmap for advancing embodied intelligence through multimodal information fusion across robotic domains.&lt;/p&gt;</content:encoded></item><item><title>4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</title><link>https://arxiv.org/abs/2512.14235v1</link><guid>http://arxiv.org/abs/2512.14235v1</guid><pubDate>Tue, 16 Dec 2025 09:43:05 +0000</pubDate><dc:creator>Jimmie Kwok</dc:creator><dc:creator>Holger Caesar</dc:creator><dc:creator>Andras Palffy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.
Published: 2025-12-16T09:43:05+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jimmie Kwok; Holger Caesar; Andras Palffy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.&lt;/p&gt;</content:encoded></item><item><title>Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection</title><link>https://arxiv.org/abs/2512.12884v1</link><guid>http://arxiv.org/abs/2512.12884v1</guid><pubDate>Sun, 14 Dec 2025 23:56:16 +0000</pubDate><dc:creator>Xiangzhong Liu</dc:creator><dc:creator>Jiajie Zhang</dc:creator><dc:creator>Hao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/IV64158.2025.11097627</prism:doi><description>In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.
Published: 2025-12-14T23:56:16+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangzhong Liu; Jiajie Zhang; Hao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/IV64158.2025.11097627"&gt;10.1109/IV64158.2025.11097627&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.&lt;/p&gt;</content:encoded></item><item><title>CompleMatch: Boosting Time-Series Semi-Supervised Classification With Temporal-Frequency Complementarity</title><link>https://doi.org/10.1109/tpami.2025.3644603</link><guid>10.1109/tpami.2025.3644603</guid><pubDate>Mon, 15 Dec 2025 18:38:22 +0000</pubDate><dc:creator>Zhen Liu</dc:creator><dc:creator>Kun Zeng</dc:creator><dc:creator>Qianli Ma</dc:creator><dc:creator>James T. Kwok</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3644603</prism:doi><description>Time series Semi-Supervised Classification (SSC) aims to improve model performance by utilizing abundant unlabeled data in scenarios where labeled samples are limited. Previous approaches mainly focus on exploiting temporal dependencies within the time domain for SSC. However, these temporal dependencies are susceptible to sampling noise and may not effectively capture the global periodicity of features across categories. To this end, we propose a time series SSC framework called CompleMatch, leveraging the complementary information from both temporal and frequency representations for unlabeled data learning. CompleMatch simultaneously trains two deep neural networks based on time-domain and frequency-domain views, with pseudo-labels generated via label propagation in the representation space guiding the training of the opposing view's classifier. In this co-training paradigm, we incorporate a constraint term to harness the complementary nature of temporal-frequency representations, thereby enhancing the model's robustness under limited labeled data. In addition, we design a temporal-frequency contrastive learning module that integrates supervised and self-supervised signals to enhance pseudo-label quality by learning more discriminative representations. Extensive experiments demonstrate that CompleMatch surpasses state-of-the-art methods. Furthermore, analyses of model behavior (i.e., ablation studies and visualization) underscore the effectiveness of our proposed approach.
Published: 2025-12-15T18:38:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhen Liu; Kun Zeng; Qianli Ma; James T. Kwok&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3644603"&gt;10.1109/tpami.2025.3644603&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Time series Semi-Supervised Classification (SSC) aims to improve model performance by utilizing abundant unlabeled data in scenarios where labeled samples are limited. Previous approaches mainly focus on exploiting temporal dependencies within the time domain for SSC. However, these temporal dependencies are susceptible to sampling noise and may not effectively capture the global periodicity of features across categories. To this end, we propose a time series SSC framework called CompleMatch, leveraging the complementary information from both temporal and frequency representations for unlabeled data learning. CompleMatch simultaneously trains two deep neural networks based on time-domain and frequency-domain views, with pseudo-labels generated via label propagation in the representation space guiding the training of the opposing view&amp;#x27;s classifier. In this co-training paradigm, we incorporate a constraint term to harness the complementary nature of temporal-frequency representations, thereby enhancing the model&amp;#x27;s robustness under limited labeled data. In addition, we design a temporal-frequency contrastive learning module that integrates supervised and self-supervised signals to enhance pseudo-label quality by learning more discriminative representations. Extensive experiments demonstrate that CompleMatch surpasses state-of-the-art methods. Furthermore, analyses of model behavior (i.e., ablation studies and visualization) underscore the effectiveness of our proposed approach.&lt;/p&gt;</content:encoded></item><item><title>DiffORSINet: Salient Object Detection in Optical Remote Sensing Images via Conditional Diffusion Model</title><link>https://doi.org/10.1109/tgrs.2025.3644383</link><guid>10.1109/tgrs.2025.3644383</guid><pubDate>Mon, 15 Dec 2025 18:38:25 +0000</pubDate><dc:creator>Yaoyao Hou</dc:creator><dc:creator>Ting Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3644383</prism:doi><description>Salient Object Detection in optical remote sensing images (ORSI-SOD) has received increasing attention in recent years. Although some progress has been made in existing methods, there are still challenges such as ambiguous and irregular boundaries of salient targets and complex backgrounds. The existing ORSI-SOD methods have difficulty in finely dividing the boundaries of salient targets and dealing with chaotic backgrounds. To solve these problems, we propose a new network based on the diffusion model, termed DiffORSINet, which describes the ORSI-SOD task as a conditional mask generation problem. By combining RGB images and the guidance of time steps, it can gradually and accurately locate and refine the segmentation of salient targets during the denoising process. Furthermore, we design a dedicated denoising network, which includes a Fourier frequency awareness module (FFAM) and a multi-level feature fusion module (MFFM), which significantly improves the refinement ability of the network. FFAM captures and fuses the frequency-domain features by combining the Fourier transform operation and the cross-attention mechanism, enhances the intensity of some signals, and thereby refines the image details. MFFM reduces the interference of chaotic backgrounds by coordinating and fusing multi-level features and suppressing irrelevant regions. Finally, the comparative experimental results on three widely used ORSI-SOD datasets show that the method proposed in this paper is superior to other existing methods. Our code and results are available at https://github.com/hyy-qd/DiffORSINet/.
Published: 2025-12-15T18:38:25+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yaoyao Hou; Ting Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3644383"&gt;10.1109/tgrs.2025.3644383&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Salient Object Detection in optical remote sensing images (ORSI-SOD) has received increasing attention in recent years. Although some progress has been made in existing methods, there are still challenges such as ambiguous and irregular boundaries of salient targets and complex backgrounds. The existing ORSI-SOD methods have difficulty in finely dividing the boundaries of salient targets and dealing with chaotic backgrounds. To solve these problems, we propose a new network based on the diffusion model, termed DiffORSINet, which describes the ORSI-SOD task as a conditional mask generation problem. By combining RGB images and the guidance of time steps, it can gradually and accurately locate and refine the segmentation of salient targets during the denoising process. Furthermore, we design a dedicated denoising network, which includes a Fourier frequency awareness module (FFAM) and a multi-level feature fusion module (MFFM), which significantly improves the refinement ability of the network. FFAM captures and fuses the frequency-domain features by combining the Fourier transform operation and the cross-attention mechanism, enhances the intensity of some signals, and thereby refines the image details. MFFM reduces the interference of chaotic backgrounds by coordinating and fusing multi-level features and suppressing irrelevant regions. Finally, the comparative experimental results on three widely used ORSI-SOD datasets show that the method proposed in this paper is superior to other existing methods. Our code and results are available at https://github.com/hyy-qd/DiffORSINet/.&lt;/p&gt;</content:encoded></item><item><title>LLM-Informed Global-Local Contextualization for Zero-Shot Food Detection</title><link>https://doi.org/10.1016/j.patcog.2025.112928</link><guid>10.1016/j.patcog.2025.112928</guid><pubDate>Tue, 16 Dec 2025 00:05:37 +0000</pubDate><dc:creator>Xinlong Wang</dc:creator><dc:creator>Weiqing Min</dc:creator><dc:creator>Guorui Sheng</dc:creator><dc:creator>Jingru Song</dc:creator><dc:creator>Yancun Yang</dc:creator><dc:creator>Tao Yao</dc:creator><dc:creator>Shuqiang Jiang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112928</prism:doi><description>Zero-Shot Detection, the ability to detect novel objects without training samples, exhibits immense potential in an ever-changing world, particularly in scenarios requiring the identification of emerging categories. However, effectively applying ZSD to fine-grained domains, characterized by high inter-class similarity and notable intra-class diversity, remains a significant challenge. This is particularly pronounced in the food domain, where the intricate nature of food attributes—notably the pervasive visual ambiguity among related culinary categories and the extensive spectrum of appearances within each food category—severely constrains the performance of existing methods. To address these specific challenges in the food domain, we introduce Zero-Shot Food Detection with Semantic Space and Feature Fusion (ZeSF), a novel framework tailored for Zero-Shot Food Detection. ZeSF integrates two key modules: (1) Multi-Scale Context Integration Module (MSCIM) that employs dilated convolutions for hierarchical feature extraction and adaptive multi-scale fusion to capture subtle, fine-grained visual distinctions; and (2) Contextual Text Feature Enhancement Module (CTFEM) that leverages Large Language Models to generate semantically rich textual embeddings, encompassing both global attributes and discriminative local descriptors. Critically, a cross-modal alignment further harmonizes visual and textual features. Comprehensive evaluations on the UEC FOOD 256 and Food Objects With Attributes (FOWA) datasets affirm ZeSF’s superiority, achieving significant improvements in the Harmonic Mean for the Generalized ZSD setting. Crucially, we further validate the framework’s generalization capability on the MS COCO and PASCAL VOC benchmarks, where it again outperforms strong baselines. The source code will be publicly available upon publication.
Published: 2025-12-16T00:05:37+00:00
Venue: Pattern Recognition
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinlong Wang; Weiqing Min; Guorui Sheng; Jingru Song; Yancun Yang; Tao Yao; Shuqiang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112928"&gt;10.1016/j.patcog.2025.112928&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Zero-Shot Detection, the ability to detect novel objects without training samples, exhibits immense potential in an ever-changing world, particularly in scenarios requiring the identification of emerging categories. However, effectively applying ZSD to fine-grained domains, characterized by high inter-class similarity and notable intra-class diversity, remains a significant challenge. This is particularly pronounced in the food domain, where the intricate nature of food attributes—notably the pervasive visual ambiguity among related culinary categories and the extensive spectrum of appearances within each food category—severely constrains the performance of existing methods. To address these specific challenges in the food domain, we introduce Zero-Shot Food Detection with Semantic Space and Feature Fusion (ZeSF), a novel framework tailored for Zero-Shot Food Detection. ZeSF integrates two key modules: (1) Multi-Scale Context Integration Module (MSCIM) that employs dilated convolutions for hierarchical feature extraction and adaptive multi-scale fusion to capture subtle, fine-grained visual distinctions; and (2) Contextual Text Feature Enhancement Module (CTFEM) that leverages Large Language Models to generate semantically rich textual embeddings, encompassing both global attributes and discriminative local descriptors. Critically, a cross-modal alignment further harmonizes visual and textual features. Comprehensive evaluations on the UEC FOOD 256 and Food Objects With Attributes (FOWA) datasets affirm ZeSF’s superiority, achieving significant improvements in the Harmonic Mean for the Generalized ZSD setting. Crucially, we further validate the framework’s generalization capability on the MS COCO and PASCAL VOC benchmarks, where it again outperforms strong baselines. The source code will be publicly available upon publication.&lt;/p&gt;</content:encoded></item><item><title>Unrolling operator splitting in learning PDEs for object detection</title><link>https://doi.org/10.1016/j.neucom.2025.132172</link><guid>10.1016/j.neucom.2025.132172</guid><pubDate>Mon, 15 Dec 2025 17:05:34 +0000</pubDate><dc:creator>Banu Wirawan Yohanes</dc:creator><dc:creator>Philip O. Ogunbona</dc:creator><dc:creator>Wanqing Li</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132172</prism:doi><description>Object detection presents significant challenges due to the variability in object scale, location, and orientation within images. Most state-of-the-art detectors are based on convolutional or Transformer architectures, which, while effective, often result in deep, opaque models that generalise poorly and lack interpretability. In contrast, iterative algorithms offer greater transparency and generalisation, albeit at the cost of efficiency and accuracy. In this work, we reformulate object detection as a partial differential equation (PDE)-constrained optimal control problem. This formulation exploits linear combinations of fundamental differential invariants—such as translation and rotation invariance—to embed structural priors into the learning process. We solve this problem using operator splitting via the Alternating Direction Method of Multipliers (ADMM), and unroll each optimisation step into a network layer, yielding a novel architecture: ADMM-ODNet. This approach provides a principled and interpretable alternative to conventional deep networks. Experimental results on the Corel, Pascal VOC and COCO datasets demonstrate that ADMM-ODNet outperforms leading models such as Cascade Mask R-CNN, Swin Transformer, Deformable DETR, DINO, DN-and RT-DETR, and achieves performance comparable to Plain DETR and YOLO, while requiring significantly fewer parameters.
Published: 2025-12-15T17:05:34+00:00
Venue: Neurocomputing
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Banu Wirawan Yohanes; Philip O. Ogunbona; Wanqing Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132172"&gt;10.1016/j.neucom.2025.132172&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection presents significant challenges due to the variability in object scale, location, and orientation within images. Most state-of-the-art detectors are based on convolutional or Transformer architectures, which, while effective, often result in deep, opaque models that generalise poorly and lack interpretability. In contrast, iterative algorithms offer greater transparency and generalisation, albeit at the cost of efficiency and accuracy. In this work, we reformulate object detection as a partial differential equation (PDE)-constrained optimal control problem. This formulation exploits linear combinations of fundamental differential invariants—such as translation and rotation invariance—to embed structural priors into the learning process. We solve this problem using operator splitting via the Alternating Direction Method of Multipliers (ADMM), and unroll each optimisation step into a network layer, yielding a novel architecture: ADMM-ODNet. This approach provides a principled and interpretable alternative to conventional deep networks. Experimental results on the Corel, Pascal VOC and COCO datasets demonstrate that ADMM-ODNet outperforms leading models such as Cascade Mask R-CNN, Swin Transformer, Deformable DETR, DINO, DN-and RT-DETR, and achieves performance comparable to Plain DETR and YOLO, while requiring significantly fewer parameters.&lt;/p&gt;</content:encoded></item><item><title>Towards Deeper Emotional Reflection: Crafting Affective Image Filters With Generative Priors</title><link>https://doi.org/10.1109/tpami.2025.3643911</link><guid>10.1109/tpami.2025.3643911</guid><pubDate>Mon, 15 Dec 2025 18:38:22 +0000</pubDate><dc:creator>Peixuan Zhang</dc:creator><dc:creator>Shuchen Weng</dc:creator><dc:creator>Jiajun Tang</dc:creator><dc:creator>Si Li</dc:creator><dc:creator>Boxin Shi</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3643911</prism:doi><description>Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.
Published: 2025-12-15T18:38:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peixuan Zhang; Shuchen Weng; Jiajun Tang; Si Li; Boxin Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3643911"&gt;10.1109/tpami.2025.3643911&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.&lt;/p&gt;</content:encoded></item><item><title>StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion</title><link>https://arxiv.org/abs/2512.13147v1</link><guid>http://arxiv.org/abs/2512.13147v1</guid><pubDate>Mon, 15 Dec 2025 09:56:09 +0000</pubDate><dc:creator>Sangmin Hong</dc:creator><dc:creator>Suyoung Lee</dc:creator><dc:creator>Kyoung Mu Lee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.
Published: 2025-12-15T09:56:09+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sangmin Hong; Suyoung Lee; Kyoung Mu Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model&amp;#x27;s accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.&lt;/p&gt;</content:encoded></item><item><title>TSDFuse: Teacher-Student Supervised Explicit Decoupling of Shared and Distinct Features for Infrared-Visible Image Fusion</title><link>https://doi.org/10.1016/j.eswa.2025.130798</link><guid>10.1016/j.eswa.2025.130798</guid><pubDate>Tue, 16 Dec 2025 00:12:31 +0000</pubDate><dc:creator>Jie Li</dc:creator><dc:creator>Gangzhu Qiao</dc:creator><dc:creator>Jianghui Cai</dc:creator><dc:creator>Yubing Luo</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130798</prism:doi><description>Infrared-visible image fusion integrates the complementary strengths of two modalities to produce images with rich textures and clear targets. However, because features are highly entangled and difficult to constrain, the fusion process often suffers from information loss. Existing approaches commonly attempt feature disentanglement to mitigate such loss, yet they still suffer from under-disentanglement, cross-talk between decomposed components, and under-utilization of modality-specific cues. To address these issues, we introduce, to our knowledge, the first teacher-student fusion framework that explicitly supervises the shared features. The framework employs a DRM to degrade visible images into pseudo-infrared representations, which serve as explicit pseudo-labels for learning shared features. A TS-EDCRM is then designed to achieve effective separation of shared and modality-specific representations through collaborative learning and cross-reconstruction, thereby suppressing feature leakage. Finally, a FDFM refines the decoder to produce fused images with sharper details and richer information. Across four public datasets (MSRS, LLVIP, TNO, and M3FD) and twelve state-of-the-art baselines, our method delivers consistent gains on EN, SF, AG, and SD, while maintaining information fidelity and cross-modal balance on VIF and Qabf. Ablation studies show that explicit shared supervision enforces shared-feature consistency, cross-reconstruction improves the separability of modality-specific features, and decoder fine-tuning further boosts the final fusion quality. Code will be released at https://github.com/no9951lj/TSDFuse .
Published: 2025-12-16T00:12:31+00:00
Venue: Expert Systems with Applications
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Li; Gangzhu Qiao; Jianghui Cai; Yubing Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130798"&gt;10.1016/j.eswa.2025.130798&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared-visible image fusion integrates the complementary strengths of two modalities to produce images with rich textures and clear targets. However, because features are highly entangled and difficult to constrain, the fusion process often suffers from information loss. Existing approaches commonly attempt feature disentanglement to mitigate such loss, yet they still suffer from under-disentanglement, cross-talk between decomposed components, and under-utilization of modality-specific cues. To address these issues, we introduce, to our knowledge, the first teacher-student fusion framework that explicitly supervises the shared features. The framework employs a DRM to degrade visible images into pseudo-infrared representations, which serve as explicit pseudo-labels for learning shared features. A TS-EDCRM is then designed to achieve effective separation of shared and modality-specific representations through collaborative learning and cross-reconstruction, thereby suppressing feature leakage. Finally, a FDFM refines the decoder to produce fused images with sharper details and richer information. Across four public datasets (MSRS, LLVIP, TNO, and M3FD) and twelve state-of-the-art baselines, our method delivers consistent gains on EN, SF, AG, and SD, while maintaining information fidelity and cross-modal balance on VIF and Qabf. Ablation studies show that explicit shared supervision enforces shared-feature consistency, cross-reconstruction improves the separability of modality-specific features, and decoder fine-tuning further boosts the final fusion quality. Code will be released at https://github.com/no9951lj/TSDFuse .&lt;/p&gt;</content:encoded></item><item><title>SuperCLIP: CLIP with Simple Classification Supervision</title><link>https://arxiv.org/abs/2512.14480v1</link><guid>http://arxiv.org/abs/2512.14480v1</guid><pubDate>Tue, 16 Dec 2025 15:11:53 +0000</pubDate><dc:creator>Weiheng Zhao</dc:creator><dc:creator>Zilong Huang</dc:creator><dc:creator>Jiashi Feng</dc:creator><dc:creator>Xinggang Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.
Published: 2025-12-16T15:11:53+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiheng Zhao; Zilong Huang; Jiashi Feng; Xinggang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP&amp;#x27;s training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP&amp;#x27;s ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP&amp;#x27;s small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.&lt;/p&gt;</content:encoded></item><item><title>VajraV1 -- The most accurate Real Time Object Detector of the YOLO family</title><link>https://arxiv.org/abs/2512.13834v1</link><guid>http://arxiv.org/abs/2512.13834v1</guid><pubDate>Mon, 15 Dec 2025 19:16:15 +0000</pubDate><dc:creator>Naman Balbir Singh Makkar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.
Published: 2025-12-15T19:16:15+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Naman Balbir Singh Makkar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.&lt;/p&gt;</content:encoded></item><item><title>Arithmetic-Intensity-Aware Quantization</title><link>https://arxiv.org/abs/2512.14090v1</link><guid>http://arxiv.org/abs/2512.14090v1</guid><pubDate>Tue, 16 Dec 2025 04:59:08 +0000</pubDate><dc:creator>Taig Singh</dc:creator><dc:creator>Shreshth Rajan</dc:creator><dc:creator>Nikhil Iyer</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.
Published: 2025-12-16T04:59:08+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Taig Singh; Shreshth Rajan; Nikhil Iyer&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;As modern neural networks become increasingly memory-bound, inference throughput is limited by DRAM bandwidth rather than compute. We present Arithmetic-Intensity-Aware Quantization (AIQ), a mixed precision quantization framework that chooses per-layer bit-widths to maximize arithmetic intensity (AI) while minimizing accuracy loss. AIQ is a post-training quantization method that uses search algorithms over per-layer quantization schemes to minimize a weighted loss over AI and accuracy. On ResNet-20/CIFAR-10, AIQ increases AI by ~50% over an FP32 baseline while keeping test accuracy within ~1 percentage point, and outperforming global uniform quantization schemes. On a memory-bound MobileNetV2 architecture, AIQ configurations give a 1.66x higher throughput than the FP32 baseline while keeping test accuracy within 1 percentage point. We also find that AIQ naturally quantizes larger layers more aggressively.&lt;/p&gt;</content:encoded></item><item><title>SSM-Det: State Space Model-Based Object Detector for Intelligent Transportation System</title><link>https://doi.org/10.1109/tits.2025.3640934</link><guid>10.1109/tits.2025.3640934</guid><pubDate>Mon, 15 Dec 2025 18:39:47 +0000</pubDate><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Chunmian Lin</dc:creator><dc:creator>Kan Guo</dc:creator><dc:creator>Jiangang Guo</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3640934</prism:doi><description>The State Space Model (SSM) has been a growth of interest in computer vision due to its long-term dependency modeling with linear complexity. Despite massive endeavor, it has not been extensively explored in intelligent transportation system (ITS) yet. In this paper, we propose State Space Model-based object Detector (SSM-Det), that is meticulously curated with Direction-aware Visual State Space Encoder (D-VSSE). Specifically, it customizes multi-path pixel exchange and patch re-arrangement via four-direction scanning mechanism, promoting for information communication. To bridge the information bottleneck across high-low level, we further design Split-Fusion (SF) and Skip-Connection (SC) modules for contextual feature propagation before decoding: SF performs multi-channel semantic separation and re-weighting in global-local scope, while SC is responsible for cross-layer feature interaction in a cascaded manner. Empirical studies is conducted on both VisDrone2019-DET and SEU_PML benchmarks, and our proposed SSM-Det reports the state-of-the-art performance against all counterparts by a substantial margin, while maintaining the real-time inference speed. We hope this work contributes to the in-depth investigation of SSM-based detector for intelligent transportation applications. The code is available at https://buaawjq.github.io/SSM-Det/.
Published: 2025-12-15T18:39:47+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaqi Wang; Chunmian Lin; Kan Guo; Jiangang Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3640934"&gt;10.1109/tits.2025.3640934&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;The State Space Model (SSM) has been a growth of interest in computer vision due to its long-term dependency modeling with linear complexity. Despite massive endeavor, it has not been extensively explored in intelligent transportation system (ITS) yet. In this paper, we propose State Space Model-based object Detector (SSM-Det), that is meticulously curated with Direction-aware Visual State Space Encoder (D-VSSE). Specifically, it customizes multi-path pixel exchange and patch re-arrangement via four-direction scanning mechanism, promoting for information communication. To bridge the information bottleneck across high-low level, we further design Split-Fusion (SF) and Skip-Connection (SC) modules for contextual feature propagation before decoding: SF performs multi-channel semantic separation and re-weighting in global-local scope, while SC is responsible for cross-layer feature interaction in a cascaded manner. Empirical studies is conducted on both VisDrone2019-DET and SEU_PML benchmarks, and our proposed SSM-Det reports the state-of-the-art performance against all counterparts by a substantial margin, while maintaining the real-time inference speed. We hope this work contributes to the in-depth investigation of SSM-based detector for intelligent transportation applications. The code is available at https://buaawjq.github.io/SSM-Det/.&lt;/p&gt;</content:encoded></item><item><title>APR-BiCA: LiDAR-based absolute pose regression with bidirectional cross attention and gating unit</title><link>https://doi.org/10.1016/j.neucom.2025.132404</link><guid>10.1016/j.neucom.2025.132404</guid><pubDate>Mon, 15 Dec 2025 16:04:44 +0000</pubDate><dc:creator>Jianlong Dai</dc:creator><dc:creator>Hui Wang</dc:creator><dc:creator>Yuqian Zhao</dc:creator><dc:creator>Zhihua Liu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132404</prism:doi><description>LiDAR localization is a critical component in fields like intelligent robots and autonomous driving. Absolute pose regression (APR) techniques directly infer global poses from input point clouds through end-to-end regression, achieving superior computational efficiency. However, APR struggles with dynamic objects and environmental noise in large-scale scenarios. To address this, we propose an APR network called APR-BiCA to fuse complementary information from raw point clouds and range images, which aims to improve the localization accuracy of robots in large-scale autonomous driving scenarios. The APR-BiCA incorporates two distinct branches: one extracts features from the raw point cloud to capture key features and build global point relationships, while the other processes the range image derived from the point cloud to extracts robust structural features. Additionally, a bidirectional cross attention mechanism combined with a gating unit-based fusion module is designed to facilitate effective inter-modal feature interaction, thereby enhancing the feature representational capability to support efficient handling of large-scale environments. Experimental results on the Oxford RobotCar and NCLT datasets demonstrate the superior performance of APR-BiCA, while maintaining exceptional efficiency. This well-balanced combination of accuracy and efficiency underscores its potential to advance LiDAR-based localization technology and drive its practical application in real-world autonomous driving systems.
Published: 2025-12-15T16:04:44+00:00
Venue: Neurocomputing
Score: 0.777 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianlong Dai; Hui Wang; Yuqian Zhao; Zhihua Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132404"&gt;10.1016/j.neucom.2025.132404&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (must_read)&lt;/p&gt;
&lt;p&gt;LiDAR localization is a critical component in fields like intelligent robots and autonomous driving. Absolute pose regression (APR) techniques directly infer global poses from input point clouds through end-to-end regression, achieving superior computational efficiency. However, APR struggles with dynamic objects and environmental noise in large-scale scenarios. To address this, we propose an APR network called APR-BiCA to fuse complementary information from raw point clouds and range images, which aims to improve the localization accuracy of robots in large-scale autonomous driving scenarios. The APR-BiCA incorporates two distinct branches: one extracts features from the raw point cloud to capture key features and build global point relationships, while the other processes the range image derived from the point cloud to extracts robust structural features. Additionally, a bidirectional cross attention mechanism combined with a gating unit-based fusion module is designed to facilitate effective inter-modal feature interaction, thereby enhancing the feature representational capability to support efficient handling of large-scale environments. Experimental results on the Oxford RobotCar and NCLT datasets demonstrate the superior performance of APR-BiCA, while maintaining exceptional efficiency. This well-balanced combination of accuracy and efficiency underscores its potential to advance LiDAR-based localization technology and drive its practical application in real-world autonomous driving systems.&lt;/p&gt;</content:encoded></item><item><title>Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation</title><link>https://arxiv.org/abs/2512.12595v1</link><guid>http://arxiv.org/abs/2512.12595v1</guid><pubDate>Sun, 14 Dec 2025 08:28:50 +0000</pubDate><dc:creator>Karthikeya KV</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.
Published: 2025-12-14T08:28:50+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Karthikeya KV&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.&lt;/p&gt;</content:encoded></item><item><title>OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving</title><link>https://arxiv.org/abs/2512.14225v1</link><guid>http://arxiv.org/abs/2512.14225v1</guid><pubDate>Tue, 16 Dec 2025 09:18:15 +0000</pubDate><dc:creator>Tao Tang</dc:creator><dc:creator>Enhui Ma</dc:creator><dc:creator>xia zhou</dc:creator><dc:creator>Letian Wang</dc:creator><dc:creator>Tianyi Yan</dc:creator><dc:creator>Xueyang Zhang</dc:creator><dc:creator>Kun Zhan</dc:creator><dc:creator>Peng Jia</dc:creator><dc:creator>XianPeng Lang</dc:creator><dc:creator>Jia-Wang Bian</dc:creator><dc:creator>Kaicheng Yu</dc:creator><dc:creator>Xiaodan Liang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.
Published: 2025-12-16T09:18:15+00:00
Venue: arXiv
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Tang; Enhui Ma; xia zhou; Letian Wang; Tianyi Yan; Xueyang Zhang; Kun Zhan; Peng Jia; XianPeng Lang; Jia-Wang Bian; Kaicheng Yu; Xiaodan Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.&lt;/p&gt;</content:encoded></item><item><title>UDMMColor: A Unified Diffusion Model for Multi-Modal Colorization</title><link>https://doi.org/10.1109/tcsvt.2025.3643915</link><guid>10.1109/tcsvt.2025.3643915</guid><pubDate>Mon, 15 Dec 2025 18:40:24 +0000</pubDate><dc:creator>Yan Zhai</dc:creator><dc:creator>Zerui Han</dc:creator><dc:creator>Zhulin Tao</dc:creator><dc:creator>Xianglin Huang</dc:creator><dc:creator>Jinshan Pan</dc:creator><dc:creator>Jinhui Tang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3643915</prism:doi><description>Diffusion model-based networks have been widely applied in the field of image generation and have gradually demonstrated a strong potential in image colorization tasks. However, despite the emergence of various colorization diffusion models, two major challenges remain: (1) the lack of effective control over the colorization process and (2) the prevalent issue of color bleeding. Integrating suitable conditional control can effectively alleviate these challenges. To this end, we propose a unified multi-modal diffusion model that harnesses diverse modality information to achieve flexible and high-quality colorization. Specifically, we introduce a Stroke-Adapter that extracts and integrates stroke prompt, enhancing user control over color distribution. Additionally, we design an Edge-Guided Attention mechanism to effectively inject edge information into the colorization process, significantly reducing color bleeding artifacts. Extensive comparative experiments demonstrate that our method outperforms state-of-the-art image colorization approaches in both qualitative and quantitative evaluations, achieving superior colorization results with enhanced controllability.
Published: 2025-12-15T18:40:24+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yan Zhai; Zerui Han; Zhulin Tao; Xianglin Huang; Jinshan Pan; Jinhui Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3643915"&gt;10.1109/tcsvt.2025.3643915&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion model-based networks have been widely applied in the field of image generation and have gradually demonstrated a strong potential in image colorization tasks. However, despite the emergence of various colorization diffusion models, two major challenges remain: (1) the lack of effective control over the colorization process and (2) the prevalent issue of color bleeding. Integrating suitable conditional control can effectively alleviate these challenges. To this end, we propose a unified multi-modal diffusion model that harnesses diverse modality information to achieve flexible and high-quality colorization. Specifically, we introduce a Stroke-Adapter that extracts and integrates stroke prompt, enhancing user control over color distribution. Additionally, we design an Edge-Guided Attention mechanism to effectively inject edge information into the colorization process, significantly reducing color bleeding artifacts. Extensive comparative experiments demonstrate that our method outperforms state-of-the-art image colorization approaches in both qualitative and quantitative evaluations, achieving superior colorization results with enhanced controllability.&lt;/p&gt;</content:encoded></item></channel></rss>