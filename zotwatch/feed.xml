<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 02 Dec 2025 02:47:13 +0000</lastBuildDate><item><title>LSDFormer: Lightweight SAR Ship Detection Enhanced With Efficient Multi-Attention and Structural Reparameterization</title><link>https://doi.org/10.1109/jstars.2025.3639164</link><guid>10.1109/jstars.2025.3639164</guid><pubDate>Mon, 01 Dec 2025 18:24:44 +0000</pubDate><dc:creator>Rui Jiang</dc:creator><dc:creator>Hang Shi</dc:creator><dc:creator>Jiahong Ni</dc:creator><dc:creator>Jiatao Li</dc:creator><dc:creator>Yi Feng</dc:creator><dc:creator>Xinqiang Chen</dc:creator><dc:creator>Yinlin Li</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3639164</prism:doi><description>Ship detection in Synthetic Aperture Radar (SAR) images faces challenges such as strong background interference, varying ship appearance and distribution and high real-time requirements. Although attention-based deep learning methods dominate this field, the design of lightweight models with efficient attention mechanisms capable of addressing the above challenges remains underexplored. To address this issue, we propose a lightweight SAR ship detection model named LSDFormer, which is built upon the MetaFormer architecture and consists of an efficient multi-attention enhanced backbone and neck and a structural reparameterization enhanced head. We employ two lightweight modules for the backbone and neck: a PoolFormer-based feature extraction module with efficient channel modulation attention is proposed to enhance ship features and suppress background interference; a downsampling module using efficient channel aggregation attention and group convolutions is introduced to enrich ship features. The position-sensitive attention from YOLOv11 is also introduced to handle variations in ship appearance and distribution. These three attentions are integrated into an efficient multi-attention mechanism. Furthermore, a structural reparameterization based detection branch is proposed for the head of LSDFormer, which enhances ship features while reducing model complexity. Extensive experiments on SSDD and HRSID datasets demonstrate the superiority and effectiveness of LSDFormer, achieving AP50 of 98.5 ± 0.4 % \bf {98.5\pm 0.4\%} and 92.8 ± 0.2 % \bf {92.8\pm 0.2\%} , respectively, with only 1.5 \bf {1.5} M parameters and 4.1 \bf {4.1} GFLOPs. The average processing time per image is 4.9 \bf {4.9} ms on SSDD and 4.2 \bf {4.2} ms on HRSID, confirming its real-time performance.
Published: 2025-12-01T18:24:44+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.834 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Jiang; Hang Shi; Jiahong Ni; Jiatao Li; Yi Feng; Xinqiang Chen; Yinlin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3639164"&gt;10.1109/jstars.2025.3639164&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.834 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection in Synthetic Aperture Radar (SAR) images faces challenges such as strong background interference, varying ship appearance and distribution and high real-time requirements. Although attention-based deep learning methods dominate this field, the design of lightweight models with efficient attention mechanisms capable of addressing the above challenges remains underexplored. To address this issue, we propose a lightweight SAR ship detection model named LSDFormer, which is built upon the MetaFormer architecture and consists of an efficient multi-attention enhanced backbone and neck and a structural reparameterization enhanced head. We employ two lightweight modules for the backbone and neck: a PoolFormer-based feature extraction module with efficient channel modulation attention is proposed to enhance ship features and suppress background interference; a downsampling module using efficient channel aggregation attention and group convolutions is introduced to enrich ship features. The position-sensitive attention from YOLOv11 is also introduced to handle variations in ship appearance and distribution. These three attentions are integrated into an efficient multi-attention mechanism. Furthermore, a structural reparameterization based detection branch is proposed for the head of LSDFormer, which enhances ship features while reducing model complexity. Extensive experiments on SSDD and HRSID datasets demonstrate the superiority and effectiveness of LSDFormer, achieving AP50 of 98.5 ± 0.4 % \bf {98.5\pm 0.4\%} and 92.8 ± 0.2 % \bf {92.8\pm 0.2\%} , respectively, with only 1.5 \bf {1.5} M parameters and 4.1 \bf {4.1} GFLOPs. The average processing time per image is 4.9 \bf {4.9} ms on SSDD and 4.2 \bf {4.2} ms on HRSID, confirming its real-time performance.&lt;/p&gt;</content:encoded></item><item><title>Diffusion-Based Text-Guided Image Generation with Fine-Grained Spatial Object-Attribute Relationships</title><link>https://doi.org/10.1109/tcsvt.2025.3639218</link><guid>10.1109/tcsvt.2025.3639218</guid><pubDate>Mon, 01 Dec 2025 18:25:50 +0000</pubDate><dc:creator>Fuxiang Wu</dc:creator><dc:creator>Liu Liu</dc:creator><dc:creator>Fusheng Hao</dc:creator><dc:creator>Ziliang Ren</dc:creator><dc:creator>Dacheng Tao</dc:creator><dc:creator>Xinyu Wu</dc:creator><dc:creator>Jun Cheng</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3639218</prism:doi><description>Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.
Published: 2025-12-01T18:25:50+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.823 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuxiang Wu; Liu Liu; Fusheng Hao; Ziliang Ren; Dacheng Tao; Xinyu Wu; Jun Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3639218"&gt;10.1109/tcsvt.2025.3639218&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.823 (must_read)&lt;/p&gt;
&lt;p&gt;Expressing and controlling fine-grained spatial attributes of objects in large-scale models presents significant challenges, as these spatial attributes are often difficult to describe textually and exhaustive enumeration is impractical. This hinders effective alignment with user preferences regarding spatial attribute-object relationships in fine-grained synthesis tasks. To tackle this problem, we propose AttrObjDiff, a novel framework built on the pre-trained Stable Diffusion model to integrate spatial attribute maps. Firstly, AttrObjDiff constrains the denoising step using trainable cross-attention fusion modules, attribute-enhancing cross-attention and LoRAs. The fusion modules take layout features extracted by a frozen ControlNet and corresponding fine-grained attribute maps as inputs to generate joint constraint features of spatial attribute-object relationships. We leverage attribute-enhancing cross-attention within the U-Net to further refine these spatial attributes. Finally, LoRAs are employed to align with these joint constraint features of finegrained relationships. Secondly, AttrObjDiff enhances the reverse process with lightweight noise reranking models to improve spatial object-attribute alignment. The reranking models select semantic noises related to fine-grained relationships, improving synthesis quality without significantly increasing computational costs. Experimental results demonstrate that our method can generate high-quality images guided by fine-grained spatial object-attribute relationships, improving synthesis controllability and semantic consistency.&lt;/p&gt;</content:encoded></item><item><title>Refinement-Guided Critique Learning: A Framework for Training Critique Models</title><link>https://doi.org/10.1016/j.inffus.2025.104002</link><guid>10.1016/j.inffus.2025.104002</guid><pubDate>Mon, 01 Dec 2025 16:11:19 +0000</pubDate><dc:creator>Chao Xiang</dc:creator><dc:creator>Junhao Zheng</dc:creator><dc:creator>Xinyu Mu</dc:creator><dc:creator>Tianshu Yu</dc:creator><dc:creator>Li Zhang</dc:creator><dc:creator>Chuxiong Sun</dc:creator><dc:creator>Lijuan Shi</dc:creator><dc:creator>Feng Wang</dc:creator><dc:creator>Mingchuan Yang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104002</prism:doi><description>Large language models have shown exceptional assessment and analytical abilities, offering valuable insights and detecting deficiencies across diverse tasks. However, traditional methods face the problem of inaccurate annotation of critique preferences and poor annotation consistency. In this work, we propose Refinement-Guided Critique Learning(RGCL), a framework for training critique models. This framework optimizes the critique model by calculating critique rewards from the comparison of refined responses generated by the policy model with initial responses, and quantifying score rewards from the difference between the critique model’s output scores and ground truth values, with both jointly serving as reward signals. We evaluate the RGCL framework across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes.
Published: 2025-12-01T16:11:19+00:00
Venue: Information Fusion
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chao Xiang; Junhao Zheng; Xinyu Mu; Tianshu Yu; Li Zhang; Chuxiong Sun; Lijuan Shi; Feng Wang; Mingchuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104002"&gt;10.1016/j.inffus.2025.104002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models have shown exceptional assessment and analytical abilities, offering valuable insights and detecting deficiencies across diverse tasks. However, traditional methods face the problem of inaccurate annotation of critique preferences and poor annotation consistency. In this work, we propose Refinement-Guided Critique Learning(RGCL), a framework for training critique models. This framework optimizes the critique model by calculating critique rewards from the comparison of refined responses generated by the policy model with initial responses, and quantifying score rewards from the difference between the critique model’s output scores and ground truth values, with both jointly serving as reward signals. We evaluate the RGCL framework across five tasks, i.e., dialog generation, summarization, question answering, mathematical reasoning, and code generation, and show that it significantly outperforms traditional methods and open-source models in terms of critique quality and refinement outcomes.&lt;/p&gt;</content:encoded></item><item><title>Self-supervised despeckling based solely on SAR intensity images: A general strategy</title><link>https://doi.org/10.1016/j.isprsjprs.2025.11.025</link><guid>10.1016/j.isprsjprs.2025.11.025</guid><pubDate>Mon, 01 Dec 2025 22:06:47 +0000</pubDate><dc:creator>Liang Chen</dc:creator><dc:creator>Yifei Yin</dc:creator><dc:creator>Hao Shi</dc:creator><dc:creator>Jingfei He</dc:creator><dc:creator>Wei Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.11.025</prism:doi><description>Speckle noise is generated along with the SAR imaging mechanism and degrades the quality of SAR images, leading to difficult interpretation. Hence, despeckling is an indispensable step in SAR pre-processing. Fortunately, supervised learning (SL) has proven to be a progressive method for SAR image despeckling. SL methods necessitate the availability of both original SAR images and their speckle-free counterparts during training, whilst speckle-free SAR images do not exist in the real world. Even though there are several substitutes for speckle-free images, the domain gap leads to poor performance and adaptability. Self-supervision provides an approach to training without clean reference. However, most self-supervised methods introduce additional requirements on speckle modeling or specific data, posing challenges in real-world applications. To address these challenges, we propose a general Self-supervised Despeckling Strategy for SAR images (SDS-SAR) that relies solely on speckled intensity data for training. Firstly, the theoretical feasibility of SAR image despeckling without speckle-free images is established. A self-supervised despeckling criteria suitable for diverse SAR images is proposed. Subsequently, a Random-Aware sub-SAMpler with Projection correLation Estimation (RA-SAMPLE) is put forth. Mutually independent training pairs can be derived from actual SAR intensity images. Furthermore, a multi-feature loss function is introduced, consisting of a despeckling term, a regularization term, and a perception term. The performance of speckle suppression and texture preservation is well-balanced. Experiments reveal that the proposed method performs comparably to supervised approaches on synthetic data and outperforms them on actual data. Both visual and quantitative evaluations confirm its superiority over state-of-the-art despeckling techniques. Moreover, the results demonstrates that SDS-SAR provides a novel solution for noise suppression in other multiplicative coherent systems. The trained model and dataset will be available at https://github.com/YYF121/SDS-SAR .
Published: 2025-12-01T22:06:47+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liang Chen; Yifei Yin; Hao Shi; Jingfei He; Wei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.11.025"&gt;10.1016/j.isprsjprs.2025.11.025&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;Speckle noise is generated along with the SAR imaging mechanism and degrades the quality of SAR images, leading to difficult interpretation. Hence, despeckling is an indispensable step in SAR pre-processing. Fortunately, supervised learning (SL) has proven to be a progressive method for SAR image despeckling. SL methods necessitate the availability of both original SAR images and their speckle-free counterparts during training, whilst speckle-free SAR images do not exist in the real world. Even though there are several substitutes for speckle-free images, the domain gap leads to poor performance and adaptability. Self-supervision provides an approach to training without clean reference. However, most self-supervised methods introduce additional requirements on speckle modeling or specific data, posing challenges in real-world applications. To address these challenges, we propose a general Self-supervised Despeckling Strategy for SAR images (SDS-SAR) that relies solely on speckled intensity data for training. Firstly, the theoretical feasibility of SAR image despeckling without speckle-free images is established. A self-supervised despeckling criteria suitable for diverse SAR images is proposed. Subsequently, a Random-Aware sub-SAMpler with Projection correLation Estimation (RA-SAMPLE) is put forth. Mutually independent training pairs can be derived from actual SAR intensity images. Furthermore, a multi-feature loss function is introduced, consisting of a despeckling term, a regularization term, and a perception term. The performance of speckle suppression and texture preservation is well-balanced. Experiments reveal that the proposed method performs comparably to supervised approaches on synthetic data and outperforms them on actual data. Both visual and quantitative evaluations confirm its superiority over state-of-the-art despeckling techniques. Moreover, the results demonstrates that SDS-SAR provides a novel solution for noise suppression in other multiplicative coherent systems. The trained model and dataset will be available at https://github.com/YYF121/SDS-SAR .&lt;/p&gt;</content:encoded></item><item><title>Rethinking Domain-Agnostic Continual Learning via Frequency Completeness Learning</title><link>https://doi.org/10.1016/j.inffus.2025.103961</link><guid>10.1016/j.inffus.2025.103961</guid><pubDate>Mon, 01 Dec 2025 16:11:22 +0000</pubDate><dc:creator>Jian Peng</dc:creator><dc:creator>Haitao Zhang</dc:creator><dc:creator>Jing Shen</dc:creator><dc:creator>Zeyi Li</dc:creator><dc:creator>Jiayi Ma</dc:creator><dc:creator>Haifeng Li</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.103961</prism:doi><description>Continual learning addresses knowledge acquisition while mitigating catastrophic forgetting in evolving task environments. Current spatial domain approaches exhibit limitations in cross-domain scenarios with unknown domain shifts. We reformulate cross-domain continual learning as an extension of single-domain generalization, introducing a novel frequency domain perspective that remains underexplored in continual learning research. Our analysis reveals the Forgetting Frequency Bias Hypothesis: model forgetting escalates with increasing frequency distribution gaps between tasks. Specifically, task-specific frequency overfitting emerges as a critical factor, where closer inter-task frequency distributions correlate with reduced forgetting. Building on this insight, we propose Frequency-Completeness Learning (FCL), a dual-path framework that disentangles high/low-frequency components through spectral reconstruction to enhance frequency diversity. Complementing this, we develop Frequency Domain Shuffling (FDS), a semantic-preserving augmentation strategy that improves style diversity while maintaining domain-invariant features. Extensive experiments on incremental classification (CIFAR-100, ImageNet-100, ImageNet-R) and semantic segmentation demonstrate FCL’s effectiveness. Our method achieves up to 10% improvement over baselines when integrated with existing continual learning techniques. The consistent performance gains across arbitrary domain scenarios underscore the importance of frequency completeness in addressing cross-domain continual learning challenges. The source code is available at https://github.com/GeoX-Lab/FCL .
Published: 2025-12-01T16:11:22+00:00
Venue: Information Fusion
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Peng; Haitao Zhang; Jing Shen; Zeyi Li; Jiayi Ma; Haifeng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.103961"&gt;10.1016/j.inffus.2025.103961&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Continual learning addresses knowledge acquisition while mitigating catastrophic forgetting in evolving task environments. Current spatial domain approaches exhibit limitations in cross-domain scenarios with unknown domain shifts. We reformulate cross-domain continual learning as an extension of single-domain generalization, introducing a novel frequency domain perspective that remains underexplored in continual learning research. Our analysis reveals the Forgetting Frequency Bias Hypothesis: model forgetting escalates with increasing frequency distribution gaps between tasks. Specifically, task-specific frequency overfitting emerges as a critical factor, where closer inter-task frequency distributions correlate with reduced forgetting. Building on this insight, we propose Frequency-Completeness Learning (FCL), a dual-path framework that disentangles high/low-frequency components through spectral reconstruction to enhance frequency diversity. Complementing this, we develop Frequency Domain Shuffling (FDS), a semantic-preserving augmentation strategy that improves style diversity while maintaining domain-invariant features. Extensive experiments on incremental classification (CIFAR-100, ImageNet-100, ImageNet-R) and semantic segmentation demonstrate FCL’s effectiveness. Our method achieves up to 10% improvement over baselines when integrated with existing continual learning techniques. The consistent performance gains across arbitrary domain scenarios underscore the importance of frequency completeness in addressing cross-domain continual learning challenges. The source code is available at https://github.com/GeoX-Lab/FCL .&lt;/p&gt;</content:encoded></item><item><title>PowerCLIP: Powerset Alignment for Contrastive Pre-Training</title><link>https://arxiv.org/abs/2511.23170v1</link><guid>http://arxiv.org/abs/2511.23170v1</guid><pubDate>Fri, 28 Nov 2025 13:28:18 +0000</pubDate><dc:creator>Masaki Kawamura</dc:creator><dc:creator>Nakamasa Inoue</dc:creator><dc:creator>Rintaro Yanagi</dc:creator><dc:creator>Hirokatsu Kataoka</dc:creator><dc:creator>Rio Yokota</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.
Published: 2025-11-28T13:28:18+00:00
Venue: arXiv
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Masaki Kawamura; Nakamasa Inoue; Rintaro Yanagi; Hirokatsu Kataoka; Rio Yokota&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.&lt;/p&gt;</content:encoded></item><item><title>ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning</title><link>https://arxiv.org/abs/2512.00831v1</link><guid>http://arxiv.org/abs/2512.00831v1</guid><pubDate>Sun, 30 Nov 2025 10:39:53 +0000</pubDate><dc:creator>Yuchen Zeng</dc:creator><dc:creator>Shuibai Zhang</dc:creator><dc:creator>Wonjun Kang</dc:creator><dc:creator>Shutong Wu</dc:creator><dc:creator>Lynnix Zou</dc:creator><dc:creator>Ying Fan</dc:creator><dc:creator>Heeju Kim</dc:creator><dc:creator>Ziqian Lin</dc:creator><dc:creator>Jungtaek Kim</dc:creator><dc:creator>Hyung Il Koo</dc:creator><dc:creator>Dimitris Papailiopoulos</dc:creator><dc:creator>Kangwook Lee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning "algorithms" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.
Published: 2025-11-30T10:39:53+00:00
Venue: arXiv
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchen Zeng; Shuibai Zhang; Wonjun Kang; Shutong Wu; Lynnix Zou; Ying Fan; Heeju Kim; Ziqian Lin; Jungtaek Kim; Hyung Il Koo; Dimitris Papailiopoulos; Kangwook Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning &amp;quot;algorithms&amp;quot; remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.&lt;/p&gt;</content:encoded></item><item><title>Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies</title><link>https://arxiv.org/abs/2512.00619v1</link><guid>http://arxiv.org/abs/2512.00619v1</guid><pubDate>Sat, 29 Nov 2025 20:20:52 +0000</pubDate><dc:creator>Goutham Nalagatla</dc:creator><dc:creator>Shreyas Grandhe</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based gen- erative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.
Published: 2025-11-29T20:20:52+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Goutham Nalagatla; Shreyas Grandhe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based gen- erative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.&lt;/p&gt;</content:encoded></item><item><title>Multimodal cross fusion Mamba network for remote sensing image semantic segmentation with complementary masked self-supervision</title><link>https://doi.org/10.1016/j.jag.2025.104960</link><guid>10.1016/j.jag.2025.104960</guid><pubDate>Mon, 01 Dec 2025 08:22:58 +0000</pubDate><dc:creator>Xiao Liu</dc:creator><dc:creator>Tao Wang</dc:creator><dc:creator>Fei Jin</dc:creator><dc:creator>Jie Rui</dc:creator><dc:creator>Shuxiang Wang</dc:creator><dc:creator>Ziheng Huang</dc:creator><dc:creator>Yujie Zou</dc:creator><dc:creator>Xiaowei Yu</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.104960</prism:doi><description>Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .
Published: 2025-12-01T08:22:58+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiao Liu; Tao Wang; Fei Jin; Jie Rui; Shuxiang Wang; Ziheng Huang; Yujie Zou; Xiaowei Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.104960"&gt;10.1016/j.jag.2025.104960&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning-based multimodal remote sensing image (RSI) semantic segmentation models have garnered significant attention owing to their ability to leverage complementary information across modalities, resulting in more robust and accurate Earth observation. However, most existing approaches rely predominantly on convolutional neural networks with limited receptive fields or transformer-based architectures that are computationally intensive. In this study, we proposed a multimodal cross fusion Mamba (MCF-Mamba) network for multimodal RSI semantic segmentation, aiming to collaboratively enhance accuracy and efficiency. The proposed network features three core components: a dual-branch VMamba encoder, a multimodal cross-Mamba fusion module, and a U-shaped Mamba decoder. The architecture with a unified structure centered on the selective state space model ensures that it achieves global perception with linear complexity. Furthermore, for addressing the constraint of limited labeled samples in practical applications, we developed a generative multimodal complementary masked self-supervised (CMSS) strategy. It leverages abundant unlabeled RSIs to learn generalized multimodal representations by modeling intermodal complementary consistency. Extensive experiments on three public datasets involving optical-SAR and optical-DEM modalities demonstrated that the proposed network and strategy are superior to other advanced segmentation models and self-supervised methods in land cover mapping and building extraction tasks. The MCF-Mamba network achieves the highest accuracy while significantly reducing both model size and computational cost, and further improves the accuracy and generalization through the CMSS strategy. The source code is available at https://github.com/Xiao-RS/MCFMamba_CMSS .&lt;/p&gt;</content:encoded></item><item><title>Causal Reasoning Meets Heuristic Strategies: Enhancing RAG through Fine-Tuning and Knowledge Interaction</title><link>https://doi.org/10.1016/j.knosys.2025.114976</link><guid>10.1016/j.knosys.2025.114976</guid><pubDate>Mon, 01 Dec 2025 16:47:57 +0000</pubDate><dc:creator>Xun Luo</dc:creator><dc:creator>Yuzhong Chen</dc:creator><dc:creator>Yanhao Tu</dc:creator><dc:creator>Wenju Qiu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.114976</prism:doi><description>Retrieval-augmented Generation (RAG) enhances large language models (LLMs) with external knowledge, but traditional approaches typically rely on surface-level relevance and lack robustness to noisy or conflicting information. In real-world scenarios, users often pose complex queries that require accurate, contextually grounded reasoning, posing two key challenges for RAG systems. The first challenge is how to extract truly supportive evidence from noisy or topically similar but uninformative documents. The second challenge is how to resolve conflicts between internal parameterized knowledge and external knowledge from retrieved documents. To address these challenges, we propose CRGS-RAG, a framework that incorporates the Causal Reasoning Fine-Tuning Strategy and Game-Theory-Inspired Knowledge Fusion Strategy. The Causal Reasoning Fine-Tuning Strategy improves model robustness by training it to focus on causally relevant evidence, while the Game-Theory-Inspired Knowledge Fusion Strategy enables CRGS-RAG to adaptively integrate internal parameterized knowledge and external knowledge from retrieved documents under conflicting conditions. Experiments on five open-domain QA benchmark datasets show that CRGS-RAG consistently outperforms the state-of-the-art RAG baselines in accuracy and consistency. Furthermore, ablation studies reveal that the Causal Reasoning Fine-Tuning Strategy significantly enhances CRGS-RAG’s reasoning ability under noisy retrieval, while the Game-Theory-Inspired Knowledge Fusion Strategy module improves factual alignment and robustness in fusing knowledge from multiple sources. To facilitate reproduction, our code is available at https://github.com/yuanlill/CRGS-RAG .
Published: 2025-12-01T16:47:57+00:00
Venue: Knowledge-Based Systems
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xun Luo; Yuzhong Chen; Yanhao Tu; Wenju Qiu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.114976"&gt;10.1016/j.knosys.2025.114976&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Retrieval-augmented Generation (RAG) enhances large language models (LLMs) with external knowledge, but traditional approaches typically rely on surface-level relevance and lack robustness to noisy or conflicting information. In real-world scenarios, users often pose complex queries that require accurate, contextually grounded reasoning, posing two key challenges for RAG systems. The first challenge is how to extract truly supportive evidence from noisy or topically similar but uninformative documents. The second challenge is how to resolve conflicts between internal parameterized knowledge and external knowledge from retrieved documents. To address these challenges, we propose CRGS-RAG, a framework that incorporates the Causal Reasoning Fine-Tuning Strategy and Game-Theory-Inspired Knowledge Fusion Strategy. The Causal Reasoning Fine-Tuning Strategy improves model robustness by training it to focus on causally relevant evidence, while the Game-Theory-Inspired Knowledge Fusion Strategy enables CRGS-RAG to adaptively integrate internal parameterized knowledge and external knowledge from retrieved documents under conflicting conditions. Experiments on five open-domain QA benchmark datasets show that CRGS-RAG consistently outperforms the state-of-the-art RAG baselines in accuracy and consistency. Furthermore, ablation studies reveal that the Causal Reasoning Fine-Tuning Strategy significantly enhances CRGS-RAG’s reasoning ability under noisy retrieval, while the Game-Theory-Inspired Knowledge Fusion Strategy module improves factual alignment and robustness in fusing knowledge from multiple sources. To facilitate reproduction, our code is available at https://github.com/yuanlill/CRGS-RAG .&lt;/p&gt;</content:encoded></item><item><title>Bidirectional parallel multi-layer multi-scale hybrid network</title><link>https://doi.org/10.1016/j.neucom.2025.132255</link><guid>10.1016/j.neucom.2025.132255</guid><pubDate>Mon, 01 Dec 2025 16:08:14 +0000</pubDate><dc:creator>Chunguang Yue</dc:creator><dc:creator>Jinbao Li</dc:creator><dc:creator>Donghuan Zhang</dc:creator><dc:creator>Xiaowei Liu</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132255</prism:doi><description>Although Vision Transformer (ViT) can directly process images, the division of images into patches lacks internal interaction in patches and has a single feature scale, leading to suboptimal performance in dense prediction tasks. Most existing research focuses on serial networks that combine the strengths of CNN and ViT to address these issues, but this often disrupts the ViT structure and introduces additional pretraining costs. In this paper, we propose BiPNet (the Bidirectional Parallel Multi-layer Multi-scale Hybrid Network), which addresses the aforementioned issues by facilitating information interaction between CNNs and Transformers and can directly leverage existing ViT pre-trained weights. Compared to existing methods, our Bidirectional Parallel Multi-Layer Multi-Scale Hybrid Network has the following advantages: 1.The CNN and Transformer are used in parallel to fully retain the ViT architecture, making use of existing pre-trained models. 2.A 3M (multi-layer, multi-scale convolutional module) is proposed to handle the spatial pyramid information of CNNs, addressing the problem of insufficient local feature interaction and single feature representation within ViT. 3.A simple CNN-Transformer BiLGM (bidirectional local-global interaction module) is introduced, which performs both local-global interaction and balances high and low-frequency semantics, making it beneficial for handling dense prediction tasks. It achieves 63.9 % " role="presentation"&gt; on COCO val2017 and 62.0 % mIoU on ADE20K with its super-large model without using additional training data.
Published: 2025-12-01T16:08:14+00:00
Venue: Neurocomputing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunguang Yue; Jinbao Li; Donghuan Zhang; Xiaowei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132255"&gt;10.1016/j.neucom.2025.132255&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Although Vision Transformer (ViT) can directly process images, the division of images into patches lacks internal interaction in patches and has a single feature scale, leading to suboptimal performance in dense prediction tasks. Most existing research focuses on serial networks that combine the strengths of CNN and ViT to address these issues, but this often disrupts the ViT structure and introduces additional pretraining costs. In this paper, we propose BiPNet (the Bidirectional Parallel Multi-layer Multi-scale Hybrid Network), which addresses the aforementioned issues by facilitating information interaction between CNNs and Transformers and can directly leverage existing ViT pre-trained weights. Compared to existing methods, our Bidirectional Parallel Multi-Layer Multi-Scale Hybrid Network has the following advantages: 1.The CNN and Transformer are used in parallel to fully retain the ViT architecture, making use of existing pre-trained models. 2.A 3M (multi-layer, multi-scale convolutional module) is proposed to handle the spatial pyramid information of CNNs, addressing the problem of insufficient local feature interaction and single feature representation within ViT. 3.A simple CNN-Transformer BiLGM (bidirectional local-global interaction module) is introduced, which performs both local-global interaction and balances high and low-frequency semantics, making it beneficial for handling dense prediction tasks. It achieves 63.9 % &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; on COCO val2017 and 62.0 % mIoU on ADE20K with its super-large model without using additional training data.&lt;/p&gt;</content:encoded></item><item><title>Energy-Efficient Vision Transformer Inference for Edge-AI Deployment</title><link>https://arxiv.org/abs/2511.23166v1</link><guid>http://arxiv.org/abs/2511.23166v1</guid><pubDate>Fri, 28 Nov 2025 13:24:08 +0000</pubDate><dc:creator>Nursultan Amanzhol</dc:creator><dc:creator>Jurn-Gyu Park</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).
Published: 2025-11-28T13:24:08+00:00
Venue: arXiv
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nursultan Amanzhol; Jurn-Gyu Park&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;The growing deployment of Vision Transformers (ViTs) on energy-constrained devices requires evaluation methods that go beyond accuracy alone. We present a two-stage pipeline for assessing ViT energy efficiency that combines device-agnostic model selection with device-related measurements. We benchmark 13 ViT models on ImageNet-1K and CIFAR-10, running inference on NVIDIA Jetson TX2 (edge device) and an NVIDIA RTX 3050 (mobile GPU). The device-agnostic stage uses the NetScore metric for screening; the device-related stage ranks models with the Sustainable Accuracy Metric (SAM). Results show that hybrid models such as LeViT_Conv_192 reduce energy by up to 53% on TX2 relative to a ViT baseline (e.g., SAM5=1.44 on TX2/CIFAR-10), while distilled models such as TinyViT-11M_Distilled excel on the mobile GPU (e.g., SAM5=1.72 on RTX 3050/CIFAR-10 and SAM5=0.76 on RTX 3050/ImageNet-1K).&lt;/p&gt;</content:encoded></item><item><title>Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs</title><link>https://arxiv.org/abs/2511.22826v1</link><guid>http://arxiv.org/abs/2511.22826v1</guid><pubDate>Fri, 28 Nov 2025 01:21:29 +0000</pubDate><dc:creator>Tianle Chen</dc:creator><dc:creator>Chaitanya Chakka</dc:creator><dc:creator>Arjun Reddy Akula</dc:creator><dc:creator>Xavier Thomas</dc:creator><dc:creator>Deepti Ghadiyaram</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.
Published: 2025-11-28T01:21:29+00:00
Venue: arXiv
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianle Chen; Chaitanya Chakka; Arjun Reddy Akula; Xavier Thomas; Deepti Ghadiyaram&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model&amp;#x27;s reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.&lt;/p&gt;</content:encoded></item><item><title>Vision Bridge Transformer at Scale</title><link>https://arxiv.org/abs/2511.23199v1</link><guid>http://arxiv.org/abs/2511.23199v1</guid><pubDate>Fri, 28 Nov 2025 14:03:39 +0000</pubDate><dc:creator>Zhenxiong Tan</dc:creator><dc:creator>Zeqing Wang</dc:creator><dc:creator>Xingyi Yang</dc:creator><dc:creator>Songhua Liu</dc:creator><dc:creator>Xinchao Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.
Published: 2025-11-28T14:03:39+00:00
Venue: arXiv
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenxiong Tan; Zeqing Wang; Xingyi Yang; Songhua Liu; Xinchao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.&lt;/p&gt;</content:encoded></item><item><title>Hybrid-stage Association with Dynamicity Adaptation and Enhanced Cues for Multi-object Tracking and Segmentation</title><link>https://doi.org/10.1016/j.patcog.2025.112803</link><guid>10.1016/j.patcog.2025.112803</guid><pubDate>Sun, 30 Nov 2025 15:08:25 +0000</pubDate><dc:creator>Longtao Chen</dc:creator><dc:creator>Guoxing Liao</dc:creator><dc:creator>Yifan Shi</dc:creator><dc:creator>Jing Lou</dc:creator><dc:creator>Fenglei Xu</dc:creator><dc:creator>Huanqiang Zeng</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112803</prism:doi><description>The varying degrees of dynamicity in objects present significant challenges for multi-object tracking and segmentation (MOTS), often manifesting as transitions between severe and minor deformations and occlusions. Currently, mainstream data association methodologies in MOTS rely on one-time single-stage or patchwork-style multi-stage strategy, and those dependent on predefined cues struggle to adapt to variable dynamicity. To address this issue, we propose HD-Track, a Hybrid-stage data association approach with Dynamicity Adaptation and Enhanced Cues. The Hybrid-stage strategy performs data association through pre-association and re-correction association stages. First, we exploit appearance cue sensitivity to dynamicity variations to project object dynamicity via pre-association. Second, we introduce Dynamicity Adaptation, featuring Dynamicity Selection to choose reliable appearance cues based on pre-association results, and Occlusion Dynamicity Fusing to dynamically integrate appearance and motion cues based on historical mask area variations, enhancing re-correction association robustness. Additionally, we propose a Mask-based Attention Mechanism and a Quad-triangle Transformation, collectively known as Enhanced Cues, to strengthen the robustness of both cues. Our extensive experiments on the MOTS20 and KITTI MOTS datasets demonstrate that HD-Track delivers reliable performance across diverse scenarios.
Published: 2025-11-30T15:08:25+00:00
Venue: Pattern Recognition
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Longtao Chen; Guoxing Liao; Yifan Shi; Jing Lou; Fenglei Xu; Huanqiang Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112803"&gt;10.1016/j.patcog.2025.112803&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;The varying degrees of dynamicity in objects present significant challenges for multi-object tracking and segmentation (MOTS), often manifesting as transitions between severe and minor deformations and occlusions. Currently, mainstream data association methodologies in MOTS rely on one-time single-stage or patchwork-style multi-stage strategy, and those dependent on predefined cues struggle to adapt to variable dynamicity. To address this issue, we propose HD-Track, a Hybrid-stage data association approach with Dynamicity Adaptation and Enhanced Cues. The Hybrid-stage strategy performs data association through pre-association and re-correction association stages. First, we exploit appearance cue sensitivity to dynamicity variations to project object dynamicity via pre-association. Second, we introduce Dynamicity Adaptation, featuring Dynamicity Selection to choose reliable appearance cues based on pre-association results, and Occlusion Dynamicity Fusing to dynamically integrate appearance and motion cues based on historical mask area variations, enhancing re-correction association robustness. Additionally, we propose a Mask-based Attention Mechanism and a Quad-triangle Transformation, collectively known as Enhanced Cues, to strengthen the robustness of both cues. Our extensive experiments on the MOTS20 and KITTI MOTS datasets demonstrate that HD-Track delivers reliable performance across diverse scenarios.&lt;/p&gt;</content:encoded></item><item><title>Dilated Transformation-Guided Unsupervised Multimodal Learning for Hyperspectral and Multispectral Image Fusion</title><link>https://doi.org/10.1109/tgrs.2025.3636047</link><guid>10.1109/tgrs.2025.3636047</guid><pubDate>Mon, 01 Dec 2025 18:24:33 +0000</pubDate><dc:creator>Yuanchao Su</dc:creator><dc:creator>Sheng Li</dc:creator><dc:creator>Yicong Zhou</dc:creator><dc:creator>Lianru Gao</dc:creator><dc:creator>Mengying Jiang</dc:creator><dc:creator>Xu Sun</dc:creator><dc:creator>Haiwei Li</dc:creator><dc:creator>Enke Hou</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3636047</prism:doi><description>Multimodal fusion widely uses convolutional layers to capture local correlations and adjust feature dimensions. However, the progressive expansion of the receptive field in convolutional layers often compromises spatial context retention, leading to the loss of fine details. Furthermore, the fixed-size kernels typically used in standard convolution restrict the network’s ability to capture multiscale contextual details. To address this limitation, this paper develops a dilated transformation-guided unsupervised multimodal learning (DTUML) method to fuse a high-resolution multispectral image (HR-MSI) and a low-resolution hyperspectral image (LR-HSI), thereby generating a high-resolution hyperspectral image (HR-HSI). Our DTUML adopts a dual-stream encoder architecture to conduct multimodal data, where one stream focuses on preserving spectral information from LR-HSIs, while the other emphasizes the acquisition of spatial details from HR-MSIs. These complementary features are subsequently integrated to ensure spectral fidelity and retain spatial detail. Then, a convolutional layer restores dimensional consistency and outputs an HR-HSI. Extensive experiments demonstrate the effectiveness of DTUML, showing superior performance and strong competitiveness compared to state-of-the-art methods. Code: https://github.com/yuanchaosu/TGRS-DTUML.
Published: 2025-12-01T18:24:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuanchao Su; Sheng Li; Yicong Zhou; Lianru Gao; Mengying Jiang; Xu Sun; Haiwei Li; Enke Hou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3636047"&gt;10.1109/tgrs.2025.3636047&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal fusion widely uses convolutional layers to capture local correlations and adjust feature dimensions. However, the progressive expansion of the receptive field in convolutional layers often compromises spatial context retention, leading to the loss of fine details. Furthermore, the fixed-size kernels typically used in standard convolution restrict the network’s ability to capture multiscale contextual details. To address this limitation, this paper develops a dilated transformation-guided unsupervised multimodal learning (DTUML) method to fuse a high-resolution multispectral image (HR-MSI) and a low-resolution hyperspectral image (LR-HSI), thereby generating a high-resolution hyperspectral image (HR-HSI). Our DTUML adopts a dual-stream encoder architecture to conduct multimodal data, where one stream focuses on preserving spectral information from LR-HSIs, while the other emphasizes the acquisition of spatial details from HR-MSIs. These complementary features are subsequently integrated to ensure spectral fidelity and retain spatial detail. Then, a convolutional layer restores dimensional consistency and outputs an HR-HSI. Extensive experiments demonstrate the effectiveness of DTUML, showing superior performance and strong competitiveness compared to state-of-the-art methods. Code: https://github.com/yuanchaosu/TGRS-DTUML.&lt;/p&gt;</content:encoded></item><item><title>Illumination-aware Multimodal Hierarchical Fusion Network for RGB-Infrared Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3636590</link><guid>10.1109/tgrs.2025.3636590</guid><pubDate>Mon, 01 Dec 2025 18:24:33 +0000</pubDate><dc:creator>Ting Lu</dc:creator><dc:creator>Jiacheng Lu</dc:creator><dc:creator>Wei Fu</dc:creator><dc:creator>Yifan Xi</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3636590</prism:doi><description>RGB-infrared (RGB-IR) object detection has attracted significant attention in drone-based applications due to its robustness under all-weather conditions. How to effectively fuse the complementary information in both modalities is one key for accurate object detection. However, the performance is limited by the inherent differences between modalities and the varying illumination conditions across different weather scenarios. Focused on this issue, we propose an illumination-aware multimodal hierarchical fusion network (IMHFNet) for RGB-IR object detection. First, an illumination aware module (IAM) is designed to extract local illumination features from RGB image, which is used to guide the subsequent multimodal feature fusion process. Then, considering the differences in semantic expression and detail representation of different feature layers of multimodal data, we separately design shallow and deep feature fusion strategies. In specific, the shallow feature fusion module is constructed based on convolutional operators and illumination-guided adaptive weight fusion, focusing on capturing and enhancing local detail information. For the deep feature fusion, illumination feature is incorporated as an auxiliary information, to guide the global semantic information integration across different modalities via adopting a transformer structure. In this work, we also construct a new drone-based RGB-IR dataset, named by DroneShip. It contains 4,306 images annotated with 17,054 oriented ship object instances, which covers a wide range of natural illumination conditions from daytime to nighttime. Finally, to validate the effectiveness of the proposed method, we evaluate the IMHFNet on the constructed DroneShip and two publicly available RGB-IR datasets (KAIST and DroneVehicle), which respectively focus on ship, pedestrian and vehicle targets. Experimental results on all three datasets consistently demonstrate the effectiveness and robustness of IMHFNet across diverse scenarios...
Published: 2025-12-01T18:24:33+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ting Lu; Jiacheng Lu; Wei Fu; Yifan Xi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3636590"&gt;10.1109/tgrs.2025.3636590&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;RGB-infrared (RGB-IR) object detection has attracted significant attention in drone-based applications due to its robustness under all-weather conditions. How to effectively fuse the complementary information in both modalities is one key for accurate object detection. However, the performance is limited by the inherent differences between modalities and the varying illumination conditions across different weather scenarios. Focused on this issue, we propose an illumination-aware multimodal hierarchical fusion network (IMHFNet) for RGB-IR object detection. First, an illumination aware module (IAM) is designed to extract local illumination features from RGB image, which is used to guide the subsequent multimodal feature fusion process. Then, considering the differences in semantic expression and detail representation of different feature layers of multimodal data, we separately design shallow and deep feature fusion strategies. In specific, the shallow feature fusion module is constructed based on convolutional operators and illumination-guided adaptive weight fusion, focusing on capturing and enhancing local detail information. For the deep feature fusion, illumination feature is incorporated as an auxiliary information, to guide the global semantic information integration across different modalities via adopting a transformer structure. In this work, we also construct a new drone-based RGB-IR dataset, named by DroneShip. It contains 4,306 images annotated with 17,054 oriented ship object instances, which covers a wide range of natural illumination conditions from daytime to nighttime. Finally, to validate the effectiveness of the proposed method, we evaluate the IMHFNet on the constructed DroneShip and two publicly available RGB-IR datasets (KAIST and DroneVehicle), which respectively focus on ship, pedestrian and vehicle targets. Experimental results on all three datasets consistently demonstrate the effectiveness and robustness of IMHFNet across diverse scenarios...&lt;/p&gt;</content:encoded></item><item><title>Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day</title><link>https://arxiv.org/abs/2511.23220v1</link><guid>http://arxiv.org/abs/2511.23220v1</guid><pubDate>Fri, 28 Nov 2025 14:26:46 +0000</pubDate><dc:creator>Milad Abdollahzadeh</dc:creator><dc:creator>Abdul Raheem</dc:creator><dc:creator>Zilong Zhao</dc:creator><dc:creator>Uzair Javaid</dc:creator><dc:creator>Kevin Yee</dc:creator><dc:creator>Nalam Venkata Abhishek</dc:creator><dc:creator>Tram Truong-Huu</dc:creator><dc:creator>Biplab Sikdar</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.
Published: 2025-11-28T14:26:46+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Milad Abdollahzadeh; Abdul Raheem; Zilong Zhao; Uzair Javaid; Kevin Yee; Nalam Venkata Abhishek; Tram Truong-Huu; Biplab Sikdar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.&lt;/p&gt;</content:encoded></item><item><title>ORION: Teaching Language Models to Reason Efficiently in the Language of Thought</title><link>https://arxiv.org/abs/2511.22891v1</link><guid>http://arxiv.org/abs/2511.22891v1</guid><pubDate>Fri, 28 Nov 2025 05:41:55 +0000</pubDate><dc:creator>Kumar Tanmay</dc:creator><dc:creator>Kriti Aggarwal</dc:creator><dc:creator>Paul Pu Liang</dc:creator><dc:creator>Subhabrata Mukherjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose "thinking" tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.
Published: 2025-11-28T05:41:55+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kumar Tanmay; Kriti Aggarwal; Paul Pu Liang; Subhabrata Mukherjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) achieve strong performance in mathematics, code generation, and task planning, but their reliance on long chains of verbose &amp;quot;thinking&amp;quot; tokens leads to high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis, which posits that human reasoning operates over a symbolic, compositional mental language called Mentalese, we introduce a framework that trains models to reason in a similarly compact style. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To improve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that rewards concise solutions that stay correct, while still allowing longer reasoning when needed. Applied to Mentalese-aligned models, SLPO yields significantly higher compression rates by enabling concise reasoning that preserves the benefits of detailed thinking without the computational overhead. Across benchmarks including AIME 2024 and 2025, MinervaMath, OlympiadBench, Math500, and AMC, our ORION models produce reasoning traces with 4-16x fewer tokens, achieve up to 5x lower inference latency, and reduce training costs by 7-9x relative to the DeepSeek R1 Distilled model, while maintaining 90-98% of its accuracy. ORION also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2x compression. These results show that Mentalese-style compressed reasoning offers a step toward human-like cognitive efficiency, enabling real-time, cost-effective reasoning without sacrificing accuracy.&lt;/p&gt;</content:encoded></item><item><title>Empowering artificial intelligence with homomorphic encryption for secure deep reinforcement learning</title><link>https://doi.org/10.1038/s42256-025-01135-2</link><guid>10.1038/s42256-025-01135-2</guid><pubDate>Mon, 01 Dec 2025 10:02:38 +0000</pubDate><dc:creator>Chi-Hieu Nguyen</dc:creator><dc:creator>Thai Hoang Dinh</dc:creator><dc:creator>Diep N. Nguyen</dc:creator><dc:creator>Kristin Lauter</dc:creator><dc:creator>Miran Kim</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01135-2</prism:doi><description>Deep reinforcement learning (DRL) demonstrates significant potential in solving complex control and decision-making problems, but it may inadvertently expose sensitive, environment-specific information, raising privacy and security concerns for computer systems, humans and organizations. This work introduces a privacy-preserving framework using homomorphic encryption and advanced learning algorithms to secure DRL processes. Our framework enables the encryption of sensitive information, including states, actions and rewards, before sharing it with an untrusted processing platform. This encryption ensures data privacy, prevents unauthorized access and maintains compliance with data protection laws throughout the learning process. In addition, we develop innovative algorithms to efficiently handle a wide range of encrypted control tasks. Our core innovation is the homomorphic encryption-compatible Adam optimizer, which reparameterizes momentum values to bypass the need for high-degree polynomial approximations of inverse square roots on encrypted data. This adaptation, previously unexplored in homomorphic encryption-based ML research, enables stable and efficient training with adaptive learning rates in encrypted domains, addressing a critical bottleneck for privacy-preserving DRL with sparse rewards. Evaluations on standard DRL benchmarks demonstrate that our encrypted DRL performs comparably with its unencrypted counterpart (with a gap of less than 10%) and maintaining data confidentiality with homomorphic encryption. This work facilitates the integration of privacy-preserving DRL into real-world applications, addressing critical privacy concerns, and promoting the ethical advancement of artificial intelligence. A secure artificial intelligence framework is introduced that leverages homomorphic encryption to safeguard sensitive information in deep reinforcement learning, achieving accurate decision-making and ensuring data privacy and confidentiality.
Published: 2025-12-01T10:02:38+00:00
Venue: Nature Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chi-Hieu Nguyen; Thai Hoang Dinh; Diep N. Nguyen; Kristin Lauter; Miran Kim&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01135-2"&gt;10.1038/s42256-025-01135-2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Deep reinforcement learning (DRL) demonstrates significant potential in solving complex control and decision-making problems, but it may inadvertently expose sensitive, environment-specific information, raising privacy and security concerns for computer systems, humans and organizations. This work introduces a privacy-preserving framework using homomorphic encryption and advanced learning algorithms to secure DRL processes. Our framework enables the encryption of sensitive information, including states, actions and rewards, before sharing it with an untrusted processing platform. This encryption ensures data privacy, prevents unauthorized access and maintains compliance with data protection laws throughout the learning process. In addition, we develop innovative algorithms to efficiently handle a wide range of encrypted control tasks. Our core innovation is the homomorphic encryption-compatible Adam optimizer, which reparameterizes momentum values to bypass the need for high-degree polynomial approximations of inverse square roots on encrypted data. This adaptation, previously unexplored in homomorphic encryption-based ML research, enables stable and efficient training with adaptive learning rates in encrypted domains, addressing a critical bottleneck for privacy-preserving DRL with sparse rewards. Evaluations on standard DRL benchmarks demonstrate that our encrypted DRL performs comparably with its unencrypted counterpart (with a gap of less than 10%) and maintaining data confidentiality with homomorphic encryption. This work facilitates the integration of privacy-preserving DRL into real-world applications, addressing critical privacy concerns, and promoting the ethical advancement of artificial intelligence. A secure artificial intelligence framework is introduced that leverages homomorphic encryption to safeguard sensitive information in deep reinforcement learning, achieving accurate decision-making and ensuring data privacy and confidentiality.&lt;/p&gt;</content:encoded></item><item><title>Geometry Gated Multi-view Stereo for 3D Reconstruction</title><link>https://doi.org/10.1016/j.neucom.2025.132264</link><guid>10.1016/j.neucom.2025.132264</guid><pubDate>Mon, 01 Dec 2025 16:08:04 +0000</pubDate><dc:creator>Han Li</dc:creator><dc:creator>Guohua Gou</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Weicheng Jiang</dc:creator><dc:creator>Haigang Sui</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132264</prism:doi><description>Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.
Published: 2025-12-01T16:08:04+00:00
Venue: Neurocomputing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Li; Guohua Gou; Hao Zhang; Weicheng Jiang; Haigang Sui&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132264"&gt;10.1016/j.neucom.2025.132264&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-view stereo (MVS) aims to reconstruct accurate 3D scenes from multiple images. Currently, deep learning-based MVS methods typically estimate depth maps by regressing cost volumes. Therefore, the accuracy of geometric information encoded in the cost volume and the aggregation methods are crucial to the performance of MVS reconstruction. However, existing approaches lack sufficient optimization in cost volume construction and interaction. Moreover, conventional 3D convolutions often result in high computational complexity.
To address these challenges, this work proposes a Geometry-gated Multi-view Stereo Network (GGMVS), aiming to optimize feature representation in cost volume construction and the cost volume fusion mechanism, thereby improving both the accuracy and efficiency of MVS reconstruction. First, we design a Geometric Matching Enhancement Network (GME) to optimize the quality of cost volume construction. GME captures fine-grained features from multiple views and achieves dynamic feature propagation in a top-down manner. Second, we introduce a Cross-attention Volume Fusion Module (CVF) to strengthen inter-scale cost volume interactions. CVF leverages a cross-attention mechanism to globally integrate information from cost volumes at different scales, facilitating effective multi-scale geometric information fusion. Finally, we propose a Gated Volume Fusion Module (GVF) to enable refined filtering of cost volume information. GVF generates gating signals to dynamically filter and integrate high-confidence information from different cost volumes, providing precise inputs for the aggregation unit.
Experimental results on the DTU and T&amp;amp;T datasets demonstrate that GGMVS significantly reduces memory consumption and runtime while maintaining competitive accuracy. Furthermore, validation on the ETH3D dataset further confirms the excellent generalization capability of GGMVS.&lt;/p&gt;</content:encoded></item><item><title>Adversarial Training for Process Reward Models</title><link>https://arxiv.org/abs/2511.22888v1</link><guid>http://arxiv.org/abs/2511.22888v1</guid><pubDate>Fri, 28 Nov 2025 05:32:01 +0000</pubDate><dc:creator>Gurusha Juneja</dc:creator><dc:creator>Deepak Nathani</dc:creator><dc:creator>William Yang Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.
Published: 2025-11-28T05:32:01+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gurusha Juneja; Deepak Nathani; William Yang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Process Reward Models (PRMs) enhance reasoning ability of LLMs by providing step-level supervision. However, their widespread adoption is limited due to expensive manual step-level annotation and poor generalization of static training data to novel errors. We introduce Adversarially Trained PRMs (\texttt{APRM}), where a Generator ($G$) learns to produce reasoning errors to deceive a PRM ($R$), while $R$ concurrently learns to detect them. This interaction yields progressively harder negatives for $R$, improving its robustness and generalization to novel errors without requiring manual step-level labels. Averaged across diverse mathematical reasoning benchmarks, \texttt{APRM} improves solver accuracy by $+3.4$ percentage points (pp) over the strongest PRM baseline. \texttt{APRM} achieves gains of $+5.3$ pp on out-of-distribution tasks.&lt;/p&gt;</content:encoded></item><item><title>RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video</title><link>https://arxiv.org/abs/2511.22950v1</link><guid>http://arxiv.org/abs/2511.22950v1</guid><pubDate>Fri, 28 Nov 2025 07:51:02 +0000</pubDate><dc:creator>Haiyang Mei</dc:creator><dc:creator>Qiming Huang</dc:creator><dc:creator>Hai Ci</dc:creator><dc:creator>Mike Zheng Shou</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.
Published: 2025-11-28T07:51:02+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haiyang Mei; Qiming Huang; Hai Ci; Mike Zheng Shou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.&lt;/p&gt;</content:encoded></item><item><title>WUSH: Near-Optimal Adaptive Transforms for LLM Quantization</title><link>https://arxiv.org/abs/2512.00956v1</link><guid>http://arxiv.org/abs/2512.00956v1</guid><pubDate>Sun, 30 Nov 2025 16:17:34 +0000</pubDate><dc:creator>Jiale Chen</dc:creator><dc:creator>Vage Egiazarian</dc:creator><dc:creator>Torsten Hoefler</dc:creator><dc:creator>Dan Alistarh</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.
Published: 2025-11-30T16:17:34+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiale Chen; Vage Egiazarian; Torsten Hoefler; Dan Alistarh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.&lt;/p&gt;</content:encoded></item><item><title>SpeContext: Enabling Efficient Long-context Reasoning with Speculative Context Sparsity in LLMs</title><link>https://arxiv.org/abs/2512.00722v1</link><guid>http://arxiv.org/abs/2512.00722v1</guid><pubDate>Sun, 30 Nov 2025 04:32:43 +0000</pubDate><dc:creator>Jiaming Xu</dc:creator><dc:creator>Jiayi Pan</dc:creator><dc:creator>Hanzhen Wang</dc:creator><dc:creator>Yongkang Zhou</dc:creator><dc:creator>Jiancai Ye</dc:creator><dc:creator>Yu Wang</dc:creator><dc:creator>Guohao Dai</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving &gt; 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.
Published: 2025-11-30T04:32:43+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaming Xu; Jiayi Pan; Hanzhen Wang; Yongkang Zhou; Jiancai Ye; Yu Wang; Guohao Dai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we point out that the objective of the retrieval algorithms is to align with the LLM, which is similar to the objective of knowledge distillation in LLMs. We analyze the similarity in information focus between the distilled language model(DLM) and the original LLM from the perspective of information theory, and thus propose a novel paradigm that leverages a DLM as the retrieval algorithm. Based on the insight, we present SpeContext, an algorithm and system co-design for long-context reasoning. (1) At the algorithm level, SpeContext proposes lightweight retrieval head based on the head-level attention weights of DLM, achieving &amp;gt; 90% parameters reduction by pruning the redundancy. (2) At the system level, SpeContext designs an asynchronous prefetch dataflow via the elastic loading strategy, effectively overlapping KV cache retrieval with the LLM computation. (3) At the compilation level, SpeContext constructs the theoretical memory model and implements an adaptive memory management system to achieve acceleration by maximizing GPU memory utilization. We deploy and evaluate SpeContext in two resourceconstrained environments, cloud and edge. Extensive experiments show that, compared with the Huggingface framework, SpeContext achieves up to 24.89x throughput improvement in cloud and 10.06x speedup in edge with negligible accuracy loss, pushing the Pareto frontier of accuracy and throughput.&lt;/p&gt;</content:encoded></item><item><title>Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs</title><link>https://arxiv.org/abs/2512.00908v1</link><guid>http://arxiv.org/abs/2512.00908v1</guid><pubDate>Sun, 30 Nov 2025 14:19:36 +0000</pubDate><dc:creator>Xinzhu Chen</dc:creator><dc:creator>Xuesheng Li</dc:creator><dc:creator>Zhongxiang Sun</dc:creator><dc:creator>Weijie Yu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.
Published: 2025-11-30T14:19:36+00:00
Venue: arXiv
Score: 0.786 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinzhu Chen; Xuesheng Li; Zhongxiang Sun; Weijie Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (consider)&lt;/p&gt;
&lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.&lt;/p&gt;</content:encoded></item><item><title>Probing the "Psyche'' of Large Reasoning Models: Understanding Through a Human Lens</title><link>https://arxiv.org/abs/2512.00729v1</link><guid>http://arxiv.org/abs/2512.00729v1</guid><pubDate>Sun, 30 Nov 2025 04:49:44 +0000</pubDate><dc:creator>Yuxiang Chen</dc:creator><dc:creator>Zuohan Wu</dc:creator><dc:creator>Ziwei Wang</dc:creator><dc:creator>Xiangning Yu</dc:creator><dc:creator>Xujia Li</dc:creator><dc:creator>Linyi Yang</dc:creator><dc:creator>Mengyue Yang</dc:creator><dc:creator>Jun Wang</dc:creator><dc:creator>Lei Chen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche'' of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks'' (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.
Published: 2025-11-30T04:49:44+00:00
Venue: arXiv
Score: 0.786 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxiang Chen; Zuohan Wu; Ziwei Wang; Xiangning Yu; Xujia Li; Linyi Yang; Mengyue Yang; Jun Wang; Lei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (consider)&lt;/p&gt;
&lt;p&gt;Large reasoning models (LRMs) have garnered significant attention from researchers owing to their exceptional capability in addressing complex tasks. Motivated by the observed human-like behaviors in their reasoning processes, this paper introduces a comprehensive taxonomy to characterize atomic reasoning steps and probe the ``psyche&amp;#x27;&amp;#x27; of LRM intelligence. Specifically, it comprises five groups and seventeen categories derived from human mental processes, thereby grounding the understanding of LRMs in an interdisciplinary perspective. The taxonomy is then applied for an in-depth understanding of current LRMs, resulting in a distinct labeled dataset that comprises 277,534 atomic reasoning steps. Using this resource, we analyze contemporary LRMs and distill several actionable takeaways for improving training and post-training of reasoning models. Notably, our analysis reveals that prevailing post-answer ``double-checks&amp;#x27;&amp;#x27; (self-monitoring evaluations) are largely superficial and rarely yield substantive revisions. Thus, incentivizing comprehensive multi-step reflection, rather than simple self-monitoring, may offer a more effective path forward. To complement the taxonomy, an automatic annotation framework, named CAPO, is proposed to leverage large language models (LLMs) for generating the taxonomy-based annotations. Experimental results demonstrate that CAPO achieves higher consistency with human experts compared to baselines, facilitating a scalable and comprehensive analysis of LRMs from a human cognitive perspective. Together, the taxonomy, CAPO, and the derived insights provide a principled, scalable path toward understanding and advancing LRM reasoning.&lt;/p&gt;</content:encoded></item><item><title>Experts are all you need: A Composable Framework for Large Language Model Inference</title><link>https://arxiv.org/abs/2511.22955v1</link><guid>http://arxiv.org/abs/2511.22955v1</guid><pubDate>Fri, 28 Nov 2025 08:00:16 +0000</pubDate><dc:creator>Shrihari Sridharan</dc:creator><dc:creator>Sourjya Roy</dc:creator><dc:creator>Anand Raghunathan</dc:creator><dc:creator>Kaushik Roy</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or "experts". However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential "plan--act--observe" loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.
Published: 2025-11-28T08:00:16+00:00
Venue: arXiv
Score: 0.785 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shrihari Sridharan; Sourjya Roy; Anand Raghunathan; Kaushik Roy&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (consider)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) have achieved state-of-the-art accuracies in a variety of natural language processing (NLP) tasks. However, this success comes at the cost of increased model sizes which leads to additional computational burden. Mixture of Experts (MoEs) overcome this bottleneck by decoupling model capacity from computation by only activating a subset of parameters or &amp;quot;experts&amp;quot;. However, these models require joint pretraining of these experts along with the router and do not model multi-step reasoning. In contrast, multi-agent frameworks improve reasoning by decomposing complex problems into modular subtasks. However, these frameworks rely on sequential &amp;quot;plan--act--observe&amp;quot; loops, which introduce significant latency. Our work, Comp-LLM, addresses these challenges by introducing a composable inference framework that enables cross-expert collaboration via an explicit sub-query dependency graph. Comp-LLM consists of three components: (1) A Sub-query Generator that decomposes an input query, assigns each sub-query to an appropriate expert using embedding similarity, and constructs a dependency graph; (2) A Query Executor that processes nodes in the graph and identifies opportunities for parallelism based on dependencies and resource constraints; and (3) A Response Aggregator that synthesizes intermediate expert responses into a coherent final answer. Across several benchmarks, Comp-LLM achieves up to 11.01% accuracy improvement over monolithic LLMs of similar size, while offering 1.67x--3.56x reduction in model size with no significant degradation relative to the largest model in its family. Additionally, Comp-LLM provides 1.1x--1.7x latency improvement compared to sequential sub-query processing.&lt;/p&gt;</content:encoded></item><item><title>Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction</title><link>https://arxiv.org/abs/2511.23476v1</link><guid>http://arxiv.org/abs/2511.23476v1</guid><pubDate>Fri, 28 Nov 2025 18:59:47 +0000</pubDate><dc:creator>Bao Shu</dc:creator><dc:creator>Yan Cai</dc:creator><dc:creator>Jianjian Sun</dc:creator><dc:creator>Chunrui Han</dc:creator><dc:creator>En Yu</dc:creator><dc:creator>Liang Zhao</dc:creator><dc:creator>Jingcheng Hu</dc:creator><dc:creator>Yinmin Zhang</dc:creator><dc:creator>Haoran Lv</dc:creator><dc:creator>Yuang Peng</dc:creator><dc:creator>Zheng Ge</dc:creator><dc:creator>Xiangyu Zhang</dc:creator><dc:creator>Daxin Jiang</dc:creator><dc:creator>Xiangyu Yue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.
Published: 2025-11-28T18:59:47+00:00
Venue: arXiv
Score: 0.785 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bao Shu; Yan Cai; Jianjian Sun; Chunrui Han; En Yu; Liang Zhao; Jingcheng Hu; Yinmin Zhang; Haoran Lv; Yuang Peng; Zheng Ge; Xiangyu Zhang; Daxin Jiang; Xiangyu Yue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (consider)&lt;/p&gt;
&lt;p&gt;Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model&amp;#x27;s active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Object-Centric Data Synthesis for Category-level Object Detection</title><link>https://arxiv.org/abs/2511.23450v1</link><guid>http://arxiv.org/abs/2511.23450v1</guid><pubDate>Fri, 28 Nov 2025 18:41:46 +0000</pubDate><dc:creator>Vikhyat Agarwal</dc:creator><dc:creator>Jiayi Cora Guo</dc:creator><dc:creator>Declan Hoban</dc:creator><dc:creator>Sissi Zhang</dc:creator><dc:creator>Nicholas Moran</dc:creator><dc:creator>Peter Cho</dc:creator><dc:creator>Srilakshmi Pattabiraman</dc:creator><dc:creator>Shantanu Joshi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep learning approaches to object detection have achieved reliable detection of specific object classes in images. However, extending a model's detection capability to new object classes requires large amounts of annotated training data, which is costly and time-consuming to acquire, especially for long-tailed classes with insufficient representation in existing datasets. Here, we introduce the object-centric data setting, when limited data is available in the form of object-centric data (multi-view images or 3D models), and systematically evaluate the performance of four different data synthesis methods to finetune object detection models on novel object categories in this setting. The approaches are based on simple image processing techniques, 3D rendering, and image diffusion models, and use object-centric data to synthesize realistic, cluttered images with varying contextual coherence and complexity. We assess how these methods enable models to achieve category-level generalization in real-world data, and demonstrate significant performance boosts within this data-constrained experimental setting.
Published: 2025-11-28T18:41:46+00:00
Venue: arXiv
Score: 0.785 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Vikhyat Agarwal; Jiayi Cora Guo; Declan Hoban; Sissi Zhang; Nicholas Moran; Peter Cho; Srilakshmi Pattabiraman; Shantanu Joshi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (consider)&lt;/p&gt;
&lt;p&gt;Deep learning approaches to object detection have achieved reliable detection of specific object classes in images. However, extending a model&amp;#x27;s detection capability to new object classes requires large amounts of annotated training data, which is costly and time-consuming to acquire, especially for long-tailed classes with insufficient representation in existing datasets. Here, we introduce the object-centric data setting, when limited data is available in the form of object-centric data (multi-view images or 3D models), and systematically evaluate the performance of four different data synthesis methods to finetune object detection models on novel object categories in this setting. The approaches are based on simple image processing techniques, 3D rendering, and image diffusion models, and use object-centric data to synthesize realistic, cluttered images with varying contextual coherence and complexity. We assess how these methods enable models to achieve category-level generalization in real-world data, and demonstrate significant performance boosts within this data-constrained experimental setting.&lt;/p&gt;</content:encoded></item></channel></rss>