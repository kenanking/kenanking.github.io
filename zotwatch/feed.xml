<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 23 Dec 2025 02:50:02 +0000</lastBuildDate><item><title>Boosting Faithful Multi-modal LLMs via Complementary Visual Grounding</title><link>https://doi.org/10.1109/tip.2025.3644140</link><guid>10.1109/tip.2025.3644140</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Zheren Fu</dc:creator><dc:creator>Zhendong Mao</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Yongdong Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644140</prism:doi><description>Multimodal Large Language Models (MLLMs) exhibit impressive performance across vision-language tasks, but still face the hallucination challenges, where generated texts are factually inconsistent with visual input. Existing mitigation methods focus on surface symptoms of hallucination and heavily rely on post-hoc corrections, extensive data curation, or costly inference schemes. In this work, we identify two key factors of MLLM hallucination: Insufficient Visual Context, where ambiguous visual contexts lead to language speculation, and Progressive Textual Drift, where model attention strays from visual inputs in longer responses. To address these problems, we propose a novel Complementary Visual Grounding (CVG) framework. CVG exploits the intrinsic architecture of MLLMs, without requiring any external tools, models, or additional data. CVG first disentangles visual context into two complementary branches based on query relevance, then maintains steadfast visual grounding during the auto-regressive generation. Finally, it contrasts the output distributions of two branches to produce a faithful response. Extensive experiments on various hallucination and general benchmarks demonstrate that CVG achieves state-of-the-art performances across MLLM architectures and scales.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheren Fu; Zhendong Mao; Lei Zhang; Yongdong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644140"&gt;10.1109/tip.2025.3644140&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) exhibit impressive performance across vision-language tasks, but still face the hallucination challenges, where generated texts are factually inconsistent with visual input. Existing mitigation methods focus on surface symptoms of hallucination and heavily rely on post-hoc corrections, extensive data curation, or costly inference schemes. In this work, we identify two key factors of MLLM hallucination: Insufficient Visual Context, where ambiguous visual contexts lead to language speculation, and Progressive Textual Drift, where model attention strays from visual inputs in longer responses. To address these problems, we propose a novel Complementary Visual Grounding (CVG) framework. CVG exploits the intrinsic architecture of MLLMs, without requiring any external tools, models, or additional data. CVG first disentangles visual context into two complementary branches based on query relevance, then maintains steadfast visual grounding during the auto-regressive generation. Finally, it contrasts the output distributions of two branches to produce a faithful response. Extensive experiments on various hallucination and general benchmarks demonstrate that CVG achieves state-of-the-art performances across MLLM architectures and scales.&lt;/p&gt;</content:encoded></item><item><title>PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation</title><link>https://doi.org/10.1109/tip.2025.3644785</link><guid>10.1109/tip.2025.3644785</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Mengyuan Liu</dc:creator><dc:creator>Jiajie Liu</dc:creator><dc:creator>Jinyan Zhang</dc:creator><dc:creator>Wenhao Li</dc:creator><dc:creator>Junsong Yuan</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644785</prism:doi><description>The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengyuan Liu; Jiajie Liu; Jinyan Zhang; Wenhao Li; Junsong Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644785"&gt;10.1109/tip.2025.3644785&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.&lt;/p&gt;</content:encoded></item><item><title>A YOLO-based Polymerized Head-auxiliary Structures for Target Detection in Remote Sensing Images</title><link>https://doi.org/10.1016/j.patcog.2025.112961</link><guid>10.1016/j.patcog.2025.112961</guid><pubDate>Sun, 21 Dec 2025 06:49:30 +0000</pubDate><dc:creator>Yalu Zhang</dc:creator><dc:creator>Sixiang Quan</dc:creator><dc:creator>Hai Xiao</dc:creator><dc:creator>Jun Liu</dc:creator><dc:creator>Zhenfeng Shao</dc:creator><dc:creator>Zhihui Wang</dc:creator><dc:creator>Yingying Peng</dc:creator><dc:creator>Huali Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112961</prism:doi><description>Target detection tasks are now widely applied in the field of remote sensing. However, remote sensing target detection tasks are confronted with problems such as cluttered backgrounds and large scale variations. To address these issues, this paper proposes a high-precision aggregation head-auxiliary target detector (PHAS-YOLO). PHAS-YOLO includes two innovative plug-and-play modules: the spatial awareness attention module (SAAM) and the convolutional re-calibration multiscale feature fusion module (CRMSFF), as well as the context aggregation bidirectional connection structure (CABi-FPN) and the adaptive auxiliary head structure (AAHS). The proposed modules enable the model to have good spatial feature aggregation capabilities to retain key feature information, incorporate an adaptive weighting mechanism to reduce information loss caused by the fusion of different scales, and refine the features of the images to be detected. A series of experiments were conducted on three public remote sensing target detection datasets, namely DIOR, DOTAv1.0, and HRRSD, to verify the effectiveness and superiority of the proposed method in remote sensing target detection tasks.
Published: 2025-12-21T06:49:30+00:00
Venue: Pattern Recognition
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yalu Zhang; Sixiang Quan; Hai Xiao; Jun Liu; Zhenfeng Shao; Zhihui Wang; Yingying Peng; Huali Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112961"&gt;10.1016/j.patcog.2025.112961&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Target detection tasks are now widely applied in the field of remote sensing. However, remote sensing target detection tasks are confronted with problems such as cluttered backgrounds and large scale variations. To address these issues, this paper proposes a high-precision aggregation head-auxiliary target detector (PHAS-YOLO). PHAS-YOLO includes two innovative plug-and-play modules: the spatial awareness attention module (SAAM) and the convolutional re-calibration multiscale feature fusion module (CRMSFF), as well as the context aggregation bidirectional connection structure (CABi-FPN) and the adaptive auxiliary head structure (AAHS). The proposed modules enable the model to have good spatial feature aggregation capabilities to retain key feature information, incorporate an adaptive weighting mechanism to reduce information loss caused by the fusion of different scales, and refine the features of the images to be detected. A series of experiments were conducted on three public remote sensing target detection datasets, namely DIOR, DOTAv1.0, and HRRSD, to verify the effectiveness and superiority of the proposed method in remote sensing target detection tasks.&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Survey and Taxonomy of Mamba: Applications, Challenges, and Future Directions</title><link>https://doi.org/10.1016/j.inffus.2025.104094</link><guid>10.1016/j.inffus.2025.104094</guid><pubDate>Mon, 22 Dec 2025 07:49:44 +0000</pubDate><dc:creator>Qiguang Miao</dc:creator><dc:creator>Linxing Jia</dc:creator><dc:creator>Kun Xie</dc:creator><dc:creator>Kaiyuan Fu</dc:creator><dc:creator>Zongkai Yang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104094</prism:doi><description>Transformer-based architectures have achieved notable success across natural language processing, computer vision, and multimodal learning, yet they face persistent challenges such as high computational complexity and limited adaptability to dynamic environments. State Space Models (SSMs) have emerged as a competitive alternative, offering linear-time complexity and the ability to implicitly capture long-range dependencies. Building on this foundation, the Mamba model introduces time-varying parameterization to dynamically adjust state transitions based on input context, combined with selective state updates, content-aware scanning strategies, and hardware-efficient design. These innovations enable Mamba to maintain linear complexity while delivering higher throughput and significantly reduced memory consumption compared to both Transformer-based and conventional SSM architectures. This survey systematically reviews the theoretical foundations, architectural innovations, and application progress of the Mamba model. First, we trace the evolution of SSMs, highlighting the key design principles that underpin Mamba’s dynamic state transition and selective computation mechanisms. Second, we summarize Mamba’s structural innovations in modeling dynamics and multimodal fusion, categorizing its applications across multiple modalities, including vision, speech, point clouds, and multimodal data. Finally, we evaluate representative applications in medical image analysis, recommendation systems, reinforcement learning, and generative modeling, identifying advantages, limitations, and open challenges. The review concludes by outlining future research directions focused on improving generalization, causal reasoning, interpretability, and computational efficiency. This work aims to provide a concise yet comprehensive reference for researchers and practitioners, promoting further development and deployment of Mamba-based architectures across diverse real-world scenarios.
Published: 2025-12-22T07:49:44+00:00
Venue: Information Fusion
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiguang Miao; Linxing Jia; Kun Xie; Kaiyuan Fu; Zongkai Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104094"&gt;10.1016/j.inffus.2025.104094&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer-based architectures have achieved notable success across natural language processing, computer vision, and multimodal learning, yet they face persistent challenges such as high computational complexity and limited adaptability to dynamic environments. State Space Models (SSMs) have emerged as a competitive alternative, offering linear-time complexity and the ability to implicitly capture long-range dependencies. Building on this foundation, the Mamba model introduces time-varying parameterization to dynamically adjust state transitions based on input context, combined with selective state updates, content-aware scanning strategies, and hardware-efficient design. These innovations enable Mamba to maintain linear complexity while delivering higher throughput and significantly reduced memory consumption compared to both Transformer-based and conventional SSM architectures. This survey systematically reviews the theoretical foundations, architectural innovations, and application progress of the Mamba model. First, we trace the evolution of SSMs, highlighting the key design principles that underpin Mamba’s dynamic state transition and selective computation mechanisms. Second, we summarize Mamba’s structural innovations in modeling dynamics and multimodal fusion, categorizing its applications across multiple modalities, including vision, speech, point clouds, and multimodal data. Finally, we evaluate representative applications in medical image analysis, recommendation systems, reinforcement learning, and generative modeling, identifying advantages, limitations, and open challenges. The review concludes by outlining future research directions focused on improving generalization, causal reasoning, interpretability, and computational efficiency. This work aims to provide a concise yet comprehensive reference for researchers and practitioners, promoting further development and deployment of Mamba-based architectures across diverse real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Cross-Scale Feature Aggregation for Few-shot Object Detection</title><link>https://doi.org/10.1016/j.neucom.2025.132514</link><guid>10.1016/j.neucom.2025.132514</guid><pubDate>Mon, 22 Dec 2025 16:56:56 +0000</pubDate><dc:creator>Anni Wang</dc:creator><dc:creator>Penglin Zhang</dc:creator><dc:creator>Jinhan Li</dc:creator><dc:creator>Jian Chao</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132514</prism:doi><description>In recent years, deep learning algorithms have achieved remarkable success across a wide range of applications. However, high-performance models generally require large quantities of annotated data to achieve optimal results, and in many practical scenarios, obtaining high-quality labeled samples remains a significant challenge, limiting the applicability of deep learning techniques to object detection tasks. Conventional deep learning approaches to object detection heavily rely on visual features extracted from query images to generate region proposals. Consequently, these methods struggle to meet detection requirements in complex environments, especially when confronted with newly introduced categories. To address these limitations, recent research efforts have shifted toward few-shot and zero-shot detection strategies. These emerging approaches enable the recognition of unseen objects in new domains using a relatively small number of annotated examples. Such methods typically employ self-supervised learning mechanisms to extract features without relying on predefined category-specific knowledge, which significantly enhances cross-domain generalization performance. Inspired by this paradigm, this paper proposes a dual-branch cross-domain adaptive object detection algorithm. The proposed method introduces a multi-scale cross-branch feature extraction module designed to enhance the model’s self-supervised learning capabilities. Furthermore, it incorporates a support branch feature aggregation module, which provides effective guidance for both location and category predictions in the query branch. This design enables accurate cross-domain adaptive learning. To validate the effectiveness of the proposed algorithm, comparative experiments were conducted on publicly available datasets using state-of-the-art detection methods as baseline models. The experimental results demonstrate that the proposed approach achieves superior performance on cross-domain object detection tasks.
Published: 2025-12-22T16:56:56+00:00
Venue: Neurocomputing
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anni Wang; Penglin Zhang; Jinhan Li; Jian Chao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132514"&gt;10.1016/j.neucom.2025.132514&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, deep learning algorithms have achieved remarkable success across a wide range of applications. However, high-performance models generally require large quantities of annotated data to achieve optimal results, and in many practical scenarios, obtaining high-quality labeled samples remains a significant challenge, limiting the applicability of deep learning techniques to object detection tasks. Conventional deep learning approaches to object detection heavily rely on visual features extracted from query images to generate region proposals. Consequently, these methods struggle to meet detection requirements in complex environments, especially when confronted with newly introduced categories. To address these limitations, recent research efforts have shifted toward few-shot and zero-shot detection strategies. These emerging approaches enable the recognition of unseen objects in new domains using a relatively small number of annotated examples. Such methods typically employ self-supervised learning mechanisms to extract features without relying on predefined category-specific knowledge, which significantly enhances cross-domain generalization performance. Inspired by this paradigm, this paper proposes a dual-branch cross-domain adaptive object detection algorithm. The proposed method introduces a multi-scale cross-branch feature extraction module designed to enhance the model’s self-supervised learning capabilities. Furthermore, it incorporates a support branch feature aggregation module, which provides effective guidance for both location and category predictions in the query branch. This design enables accurate cross-domain adaptive learning. To validate the effectiveness of the proposed algorithm, comparative experiments were conducted on publicly available datasets using state-of-the-art detection methods as baseline models. The experimental results demonstrate that the proposed approach achieves superior performance on cross-domain object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>Cross-Frequency Attention and Color Contrast Constraint for Remote Sensing Dehazing</title><link>https://doi.org/10.1109/tip.2025.3644167</link><guid>10.1109/tip.2025.3644167</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Yuxin Feng</dc:creator><dc:creator>Jufeng Li</dc:creator><dc:creator>Tao Huang</dc:creator><dc:creator>Fangfang Wu</dc:creator><dc:creator>Yakun Ju</dc:creator><dc:creator>Chunxu Li</dc:creator><dc:creator>Weisheng Dong</dc:creator><dc:creator>Alex C. Kot</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644167</prism:doi><description>Current deep learning-based methods for remote sensing image dehazing have developed rapidly, yet they still commonly struggle to simultaneously preserve fine texture details and restore accurate colors. The fundamental reason lies in the insufficient modeling of high-frequency information that captures structural details, as well as the lack of effective constraints for color restoration. To address the insufficient modeling of global high-frequency information, we first develop an omni-directional high-frequency feature inpainting mechanism that leverages the wavelet transform to extract multi-directional high-frequency components. While maintaining the advantage of linear complexity, it models global long-range texture dependencies through cross-frequency perception. Then, to further strengthen local high-frequency representation, we design a high-frequency prompt attention module that dynamically injects wavelet-domain optimized high-frequency features as cross-level guidance signals, significantly enhancing the model’s capability in edge sharpness restoration and texture detail reconstruction. Further, to alleviate the problem of inaccurate color restoration, we propose a color contrast loss function based on the HSV color space, which explicitly models the statistical distribution differences of brightness and saturation in hazy regions, guiding the model to generate dehazed images with consistent colors and natural visual appearance. Finally, extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing approaches in both texture detail restoration and color consistency. Further results and code available at: https://github.com/fyxnl/C4RSD.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxin Feng; Jufeng Li; Tao Huang; Fangfang Wu; Yakun Ju; Chunxu Li; Weisheng Dong; Alex C. Kot&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644167"&gt;10.1109/tip.2025.3644167&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Current deep learning-based methods for remote sensing image dehazing have developed rapidly, yet they still commonly struggle to simultaneously preserve fine texture details and restore accurate colors. The fundamental reason lies in the insufficient modeling of high-frequency information that captures structural details, as well as the lack of effective constraints for color restoration. To address the insufficient modeling of global high-frequency information, we first develop an omni-directional high-frequency feature inpainting mechanism that leverages the wavelet transform to extract multi-directional high-frequency components. While maintaining the advantage of linear complexity, it models global long-range texture dependencies through cross-frequency perception. Then, to further strengthen local high-frequency representation, we design a high-frequency prompt attention module that dynamically injects wavelet-domain optimized high-frequency features as cross-level guidance signals, significantly enhancing the model’s capability in edge sharpness restoration and texture detail reconstruction. Further, to alleviate the problem of inaccurate color restoration, we propose a color contrast loss function based on the HSV color space, which explicitly models the statistical distribution differences of brightness and saturation in hazy regions, guiding the model to generate dehazed images with consistent colors and natural visual appearance. Finally, extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing approaches in both texture detail restoration and color consistency. Further results and code available at: https://github.com/fyxnl/C4RSD.&lt;/p&gt;</content:encoded></item><item><title>A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation</title><link>https://doi.org/10.1109/tip.2025.3644789</link><guid>10.1109/tip.2025.3644789</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Jianghao Wu</dc:creator><dc:creator>Xiangde Luo</dc:creator><dc:creator>Yubo Zhou</dc:creator><dc:creator>Lianming Wu</dc:creator><dc:creator>Guotai Wang</dc:creator><dc:creator>Shaoting Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644789</prism:doi><description>Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose A3-TTA, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianghao Wu; Xiangde Luo; Yubo Zhou; Lianming Wu; Guotai Wang; Shaoting Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644789"&gt;10.1109/tip.2025.3644789&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose A3-TTA, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.&lt;/p&gt;</content:encoded></item><item><title>CCSFuse: Collaborative Compensation and Selective Fusion for UAV-based RGB-IR Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3647293</link><guid>10.1109/tgrs.2025.3647293</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Tao Zhang</dc:creator><dc:creator>Ruitao Lu</dc:creator><dc:creator>Xiaogang Yang</dc:creator><dc:creator>Dingwen Zhang</dc:creator><dc:creator>Yansheng Li</dc:creator><dc:creator>Xueli Xie</dc:creator><dc:creator>Yunsong Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647293</prism:doi><description>Visible-Infrared (RGB-IR) object detection plays a crucial role in UAV-based vision tasks. However, existing methods still suffer from learning bias caused by imbalanced information distribution and inaccurate fusion due to modal conflicts. Inspired by the human multisensory information processing mechanism, we propose a novel “CompensationFusion” progressive detection framework, CCSFuse, to fully exploit the complementary relationship between modalities while eliminating conflict interference. Specifically, we design a cross-modal feature compensation module, which establishes inter-modal information interaction to achieve mutual complementarity and enhancement during feature extraction, effectively mitigating the issue of imbalanced modal information distribution. Additionally, we introduce an adaptive feature-selection fusion module to address modal conflicts. We employ a cross-modal channel attention to calibrate channel features of different modalities and utilizes a selective fusion strategy to dynamically assess modal importance, thereby achieving adaptive modal fusion. Finally, we validate the effectiveness of CCSFuse on the DroneVehicle and LLVIP datasets. The results confirm that CCSFuse significantly improves the efficiency of feature optimization and integration. In UAV-based object detection scenarios, CCSFuse outperforms state-of-the-art methods in both qualitative and quantitative comparisons, particularly for small objects and low-quality modalities. The code is available at https://github.com/ZhangT-xxl/CCSFuse.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Zhang; Ruitao Lu; Xiaogang Yang; Dingwen Zhang; Yansheng Li; Xueli Xie; Yunsong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647293"&gt;10.1109/tgrs.2025.3647293&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Visible-Infrared (RGB-IR) object detection plays a crucial role in UAV-based vision tasks. However, existing methods still suffer from learning bias caused by imbalanced information distribution and inaccurate fusion due to modal conflicts. Inspired by the human multisensory information processing mechanism, we propose a novel “CompensationFusion” progressive detection framework, CCSFuse, to fully exploit the complementary relationship between modalities while eliminating conflict interference. Specifically, we design a cross-modal feature compensation module, which establishes inter-modal information interaction to achieve mutual complementarity and enhancement during feature extraction, effectively mitigating the issue of imbalanced modal information distribution. Additionally, we introduce an adaptive feature-selection fusion module to address modal conflicts. We employ a cross-modal channel attention to calibrate channel features of different modalities and utilizes a selective fusion strategy to dynamically assess modal importance, thereby achieving adaptive modal fusion. Finally, we validate the effectiveness of CCSFuse on the DroneVehicle and LLVIP datasets. The results confirm that CCSFuse significantly improves the efficiency of feature optimization and integration. In UAV-based object detection scenarios, CCSFuse outperforms state-of-the-art methods in both qualitative and quantitative comparisons, particularly for small objects and low-quality modalities. The code is available at https://github.com/ZhangT-xxl/CCSFuse.&lt;/p&gt;</content:encoded></item><item><title>HCMA-Net: Hierarchical Cross-Modality Aggregation Network for Multimodal Remote Sensing Image Classification</title><link>https://doi.org/10.1109/tgrs.2025.3646806</link><guid>10.1109/tgrs.2025.3646806</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Wenping Ma</dc:creator><dc:creator>Hekai Zhang</dc:creator><dc:creator>Mengru Ma</dc:creator><dc:creator>Boyou Xue</dc:creator><dc:creator>Hao Zhu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646806</prism:doi><description>While multimodal remote sensing images provide complementary information in different imaging ways, effectively aggregating and jointly learning from these heterogeneous features is non-trivial, primarily due to the inherent modality gap that hinders semantic alignment and feature interoperability. To overcome these issues, we propose a Hierarchical Cross-Modality Aggregation Network (HCMA-Net). It introduces two novel components: the hierarchical feature aggregation and enhancement module (HFAE-Module) and the cross-modality interactive feature extraction module (CMIFE-Module). The HFAE-Module tackles the modality gap and enables cross-scale interaction through its Hierarchical Cross-Modality Feature Aggregation (HCMFA) mechanism, which incorporates Cross-Spectral and Spatial Aggregation Non-Local Attention layers (CSNLA and SANLA) to align features and aggregate contextual information across spectral and spatial dimensions. The CMIFE-Module addresses the optimization conflict by leveraging a dual-attention design; it uses self-attention to reinforce intra-modal coherence and cross-attention to dynamically extract and fuse complementary inter-modal features, thereby maximizing complementarity while avoiding the dilution of discriminative features and preventing negative transfer. Experiments on four real-world datasets (Hohhot, Nanjing, Xi’an, Houston2013) demonstrate that HCMA-Net consistently achieves outstanding classification results. The code is available at: https://github.com/sun740936222/HCMA-Net.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenping Ma; Hekai Zhang; Mengru Ma; Boyou Xue; Hao Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646806"&gt;10.1109/tgrs.2025.3646806&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;While multimodal remote sensing images provide complementary information in different imaging ways, effectively aggregating and jointly learning from these heterogeneous features is non-trivial, primarily due to the inherent modality gap that hinders semantic alignment and feature interoperability. To overcome these issues, we propose a Hierarchical Cross-Modality Aggregation Network (HCMA-Net). It introduces two novel components: the hierarchical feature aggregation and enhancement module (HFAE-Module) and the cross-modality interactive feature extraction module (CMIFE-Module). The HFAE-Module tackles the modality gap and enables cross-scale interaction through its Hierarchical Cross-Modality Feature Aggregation (HCMFA) mechanism, which incorporates Cross-Spectral and Spatial Aggregation Non-Local Attention layers (CSNLA and SANLA) to align features and aggregate contextual information across spectral and spatial dimensions. The CMIFE-Module addresses the optimization conflict by leveraging a dual-attention design; it uses self-attention to reinforce intra-modal coherence and cross-attention to dynamically extract and fuse complementary inter-modal features, thereby maximizing complementarity while avoiding the dilution of discriminative features and preventing negative transfer. Experiments on four real-world datasets (Hohhot, Nanjing, Xi’an, Houston2013) demonstrate that HCMA-Net consistently achieves outstanding classification results. The code is available at: https://github.com/sun740936222/HCMA-Net.&lt;/p&gt;</content:encoded></item><item><title>Local Saliency-Guided Dynamic Matching for Cross-Modal Remote Sensing Image-Text Retrieval</title><link>https://doi.org/10.1109/tgrs.2025.3646809</link><guid>10.1109/tgrs.2025.3646809</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Jie Shao</dc:creator><dc:creator>Yiran Xie</dc:creator><dc:creator>Pengda Wang</dc:creator><dc:creator>Guohao Feng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646809</prism:doi><description>Remote sensing cross-modal text-image retrieval (RSCTIR) has emerged as a fundamental task in remote sensing analysis, aiming to bridge the semantic gap between visual and textual modalities under complex scenarios. Despite recent progress, salient information representation and accurate cross-modal alignments remain significant challenges. In this work, we demonstrate that the correlation between visual and textual features is a valuable cue that can be leveraged to enhance saliency analysis and metric learning in cross-modal retrieval. To this end, we propose a Local Saliency Mining module that uses textual attention to guide the extraction of visual salient features. Additionally, we introduce a Multi-granularity Similarity Contrastive Loss and a Dynamic Similarity Matching Loss to enhance semantic alignment. Finally, a graph-based diffusion reranking algorithm is developed to optimize retrieval ranking by leveraging the intrinsic manifold structure of multimodal data, thereby mitigating local optima during inference. Extensive experiments on the RSICD, RSITMD and UCM-Captions datasets confirm the effectiveness and superiority of our method over state-of-the-art approaches.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Shao; Yiran Xie; Pengda Wang; Guohao Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646809"&gt;10.1109/tgrs.2025.3646809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;Remote sensing cross-modal text-image retrieval (RSCTIR) has emerged as a fundamental task in remote sensing analysis, aiming to bridge the semantic gap between visual and textual modalities under complex scenarios. Despite recent progress, salient information representation and accurate cross-modal alignments remain significant challenges. In this work, we demonstrate that the correlation between visual and textual features is a valuable cue that can be leveraged to enhance saliency analysis and metric learning in cross-modal retrieval. To this end, we propose a Local Saliency Mining module that uses textual attention to guide the extraction of visual salient features. Additionally, we introduce a Multi-granularity Similarity Contrastive Loss and a Dynamic Similarity Matching Loss to enhance semantic alignment. Finally, a graph-based diffusion reranking algorithm is developed to optimize retrieval ranking by leveraging the intrinsic manifold structure of multimodal data, thereby mitigating local optima during inference. Extensive experiments on the RSICD, RSITMD and UCM-Captions datasets confirm the effectiveness and superiority of our method over state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>CLIP2RS: Leveraging Pretrained Vision-Language Model for Semantic Segmentation of Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3647015</link><guid>10.1109/tgrs.2025.3647015</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Yinghui Xing</dc:creator><dc:creator>Dexuan Kong</dc:creator><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Ziyi Li</dc:creator><dc:creator>Qingyi Li</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647015</prism:doi><description>Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.783 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinghui Xing; Dexuan Kong; Shizhou Zhang; Ziyi Li; Qingyi Li; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647015"&gt;10.1109/tgrs.2025.3647015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.&lt;/p&gt;</content:encoded></item><item><title>Bi-Grid Reconstruction for Image Anomaly Detection</title><link>https://doi.org/10.1109/tip.2025.3644787</link><guid>10.1109/tip.2025.3644787</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Aimin Feng</dc:creator><dc:creator>Huichuan Huang</dc:creator><dc:creator>Guangyu Wei</dc:creator><dc:creator>Wenlong Sun</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644787</prism:doi><description>In the domain of image anomaly detection, significant progress has been made in un- and self-supervised methods with datasets containing only normal samples. Although these methods perform well in general industrial anomaly detection scenarios, they often struggle with over- or under-detection when faced with fine-grained anomalies in products. In this paper, we propose GRAD: Bi-Grid Reconstruction for Image Anomaly Detection, which utilizes two continuous grids to detect anomalies from both normal and abnormal perspectives. In this work: 1) Grids are served as feature repositories to assist in the reconstruction task, achieving stronger generalization compared to discrete storage, while also helping to avoid the Identical Shortcut (IS) problem common in general reconstruction methods. 2) An additional grid storing abnormal features is introduced alongside the normal grid storing normal features, which refines the boundaries of normal features, thereby enhancing GRAD’s detection performance for fine-grained defects. 3) The Feature Block Pasting (FBP) module is designed to synthesize a variety of anomalies at the feature level, enabling the rapid deployment of the abnormal grid. Additionally, benefiting from the powerful representation capabilities of grids, GRAD is suitable for a unified task setting, requiring only a single model to be trained for multiple classes. GRAD has been comprehensively tested on classic industrial datasets including MVTecAD, VisA, and the newest GoodsAD dataset, showing significant improvement over current state-of-the-art methods.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aimin Feng; Huichuan Huang; Guangyu Wei; Wenlong Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644787"&gt;10.1109/tip.2025.3644787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;In the domain of image anomaly detection, significant progress has been made in un- and self-supervised methods with datasets containing only normal samples. Although these methods perform well in general industrial anomaly detection scenarios, they often struggle with over- or under-detection when faced with fine-grained anomalies in products. In this paper, we propose GRAD: Bi-Grid Reconstruction for Image Anomaly Detection, which utilizes two continuous grids to detect anomalies from both normal and abnormal perspectives. In this work: 1) Grids are served as feature repositories to assist in the reconstruction task, achieving stronger generalization compared to discrete storage, while also helping to avoid the Identical Shortcut (IS) problem common in general reconstruction methods. 2) An additional grid storing abnormal features is introduced alongside the normal grid storing normal features, which refines the boundaries of normal features, thereby enhancing GRAD’s detection performance for fine-grained defects. 3) The Feature Block Pasting (FBP) module is designed to synthesize a variety of anomalies at the feature level, enabling the rapid deployment of the abnormal grid. Additionally, benefiting from the powerful representation capabilities of grids, GRAD is suitable for a unified task setting, requiring only a single model to be trained for multiple classes. GRAD has been comprehensively tested on classic industrial datasets including MVTecAD, VisA, and the newest GoodsAD dataset, showing significant improvement over current state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Seg-LLaVA: A small-scale large vision-language model with external visual prompts</title><link>https://doi.org/10.1016/j.neucom.2025.132423</link><guid>10.1016/j.neucom.2025.132423</guid><pubDate>Mon, 22 Dec 2025 07:44:54 +0000</pubDate><dc:creator>Tianxing Guo</dc:creator><dc:creator>Huanyu Liu</dc:creator><dc:creator>Jiazheng Wen</dc:creator><dc:creator>Junbao Li</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132423</prism:doi><description>With recent significant advancements in large vision-language models (LVLMs), image-text understanding capabilities have substantially improved. However, a notable gap remains in fine-grained region understanding. Moreover, the resource consumption for training and testing large-scale LVLMs is immense, making them less accessible to researchers with limited resources. In this paper, we propose a small-scale LVLM, Seg-LLaVA, which employs a lightweight visual prompting method that leverages a semantic segmenter and a small-scale large language model (LLM). By integrating fine-grained knowledge generated by a specialized instance segmentation model with the original image into a multi-layer linear model, we enable the model to perceive object boundaries and types in the image without significantly increasing the number of training parameters, thereby greatly enhancing its visual understanding capabilities. Additionally, we adopt an efficient training approach, allowing Seg-LLaVA to achieve outstanding performance while further reducing resource requirements. Experimental results show that our model excels across multiple benchmarks and demonstrates strong fine-grained perception capabilities.
Published: 2025-12-22T07:44:54+00:00
Venue: Neurocomputing
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianxing Guo; Huanyu Liu; Jiazheng Wen; Junbao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132423"&gt;10.1016/j.neucom.2025.132423&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;With recent significant advancements in large vision-language models (LVLMs), image-text understanding capabilities have substantially improved. However, a notable gap remains in fine-grained region understanding. Moreover, the resource consumption for training and testing large-scale LVLMs is immense, making them less accessible to researchers with limited resources. In this paper, we propose a small-scale LVLM, Seg-LLaVA, which employs a lightweight visual prompting method that leverages a semantic segmenter and a small-scale large language model (LLM). By integrating fine-grained knowledge generated by a specialized instance segmentation model with the original image into a multi-layer linear model, we enable the model to perceive object boundaries and types in the image without significantly increasing the number of training parameters, thereby greatly enhancing its visual understanding capabilities. Additionally, we adopt an efficient training approach, allowing Seg-LLaVA to achieve outstanding performance while further reducing resource requirements. Experimental results show that our model excels across multiple benchmarks and demonstrates strong fine-grained perception capabilities.&lt;/p&gt;</content:encoded></item><item><title>Automatic Contrastive Chain-of-Thought Prompting: Learning from Reasoning Errors of Large Language Models</title><link>https://doi.org/10.1016/j.eswa.2025.130919</link><guid>10.1016/j.eswa.2025.130919</guid><pubDate>Mon, 22 Dec 2025 23:58:13 +0000</pubDate><dc:creator>Xi Li</dc:creator><dc:creator>Xiping Liu</dc:creator><dc:creator>Qing Shu</dc:creator><dc:creator>Zhao Tan</dc:creator><dc:creator>Changxuan Wan</dc:creator><dc:creator>Dexi Liu</dc:creator><dc:creator>Qizhi Wan</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130919</prism:doi><description>Large Language Models (LLMs) have shown strong reasoning ability through Chain-of-Thought (CoT) prompting, yet their reasoning remains vulnerable to error accumulation, undermining accuracy and trustworthiness. To mitigate these issues, contrastive approaches have been explored, aiming to improve reasoning by contrasting valid and invalid reasoning paths. However, existing contrastive approaches rely on manually crafted invalid reasoning paths, which are often incoherent and poorly comparable to real model errors. To address this issue, we propose Automatic Contrastive Chain-of-Thought (Auto-CCoT), a generalizable framework that automatically generates valid–invalid reasoning pairs from LLM outputs and dynamically selects the most informative ones via a retrieval mechanism. Auto-CCoT guides models to reinforce correct reasoning patterns while avoiding common mistakes, overcoming the limitations of manually crafted contrastive examples. Experiments on six datasets (GSM8K, AQuA, GSM-Hard, SVAMP, ASDIV, TAT-QA) show consistent improvements, with gains up to 5.1% on AQuA and 4.0% on GSM8K. Under Self-Consistency decoding, Auto-CCoT-SC yields additional gains (1.6% - 2.2%). On TAT-QA, integrating Auto-CCoT with Program-of-Thought (PoT) improves accuracy by 3.7%. Auto-CCoT generalizes across different models, providing a robust and broadly applicable framework for enhancing LLM reasoning.
Published: 2025-12-22T23:58:13+00:00
Venue: Expert Systems with Applications
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xi Li; Xiping Liu; Qing Shu; Zhao Tan; Changxuan Wan; Dexi Liu; Qizhi Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130919"&gt;10.1016/j.eswa.2025.130919&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Large Language Models (LLMs) have shown strong reasoning ability through Chain-of-Thought (CoT) prompting, yet their reasoning remains vulnerable to error accumulation, undermining accuracy and trustworthiness. To mitigate these issues, contrastive approaches have been explored, aiming to improve reasoning by contrasting valid and invalid reasoning paths. However, existing contrastive approaches rely on manually crafted invalid reasoning paths, which are often incoherent and poorly comparable to real model errors. To address this issue, we propose Automatic Contrastive Chain-of-Thought (Auto-CCoT), a generalizable framework that automatically generates valid–invalid reasoning pairs from LLM outputs and dynamically selects the most informative ones via a retrieval mechanism. Auto-CCoT guides models to reinforce correct reasoning patterns while avoiding common mistakes, overcoming the limitations of manually crafted contrastive examples. Experiments on six datasets (GSM8K, AQuA, GSM-Hard, SVAMP, ASDIV, TAT-QA) show consistent improvements, with gains up to 5.1% on AQuA and 4.0% on GSM8K. Under Self-Consistency decoding, Auto-CCoT-SC yields additional gains (1.6% - 2.2%). On TAT-QA, integrating Auto-CCoT with Program-of-Thought (PoT) improves accuracy by 3.7%. Auto-CCoT generalizes across different models, providing a robust and broadly applicable framework for enhancing LLM reasoning.&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Representation Learning from Sparse Transformation Analysis</title><link>https://doi.org/10.1109/tpami.2025.3645586</link><guid>10.1109/tpami.2025.3645586</guid><pubDate>Mon, 22 Dec 2025 18:41:22 +0000</pubDate><dc:creator>Yue Song</dc:creator><dc:creator>T. Anderson Keller</dc:creator><dc:creator>Yisong Yue</dc:creator><dc:creator>Pietro Perona</dc:creator><dc:creator>Max Welling</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3645586</prism:doi><description>There is a vast literature on representation learning based on principles such as coding efficiency, statistical independence, causality, controllability, or symmetry. In this paper we propose to learn representations from sequence data by factorizing the transformations of the latent variables into sparse components. Input data are first encoded as distributions of latent activations and subsequently transformed using a probability flow model, before being decoded to predict a future input state. The flow model is decomposed into a number of rotational (divergence-free) vector fields and a number of potential flow (curl-free) fields. Our sparsity prior encourages only a small number of these fields to be active at any instant and infers the speed with which the probability flows along these fields. Training this model is completely unsupervised using a standard variational objective and results in a new form of disentangled representations where the input is not only represented by a combination of independent factors, but also by a combination of independent transformation primitives given by the learned flow fields. When viewing the transformations as symmetries one may interpret this as learning approximately equivariant representations. Empirically we demonstrate that this model achieves state of the art in terms of both data likelihood and unsupervised approximate equivariance errors on datasets composed of sequence transformations.
Published: 2025-12-22T18:41:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Song; T. Anderson Keller; Yisong Yue; Pietro Perona; Max Welling&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3645586"&gt;10.1109/tpami.2025.3645586&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;There is a vast literature on representation learning based on principles such as coding efficiency, statistical independence, causality, controllability, or symmetry. In this paper we propose to learn representations from sequence data by factorizing the transformations of the latent variables into sparse components. Input data are first encoded as distributions of latent activations and subsequently transformed using a probability flow model, before being decoded to predict a future input state. The flow model is decomposed into a number of rotational (divergence-free) vector fields and a number of potential flow (curl-free) fields. Our sparsity prior encourages only a small number of these fields to be active at any instant and infers the speed with which the probability flows along these fields. Training this model is completely unsupervised using a standard variational objective and results in a new form of disentangled representations where the input is not only represented by a combination of independent factors, but also by a combination of independent transformation primitives given by the learned flow fields. When viewing the transformations as symmetries one may interpret this as learning approximately equivariant representations. Empirically we demonstrate that this model achieves state of the art in terms of both data likelihood and unsupervised approximate equivariance errors on datasets composed of sequence transformations.&lt;/p&gt;</content:encoded></item><item><title>Toward Effective Model Merging in Semantic Segmentation</title><link>https://doi.org/10.1109/tnnls.2025.3626606</link><guid>10.1109/tnnls.2025.3626606</guid><pubDate>Mon, 22 Dec 2025 18:42:24 +0000</pubDate><dc:creator>Haotian Chen</dc:creator><dc:creator>Yanyu Xu</dc:creator><dc:creator>Yonghui Xu</dc:creator><dc:creator>Yu Zhang</dc:creator><dc:creator>Yixin Zhang</dc:creator><dc:creator>Fang Wang</dc:creator><dc:creator>Lizhen Cui</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3626606</prism:doi><description>Model merging has become a popular approach for combining individual models into a single model that inherits their capabilities and achieves improved performance. However, its success has not yet been transferred to semantic segmentation tasks due to two major challenges: 1) current model merging methods predominantly employ static merging strategies with fixed coefficients, limiting their ability to incorporate task-specific prior knowledge and 2) semantic segmentation faces large distribution shifts across multiple domains, causing negative transfer in the merged model. In this article, we propose an effective model merging approach for semantic segmentation, named M2Seg. To dramatically integrate relevant priors based on the input data, we propose a novel SVD-structured MoE module for adaptive merging. To address the severe distribution shifts, we further introduce a test-time dynamic calibration function designed to minimize discrepancies between training and test statistics. Additionally, historical information is leveraged to refine activation statistics during inference. Recognizing that unreliable data can negatively impact update directions, we develop a pixel-efficient entropy minimization mechanism to filter unstable pixels, thus stabilizing the merging process and enhancing segmentation performance. Extensive experiments on both seen and unseen semantic segmentation tasks demonstrate the superior effectiveness and generalization capability of our proposed method. The source code and pretrained checkpoints are available at https://github.com/cht619/MMSeg
Published: 2025-12-22T18:42:24+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.780 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haotian Chen; Yanyu Xu; Yonghui Xu; Yu Zhang; Yixin Zhang; Fang Wang; Lizhen Cui&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3626606"&gt;10.1109/tnnls.2025.3626606&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (consider)&lt;/p&gt;
&lt;p&gt;Model merging has become a popular approach for combining individual models into a single model that inherits their capabilities and achieves improved performance. However, its success has not yet been transferred to semantic segmentation tasks due to two major challenges: 1) current model merging methods predominantly employ static merging strategies with fixed coefficients, limiting their ability to incorporate task-specific prior knowledge and 2) semantic segmentation faces large distribution shifts across multiple domains, causing negative transfer in the merged model. In this article, we propose an effective model merging approach for semantic segmentation, named M2Seg. To dramatically integrate relevant priors based on the input data, we propose a novel SVD-structured MoE module for adaptive merging. To address the severe distribution shifts, we further introduce a test-time dynamic calibration function designed to minimize discrepancies between training and test statistics. Additionally, historical information is leveraged to refine activation statistics during inference. Recognizing that unreliable data can negatively impact update directions, we develop a pixel-efficient entropy minimization mechanism to filter unstable pixels, thus stabilizing the merging process and enhancing segmentation performance. Extensive experiments on both seen and unseen semantic segmentation tasks demonstrate the superior effectiveness and generalization capability of our proposed method. The source code and pretrained checkpoints are available at https://github.com/cht619/MMSeg&lt;/p&gt;</content:encoded></item><item><title>STMG-PCNet: A Lightweight Shape-Texture Mutual-Guided Predictor-Corrector Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/jstars.2025.3646741</link><guid>10.1109/jstars.2025.3646741</guid><pubDate>Mon, 22 Dec 2025 18:41:48 +0000</pubDate><dc:creator>Zichen Zhao</dc:creator><dc:creator>Huanzhang Ling</dc:creator><dc:creator>Xiaxia Qin</dc:creator><dc:creator>Haodong Xiao</dc:creator><dc:creator>Jihao Wu</dc:creator><dc:creator>Chengwei Liu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3646741</prism:doi><description>Infrared small target detection (IRSTD) is rapidly advancing in edge applications such as micro-UAVs and autonomous vehicles. However, the small size and low brightness of targets often necessitate complex architectures in existing models, making deployment on resource-constrained edge devices challenging. Therefore, designing a model that balances high detection accuracy with low resource consumption is critical for the successful implementation of IRSTD. To address this challenge, this study proposes a lightweight shape-texture mutual-guided predictor-corrector network (STMG-PCNet). Through the collaborative design of a detection branch and a shape-aware branch, along with efficient module architectures, STMG-PCNet strikes a balance between accuracy and efficiency. Specifically, a progressive lightweight dilated dense (PLD2) module in the detection branch combines dense connections and cascaded multi-scale dilated convolutions to enhance local texture retention and precise localization. A shape-aware vision Transformer (SAViT) module in the shape-aware branch models target edges and shape features via global context interaction. In addition, a numerical predictor-corrector fusion (NPC-Fusion) module is proposed to achieve lightweight multi-stage feature fusion. Experiments on three public infrared datasets reveal that STMG-PCNet significantly outperforms existing algorithms in detection accuracy, parameters, computational efficiency, and inference speed, verifying its robustness and practicality in complex scenarios. Code is available at https://github.com/Zichen-Zhao01/STMG-PCNet.
Published: 2025-12-22T18:41:48+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.778 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zichen Zhao; Huanzhang Ling; Xiaxia Qin; Haodong Xiao; Jihao Wu; Chengwei Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3646741"&gt;10.1109/jstars.2025.3646741&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (consider)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is rapidly advancing in edge applications such as micro-UAVs and autonomous vehicles. However, the small size and low brightness of targets often necessitate complex architectures in existing models, making deployment on resource-constrained edge devices challenging. Therefore, designing a model that balances high detection accuracy with low resource consumption is critical for the successful implementation of IRSTD. To address this challenge, this study proposes a lightweight shape-texture mutual-guided predictor-corrector network (STMG-PCNet). Through the collaborative design of a detection branch and a shape-aware branch, along with efficient module architectures, STMG-PCNet strikes a balance between accuracy and efficiency. Specifically, a progressive lightweight dilated dense (PLD2) module in the detection branch combines dense connections and cascaded multi-scale dilated convolutions to enhance local texture retention and precise localization. A shape-aware vision Transformer (SAViT) module in the shape-aware branch models target edges and shape features via global context interaction. In addition, a numerical predictor-corrector fusion (NPC-Fusion) module is proposed to achieve lightweight multi-stage feature fusion. Experiments on three public infrared datasets reveal that STMG-PCNet significantly outperforms existing algorithms in detection accuracy, parameters, computational efficiency, and inference speed, verifying its robustness and practicality in complex scenarios. Code is available at https://github.com/Zichen-Zhao01/STMG-PCNet.&lt;/p&gt;</content:encoded></item><item><title>General Polarimetric Correlation Pattern: A Visualization and Characterization Tool for Target Joint-Domain Scattering Mechanisms Investigation</title><link>https://doi.org/10.1109/tgrs.2025.3647123</link><guid>10.1109/tgrs.2025.3647123</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Hao-Liang Li</dc:creator><dc:creator>Si-Wei Chen</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647123</prism:doi><description>Polarimetric radar enables the acquisition of fully polarimetric information from targets. The electromagnetic scattering response of radar targets exhibits strong dependencies on the polarization basis, time, radar frequency, and incident angle. Most existing methods for characterizing target scattering mechanisms are limited to single-domain features or independent multi-domain features, thereby hindering the achievement of fine target understanding and investigation. This work is dedicated to addressing this issue, with the core idea of transforming from the traditional paradigm of independent single-domain representation to a joint-domain processing framework. First, a polarimetric scattering tensor characterization approach is constructed to integrate target scattering information across multiple domains, including the polarimetric, time, frequency, and spatial domains. Then, a visualization and characterization tool named general polarimetric correlation pattern (GPCP) is proposed for mining and utilizing target joint-domain scattering characteristics. This tool enables the visualization of scattering diversity variations across different dimensions, which encompass rich information rarely considered in existing studies. Investigations are conducted to reveal canonical structures’ latent scattering patterns using electromagnetic computation data. On this basis, a set of polarimetric features, including 8 geometric features and 8 statistical features, are derived to quantitatively characterize these patterns. Quantitative investigations based on the target-to-clutter ratio (TCR) index with Radarsat-2 and Gaofen-3 polarimetric synthetic aperture radar (PolSAR) datasets demonstrate the superiority of the proposed polarimetric features in enhancing ship target contrast. Finally, potential future applications of the proposed GPCP are discussed.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao-Liang Li; Si-Wei Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647123"&gt;10.1109/tgrs.2025.3647123&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Polarimetric radar enables the acquisition of fully polarimetric information from targets. The electromagnetic scattering response of radar targets exhibits strong dependencies on the polarization basis, time, radar frequency, and incident angle. Most existing methods for characterizing target scattering mechanisms are limited to single-domain features or independent multi-domain features, thereby hindering the achievement of fine target understanding and investigation. This work is dedicated to addressing this issue, with the core idea of transforming from the traditional paradigm of independent single-domain representation to a joint-domain processing framework. First, a polarimetric scattering tensor characterization approach is constructed to integrate target scattering information across multiple domains, including the polarimetric, time, frequency, and spatial domains. Then, a visualization and characterization tool named general polarimetric correlation pattern (GPCP) is proposed for mining and utilizing target joint-domain scattering characteristics. This tool enables the visualization of scattering diversity variations across different dimensions, which encompass rich information rarely considered in existing studies. Investigations are conducted to reveal canonical structures’ latent scattering patterns using electromagnetic computation data. On this basis, a set of polarimetric features, including 8 geometric features and 8 statistical features, are derived to quantitatively characterize these patterns. Quantitative investigations based on the target-to-clutter ratio (TCR) index with Radarsat-2 and Gaofen-3 polarimetric synthetic aperture radar (PolSAR) datasets demonstrate the superiority of the proposed polarimetric features in enhancing ship target contrast. Finally, potential future applications of the proposed GPCP are discussed.&lt;/p&gt;</content:encoded></item><item><title>MGSANet: A Multiscale Graph Spatial Alignment Network for Weakly Aligned RGB-Thermal Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3647051</link><guid>10.1109/tgrs.2025.3647051</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Qingwang Wang</dc:creator><dc:creator>Yuxuan Sun</dc:creator><dc:creator>Tao Shen</dc:creator><dc:creator>Mohammad Al-Antary</dc:creator><dc:creator>Hisham Alasmary</dc:creator><dc:creator>Muhammad Waqas</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647051</prism:doi><description>The current mainstream research on color-thermal (i.e., RGB-T) object detection assumes that the RGB and thermal images are strictly aligned. However, in practical situations, due to the insufficient spatiotemporal synchronization, stereo disparity of the camera in the installation position, and the errors in the image pairs registration process, the position of the same objects in RGB and thermal images are not completely overlapped. The position shift can cause distortion, trailing, and blurring issues during the image fusion process, leading to a decrease in model detection accuracy. To address this challenge, we propose a novel multiscale graph spatial alignment network (MGSANet), which can effectively alleviate the negative effects of cross-modal image misalignment. Specifically, we represent the feature maps extracted from RGB and thermal images by backbone network as a graph structure, and use graph attention network (GAT) to model the spatial position deviation relationship. Furthermore, considering the multiscale characteristics of the objects, we represent the feature maps with multiscale graphs. We then align RGB and thermal feature maps in a potential feature space according to the learned deviation relationship for object detection. In addition, considering the scarcity of RGB-T datasets from the perspective of unmanned aerial vehicle (UAV), and to verify the object detection performance on different platforms, we construct an RGB-T object detection dataset collected by the UAV platform, named KUSTDrone. We conducted experiments on datasets collected by vehicle and UAV platforms respectively. Experimental results demonstrate that MGSANet outperforms the competitive methods for weakly aligned RGB-T object detection. The dataset will be accessible at https://github.com/KustTeamWQW/KUSTDrone with a license, and the code will also be accessible at https://github.com/KustTeamWQW/MGSANet.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.776 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qingwang Wang; Yuxuan Sun; Tao Shen; Mohammad Al-Antary; Hisham Alasmary; Muhammad Waqas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647051"&gt;10.1109/tgrs.2025.3647051&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (consider)&lt;/p&gt;
&lt;p&gt;The current mainstream research on color-thermal (i.e., RGB-T) object detection assumes that the RGB and thermal images are strictly aligned. However, in practical situations, due to the insufficient spatiotemporal synchronization, stereo disparity of the camera in the installation position, and the errors in the image pairs registration process, the position of the same objects in RGB and thermal images are not completely overlapped. The position shift can cause distortion, trailing, and blurring issues during the image fusion process, leading to a decrease in model detection accuracy. To address this challenge, we propose a novel multiscale graph spatial alignment network (MGSANet), which can effectively alleviate the negative effects of cross-modal image misalignment. Specifically, we represent the feature maps extracted from RGB and thermal images by backbone network as a graph structure, and use graph attention network (GAT) to model the spatial position deviation relationship. Furthermore, considering the multiscale characteristics of the objects, we represent the feature maps with multiscale graphs. We then align RGB and thermal feature maps in a potential feature space according to the learned deviation relationship for object detection. In addition, considering the scarcity of RGB-T datasets from the perspective of unmanned aerial vehicle (UAV), and to verify the object detection performance on different platforms, we construct an RGB-T object detection dataset collected by the UAV platform, named KUSTDrone. We conducted experiments on datasets collected by vehicle and UAV platforms respectively. Experimental results demonstrate that MGSANet outperforms the competitive methods for weakly aligned RGB-T object detection. The dataset will be accessible at https://github.com/KustTeamWQW/KUSTDrone with a license, and the code will also be accessible at https://github.com/KustTeamWQW/MGSANet.&lt;/p&gt;</content:encoded></item><item><title>Jo-SNC: Combating Noisy Labels Through Fostering Self- and Neighbor-Consistency</title><link>https://doi.org/10.1109/tpami.2025.3646737</link><guid>10.1109/tpami.2025.3646737</guid><pubDate>Mon, 22 Dec 2025 18:41:22 +0000</pubDate><dc:creator>Zeren Sun</dc:creator><dc:creator>Yazhou Yao</dc:creator><dc:creator>Tongliang Liu</dc:creator><dc:creator>Zechao Li</dc:creator><dc:creator>Fumin Shen</dc:creator><dc:creator>Jinhui Tang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646737</prism:doi><description>Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (Joint sample selection and model regularization based on Self- and Neighbor-Consistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the “likelihood” of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods. Our code and models have been made publicly available at https://github.com/NUST-Machine-Intelligence-Laboratory/Jo-SNC.
Published: 2025-12-22T18:41:22+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zeren Sun; Yazhou Yao; Tongliang Liu; Zechao Li; Fumin Shen; Jinhui Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646737"&gt;10.1109/tpami.2025.3646737&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Label noise is pervasive in various real-world scenarios, posing challenges in supervised deep learning. Deep networks are vulnerable to such label-corrupted samples due to the memorization effect. One major stream of previous methods concentrates on identifying clean data for training. However, these methods often neglect imbalances in label noise across different mini-batches and devote insufficient attention to out-of-distribution noisy data. To this end, we propose a noise-robust method named Jo-SNC (Joint sample selection and model regularization based on Self- and Neighbor-Consistency). Specifically, we propose to employ the Jensen-Shannon divergence to measure the “likelihood” of a sample being clean or out-of-distribution. This process factors in the nearest neighbors of each sample to reinforce the reliability of clean sample identification. We design a self-adaptive, data-driven thresholding scheme to adjust per-class selection thresholds. While clean samples undergo conventional training, detected in-distribution and out-of-distribution noisy samples are trained following partial label learning and negative learning, respectively. Finally, we advance the model performance further by proposing a triplet consistency regularization that promotes self-prediction consistency, neighbor-prediction consistency, and feature consistency. Extensive experiments on various benchmark datasets and comprehensive ablation studies demonstrate the effectiveness and superiority of our approach over existing state-of-the-art methods. Our code and models have been made publicly available at https://github.com/NUST-Machine-Intelligence-Laboratory/Jo-SNC.&lt;/p&gt;</content:encoded></item><item><title>Weakly-Localized Ship Velocity Estimation From Optical Image</title><link>https://doi.org/10.1109/jstars.2025.3647058</link><guid>10.1109/jstars.2025.3647058</guid><pubDate>Mon, 22 Dec 2025 18:41:48 +0000</pubDate><dc:creator>Jishuai Zhu</dc:creator><dc:creator>Ziheng Zeng</dc:creator><dc:creator>Yaxiong Chen</dc:creator><dc:creator>Shengwu Xiong</dc:creator><dc:creator>Sai Zhong</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647058</prism:doi><description>Estimating ship velocity from remote sensing imagery is crucial for maritime surveillance, traffic monitoring, and the detection of illegal activities. Traditional approaches that rely on automatic identification system data often face challenges such as delayed updates, deliberate signal shutdowns, and spoofing. Recent methods based on synthetic aperture radar imagery typically require additional annotations and remain dependent on hand-crafted geometric assumptions. In this work, our method, VESSEL (Velocity EStimation with weakly Supervised End-to-end Learning), learns to identify motion-relevant regions without explicit supervision on vessels or wakes, substantially reducing annotation overhead and enabling broader applicability across diverse oceanic environments. Experiments on a proprietary optical dataset demonstrate that our method achieves superior performance compared to the state-of-the-art method, particularly when wakes are clearly visible. Unlike previous methods restricted to Kelvin wakes, our approach is generalizable to more complex wake scenarios, such as turbulent wakes, where traditional methods struggle to apply. The study highlights the potential of learning-based strategies for robust and scalable ship velocity estimation.
Published: 2025-12-22T18:41:48+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jishuai Zhu; Ziheng Zeng; Yaxiong Chen; Shengwu Xiong; Sai Zhong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647058"&gt;10.1109/jstars.2025.3647058&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Estimating ship velocity from remote sensing imagery is crucial for maritime surveillance, traffic monitoring, and the detection of illegal activities. Traditional approaches that rely on automatic identification system data often face challenges such as delayed updates, deliberate signal shutdowns, and spoofing. Recent methods based on synthetic aperture radar imagery typically require additional annotations and remain dependent on hand-crafted geometric assumptions. In this work, our method, VESSEL (Velocity EStimation with weakly Supervised End-to-end Learning), learns to identify motion-relevant regions without explicit supervision on vessels or wakes, substantially reducing annotation overhead and enabling broader applicability across diverse oceanic environments. Experiments on a proprietary optical dataset demonstrate that our method achieves superior performance compared to the state-of-the-art method, particularly when wakes are clearly visible. Unlike previous methods restricted to Kelvin wakes, our approach is generalizable to more complex wake scenarios, such as turbulent wakes, where traditional methods struggle to apply. The study highlights the potential of learning-based strategies for robust and scalable ship velocity estimation.&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm</title><link>https://doi.org/10.1007/s11263-025-02614-0</link><guid>10.1007/s11263-025-02614-0</guid><pubDate>Mon, 22 Dec 2025 20:38:54 +0000</pubDate><dc:creator>Fuxiang Huang</dc:creator><dc:creator>Xiaowei Fu</dc:creator><dc:creator>Shiyu Ye</dc:creator><dc:creator>Lina Ma</dc:creator><dc:creator>Wen Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><dc:creator>David Zhang</dc:creator><dc:creator>Lei Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02614-0</prism:doi><description>Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation. Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.
Published: 2025-12-22T20:38:54+00:00
Venue: International Journal of Computer Vision
Score: 0.773 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fuxiang Huang; Xiaowei Fu; Shiyu Ye; Lina Ma; Wen Li; Xinbo Gao; David Zhang; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02614-0"&gt;10.1007/s11263-025-02614-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (consider)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation. Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.&lt;/p&gt;</content:encoded></item><item><title>Complex-Valued Convolutional Neural Network With Learnable Activation Function for Frequency-Domain Radar Signal Processing</title><link>https://doi.org/10.1109/taes.2025.3646567</link><guid>10.1109/taes.2025.3646567</guid><pubDate>Mon, 22 Dec 2025 18:44:07 +0000</pubDate><dc:creator>Mainak Chakraborty</dc:creator><dc:creator>Masoud Daneshtalab</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3646567</prism:doi><description>Recent advancements in deep learning and the availability of open-source datasets have enabled real-valued Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to achieve high performance in Synthetic Aperture Radar (SAR) target recognition, SAR-based land use and land cover classification, radar micro-Doppler signature-based Human Activity Recognition (HAR), and Small Unmanned Aerial Vehicle (SUAV) target recognition. However, their high computational cost and resource requirements limit deployment in resource-constrained environments. Frequency-domain complex-valued CNNs have recently emerged as a promising alternative, leveraging the convolution theorem to perform efficient spectral convolutions, reducing computational complexity while preserving both amplitude and phase characteristics of SAR and Continuous-wave (CW) radar signals. Despite their potential, adoption remains constrained by the need for frequency-adaptive complex-valued layers, robust spectral complex-valued activation functions, and efficient parameter initialization methods. Traditional frequency-domain CVNNs often require frequent Fourier transform (FFT/IFFT) transitions ( O ( n log n ) \mathcal {O}(n \log n) complexity) for spatial-domain pooling and activations, increasing computational overhead. Additionally, many existing complex-valued CNNs employ real-valued activation functions on complex tensors in a split-type manner, which might destroy phase-magnitude relationships and reduce effectiveness for phase-sensitive tasks. Moreover, architectures that use complex-valued weights but rely on real-valued activation functions suffer from phase distortion, limited expressiveness, and mathematical inconsistency. Considering these limitations, we propose a frequency-adaptive complex-valued CNN with a complex-valued learnable activation function designed for SAR-based analysis, SUAV detection, and HAR. Our model operates entirely in the frequency domain and processes only complex-valued data. Exte...
Published: 2025-12-22T18:44:07+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.773 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mainak Chakraborty; Masoud Daneshtalab&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3646567"&gt;10.1109/taes.2025.3646567&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.773 (consider)&lt;/p&gt;
&lt;p&gt;Recent advancements in deep learning and the availability of open-source datasets have enabled real-valued Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to achieve high performance in Synthetic Aperture Radar (SAR) target recognition, SAR-based land use and land cover classification, radar micro-Doppler signature-based Human Activity Recognition (HAR), and Small Unmanned Aerial Vehicle (SUAV) target recognition. However, their high computational cost and resource requirements limit deployment in resource-constrained environments. Frequency-domain complex-valued CNNs have recently emerged as a promising alternative, leveraging the convolution theorem to perform efficient spectral convolutions, reducing computational complexity while preserving both amplitude and phase characteristics of SAR and Continuous-wave (CW) radar signals. Despite their potential, adoption remains constrained by the need for frequency-adaptive complex-valued layers, robust spectral complex-valued activation functions, and efficient parameter initialization methods. Traditional frequency-domain CVNNs often require frequent Fourier transform (FFT/IFFT) transitions ( O ( n log n ) \mathcal {O}(n \log n) complexity) for spatial-domain pooling and activations, increasing computational overhead. Additionally, many existing complex-valued CNNs employ real-valued activation functions on complex tensors in a split-type manner, which might destroy phase-magnitude relationships and reduce effectiveness for phase-sensitive tasks. Moreover, architectures that use complex-valued weights but rely on real-valued activation functions suffer from phase distortion, limited expressiveness, and mathematical inconsistency. Considering these limitations, we propose a frequency-adaptive complex-valued CNN with a complex-valued learnable activation function designed for SAR-based analysis, SUAV detection, and HAR. Our model operates entirely in the frequency domain and processes only complex-valued data. Exte...&lt;/p&gt;</content:encoded></item><item><title>Ship Detection by Combined Using DoP Fluctuation and Averaged Intensity Information of PolSAR Data</title><link>https://doi.org/10.1109/lgrs.2025.3647289</link><guid>10.1109/lgrs.2025.3647289</guid><pubDate>Mon, 22 Dec 2025 18:45:21 +0000</pubDate><dc:creator>Koki Oketani</dc:creator><dc:creator>Fang Shang</dc:creator><dc:creator>Naoto Kishi</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3647289</prism:doi><description>In this work, we propose a new ship detecting method by introducing the DoP fluctuation information and combining it with intensity information. The high fluctuation level of DoP stably appears when there is man-made target. The intensity information and its threshold used in the method are proposed to be adjusted for line spacing and background intensity level to ensure the parameter set can be generally used. With proper thresholds, ship target basically has positive responses in the DoP fluctuation and intensity results. However, not all the both responses are caused by ship target. After deleting pseudo responses, ship targets can be identified. The detecting accuracy of the proposed method is tested by using ALOS2-PALSAR2 datasets.
Published: 2025-12-22T18:45:21+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.772 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Koki Oketani; Fang Shang; Naoto Kishi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3647289"&gt;10.1109/lgrs.2025.3647289&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.772 (consider)&lt;/p&gt;
&lt;p&gt;In this work, we propose a new ship detecting method by introducing the DoP fluctuation information and combining it with intensity information. The high fluctuation level of DoP stably appears when there is man-made target. The intensity information and its threshold used in the method are proposed to be adjusted for line spacing and background intensity level to ensure the parameter set can be generally used. With proper thresholds, ship target basically has positive responses in the DoP fluctuation and intensity results. However, not all the both responses are caused by ship target. After deleting pseudo responses, ship targets can be identified. The detecting accuracy of the proposed method is tested by using ALOS2-PALSAR2 datasets.&lt;/p&gt;</content:encoded></item><item><title>Degradation-Aware Prompted Transformer for Unified Medical Image Restoration</title><link>https://doi.org/10.1109/tip.2025.3644795</link><guid>10.1109/tip.2025.3644795</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Jinbao Wei</dc:creator><dc:creator>Gang Yang</dc:creator><dc:creator>Zhijie Wang</dc:creator><dc:creator>Shimin Tao</dc:creator><dc:creator>Aiping Liu</dc:creator><dc:creator>Xun Chen</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644795</prism:doi><description>Medical image restoration (MedIR) aims to recover high-quality images from degraded inputs, yet faces unique challenges from physics-driven degradations and multi-modal task interference. While existing all-in-one methods handle natural image degradations well, they struggle with medical scenarios due to limited degradation perception and suboptimal multi-task optimization. In response, we introduce DaPT, a Degradation-aware Prompted Transformer, which integrates dynamic prompt learning and modular expert mining for unified MedIR. First, DaPT introduces spatially compact prompts with optimal transport regularization, amplifying inter-prompt differences to capture diverse degradation patterns. Second, a mixture of experts dynamically routes inputs to specialized modules via prompt guidance, resolving task conflicts while reducing computational overhead. The synergy of prompt learning and expert mining further enables robust restoration across multi-modal medical data, offering a practical solution for clinical imaging. Extensive experiments across multiple modalities (MRI, CT, PET) and diverse degradations, covering both in-distribution and out-of-distribution scenarios, demonstrate that DaPT consistently outperforms state-of-the-art methods and generalizes reliably to unseen settings, underscoring its robustness, effectiveness, and clinical practicality. The source code will be released at https://github.com/weijinbao1998/DaPT.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.771 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinbao Wei; Gang Yang; Zhijie Wang; Shimin Tao; Aiping Liu; Xun Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644795"&gt;10.1109/tip.2025.3644795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (consider)&lt;/p&gt;
&lt;p&gt;Medical image restoration (MedIR) aims to recover high-quality images from degraded inputs, yet faces unique challenges from physics-driven degradations and multi-modal task interference. While existing all-in-one methods handle natural image degradations well, they struggle with medical scenarios due to limited degradation perception and suboptimal multi-task optimization. In response, we introduce DaPT, a Degradation-aware Prompted Transformer, which integrates dynamic prompt learning and modular expert mining for unified MedIR. First, DaPT introduces spatially compact prompts with optimal transport regularization, amplifying inter-prompt differences to capture diverse degradation patterns. Second, a mixture of experts dynamically routes inputs to specialized modules via prompt guidance, resolving task conflicts while reducing computational overhead. The synergy of prompt learning and expert mining further enables robust restoration across multi-modal medical data, offering a practical solution for clinical imaging. Extensive experiments across multiple modalities (MRI, CT, PET) and diverse degradations, covering both in-distribution and out-of-distribution scenarios, demonstrate that DaPT consistently outperforms state-of-the-art methods and generalizes reliably to unseen settings, underscoring its robustness, effectiveness, and clinical practicality. The source code will be released at https://github.com/weijinbao1998/DaPT.&lt;/p&gt;</content:encoded></item><item><title>A Multi-Scale Attention Transformer for Martian Dust Devil Detection in Remote Sensing Imagery</title><link>https://doi.org/10.1109/jstars.2025.3646776</link><guid>10.1109/jstars.2025.3646776</guid><pubDate>Mon, 22 Dec 2025 18:41:48 +0000</pubDate><dc:creator>Gan Liu</dc:creator><dc:creator>Linlin Shi</dc:creator><dc:creator>Jialong Lai</dc:creator><dc:creator>Feifei Cui</dc:creator><dc:creator>Xiaoping Zhang</dc:creator><dc:creator>Yi Xu</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3646776</prism:doi><description>Detecting Martian dust devils remains a challenging task due to the scarcity of high-quality annotated data, significant variations in scale, blurred boundaries, and complex surface textures. To address these difficulties, we construct a cross-regional, manually annotated benchmark dataset named MDD-Human and propose a novel Transformer-based detection network, MDT (Mars Dust Devil Detection Transformer). The model adopts FasterNet as its backbone to ensure a balance between computational efficiency and feature extraction capability. A key innovation lies in the multi-scale attention fusion module, which incorporates hierarchical fusion strategies and hybrid attention mechanisms to effectively enhance the representation of dust devil features under diverse Martian terrains. In addition, we introduce a shape-aware localization loss function, SMIoU (Shape-Augmented Minimum Point Distance IoU), which improves geometric sensitivity by integrating corner distance constraints and structural shape priors. Experimental results on the MDD-Human dataset demonstrate that MDT achieves 92.7% Precision, 90.8% Recall, 92.4% mAP@50, and 91.8% F1-score, outperforming several classical and state-of-the-art detectors. Further tests on unseen THEMIS and CRISM datasets confirm the model's strong cross-source generalization, highlighting its robustness and applicability in diverse Martian imaging scenarios.
Published: 2025-12-22T18:41:48+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gan Liu; Linlin Shi; Jialong Lai; Feifei Cui; Xiaoping Zhang; Yi Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3646776"&gt;10.1109/jstars.2025.3646776&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;Detecting Martian dust devils remains a challenging task due to the scarcity of high-quality annotated data, significant variations in scale, blurred boundaries, and complex surface textures. To address these difficulties, we construct a cross-regional, manually annotated benchmark dataset named MDD-Human and propose a novel Transformer-based detection network, MDT (Mars Dust Devil Detection Transformer). The model adopts FasterNet as its backbone to ensure a balance between computational efficiency and feature extraction capability. A key innovation lies in the multi-scale attention fusion module, which incorporates hierarchical fusion strategies and hybrid attention mechanisms to effectively enhance the representation of dust devil features under diverse Martian terrains. In addition, we introduce a shape-aware localization loss function, SMIoU (Shape-Augmented Minimum Point Distance IoU), which improves geometric sensitivity by integrating corner distance constraints and structural shape priors. Experimental results on the MDD-Human dataset demonstrate that MDT achieves 92.7% Precision, 90.8% Recall, 92.4% mAP@50, and 91.8% F1-score, outperforming several classical and state-of-the-art detectors. Further tests on unseen THEMIS and CRISM datasets confirm the model&amp;#x27;s strong cross-source generalization, highlighting its robustness and applicability in diverse Martian imaging scenarios.&lt;/p&gt;</content:encoded></item><item><title>ViTextVQA: A Large-Scale Visual Question Answering Dataset and a Novel Multimodal Feature Fusion Method for Vietnamese Text Comprehension in Images</title><link>https://doi.org/10.1016/j.eswa.2025.130839</link><guid>10.1016/j.eswa.2025.130839</guid><pubDate>Mon, 22 Dec 2025 16:20:52 +0000</pubDate><dc:creator>Quan Van Nguyen</dc:creator><dc:creator>Dan Quang Tran</dc:creator><dc:creator>Huy Quang Pham</dc:creator><dc:creator>Thang Kien-Bao Nguyen</dc:creator><dc:creator>Nghia Hieu Nguyen</dc:creator><dc:creator>Kiet Van Nguyen</dc:creator><dc:creator>Ngan Luu-Thuy Nguyen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130839</prism:doi><description>Visual Question Answering (VQA) is a challenging task that requires the joint understanding of natural language and visual content. While early research primarily focused on recognizing objects and scene context, it often overlooked scene text-an essential source of explicit semantic information. This paper introduces ViTextVQA ( Vi etnamese Text -based V isual Q uestion A nswering), the first large-scale Vietnamese dataset specializing in text-based VQA. The dataset contains over 16,000 images and over 50,000 question-answer pairs. To tackle this task efficiently, ViTextBLIP-2 (Vietnamese Text-based Bootstrapped Language-Image Model via Fine-tuning) is proposed, a novel multimodal feature fusion method designed to optimize Vietnamese text-based VQA. Experiments with state-of-the-art models highlight the importance of token ordering in OCR text for answer generation, leading to significant performance improvements. The ViTextVQA dataset is publicly available for research purposes 1 .
Published: 2025-12-22T16:20:52+00:00
Venue: Expert Systems with Applications
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Quan Van Nguyen; Dan Quang Tran; Huy Quang Pham; Thang Kien-Bao Nguyen; Nghia Hieu Nguyen; Kiet Van Nguyen; Ngan Luu-Thuy Nguyen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130839"&gt;10.1016/j.eswa.2025.130839&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Visual Question Answering (VQA) is a challenging task that requires the joint understanding of natural language and visual content. While early research primarily focused on recognizing objects and scene context, it often overlooked scene text-an essential source of explicit semantic information. This paper introduces ViTextVQA ( Vi etnamese Text -based V isual Q uestion A nswering), the first large-scale Vietnamese dataset specializing in text-based VQA. The dataset contains over 16,000 images and over 50,000 question-answer pairs. To tackle this task efficiently, ViTextBLIP-2 (Vietnamese Text-based Bootstrapped Language-Image Model via Fine-tuning) is proposed, a novel multimodal feature fusion method designed to optimize Vietnamese text-based VQA. Experiments with state-of-the-art models highlight the importance of token ordering in OCR text for answer generation, leading to significant performance improvements. The ViTextVQA dataset is publicly available for research purposes 1 .&lt;/p&gt;</content:encoded></item><item><title>Denoising Attribution Maps through Gradient Analysis of Critical Parameters</title><link>https://doi.org/10.1016/j.patcog.2025.112954</link><guid>10.1016/j.patcog.2025.112954</guid><pubDate>Sun, 21 Dec 2025 22:39:38 +0000</pubDate><dc:creator>Seungeon Lee</dc:creator><dc:creator>Heejin Bin</dc:creator><dc:creator>Sungwon Han</dc:creator><dc:creator>Meeyoung Cha</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112954</prism:doi><description>Many post-hoc explainable feature attribution techniques analyze gradient propagation and understand decisions of deep learning models. However, conventional gradient analysis used to generate such maps can be noisy, potentially compromising the reliability of explanations. In this work, we introduce a robust method for computing feature attributions by identifying critical parameters and refining gradient propagation through these parameters. This method reduces the impact of non-critical parameters, mitigating the effect of feature leakage and randomized initialization, which introduce noise in attribution maps. We implemented this concept as an add-on module, called CriGrad , and evaluated its efficacy using three benchmarks and seven explainable models. Our results show that focusing on critical parameters improves explanability in 93% of the cases, demonstrating its effectiveness and improved reliability.
Published: 2025-12-21T22:39:38+00:00
Venue: Pattern Recognition
Score: 0.765 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Seungeon Lee; Heejin Bin; Sungwon Han; Meeyoung Cha&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112954"&gt;10.1016/j.patcog.2025.112954&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (consider)&lt;/p&gt;
&lt;p&gt;Many post-hoc explainable feature attribution techniques analyze gradient propagation and understand decisions of deep learning models. However, conventional gradient analysis used to generate such maps can be noisy, potentially compromising the reliability of explanations. In this work, we introduce a robust method for computing feature attributions by identifying critical parameters and refining gradient propagation through these parameters. This method reduces the impact of non-critical parameters, mitigating the effect of feature leakage and randomized initialization, which introduce noise in attribution maps. We implemented this concept as an add-on module, called CriGrad , and evaluated its efficacy using three benchmarks and seven explainable models. Our results show that focusing on critical parameters improves explanability in 93% of the cases, demonstrating its effectiveness and improved reliability.&lt;/p&gt;</content:encoded></item><item><title>ZERO-PDE: Zero-shot Joint Denoising and Enhancement for Infrared Polarization Images</title><link>https://doi.org/10.1016/j.knosys.2025.115158</link><guid>10.1016/j.knosys.2025.115158</guid><pubDate>Mon, 22 Dec 2025 23:57:49 +0000</pubDate><dc:creator>Xian-Meng Meng</dc:creator><dc:creator>Yun-You Hu</dc:creator><dc:creator>Dan-Dan Zhi</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115158</prism:doi><description>Infrared polarization imaging suffers from challenges such as nonuniform noise, weak image features, and low contrast. Existing deep learning-based methods for infrared polarization image processing are constrained by limited dataset sizes, resulting in insufficient denoising and enhancement generalization in real-world scenarios. We overcome this limitation by developing a novel framework for jointly denoising and enhancing infrared polarization images without requiring external data. Specifically, we construct training sample pairs based on the principle of nonlocal self-similarity, which involves directly generating a sample database from a single noisy image. We then design a lightweight convolutional denoising module with skip connections and introduce a mutual-constrained noise loss function to enable efficient noise-to-noise learning. Moreover, we develop a polarization feature enhancement module based on Gaussian-weighted contextual implicit neural representation, combined with a no-reference loss function to guide both global and pixel-level enhancement of weak-polarization features. Extensive quantitative and qualitative experiments validate the effectiveness of the proposed method. Compared with state-of-the-art methods, our proposed approach achieves superior results in both execution efficiency and image quality. Our code is available at https://github.com/huyunyou/ZEROPDENet .
Published: 2025-12-22T23:57:49+00:00
Venue: Knowledge-Based Systems
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xian-Meng Meng; Yun-You Hu; Dan-Dan Zhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115158"&gt;10.1016/j.knosys.2025.115158&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Infrared polarization imaging suffers from challenges such as nonuniform noise, weak image features, and low contrast. Existing deep learning-based methods for infrared polarization image processing are constrained by limited dataset sizes, resulting in insufficient denoising and enhancement generalization in real-world scenarios. We overcome this limitation by developing a novel framework for jointly denoising and enhancing infrared polarization images without requiring external data. Specifically, we construct training sample pairs based on the principle of nonlocal self-similarity, which involves directly generating a sample database from a single noisy image. We then design a lightweight convolutional denoising module with skip connections and introduce a mutual-constrained noise loss function to enable efficient noise-to-noise learning. Moreover, we develop a polarization feature enhancement module based on Gaussian-weighted contextual implicit neural representation, combined with a no-reference loss function to guide both global and pixel-level enhancement of weak-polarization features. Extensive quantitative and qualitative experiments validate the effectiveness of the proposed method. Compared with state-of-the-art methods, our proposed approach achieves superior results in both execution efficiency and image quality. Our code is available at https://github.com/huyunyou/ZEROPDENet .&lt;/p&gt;</content:encoded></item><item><title>SiamDiff: A Diffusion-Driven Siamese Network for Scale-Aware Anti-UAV Tracking</title><link>https://doi.org/10.3390/rs18010018</link><guid>10.3390/rs18010018</guid><pubDate>Mon, 22 Dec 2025 08:35:27 +0000</pubDate><dc:creator>Hong Zhang</dc:creator><dc:creator>Yihao Kuang</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Lingyu Jin</dc:creator><dc:creator>Chang Xu</dc:creator><dc:creator>Yanda Meng</dc:creator><dc:creator>Bo Huang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010018</prism:doi><description>Unmanned aerial vehicle (UAV) tracking faces significant challenges due to small targets and background interference. Traditional anchor-based tracking algorithms require designing numerous proposals to capture such tiny targets, which entails unacceptable computational overhead. On the other hand, anchor-free tracking methods struggle to adapt to target scale variations, resulting in suboptimal tracking accuracy in anti-UAV tracking scenarios. To address these limitations, we pioneer the integration of diffusion models into visual tracking, proposing SiamDiff—a scale-adaptive anti-UAV framework. We reformulate the tracking task as a bounding box prediction problem, where a diffusion model is leveraged to generate scale-adaptive proposals. Furthermore, we propose a Learnable Mask Module (LMM) and a Frequency Channel Fusion Module (FCFM) to enhance discriminative feature extraction for small targets. Additionally, we design a Scale-Aware Diffusion Strategy (SADA) to boost robustness to scale variations. Experimental results on the Anti-UAV and Anti-UAV410 benchmarks demonstrate the effectiveness of our approach, achieving a State Accuracy (SA) of 71.90% and 67.03%, respectively, outperforming the baseline and other trackers. Moreover, our method shows superior adaptability to scale variations, confirming its robustness in complex anti-UAV tracking scenarios.
Published: 2025-12-22T08:35:27+00:00
Venue: Remote Sensing
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hong Zhang; Yihao Kuang; Jiaqi Wang; Lingyu Jin; Chang Xu; Yanda Meng; Bo Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010018"&gt;10.3390/rs18010018&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Unmanned aerial vehicle (UAV) tracking faces significant challenges due to small targets and background interference. Traditional anchor-based tracking algorithms require designing numerous proposals to capture such tiny targets, which entails unacceptable computational overhead. On the other hand, anchor-free tracking methods struggle to adapt to target scale variations, resulting in suboptimal tracking accuracy in anti-UAV tracking scenarios. To address these limitations, we pioneer the integration of diffusion models into visual tracking, proposing SiamDiff—a scale-adaptive anti-UAV framework. We reformulate the tracking task as a bounding box prediction problem, where a diffusion model is leveraged to generate scale-adaptive proposals. Furthermore, we propose a Learnable Mask Module (LMM) and a Frequency Channel Fusion Module (FCFM) to enhance discriminative feature extraction for small targets. Additionally, we design a Scale-Aware Diffusion Strategy (SADA) to boost robustness to scale variations. Experimental results on the Anti-UAV and Anti-UAV410 benchmarks demonstrate the effectiveness of our approach, achieving a State Accuracy (SA) of 71.90% and 67.03%, respectively, outperforming the baseline and other trackers. Moreover, our method shows superior adaptability to scale variations, confirming its robustness in complex anti-UAV tracking scenarios.&lt;/p&gt;</content:encoded></item></channel></rss>