<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 29 Nov 2025 02:33:24 +0000</lastBuildDate><item><title>DSTransNet: Dynamic Feature Selection Network with Feature Enhancement and Multi-Attention for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638454</link><guid>10.1109/tgrs.2025.3638454</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Infrared small target detection (IRSTD) has significantly benefited from UNet-based neural models in recent years. However, current methodologies face challenges in achieving optimal compromise between missed detections and false alarms. To overcome this limitation, we rethink the role of each structural component within UNet-based architectures applied for IRSTD. Accordingly, we conceptualize the UNet’s encoder as specializing in feature extraction, the skip connections in feature selection, and the decoder in fusion-based reconstruction. Building upon these conceptualizations, we propose the DSTransNet. Within the feature extraction stage, the edge shape receptive field (ESR) module enhances edge and shape feature extraction and expands the receptive field via multiple convolutional branches, thereby reducing missed detections. At the feature selection stage, the reliable dynamic selection filtering (RDSF) module employs dynamic feature selection, leveraging encoder-based self-attention and decoder-based cross-attention of the Transformer to suppress background features resembling small targets and mitigate false alarms. During the feature fusion-based reconstruction stage, the cross-attention of spaces and channels (CSCE) module emphasizes small target features via spatial and channel cross-attention, reconstructing more accurate multi-scale detection masks. Extensive experiments on the SIRST, NUDT-SIRST, and SIRST-Aug datasets demonstrate that the proposed DSTransNet method outperforms state-of-the-art IRSTD approaches. The code is available at https://github.com/RuiminHuang/DSTransNet.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.856 (must_read)</description></item><item><title>Revisiting Attention Mechanisms and Transformer Networks for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638738</link><guid>10.1109/tgrs.2025.3638738</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Infrared small target detection plays a vital role in applications such as military surveillance and space observation. Because infrared small targets exhibit weak and indistinct features, they are often submerged within cluttered backgrounds. Capturing long-range dependencies and extracting discriminative differences between targets and backgrounds are key to improving detection accuracy. However, existing attention mechanisms and transformer network architectures have limitations, which impair the ability to explore context and capture long-distance deep dependencies. In addition, the existing methods rarely consider cross-scale feature fusion. To this end, we propose a novel network specifically for infrared small target detection called RAM-TransNet. Firstly, the whole network adopts a U-Net similar multi-attention nested pure transformer structure to learn and extract longer-distance and deeper target features. Secondly, we develop a new contextual transformer block with a dual attention structure. This contextual transformer block allows us to capture dynamic and static contextual information by making the most of the contextual information between input keys in 2D feature maps. As a result, this enhances visual features’ exploration and capture capacity. In addition, we have created a new multi-hierarchical cross-scale interaction module to aid different transformer layer features in performing multi-scale information fusion and enhancing feature perception. Finally, We evaluated our proposed method using comprehensive evaluation metrics on three public datasets. Extensive experimental results demonstrate that the proposed method is highly effective and significantly outperforms state-of-the-art methods. Moreover, the noise immunity experiment indicates that our proposed method has better noise tolerance.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.851 (must_read)</description></item><item><title>InterpIoU: Robust bounding box regression loss within an interpolation-based IoU framework</title><link>https://doi.org/10.1016/j.neucom.2025.132230</link><guid>10.1016/j.neucom.2025.132230</guid><pubDate>Fri, 28 Nov 2025 07:48:13 +0000</pubDate><category>Neurocomputing</category><description>Bounding box regression (BBR) is central to object detection, where regression loss plays a key role in precise localization. Existing IoU-based losses often rely on handcrafted geometric penalties to provide gradients in non-overlapping cases and improve localization. However, these geometric penalties are inherently sensitive to box geometry, producing unstable gradients in extreme cases and a subtle misalignment with the IoU objective, which harms small objects detection and yields undesired converge behaviors such as bounding box enlargement. To address these limitations, we introduce InterpIoU, an interpolation-based IoU optimization framework that rethinks BBR beyond handcrafted penalties. By bridging predictions and ground truth with interpolated boxes, InterpIoU supplies meaningful gradients in non-overlapping cases while ensuring consistent alignment with the BBR objective. Crucially, our findings challenge the convention of using geometric penalties, demonstrating they are often unnecessary and suboptimal. Building on InterpIoU, we propose Dynamic InterpIoU, which adjusts interpolation coefficients based on IoU values, adapting to diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC demonstrate that our methods consistently outperform state-of-the-art IoU-based losses across detection frameworks, including YOLOv8 and DINO, with notable improvements for small object detection.
Published: 2025-11-28T07:48:13+00:00
Venue: Neurocomputing
Score: 0.833 (must_read)</description></item><item><title>S4DR-Net: Self-Supervised Spatial-Spectral Distance Reconstruction Network for Multispectral Point Cloud Classification</title><link>https://doi.org/10.1109/tgrs.2025.3638606</link><guid>10.1109/tgrs.2025.3638606</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Multispectral LiDAR point clouds are valuable in remote sensing for their spatial-spectral consistency, yet their high acquisition and annotation costs pose significant challenges. To mitigate this, self-supervised learning has emerged as a promising solution, reducing reliance on annotated data while improving model generalization. However, existing self-supervised frameworks for point clouds often overlook the complexity of ground object distribution in large-scale remote sensing scenarios and fail to leverage the spectral information inherent in multispectral point clouds. In this paper, we introduce the Self-Supervised Spatial-Spectral Distance Reconstruction Network (S4DR-Net), a novel self-supervised pre-training network designed for multispectral point cloud classification. Serving as the key component of the network, the Spatial-Spectral Distance Prediction module (S-SDP) effectively addresses these limitations by reconstructing the distance relationships between voxel blocks in three-dimensional Euclidean as well as spectral spaces. By jointly considering spatial and spectral distances, S-SDP enables the network to learn a unified representation that captures the intrinsic spatial-spectral consistency of multispectral point clouds. This design allows S4DR-Net to generate low-dimensional feature representations in a self-supervised manner, without reliance on manual annotations. We conducted experiments and evaluated on two real-world multispectral point cloud datasets. The results demonstrate that S4DR-Net consistently outperforms existing self-supervised pre-training methods, achieving superior accuracy and generalization compared with current state-of-the-art approaches. The code will be released at https://github.com/KustTeamWQW/S4DR-Net.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.824 (must_read)</description></item><item><title>Lightweight Local–Global Dual-Path Feature Fusion Network for Infrared Small Target Image Super-Resolution and Enhancement</title><link>https://doi.org/10.1109/tgrs.2025.3638791</link><guid>10.1109/tgrs.2025.3638791</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Infrared imaging is widely used in remote sensing and military target recognition due to its strong resistance to interference in complex environments. However, imaging mechanisms and hardware limitations cause infrared images to have low-resolution, sparse textures, and significant background noise, which severely restrict the detection of small targets such as low-altitude drones and weak thermal emitters. To overcome these limitations, we propose a lightweight Local–Global Dual-Path Feature Fusion Network (LDFF-Net) that enhances the resolution and quality of infrared images, providing high-quality inputs for subsequent detection tasks. The network includes a Small Target Feature Recognition Module (STFRM) composed of three key components. The Enhanced High Frequency Perception Module (EHFPM) strengthens high-frequency details of small targets while suppressing noise, enabling robust local feature extraction. The State-Space Model (SSM) captures long-range dependencies with linear complexity and models semantic relationships between targets and background to compensate for the limited receptive field of local features. The Adaptive Feature Fusion Unit (AFFU) combines local and global features adaptively to improve the saliency of small targets. During training, we introduce a realistic degradation process based on visible-light images to generate training samples that include complex degradation patterns and noise, which enhances the model’s robustness and generalization. Evaluation on the ARCHIVE and SIRST datasets demonstrates that LDFF-Net outperforms existing state-of-the-art methods across eight widely used full-reference and no-reference metrics, including PSNR, LPIPS, FID, and NIQE. This result confirms the model’s effectiveness in enhancing both the super-resolution and detection performance of infrared small target images. The code and pretrained model weights are publicly available at https://github.com/98Hao/LDFF-Net.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.823 (must_read)</description></item><item><title>What neuroscience can tell AI about learning in continuously changing environments</title><link>https://doi.org/10.1038/s42256-025-01146-z</link><guid>10.1038/s42256-025-01146-z</guid><pubDate>Fri, 28 Nov 2025 10:02:42 +0000</pubDate><category>Nature Machine Intelligence</category><description>Modern artificial intelligence (AI) models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task and then deployed with fixed parameters. Their training is costly, slow and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioural policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal’s behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioural tasks with shifting rules, reward probabilities or outcomes. We outline an agenda for how the links between neuroscience and AI could be tightened, thus supporting the transfer of ideas and findings between both areas and contributing to the evolving field of NeuroAI. Durstewitz et al. explore what artificial intelligence can learn from the brain’s ability to adjust quickly to changing environments. By linking neuroscience studies of flexible behaviour with advances in continual and in-context learning, this Perspective outlines ways to strengthen the exchange of ideas between the two fields and advance NeuroAI.
Published: 2025-11-28T10:02:42+00:00
Venue: Nature Machine Intelligence
Score: 0.819 (must_read)</description></item><item><title>GLANet: Global-Local Adaptive Network for Efficient Rotated Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638781</link><guid>10.1109/tgrs.2025.3638781</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Rotated object detection plays a crucial role in various visual perception tasks such as aerial photography, remote sensing imagery, and low-altitude unmanned aerial vehicle (UAV) imagery. However, targets in both high-altitude remote sensing images and low-altitude UAV images often exhibit significant scale variations, diverse orientations, and dense spatial distributions, posing formidable challenges to detection algorithms in terms of accuracy and real-time performance. To address these issues, this paper proposes a Global-Local Adaptive Network for Efficient Rotated Object Detection (GLANet), designed to enhance detection precision and efficiency in complex scenarios. GLANet incorporates a lightweight backbone network, Revisiting Mobile CNN From ViT Perspective (RepViT), which balances inference efficiency with an improved capability to represent directional structural features of objects. During feature fusion, we introduce the Geometry-Enhanced Attention guided Rotated Feature Pyramid Network (GEAR-FPN), which jointly models global semantic context and local detailed features, thereby strengthening detection performance for small-scale and densely packed targets. In the detection head, we present a Dynamic Lightweight Geometric-Aware Head (DLGA-Head) alongside a Dynamic Lightweight Global Attention (Dynamic LWGA) mechanism to strengthen the representation of target orientation and boundary information. The effectiveness of the proposed method is validated on both the DOTA and CODrone datasets. GLANet achieves an mAP of 78.12% on DOTA with competitive, near-state-of-the-art accuracy and significantly higher computational efficiency than other top-performing models. Specifically, it contains only 8.64M parameters and 35.69 GFLOPs, ensuring real-time inference while maintaining high precision. On the CODrone dataset, it further delivers improved detection performance while maintaining superior efficiency compared with existing approaches.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.817 (must_read)</description></item><item><title>Dual-Pathway Feature Separation and Gated Fusion Network for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3638739</link><guid>10.1109/tgrs.2025.3638739</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Infrared Small Target Detection (IRSTD) plays a vital role in Infrared Search and Tracking (IRST), enabling intelligent systems to accurately detect dim and small targets within cluttered thermal environments. However, most existing deep learning approaches for IRSTD employ a unified-pathway architecture that conflates saliency and edge information within a shared representation space. This limitation causes feature entanglement, hindering the network’s capacity to accurately separate and represent global saliency and fine-grained edge contours. To overcome these challenges, we propose LoveNet, a dual-pathway network architecture that explicitly separates feature learning into two specialized branches. The first is a multi-scale saliency learning branch designed to extract comprehensive structural and contrast information, capturing the global context of targets. The second is a fixed-scale edge learning branch aimed at preserving spatial details and enhancing the precision of edge contour delineation. To integrate the heterogeneous features extracted by two branches, a gated feature fusion mechanism is proposed to adaptively combine saliency and edge representations based on their spatial and semantic relevance. Furthermore, to provide robust and comprehensive supervision, a hybrid supervision strategy is designed to guide the learning process of hierarchical feature representations. Experiments on the NUDT-SIRST, IRSTD-1k, and SIRST datasets demonstrate that LoveNet consistently achieves the best segmentation performance compared to the state-of-the-art methods, while maintaining a lightweight structure suitable for real-time applications.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.814 (must_read)</description></item><item><title>Momentum-Enhanced Dual-Prototype Learning Framework for Robust Few-Shot Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tgrs.2025.3638757</link><guid>10.1109/tgrs.2025.3638757</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Prototypical network-based few-shot learning (FSL) has demonstrated promising performance for hyperspectral image (HSI) classification tasks under scarce sample conditions. However, existing prototype-based FSL methods suffer from data distribution variations among randomly sampled tasks, leading to unstable class prototype representations and weak cross-task generalization with limited samples. To address this issue, we propose a momentum-enhanced dual-prototype learning (MEDPL) framework for robust few-shot HSI classification. Firstly, a momentum-updated prototype mechanism constructs an iteratively optimized prototype memory bank. It obtains accumulated prototypes by exponentially decaying weighted fusion of historical and current prototypes, significantly suppressing noise from randomly sampled data and class center shifts caused by distribution bias. Simultaneously, a class-conditioned perturbation-augmentation strategy is introduced. It generates adaptive noise perturbations for support set features based on learnable covariance matrices to obtain enhanced prototypes, thereby improving the generalization representation capability of class prototypes across tasks. Secondly, a dual-prototype metric learning framework is designed, jointly utilizing accumulated prototypes and enhanced prototypes to synergistically enhance the model’s classification stability and cross-task generalization, thus significantly improving the robustness of few-shot classification. Experimental results demonstrate that MEDPL outperforms other few-shot hyperspectral image classification methods. Our source code is available at https://github.com/hejinrong/MEDPL.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.801 (must_read)</description></item><item><title>RDAM: Domain Adaptation under Small and Class-Imbalanced Samples</title><link>https://doi.org/10.1016/j.knosys.2025.114909</link><guid>10.1016/j.knosys.2025.114909</guid><pubDate>Fri, 28 Nov 2025 16:05:05 +0000</pubDate><category>Knowledge-Based Systems</category><description>Domain Adaptation aims to transfer knowledge from a labeled source domain to an unlabeled target domain by aligning their feature distributions. Although notable advancements have been achieved, domain adaptation remains challenging for scenarios involving small sample size and class-imbalanced datasets, where limited and imbalanced data hinder effective domain alignment. To address this issue, we propose RDAM (Feature Regeneration Domain Adaptation with Manifold Maintenance Loss), a novel framework that enhances cross domain generalization under data scarcity and class imbalance. Our key idea is to perform feature regeneration in the source domain to balance feature quantities, thereby constructing a more comprehensive and inclusive source representation whose spatial distribution effectively covers that of the target domain. To further preserve local geometric structures and improve alignment of ambiguous or boundary samples, we introduce the manifold maintenance loss, which enforces consistency in neighborhood relationships across domains. We evaluate RDAM on four time series datasets and four image domain adaptation benchmarks. Extensive experiments show that our method achieves superior accuracy and robustness across diverse modalities and imbalance settings.
Published: 2025-11-28T16:05:05+00:00
Venue: Knowledge-Based Systems
Score: 0.799 (must_read)</description></item><item><title>Spatially-Aware Adaptive Diffusion: Unifying Low-Resolution Image Fusion and Super-Resolution</title><link>https://doi.org/10.1109/tcsvt.2025.3638425</link><guid>10.1109/tcsvt.2025.3638425</guid><pubDate>Fri, 28 Nov 2025 18:44:43 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>Low-resolution visible-infrared image fusion and super-resolution (LRVIF) are critical for enhancing image quality in low-resolution scenarios, yet limited information in the input images often constrains performance. To address these challenges, we propose SaDiff, a spatially-aware adaptive diffusion model that introduces diffusion processes into LRVIF for the first time, representing a major breakthrough in the field. Leveraging the generative capabilities of diffusion models, our approach unifies and enhances image fusion and super-resolution within a cohesive framework. A key component of SaDiff is the Spatial Residual Adaptation Block, which extends the diffusion process by dynamically adapting feature representations to spatial variations in the local regions of the input images. This module maximally preserves crucial information from the input images, such as texture details and contrast, while effectively suppressing noise, ensuring robust and context-aware feature refinement. Then we further propose Direct Diffusion Synthesis, a novel mechanism that utilizes noise predictions during diffusion to generate fused images, enabling joint training of the fusion and super-resolution networks. Additionally, a Cross-Feature Fusion Module integrates texture and contrast details, producing super-resolution fused images with improved clarity and structural integrity. Extensive experiments show that SaDiff achieves state-of-the-art performance, offering a robust and unified solution to infrared-visible image fusion and super-resolution. The code for the proposed method will be made available at https://github.com/guobaoxiao/SaDiff.
Published: 2025-11-28T18:44:43+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.799 (must_read)</description></item><item><title>ERDDCI: Exact Reversible Diffusion via Dual-Chain Inversion for High-Quality Image Editing</title><link>https://doi.org/10.1109/tcsvt.2025.3638406</link><guid>10.1109/tcsvt.2025.3638406</guid><pubDate>Fri, 28 Nov 2025 18:44:43 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>Diffusion models (DMs) have been successfully applied to real image editing. These models typically invert images into latent noise vectors during the inversion process, and then edit them during the inference process. However, DMs often rely on the local linearization assumption, which assumes that the noise injected during the inversion process approximates the noise removed during the inference process. While DMs efficiently generate images under this assumption, it also accumulates errors during the diffusion process due to the assumption, ultimately negatively impacting the quality of real image reconstruction and editing. To address this issue, we propose a novel ERDDCI (Exact Reversible Diffusion via Dual-Chain Inversion). ERDDCI uses the new Dual-Chain Inversion (DCI) for joint inference to derive an exact reversible diffusion process. Using DCI, our method avoids the cumbersome optimization process in existing inversion approaches and achieves high-quality image editing. Additionally, to accommodate image operations under high guidance scales, we introduce a dynamic control strategy that enables more refined image reconstruction and editing. Our experiments demonstrate that ERDDCI significantly outperforms state-of-the-art methods in a 50-step diffusion process. It achieves rapid and precise image reconstruction with SSIM of 0.999 and LPIPS of 0.001, and delivers competitive results in image editing. The source code is available at: https://github.com/daii-y/ERDDCI.
Published: 2025-11-28T18:44:43+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.794 (must_read)</description></item><item><title>FA-ReID: Feature alignment with adversarial learning for clothing-changing person re-identification</title><link>https://doi.org/10.1016/j.neucom.2025.132237</link><guid>10.1016/j.neucom.2025.132237</guid><pubDate>Fri, 28 Nov 2025 16:41:11 +0000</pubDate><category>Neurocomputing</category><description>Person re-identification (Re-ID) under clothing-changing conditions remains a significant challenge due to the drastic intra-identity appearance variations caused by different outfits. Existing methods often struggle to effectively disentangle stable, identity-discriminative biometric cues from dominant yet unreliable clothing features. To address this, we propose a novel Synergistic Feature Alignment framework for Re-ID, termed FA-ReID, designed to learn clothing-invariant identity representations. Our framework uniquely integrates three key components. First, we employ a dual-stream architecture that processes both standard RGB images and their corresponding multi-channel human parsing maps, which provide explicit, clothing-agnostic structural information about body parts. Second, we introduce a novel Multi-Scale Cross-Attention (MSCA) module that facilitates a synergistic fusion of these two modalities. The MSCA module enables the structural features from the parsing stream to guide the RGB stream’s focus towards identity-salient regions while suppressing misleading clothing textures. Third, we design a “clothing-stripping” adversarial training strategy. By maximizing the classification entropy on clothing-related features via a gradient reversal layer, this strategy forces the feature extractor to learn a representation that is invariant to clothing styles, effectively disentangling identity cues from appearance. Extensive experiments on three challenging clothing-changing benchmarks—PRCC, LTCC, and VC-Clothes—demonstrate that FA-ReID achieves state-of-the-art performance, validating the effectiveness of our synergistic approach to learning robust and discriminative features for clothing-changing person Re-ID. The code will be made publicly available.
Published: 2025-11-28T16:41:11+00:00
Venue: Neurocomputing
Score: 0.779 (must_read)</description></item><item><title>Adaptive Momentum Mixture-of-Experts for Continual Visual Question Answering</title><link>https://doi.org/10.1109/tcsvt.2025.3637303</link><guid>10.1109/tcsvt.2025.3637303</guid><pubDate>Fri, 28 Nov 2025 18:44:43 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>Multimodal large language models (MLLMs) have attracted considerable attention for their impressive capabilities in understanding and generating visual-language content, particularly in tasks such as visual question answering (VQA). However, the rapid evolution of knowledge in real-world applications poses challenges for these models: offline training becomes increasingly costly, and exposure to non-stationary data streams often leads to catastrophic forgetting. In this paper, we propose CL-MoE+, a dual-momentum Mixture-of-Experts (MoE) framework based on MLLMs for continual VQA. Our method integrates continual learning into MLLMs to leverage the rich commonsense knowledge embedded in large language models.We introduce a Dual-Router MoE (RMoE) module that selects both global and local experts through task-level and instance-level routers, enabling robust and context-aware expert allocation. Furthermore, we design an adaptive Momentum MoE (MMoE) to update experts’ parameters based on the knowledge drift degree and their relevance to specific tasks, thereby facilitating knowledge integration without forgetting. Extensive experiments on a 10-task split of the VQA v2 benchmark demonstrate that CL-MoE+ achieves state-of-the-art performance, validating its effectiveness in both retaining historical knowledge and learning new information in the continual learning setting.
Published: 2025-11-28T18:44:43+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.779 (must_read)</description></item><item><title>GeoFormer: Boosting Object Distinguishing and Prompt Understanding for Cross-View Object Geo-Localization</title><link>https://doi.org/10.1109/tgrs.2025.3638946</link><guid>10.1109/tgrs.2025.3638946</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Cross-view object geo-localization (CVOGL) determines the geographic location of an object on the satellite view reference image. The object is indicated by a point prompt in a ground- or drone-view query image. Despite wide applications, the current CVOGL method still suffers from limited performance, and the reason for this limitation remains unknown. In this work, we analyze CVOGL and find two primary challenges that hinder its performance, i.e., 1) point prompt understanding and 2) similar-appearance object distinguishing. Therefore, we propose a novel end-to-end framework called object geo-localization transformer (GeoFormer). Specifically, we leverage the knowledge of the segment anything model (SAM) through two settings for accurate point prompt understanding, i.e., using SAM during training and inference (GeoFormer) or using SAM only in training via knowledge distillation (GeoFormer-KD). Additionally, we devise an information aggregation module (IAM) to leverage local and global perception for similar-appearance object distinguishing. Except for the public dataset, we manually annotated a new dataset that contains 1642 image pairs for further comparison. Experiments show that our method significantly outperforms the previous work. Notably, the tiny versions of our method (GeoFormer-t and GeoFormer-t-KD) maintain state-of-the-art performance while substantially reducing parameter costs. Our code and the new dataset will be made available at https://github.com/Temperature-ai/GeoFormer.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.777 (must_read)</description></item><item><title>Speckle2Self: Learning Self-Supervised Despeckling with Attention Mechanism for SAR Images</title><link>https://doi.org/10.3390/rs17233840</link><guid>10.3390/rs17233840</guid><pubDate>Fri, 28 Nov 2025 08:26:57 +0000</pubDate><category>Remote Sensing</category><description>Despite the in-depth understanding of the synthetic aperture-radar (SAR) speckle and its characteristics, despeckling remains an open issue far from being solved. Deep-learning methods with supervised training have made great progress. However, reliable reference images are inconveniently accessible or even non-existent. In this paper, we propose an end-to-end self-supervised method named Speckle2Self for SAR image despeckling, which learns mapping from noisy input to clean output using only the input noisy image itself for training. We formulate the image despeckling as a masked pixel-estimation problem, where a set of masks is carefully designed. The masked pixel values are predicted by the queries of complementary masks indicating the positions of masked pixels through an attention mechanism. Transformer architecture is employed as the network backbone. In addition, a novel loss function is also derived based on the statistics of SAR images, and meanwhile, image downsampling is used to provide guarantees on the white noise assumption involved in our Speckle2Self. We compare the proposed Speckle2Self with reference methods on both synthetic and real images. Experimental results demonstrate that the proposed Speckle2Self achieves comparable despeckling performance with supervised methods, suppressing noise while maintaining structural details. Even compared with self-supervised methods, the proposed Speckle2Self still has significant advantages in SAR image-despeckling metrics.
Published: 2025-11-28T08:26:57+00:00
Venue: Remote Sensing
Score: 0.772 (must_read)</description></item><item><title>CAIFNet: Capturing Amplitude-Invariant Features for Remote Sensing Image Change Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3638748</link><guid>10.1109/tcsvt.2025.3638748</guid><pubDate>Fri, 28 Nov 2025 18:44:43 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>Change detection (CD) is a critical task in remote sensing (RS) image analysis. Recent deep learning networks for CD focus on identifying changes after mining the features of bi-temporal images separately. However, light differences in bi-temporal images lead to the networks extracting different features from the identical objects, which may cause pseudo-changes. From the Fourier transform perspective, an image can be decomposed into amplitude and phase, where the amplitude contains most of the light information and the phase is relevant to structure information. Therefore, amplitude-invariant features of the identical objects in different light conditions are roughly the same, which are pivotal to identify real and fake changes between bi-temporal images. In this article, we propose a capturing amplitude-invariant features network (CAIFNet), which reduces dependence on amplitude and captures diverse amplitude-invariant features. Firstly, we build an amplitude pre-processing module (APM) to provide diverse processed images by randomly mixing the amplitudes of the input images with the amplitudes of the reference images and keeping the phases of the input images constant. Secondly, a quadruple-stream encoder is proposed to capture amplitude-invariant features. Specifically, it is forced to learn and capture amplitude-invariant local details and amplitude-invariant contextual semantics based on the diverse processed images under CD task-oriented constraint, both reciprocate each other to become more accurate by local attention guide strategy (LGS). Moreover, a difference enhancement module (DEM) is designed in the quadruple-stream encoder to enhance the difference features. Thirdly, a bi-stream decoder decodes the captured amplitude-invariant features in main and boundary difference perspectives, enhancing main body and boundary details of the objects in the change maps, respectively. Finally, a spatial embedded module (SEM) allows the main and boundary difference fea...
Published: 2025-11-28T18:44:43+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.769 (must_read)</description></item><item><title>Dual-Branch ViT with Polarimetric Spatial Profile for PolSAR Image Classification</title><link>https://doi.org/10.1109/lgrs.2025.3638414</link><guid>10.1109/lgrs.2025.3638414</guid><pubDate>Fri, 28 Nov 2025 18:45:26 +0000</pubDate><category>IEEE Geoscience and Remote Sensing Letters</category><description>Polarimetric Synthetic Aperture Radar (PolSAR) image classification remains challenging due to complex scattering mechanisms, speckle noise, and the difficulty of selecting the most informative features from the multitude of derivable polarimetric representations. To address these challenges, we propose a novel two-step methodology for PolSAR image classification that utilizes an advanced feature extraction technique while leveraging the strengths of a vision transformer(ViT) architecture. In the first step, advanced feature extraction techniques are explored to capture multiscale spatial scattering information and preserve crucial structural information of the PolSAR image while effectively mitigating the noise present on it. In the second step, a dual-branch ViT (DB-ViT) is proposed that simultaneously processes both the original polarimetric features and the extracted spatial features, enabling effective information fusion through a local window attention transformer (LWAT). Extensive experiments on the Flevoland AIRSAR and the San-Francisco RADARSAT-2 benchmark datasets demonstrated that our approach consistently outperforms state-of-the-art methods, achieving the highest overall accuracies of 99.50% and 99.51%, respectively.
Published: 2025-11-28T18:45:26+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.768 (must_read)</description></item><item><title>Boosting Geometric Invariants for Discriminative Forensics of Large-Scale Generated Visual Content</title><link>https://doi.org/10.1109/tip.2025.3633580</link><guid>10.1109/tip.2025.3633580</guid><pubDate>Fri, 28 Nov 2025 18:45:10 +0000</pubDate><category>IEEE Transactions on Image Processing</category><description>Generative artificial intelligence has shown great success in visual content synthesis such that humans struggle to distinguish between real and synthesized images. Forensic research seeks to reveal artifacts in such generated images, ensuring information security or improving generation capability. In this regard, the robustness and interpretability are important for the trustworthy purpose of forensic tasks. However, typical forensic models and their underlying data representations rely on empirical learning algorithms, which cannot effectively handle the high robustness and interpretability requirements beyond experience. As an effective solution, we extend the classical geometric invariants to the forensic research of large-scale generated images. Invariants are handcrafted representations with robust and interpretable geometric principles. However, their discriminability is far from the large scale of today’s forensic tasks. We boost the discriminability by extending the classical invariants to the hierarchical architecture of convolutional neural networks. The resulting overcompleteness allows for an automatic selection of task-discriminative features, while retaining the previous advantages of robustness and interpretability. From generative adversarial networks to diffusion models, the forensic with our boosted invariants demonstrates state-of-the-art discriminability against large-scale content diversity. It also exhibits high efficiency on training examples, intrinsic invariance to geometric variations, and better interpretability of the forensic process.
Published: 2025-11-28T18:45:10+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.766 (must_read)</description></item><item><title>Anchor Graph-guided Dual-target Alignment Network for Incomplete Multi-View Clustering</title><link>https://doi.org/10.1016/j.inffus.2025.104011</link><guid>10.1016/j.inffus.2025.104011</guid><pubDate>Fri, 28 Nov 2025 07:51:19 +0000</pubDate><category>Information Fusion</category><description>Deep learning has shown good performance in handling incomplete multi-view clustering tasks. However, existing deep clustering methods face challenges such as noise interference introduced by completion, insufficient information fusion, and task decoupling when dealing with high missing rates and multi-view data, limiting clustering performance. To address these issues, this paper proposes an Anchor Graph-guided Dual-target Alignment Network for Incomplete Multi-View Clustering (AGDAN). We design a GCN-attention collaborative encoder to extract view-specific features through GCN and achieve implicit completion of missing views through cross-view attention gating. We also develop a bidirectional manifold-clustering distribution alignment mechanism, where the forward alignment stage enforces geometric consistency of the manifold structure via KL divergence, and the backward alignment stage utilizes anchor graphs and spectral clustering to optimize the global clustering target distribution. Extensive experimental results show that our method performs excellently even with high missing rates and demonstrates strong adaptability in large-scale clustering tasks.
Published: 2025-11-28T07:51:19+00:00
Venue: Information Fusion
Score: 0.766 (must_read)</description></item><item><title>Efficient redundancy reduction for open-vocabulary semantic segmentation</title><link>https://doi.org/10.1016/j.neucom.2025.132229</link><guid>10.1016/j.neucom.2025.132229</guid><pubDate>Fri, 28 Nov 2025 00:52:30 +0000</pubDate><category>Neurocomputing</category><description>Open-vocabulary semantic segmentation (OVSS) is an open-world task that aims to assign each pixel within an image to a specific class defined by arbitrary text descriptions. While large-scale vision-language models have shown remarkable open-vocabulary capabilities, their image-level pretraining limits effectiveness on pixel-wise dense prediction tasks like OVSS. Recent cost-based methods narrow this granularity gap by constructing pixel-text cost maps and refining them via cost aggregation mechanisms. Despite achieving promising performance, these approaches suffer from high computational costs and long inference latency. In this paper, we identify two major sources of redundancy in the cost-based OVSS framework: redundant information introduced during cost maps construction and inefficient sequence modeling in cost aggregation. To address these issues, we propose ERR-Seg, an efficient architecture that incorporates Redundancy-Reduced Hierarchical Cost maps (RRHC) and Redundancy-Reduced Cost Aggregation (RRCA). Specifically, RRHC reduces redundant class channels by customizing a compact class vocabulary for each image and integrates hierarchical cost maps to enrich semantic representation. RRCA alleviates computational burden by performing both spatial-level and class-level sequence reduction before aggregation. Overall, ERR-Seg results in a lightweight structure for OVSS, characterized by substantial memory and computational savings without compromising accuracy. Compared to previous state-of-the-art methods on the ADE20K-847 benchmark, ERR-Seg improves performance by " role="presentation"&gt; while achieving a 3.1 " role="presentation"&gt; speedup. The project page is available at https://lchen1019.github.io/ERR-Seg .
Published: 2025-11-28T00:52:30+00:00
Venue: Neurocomputing
Score: 0.765 (must_read)</description></item><item><title>AugGen: a generative framework for continual generalized zero-shot learning</title><link>https://doi.org/10.1016/j.neucom.2025.132187</link><guid>10.1016/j.neucom.2025.132187</guid><pubDate>Fri, 28 Nov 2025 07:48:02 +0000</pubDate><category>Neurocomputing</category><description>Continual Generalized Zero-shot Learning (CGZSL) aims to address the challenges of knowledge forgetting and class imbalance that are common in Generalized Zero-shot Learning (GZSL) when applied in continuous learning scenarios. In these settings, the model must adapt to new tasks while retaining past knowledge. Generation-based methods synthesize visual features for unseen classes, which helps bridge the gap between semantic and visual spaces. However, the generative frameworks in CGZSL still suffer from limited feature diversity for unseen classes, which impairs their generalization ability. Existing generative models struggle to fully capture the diversity distribution of classes, often leading to biased feature representations. To address these problems, we propose AugGen, a novel framework combining Diversified Attribute Enhancement (DAE) and Cross-Task Feature Distillation (TFD). DAE uses prompt attributes from large language models to enrich the unseen class attributes and combines them with the original attributes through an adaptive feature fusion network to enhance diversity. TFD mitigates catastrophic forgetting by aligning feature spaces across tasks to preserve knowledge. Experimental results across five CGZSL benchmarks demonstrate the effectiveness of the AugGen approach.
Published: 2025-11-28T07:48:02+00:00
Venue: Neurocomputing
Score: 0.765 (must_read)</description></item><item><title>DBRNet: Dual-Branch Cascaded Recursive Network with Semi-Supervised Learning for Image Desnowing</title><link>https://doi.org/10.1109/tcsvt.2025.3638589</link><guid>10.1109/tcsvt.2025.3638589</guid><pubDate>Fri, 28 Nov 2025 18:44:43 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>Image desnowing is an important task in image enhancement and restoration. It aims to reduce the impact of snowfall on image quality and downstream vision tasks. Although recent methods perform well on synthetic datasets, their robustness in real-world scenarios is limited due to the complex and diverse appearance of snow particles. To address this issue, we propose DBRNet, a semi-supervised image desnowing network that improves generalization in real conditions. DBRNet adopts a cascaded recursive structure, using multiple recursive modules to progressively refine features. During training, a dual-branch strategy combining supervised and unsupervised learning is designed, utilizing synthetic paired data for labelled supervision while introducing regularization constraints using real unlabeled images. Dual-branch design is also embedded in each recursive module, enabling explicit separation and joint learning of snow removal and background recovery. Extensive experimental validation demonstrates that this method not only outperforms existing mainstream snow removal algorithms across multiple public snow removal datasets but also exhibits exceptional snow removal performance and robust generalization capabilities in real-world snowy images.
Published: 2025-11-28T18:44:43+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.762 (must_read)</description></item><item><title>MFE-STN: A Versatile Front-End Module for SAR Deception Jamming False Target Recognition</title><link>https://doi.org/10.3390/rs17233848</link><guid>10.3390/rs17233848</guid><pubDate>Fri, 28 Nov 2025 08:26:57 +0000</pubDate><category>Remote Sensing</category><description>Advanced deception countermeasures now enable adversaries to inject false targets into synthetic-aperture-radar (SAR) imagery, generating electromagnetic signatures virtually indistinguishable from genuine targets, thus destroying the separability essential for conventional recognition algorithms. To address this problem, we propose a versatile front-end Multi-Feature Extraction and Spatial Transformation Network (MFE-STN), specifically designed for the task of discriminating between true targets and deceptive false targets created by SAR jamming, which can be seamlessly integrated with existing CNN backbones without architecture modification. MFE-STN integrates three complementary operations: (i) wavelet decomposition to extract the overall geometric features and scattering distribution of the target, (ii) a manifold transformation module for non-linear alignment of heterogeneous feature spaces, and (iii) a lightweight deformable spatial transformer that compensates for local geometric distortions introduced by deceptive jamming. By analyzing seven typical parameter-mismatch effects, we construct a simulated dataset containing six representative classes—four known classes and two unseen classes. Experimental results demonstrate that inserting MFE-STN boosts the average F1-score of known targets by 12.19% and significantly improves identification accuracy for unseen targets. This confirms the module’s capability to capture discriminative signatures to distinguish genuine targets from deceptive ones while exhibiting strong cross-domain generalization capabilities.
Published: 2025-11-28T08:26:57+00:00
Venue: Remote Sensing
Score: 0.760 (must_read)</description></item><item><title>Boosting SAR ATR Trustworthiness via ERFA: An Electromagnetic Reconstruction Feature Alignment Method</title><link>https://doi.org/10.3390/rs17233855</link><guid>10.3390/rs17233855</guid><pubDate>Fri, 28 Nov 2025 12:14:16 +0000</pubDate><category>Remote Sensing</category><description>Deep learning-based synthetic aperture radar (SAR) automatic target recognition (ATR) methods exhibit a tendency to overfit specific operating conditions—such as radar parameters and background clutter—which frequently leads to high sensitivity against variations in these conditions. A novel electromagnetic reconstruction feature alignment (ERFA) method is proposed in this paper, which integrates electromagnetic reconstruction with feature alignment into a fully convolutional network, forming the ERFA-FVGGNet. The ERFA-FVGGNet comprises three modules: electromagnetic reconstruction using our proposed orthogonal matching pursuit with image-domain cropping-optimization (OMP-IC) algorithm for efficient, high-precision attributed scattering center (ASC) reconstruction and extraction; the designed FVGGNet combining transfer learning with a lightweight fully convolutional network to enhance feature extraction and generalization; and feature alignment employing a dual-loss to suppress background clutter while improving robustness and interpretability. Experimental results demonstrate that ERFA-FVGGNet boosts trustworthiness by enhancing robustness, generalization and interpretability.
Published: 2025-11-28T12:14:16+00:00
Venue: Remote Sensing
Score: 0.759 (must_read)</description></item><item><title>RISC: A Robust Interference Self-Cancellation Method for Spaceborne SAR Systems</title><link>https://doi.org/10.1109/tgrs.2025.3638796</link><guid>10.1109/tgrs.2025.3638796</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Due to the wide bandwidth and large observation area, spaceborne synthetic aperture radar (SAR) is easily interfered by other electromagnetic signals, namely radio frequency interference (RFI), which can severely degrade SAR image quality and submerge useful information. Classic parametric and non-parametric methods are used to suppress RFI as much as possible without considering the useful information. To protect the real reflected signals, semi-parametric methods, based on low-rank and sparse recovery, are proposed to mitigate RFI, but they suffer from the singular-value over-shrinking problem when RFI is not strictly low-rank, resulting in interference residues in the recovered scene. Hence, in this paper, a robust interference self-cancellation (RISC) method is proposed to protect raw ground scenes from polluted data with better extraction accuracy of RFI. The proposed model can adaptively fit in different scenes and backgrounds by using adjacent homologous interference (HI) subregions instead of the low-rank constraints, thus better protecting SAR scenes and enhancing its robustness. Based on the alternating direction method of multipliers (ADMM), we design two different solvers for the proposed optimization model, and both are tested on four different scenes of Sentinel-1 measured data. All experiments demonstrate that the proposed method has excellent performance in RFI mitigation and SAR image recovery.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.758 (must_read)</description></item><item><title>Solving Ill-Posed Regions in High Dynamic Range Reconstruction with Uncertainty-Aware Diffusion Models</title><link>https://doi.org/10.1109/tcsvt.2025.3638674</link><guid>10.1109/tcsvt.2025.3638674</guid><pubDate>Fri, 28 Nov 2025 18:44:43 +0000</pubDate><category>IEEE Transactions on Circuits and Systems for Video Technology</category><description>Learning-based approaches have achieved promising progress in High Dynamic Range (HDR) image reconstruction, particularly in ghost removal. However, they often struggle in ill-posed regions, such as areas with occlusion or saturation, where insufficient or unreliable information leads to persistent residual ghosting artifacts and structural distortions. In this paper, we present UA-Diff, an uncertainty-aware diffusion framework designed to generate visually coherent, ghost-free HDR images. Specifically, our approach introduces an Uncertainty Generation Module (UGM) that estimates pixel-wise reconstruction confidence via a probabilistic Laplacian loss, producing an uncertainty map that explicitly highlights challenging ill-posed regions. To address these regions effectively, we develop an Uncertainty-Aware Diffusion Module (UADM) that operates selectively on the average-coefficient component of a 2D discrete wavelet transform, where dominant artifacts tend to concentrate. This enables reduced computational overhead while preserving high-quality details. Moreover, we propose an Uncertainty-Guided Sampling (UGS) strategy that leverages the uncertainty map to guide the denoising process, ensuring faithful reconstruction in reliable regions and targeted refinement in uncertain areas. Extensive experiments on three public HDR benchmarks demonstrate that UA-Diff surpasses state-of-the-art methods both quantitatively and perceptually, especially in challenging ill-posed scenarios.
Published: 2025-11-28T18:44:43+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.757 (must_read)</description></item><item><title>Harnessing transformer-based attention mechanisms for multi-scale feature fusion in medical image segmentation</title><link>https://doi.org/10.1007/s10489-025-07009-9</link><guid>10.1007/s10489-025-07009-9</guid><pubDate>Fri, 28 Nov 2025 04:31:19 +0000</pubDate><category>Applied Intelligence</category><description>Extensive research has focused on developing efficient and accurate solutions for the critical task of medical image segmentation. Approaches have evolved from hand-crafted pipelines to deep convolutional neural networks (CNNs), and more recently, to Transformer-based hybrid models. Among these, hierarchical encoder–decoder architectures remain prevalent, where skip connections are crucial in transmitting spatial features from encoders to decoders. However, conventional skip connections operate in static and passive modes, and cannot adaptively fuse multi-scale features or capture semantic relationships across resolution levels. Although attention-based skip enhancements have been proposed, they are often architecture-specific and difficult to generalize. In this study, we propose TransSkip, a novel transformer-based skip connection module that embeds both self-attention and cross-attention directly within the skip path. This enables dynamic and learnable multi-scale feature fusion across encoder levels, transforming skip connections into active semantic reasoning pathways. TransSkip is modular and architecture agnostic, supporting seamless integration with a range of hierarchical encoder–decoder networks, including CNN-based, Transformer-based, and hybrid models. Extensive experiments across 2D and 3D datasets (BUSI, Kvasir-SEG, MSD-Spleen) and multiple network backbones (U-Net, TransUNet, TransAttUNet, MCV-UNet) demonstrate that TransSkip consistently improves segmentation accuracy, with statistically significant gains and minimal parameter overhead. These results highlight the potential of TransSkip as a generalizable and efficient architectural enhancement for medical image segmentation.
Published: 2025-11-28T04:31:19+00:00
Venue: Applied Intelligence
Score: 0.755 (must_read)</description></item><item><title>Advanced Semi-Supervised Hyperspectral Change Detection via Cross-Temporal Spectral Reconstruction</title><link>https://doi.org/10.1109/tgrs.2025.3638953</link><guid>10.1109/tgrs.2025.3638953</guid><pubDate>Fri, 28 Nov 2025 18:43:01 +0000</pubDate><category>IEEE Transactions on Geoscience and Remote Sensing</category><description>Hyperspectral change detection (HSI-CD) serves as an advanced technique for monitoring surface changes by leveraging spectral differences between multi-temporal hyperspectral images. Ideally, the spectral differences between unchanged areas across multi-temporal images should be minimal. However, in practical applications, due to various factors such as imaging conditions and seasonal variations, spectral differences in unchanged areas can still be significant, which may lead to an increased likelihood of false alarms in CD. In this article, we propose a distinct reconstruction-guided approach for semi-supervised HSI-CD, termed CSR-Net, which can precisely distinguish the changed or unchanged areas with significant spectral differences. Specifically, CSR-Net encodes input HSI to reconstruct the corrected spectral sequences, mitigating errors caused by spectral differences in unchanged areas due to factors such as seasonal changes and variations in land cover characteristics. Subsequently, the reconstructed spectral sequences undergo change analysis to detect changes. Moreover, we propose an innovative semi-supervised HSI-CD loss, which weights the mean squared error loss based on binary CD results. This loss introduces a constraint that enables the model to learn spectral-temporal relationships from unlabeled data, thereby facilitating the reconstruction of corrected spectral sequences and reducing false alarms. Extensive experiments conducted on four benchmark HSI-CD datasets demonstrate that the proposed CSR-Net and loss function consistently outperform existing state-of-the-art methods, even with a very limited number of labeled samples.
Published: 2025-11-28T18:43:01+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.754 (must_read)</description></item><item><title>Optical and SAR Cross-modal Hallucination Collaborative Learning for Remote Sensing Missing-modality Building Footprint Extraction</title><link>https://doi.org/10.1109/jstars.2025.3638382</link><guid>10.1109/jstars.2025.3638382</guid><pubDate>Fri, 28 Nov 2025 18:43:16 +0000</pubDate><category>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</category><description>Building footprint extraction using optical and synthetic aperture radar (SAR) images enables all-weather capability and significantly boosts performance. In practical scenarios, optical data may not be available, leading to the missing-modality challenge. To overcome this challenge, advanced methods employ mainstream knowledge distillation approaches with hallucination network schemes to improve performance. However, under complex SAR backgrounds, current hallucination network-based methods suffer from cross-modal information transfer failure between optical and hallucination models. To solve this problem, this study introduces a cross-modal hallucination collaborative learning (CMH-CL) method, consisting of two components: modality-share information alignment learning (MSAL) and multimodal fusion information alignment learning (MFAL). The MSAL method facilitates cross-modal knowledge transfer between optical and hallucination encoders, thereby enabling the hallucination model to effectively mimic the missing optical modality. The MFAL method aligns semantic information between OPT-SAR and HAL-SAR fusion heads to strengthen their semantic consistency, thereby improving HAL-SAR fusion performance. By combining MSAL and MFAL, the CMH-CL method collaboratively alleviates cross-modal transfer failure problem between the optical and hallucination models, thereby improving performance in missing-modality building footprint extraction. Extensive experimental results obtained on a public dataset demonstrate the effectiveness of the proposed CMH-CL.
Published: 2025-11-28T18:43:16+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.754 (must_read)</description></item></channel></rss>