<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Fri, 26 Dec 2025 02:43:10 +0000</lastBuildDate><item><title>You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts</title><link>https://doi.org/10.1109/tpami.2025.3647857</link><guid>10.1109/tpami.2025.3647857</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Hongkun Dou</dc:creator><dc:creator>Zeyu Li</dc:creator><dc:creator>Xingyu Jiang</dc:creator><dc:creator>Hongjue Li</dc:creator><dc:creator>Lijun Yang</dc:creator><dc:creator>Wen Yao</dc:creator><dc:creator>Yue Deng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647857</prism:doi><description>Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by \sim 90\% \sim 90\% while maintaining superior performance.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.848 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongkun Dou; Zeyu Li; Xingyu Jiang; Hongjue Li; Lijun Yang; Wen Yao; Yue Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647857"&gt;10.1109/tpami.2025.3647857&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.848 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by \sim 90\% \sim 90\% while maintaining superior performance.&lt;/p&gt;</content:encoded></item><item><title>Toward Unified Expertise: Learning a Single Vision Model from Diverse Perception</title><link>https://doi.org/10.1109/tpami.2025.3647880</link><guid>10.1109/tpami.2025.3647880</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Zitian Chen</dc:creator><dc:creator>Mingyu Ding</dc:creator><dc:creator>Yikang Shen</dc:creator><dc:creator>Erik Learned-Miller</dc:creator><dc:creator>Chuang Gan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647880</prism:doi><description>Multi-task learning (MTL) presents greater optimization challenges than single-task learning (STL) due to conflicting gradients across tasks. While parameter sharing promotes cooperation among related tasks, many tasks require specialized representations. To balance cooperation and specialization, we propose Mod-Squad [1], a modular transformer-based model composed of a “squad” of experts. Each task activates a sparse subset of experts through a differentiable matching process, guided by a novel mutual information-based loss. This modular structure avoids full backbone sharing and scales effectively with the number of tasks and dataset size. In this extended version, we generalize Mod-Squad to support multi-dataset pre-training, enabling joint learning across disjoint, single-task datasets (e.g., ImageNet, COCO, ADE20K). This is achieved via a new formulation of the mutual information loss that unifies learning across heterogeneous sources. More importantly, while most prior work in large models has focused on efficiency, few have explored adjustable efficiency. In this study, we further evaluate the model's generalization to downstream tasks and introduce a set of efficient adaptation techniques that leverage Mod-Squad's modularity for flexible finetuning-enabling dynamic adjustment of model size, parameter count, and computational cost. Additionally, we present a hybrid adaptation scheme that combines these techniques to achieve favorable performance-efficiency trade-offs. In summary, Mod-Squad provides a robust foundation for sparse modular models that can learn from diverse supervision and datasets. Its emergent modularity enables strong generalization, decomposition into high-performing components, and rapid, resource-efficient adaptation for downstream applications.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.834 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zitian Chen; Mingyu Ding; Yikang Shen; Erik Learned-Miller; Chuang Gan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647880"&gt;10.1109/tpami.2025.3647880&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.834 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-task learning (MTL) presents greater optimization challenges than single-task learning (STL) due to conflicting gradients across tasks. While parameter sharing promotes cooperation among related tasks, many tasks require specialized representations. To balance cooperation and specialization, we propose Mod-Squad [1], a modular transformer-based model composed of a “squad” of experts. Each task activates a sparse subset of experts through a differentiable matching process, guided by a novel mutual information-based loss. This modular structure avoids full backbone sharing and scales effectively with the number of tasks and dataset size. In this extended version, we generalize Mod-Squad to support multi-dataset pre-training, enabling joint learning across disjoint, single-task datasets (e.g., ImageNet, COCO, ADE20K). This is achieved via a new formulation of the mutual information loss that unifies learning across heterogeneous sources. More importantly, while most prior work in large models has focused on efficiency, few have explored adjustable efficiency. In this study, we further evaluate the model&amp;#x27;s generalization to downstream tasks and introduce a set of efficient adaptation techniques that leverage Mod-Squad&amp;#x27;s modularity for flexible finetuning-enabling dynamic adjustment of model size, parameter count, and computational cost. Additionally, we present a hybrid adaptation scheme that combines these techniques to achieve favorable performance-efficiency trade-offs. In summary, Mod-Squad provides a robust foundation for sparse modular models that can learn from diverse supervision and datasets. Its emergent modularity enables strong generalization, decomposition into high-performing components, and rapid, resource-efficient adaptation for downstream applications.&lt;/p&gt;</content:encoded></item><item><title>Causal HyperPrompter: A Framework for Unbiased Hyperspectral Camouflaged Object Tracking</title><link>https://doi.org/10.1109/tpami.2025.3648020</link><guid>10.1109/tpami.2025.3648020</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Hanzheng Wang</dc:creator><dc:creator>Wei Li</dc:creator><dc:creator>Xiang-Gen Xia</dc:creator><dc:creator>Qian Du</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648020</prism:doi><description>Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model's sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hanzheng Wang; Wei Li; Xiang-Gen Xia; Qian Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648020"&gt;10.1109/tpami.2025.3648020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral camouflaged object tracking remains a significant challenge due to the high similarity between objects and replicas in texture and color. Despite recent progress, the bias present in the tracker and the embedding token hinders the model training. Specifically, most methods rely on false-color three-channel images to fine-tune RGB-based trackers. However, it introduces a confounding effect within the RGB domain, potentially leading to harmful biases that misguide the model toward spurious correlations while neglecting the critical spectral discrimination inherent in hyperspectral images. Furthermore, current token-type embedding methods overlook the key correlations between templates and searches, ultimately confusing correlation and impairing tracking performance. To address these challenges, this paper proposes a new unbiased tracking framework named Causal HyperPrompter. It first introduces a structural causal model to disentangle and control exclusive causal factors during tracking, and incorporates a counterfactual intervention strategy to eliminate confounding variables and mitigate the bias inherited from RGB-based models. In addition, we present a novel token-type embedding module that integrates local spectral angle modeling to enhance the semantic link between template and search tokens, thereby improving the model&amp;#x27;s sensitivity to object localization. Lastly, to overcome the difficulty of manually initializing the bounding box and addressing data scarcity, we introduce a large-scale hyperspectral camouflaged object detection and tracking dataset, BihoT-130 k, consisting of 130750 annotated frames across various camouflage scenes. Extensive experiments on multiple large-scale datasets illustrate the effectiveness of our proposed methods.&lt;/p&gt;</content:encoded></item><item><title>Self-supervised representation learning for cloud detection using Sentinel-2 images</title><link>https://doi.org/10.1016/j.rse.2025.115205</link><guid>10.1016/j.rse.2025.115205</guid><pubDate>Thu, 25 Dec 2025 04:44:10 +0000</pubDate><dc:creator>Yawogan Jean Eudes Gbodjo</dc:creator><dc:creator>Lloyd Haydn Hughes</dc:creator><dc:creator>Matthieu Molinier</dc:creator><dc:creator>Devis Tuia</dc:creator><dc:creator>Jun Li</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115205</prism:doi><description>The unavoidable presence of clouds and their shadows in optical satellite imagery hinders the true spectral response of the Earth’s underlying surface. Accurate cloud and cloud shadow detection is therefore a crucial preprocessing step for optical satellite images and any downstream analysis. Various methods have been developed to address this critical task and can be broadly categorized into physical rule-based methods and learning based methods. In recent years, machine learning based methods, particularly deep learning frameworks, have proven to outperform physical rule-based models. However, these approaches are mostly fully supervised and require a large amount of pixel-level annotations whose acquisition is costly and time consuming. In this work, we propose to address cloud and cloud shadow detection in optical satellite images using self-supervised representation learning, a machine learning paradigm that focuses on extracting relevant representations from unlabeled data, which can then be used as an effective starting point to fine-tune models with few labeled data in a supervised fashion. These approaches have been shown to perform competitively with fully supervised methods without the requirement of large annotation datasets. Specifically, we assessed two self-supervised representation learning methods that use different philosophies about self-supervision: Momentum Contrast (MoCo), based on contrastive learning and DeepCluster, based on clustering. Using two publicly available Sentinel-2 cloud datasets, namely WHUS2–CD+ and CloudSEN12, we show that MoCo and DeepCluster, trained with only 25 % of the annotated data, can perform better than physical rule-based methods such as FMask and Sen2Cor, weakly supervised methods and even several fully supervised methods. These results highlight the strong applicability of self-supervised representation learning methods to the task of cloud and cloud shadow detection with self-supervised pretraining leading to fine-tuned models that outperform industry standards and achieve near state-of-the-art performance with a fraction of the data.
Published: 2025-12-25T04:44:10+00:00
Venue: Remote Sensing of Environment
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yawogan Jean Eudes Gbodjo; Lloyd Haydn Hughes; Matthieu Molinier; Devis Tuia; Jun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115205"&gt;10.1016/j.rse.2025.115205&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;The unavoidable presence of clouds and their shadows in optical satellite imagery hinders the true spectral response of the Earth’s underlying surface. Accurate cloud and cloud shadow detection is therefore a crucial preprocessing step for optical satellite images and any downstream analysis. Various methods have been developed to address this critical task and can be broadly categorized into physical rule-based methods and learning based methods. In recent years, machine learning based methods, particularly deep learning frameworks, have proven to outperform physical rule-based models. However, these approaches are mostly fully supervised and require a large amount of pixel-level annotations whose acquisition is costly and time consuming. In this work, we propose to address cloud and cloud shadow detection in optical satellite images using self-supervised representation learning, a machine learning paradigm that focuses on extracting relevant representations from unlabeled data, which can then be used as an effective starting point to fine-tune models with few labeled data in a supervised fashion. These approaches have been shown to perform competitively with fully supervised methods without the requirement of large annotation datasets. Specifically, we assessed two self-supervised representation learning methods that use different philosophies about self-supervision: Momentum Contrast (MoCo), based on contrastive learning and DeepCluster, based on clustering. Using two publicly available Sentinel-2 cloud datasets, namely WHUS2–CD+ and CloudSEN12, we show that MoCo and DeepCluster, trained with only 25 % of the annotated data, can perform better than physical rule-based methods such as FMask and Sen2Cor, weakly supervised methods and even several fully supervised methods. These results highlight the strong applicability of self-supervised representation learning methods to the task of cloud and cloud shadow detection with self-supervised pretraining leading to fine-tuned models that outperform industry standards and achieve near state-of-the-art performance with a fraction of the data.&lt;/p&gt;</content:encoded></item><item><title>Semantic-Anchored Cross-Modal Distillation Framework With Foundation Models for SAR Ship Recognition</title><link>https://doi.org/10.1109/taes.2025.3648374</link><guid>10.1109/taes.2025.3648374</guid><pubDate>Thu, 25 Dec 2025 18:28:22 +0000</pubDate><dc:creator>Xuemeng Hui</dc:creator><dc:creator>Zhunga Liu</dc:creator><dc:creator>Shun Yao</dc:creator><dc:creator>Meiqin Liu</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3648374</prism:doi><description>Synthetic Aperture Radar (SAR) offers day-and-night capability for ship recognition, but its scattering mechanism results in limited textural and spectral detail compared to optical imagery, hindering fine-grained semantic interpretation and recognition. Existing cross-modal transfer methods mainly align features or pixels between SAR and optical imagery, yet they fail to guarantee semantic consistency across modalities. To address this, we propose a Semantic-anchored Cross-modal Distillation Framework (SCDF) with foundation models. SCDF introduces textual semantic descriptors for each ship category as semantic anchors to ensure cross-modal semantic consistency, while incorporating scattering topology maps into SAR images, thus enabling effective transfer without sacrificing modality-specific discriminability. Within this framework, a language foundation model encodes semantic anchors into text embeddings as class references, formulating ship recognition as aligning visual features with semantic anchors. To enhance the alignment between SAR features and anchors, a scattering-aware student model integrates scattering topology maps with SAR imagery, emphasizing key ship structures. This alignment is further guided by a vision foundation model acting as the optical teacher, which provides reliable optical-semantic similarity for distillation. Instead of simply transferring labels or features, the semantic-anchored distillation transfers semantic discriminability from the optical domain to SAR while preserving SAR-specific scattering topology features. Extensive experiments on the FUSAR-Ship dataset and fine-grained optical datasets (FGSC-23 and FGSCR-42) demonstrate that SCDF effectively bridges SAR and optical imagery and enhances SAR ship recognition.
Published: 2025-12-25T18:28:22+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.816 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuemeng Hui; Zhunga Liu; Shun Yao; Meiqin Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3648374"&gt;10.1109/taes.2025.3648374&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.816 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) offers day-and-night capability for ship recognition, but its scattering mechanism results in limited textural and spectral detail compared to optical imagery, hindering fine-grained semantic interpretation and recognition. Existing cross-modal transfer methods mainly align features or pixels between SAR and optical imagery, yet they fail to guarantee semantic consistency across modalities. To address this, we propose a Semantic-anchored Cross-modal Distillation Framework (SCDF) with foundation models. SCDF introduces textual semantic descriptors for each ship category as semantic anchors to ensure cross-modal semantic consistency, while incorporating scattering topology maps into SAR images, thus enabling effective transfer without sacrificing modality-specific discriminability. Within this framework, a language foundation model encodes semantic anchors into text embeddings as class references, formulating ship recognition as aligning visual features with semantic anchors. To enhance the alignment between SAR features and anchors, a scattering-aware student model integrates scattering topology maps with SAR imagery, emphasizing key ship structures. This alignment is further guided by a vision foundation model acting as the optical teacher, which provides reliable optical-semantic similarity for distillation. Instead of simply transferring labels or features, the semantic-anchored distillation transfers semantic discriminability from the optical domain to SAR while preserving SAR-specific scattering topology features. Extensive experiments on the FUSAR-Ship dataset and fine-grained optical datasets (FGSC-23 and FGSCR-42) demonstrate that SCDF effectively bridges SAR and optical imagery and enhances SAR ship recognition.&lt;/p&gt;</content:encoded></item><item><title>Diffusion-Driven Self-Supervised Learning for Shape Reconstruction and Pose Estimation</title><link>https://doi.org/10.1109/tpami.2025.3647855</link><guid>10.1109/tpami.2025.3647855</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Jingtao Sun</dc:creator><dc:creator>Yaonan Wang</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Chao Ding</dc:creator><dc:creator>Mike Zheng Shou</dc:creator><dc:creator>Ajmal Mian</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647855</prism:doi><description>Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingtao Sun; Yaonan Wang; Mingtao Feng; Chao Ding; Mike Zheng Shou; Ajmal Mian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647855"&gt;10.1109/tpami.2025.3647855&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Fully-supervised category-level pose estimation aims to determine the 6-DoF poses of unseen instances from known categories, requiring expensive manual labeling costs. Recently, various self-supervised category-level pose estimation methods have been proposed to reduce the requirement of the annotated datasets. However, most methods rely on synthetic data or 3D CAD model, and they are typically limited to addressing single-object pose problems without considering multi-objective tasks or shape reconstruction. To overcome these challenges and limitations, we introduce a diffusion-driven self-supervised network for multi-object shape reconstruction and categorical pose estimation, only leveraging the shape priors. Specifically, to capture the SE(3)-equivariant pose features and 3D scale-invariant shape information, we present a Prior-Aware Pyramid 3D Point Transformer. This module adopts a point convolutional layer with radial-kernels for pose-aware learning and a 3D scale-invariant graph convolution layer for object-level shape representation. Furthermore, we introduce a Pretrain-to-Refine Self-Supervised Training Paradigm to train our network. It enables proposed network to capture the associations between shape priors and observations, addressing the challenge of intra-class shape variations by utilising the diffusion mechanism. Extensive experiments conducted on four public datasets and a self-built dataset demonstrate that our method significantly outperforms state-of-the-art self-supervised category-level baselines and even surpasses some fully-supervised instance-level and category-level methods.&lt;/p&gt;</content:encoded></item><item><title>Edge-Semantic Synergy Network with Edge-Aware Attention for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2025.3648149</link><guid>10.1109/tgrs.2025.3648149</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Maoyong Li</dc:creator><dc:creator>Yingying Gao</dc:creator><dc:creator>Xuedong Guo</dc:creator><dc:creator>Zhixiang Chen</dc:creator><dc:creator>Lei Deng</dc:creator><dc:creator>Mingli Dong</dc:creator><dc:creator>Lianqing Zhu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648149</prism:doi><description>Infrared small target detection (IRSTD) plays a crucial role in applications such as autonomous driving, environmental monitoring, and industrial inspection. However, the small size of targets, their blurred edges, and complex backgrounds often result in significant limitations of existing methods in terms of feature synergy and edge information utilization. To address these challenges, this paper proposes an Edge-Semantic Synergy Network with Edge-Aware Attention (ESSNet) for Infrared Small Target Detection. ESSNet substantially enhances detection performance by explicitly reinforcing edge information and optimizing multi-level feature interactions. Specifically, the Edge-Semantic Synergy Module (ESSM) leverages edge details at the lowest level and semantic information at the highest level to achieve long-range level modulation, thereby enhancing the synergy between edges and semantics. Additionally, ESSM integrates a Multi-Scale Edge-Aware Attention (MSEA), which embeds edge features into the attention mechanism through explicit edge supervision, effectively improving the accuracy of boundary detection. Furthermore, the Multi-Level Feature Fusion (MLFF) module is introduced to mitigate semantic loss during the decoding process via a layer-wise guidance mechanism, preserving the structural integrity of detected targets. Experiments conducted on the SIRST, NUDT-SIRST, and IRSTD-1K datasets demonstrate that ESSNet significantly outperforms existing methods on key metrics, achieving state-of-the-art performance.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Maoyong Li; Yingying Gao; Xuedong Guo; Zhixiang Chen; Lei Deng; Mingli Dong; Lianqing Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648149"&gt;10.1109/tgrs.2025.3648149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) plays a crucial role in applications such as autonomous driving, environmental monitoring, and industrial inspection. However, the small size of targets, their blurred edges, and complex backgrounds often result in significant limitations of existing methods in terms of feature synergy and edge information utilization. To address these challenges, this paper proposes an Edge-Semantic Synergy Network with Edge-Aware Attention (ESSNet) for Infrared Small Target Detection. ESSNet substantially enhances detection performance by explicitly reinforcing edge information and optimizing multi-level feature interactions. Specifically, the Edge-Semantic Synergy Module (ESSM) leverages edge details at the lowest level and semantic information at the highest level to achieve long-range level modulation, thereby enhancing the synergy between edges and semantics. Additionally, ESSM integrates a Multi-Scale Edge-Aware Attention (MSEA), which embeds edge features into the attention mechanism through explicit edge supervision, effectively improving the accuracy of boundary detection. Furthermore, the Multi-Level Feature Fusion (MLFF) module is introduced to mitigate semantic loss during the decoding process via a layer-wise guidance mechanism, preserving the structural integrity of detected targets. Experiments conducted on the SIRST, NUDT-SIRST, and IRSTD-1K datasets demonstrate that ESSNet significantly outperforms existing methods on key metrics, achieving state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>FRFSL: Feature Reconstruction based Cross-Domain Few-Shot Learning for Coastal Wetland Hyperspectral Image Classification</title><link>https://doi.org/10.1109/tip.2025.3646073</link><guid>10.1109/tip.2025.3646073</guid><pubDate>Wed, 24 Dec 2025 18:48:18 +0000</pubDate><dc:creator>Qixing Yu</dc:creator><dc:creator>Zhongwei Li</dc:creator><dc:creator>Ziqi Xin</dc:creator><dc:creator>Fangming Guo</dc:creator><dc:creator>Guangbo Ren</dc:creator><dc:creator>Jianbu Wang</dc:creator><dc:creator>Zhenggang Bi</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646073</prism:doi><description>Hyperspectral image classification (HSIC) is a valuable method for identifying coastal wetland vegetation, but challenges like environmental complexity and difficulty in distinguishing land cover types make large-scale labeling difficult. Cross-domain few-shot learning (CDFSL) offers a potential solution to limited labeling. Existing CDFSL HSIC methods have made significant progress, but still face challenges like prototype deviation, covariate shifts, and rely on complex domain alignment (DA) methods. To address these issues, a feature reconstruction-based CDFSL (FRFSL) algorithm is proposed. Within FRFSL, a Prototype Calibration Module (PCM) is designed for the prototype deviation, which employs a Bayesian inference-enhanced Gaussian Mixture Model to select reliable query features for prototype reconstruction, aligning the prototypes more closely with the actual distribution. Additionally, a ridge regression closed-form solution is incorporated into the Distance Metric Module (DMM), employing a projection matrix for prototype reconstruction to mitigate covariate shifts between the support and query sets. Features from both source and target domains are reconstructed into dynamic graphs, transforming DA into a graph matching problem guided by optimal transport theory. A novel shared transport matrix implementation algorithm is developed to achieve lightweight and interpretable alignment. Extensive experiments on three self-constructed coastal wetland datasets and one public dataset show that FRFSL outperforms eleven state-of-the-art algorithms. The code will be available at https://github.com/Yqx-ACE/TIP_2025_FRFSL.
Published: 2025-12-24T18:48:18+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qixing Yu; Zhongwei Li; Ziqi Xin; Fangming Guo; Guangbo Ren; Jianbu Wang; Zhenggang Bi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646073"&gt;10.1109/tip.2025.3646073&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral image classification (HSIC) is a valuable method for identifying coastal wetland vegetation, but challenges like environmental complexity and difficulty in distinguishing land cover types make large-scale labeling difficult. Cross-domain few-shot learning (CDFSL) offers a potential solution to limited labeling. Existing CDFSL HSIC methods have made significant progress, but still face challenges like prototype deviation, covariate shifts, and rely on complex domain alignment (DA) methods. To address these issues, a feature reconstruction-based CDFSL (FRFSL) algorithm is proposed. Within FRFSL, a Prototype Calibration Module (PCM) is designed for the prototype deviation, which employs a Bayesian inference-enhanced Gaussian Mixture Model to select reliable query features for prototype reconstruction, aligning the prototypes more closely with the actual distribution. Additionally, a ridge regression closed-form solution is incorporated into the Distance Metric Module (DMM), employing a projection matrix for prototype reconstruction to mitigate covariate shifts between the support and query sets. Features from both source and target domains are reconstructed into dynamic graphs, transforming DA into a graph matching problem guided by optimal transport theory. A novel shared transport matrix implementation algorithm is developed to achieve lightweight and interpretable alignment. Extensive experiments on three self-constructed coastal wetland datasets and one public dataset show that FRFSL outperforms eleven state-of-the-art algorithms. The code will be available at https://github.com/Yqx-ACE/TIP_2025_FRFSL.&lt;/p&gt;</content:encoded></item><item><title>A Spatio-Spectral-Temporal Progressive Algorithm for Infrared Tiny Target Detection in Cluttered Scenes</title><link>https://doi.org/10.1109/tgrs.2025.3648555</link><guid>10.1109/tgrs.2025.3648555</guid><pubDate>Thu, 25 Dec 2025 18:26:10 +0000</pubDate><dc:creator>Jiacheng Wang</dc:creator><dc:creator>Feng Pan</dc:creator><dc:creator>Xinheng Han</dc:creator><dc:creator>Xiuli Xin</dc:creator><dc:creator>Jielei Xu</dc:creator><dc:creator>Haoyuan Zhang</dc:creator><dc:creator>Weixing Li</dc:creator><dc:creator>Ji Liu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648555</prism:doi><description>Infrared tiny target detection is of great value in fields such as military reconnaissance and security early warning but faces challenges including low signal-to-noise ratio, performance-efficiency trade-offs, and detection-false alarm compromises in complex dynamic scenarios. To solve these questions, we propose a novel Spatio-Spectral-Temporal Progressive (SSTP) Algorithm, integrating spatial, spectral and temporal features for infrared tiny target detection in cluttered scenes. First, it adopts anisotropic gradient difference detection algorithm to construct a spatial candidate target set based on the anisotropic radiation characteristics of target neighborhoods. Then, we use the isolation penalty adaptive clustering algorithm to obtain boundaries via outlier-enhanced clustering, and design a multilateral context filling algorithm to generate suspected regions and fill internal boundary information. Additionally, we develop an adaptive nonlinear geometric filter for point screening using nonlinear structural features, apply a multi-scale wavelet energy filter to capture high-frequency features, and utilize a target-background local difference measurement algorithm to extract regional independence for screening. Based on the proposed single frame detection method, a multi-dimensional feature fusion-based dynamic target tracking algorithm is employed to extract moving targets. Experiments show that on multi-frame datasets DSAT and single-frame datasets SIRST, the proposed method significantly outperforms mainstream algorithms, achieving detection rates of 98.75% and 98.23% as well as false alarm rates of 2.56 × 10−6 and 10.86 × 10−6, respectively. The algorithm not only performs well in multi frame detection, but also has good performance in single frame detection. It thus provides a solution with high robustness and real-time performance for infrared early warning systems in complex environments.
Published: 2025-12-25T18:26:10+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiacheng Wang; Feng Pan; Xinheng Han; Xiuli Xin; Jielei Xu; Haoyuan Zhang; Weixing Li; Ji Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648555"&gt;10.1109/tgrs.2025.3648555&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared tiny target detection is of great value in fields such as military reconnaissance and security early warning but faces challenges including low signal-to-noise ratio, performance-efficiency trade-offs, and detection-false alarm compromises in complex dynamic scenarios. To solve these questions, we propose a novel Spatio-Spectral-Temporal Progressive (SSTP) Algorithm, integrating spatial, spectral and temporal features for infrared tiny target detection in cluttered scenes. First, it adopts anisotropic gradient difference detection algorithm to construct a spatial candidate target set based on the anisotropic radiation characteristics of target neighborhoods. Then, we use the isolation penalty adaptive clustering algorithm to obtain boundaries via outlier-enhanced clustering, and design a multilateral context filling algorithm to generate suspected regions and fill internal boundary information. Additionally, we develop an adaptive nonlinear geometric filter for point screening using nonlinear structural features, apply a multi-scale wavelet energy filter to capture high-frequency features, and utilize a target-background local difference measurement algorithm to extract regional independence for screening. Based on the proposed single frame detection method, a multi-dimensional feature fusion-based dynamic target tracking algorithm is employed to extract moving targets. Experiments show that on multi-frame datasets DSAT and single-frame datasets SIRST, the proposed method significantly outperforms mainstream algorithms, achieving detection rates of 98.75% and 98.23% as well as false alarm rates of 2.56 × 10−6 and 10.86 × 10−6, respectively. The algorithm not only performs well in multi frame detection, but also has good performance in single frame detection. It thus provides a solution with high robustness and real-time performance for infrared early warning systems in complex environments.&lt;/p&gt;</content:encoded></item><item><title>Vision-Language Models for Person Re-identification: A Survey and Outlook</title><link>https://doi.org/10.1016/j.inffus.2025.104095</link><guid>10.1016/j.inffus.2025.104095</guid><pubDate>Wed, 24 Dec 2025 16:39:57 +0000</pubDate><dc:creator>Guorong Lin</dc:creator><dc:creator>Wei-Shi Zheng</dc:creator><dc:creator>Zuoyong Li</dc:creator><dc:creator>Yao Lu</dc:creator><dc:creator>Xiaowen Ma</dc:creator><dc:creator>Zhenhua Huang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104095</prism:doi><description>Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.
Published: 2025-12-24T16:39:57+00:00
Venue: Information Fusion
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guorong Lin; Wei-Shi Zheng; Zuoyong Li; Yao Lu; Xiaowen Ma; Zhenhua Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104095"&gt;10.1016/j.inffus.2025.104095&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Person re-identification (ReID) is a crucial task aimed at retrieving individuals of interest across multiple non-overlapping cameras. Previous methods typically rely on pre-trained visual models as backbones, which are then fine-tuned on person ReID datasets to extract discriminative features. However, due to the lack of semantic alignment between visual and textual modalities in pre-trained visual models, these methods face challenges in effectively leveraging the relationships between these modalities for ReID tasks. In recent years, Vision-Language Models (VLMs) have gained significant attention due to their ability to capture rich correlations between visual and linguistic information. Inspired by this potential, numerous researchers have proposed a series of VLM-based methods to address the diverse challenges in person ReID. This paper provides a systematic review of VLMs for person ReID. Specifically, we provide a comprehensive overview of commonly used VLM frameworks and fine-tuning strategies, while offering an in-depth analysis of the advantages of VLMs in tackling person ReID tasks. Building on this, we further provide an extensive analysis of existing VLM-based person ReID methods. Based on the modalities and learning approaches involved in the person ReID, we categorize existing VLM-based methods into five main approaches: image-based, video-based, cross-modal, multi-scene, and unsupervised person ReID methods. Finally, we outline the key research challenges and potential directions for future studies in the application of VLMs to person ReID. We believe this review will provide valuable insights and serve as an essential reference for researchers working in this field.&lt;/p&gt;</content:encoded></item><item><title>SAR-to-Optical Remote Sensing Image Translation Method Based on InternImage and Cascaded Multi-Head Attention</title><link>https://doi.org/10.3390/rs18010055</link><guid>10.3390/rs18010055</guid><pubDate>Wed, 24 Dec 2025 14:27:51 +0000</pubDate><dc:creator>Cheng Xu</dc:creator><dc:creator>Yingying Kong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010055</prism:doi><description>Synthetic aperture radar (SAR), with its all-weather and all-day observation capabilities, plays a significant role in the field of remote sensing. However, due to the unique imaging mechanism of SAR, its interpretation is challenging. Translating SAR images into optical remote sensing images has become a research hotspot in recent years to enhance the interpretability of SAR images. This paper proposes a deep learning-based method for SAR-to-optical remote sensing image translation. The network comprises three parts: a global representor, a generator with cascaded multi-head attention, and a multi-scale discriminator. The global representor, built upon InternImage with deformable convolution v3 (DCNv3) as its core operator, leverages its global receptive field and adaptive spatial aggregation capabilities to extract global semantic features from SAR images. The generator follows the classic “encoder-bottleneck-decoder” structure, where the encoder focuses on extracting local detail features from SAR images. The cascaded multi-head attention module within the bottleneck layer optimizes local detail features and facilitates feature interaction between global semantics and local details. The discriminator adopts a multi-scale structure based on the local receptive field PatchGAN, enabling joint global and local discrimination. Furthermore, for the first time in SAR image translation tasks, structural similarity index metric (SSIM) loss is combined with adversarial loss, perceptual loss, and feature matching loss as the loss function. A series of experiments demonstrate the effectiveness and reliability of the proposed method. Compared to mainstream image translation methods, our method ultimately generates higher-quality optical remote sensing images that are semantically consistent, texturally authentic, clearly detailed, and visually reasonable appearances.
Published: 2025-12-24T14:27:51+00:00
Venue: Remote Sensing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheng Xu; Yingying Kong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010055"&gt;10.3390/rs18010055&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR), with its all-weather and all-day observation capabilities, plays a significant role in the field of remote sensing. However, due to the unique imaging mechanism of SAR, its interpretation is challenging. Translating SAR images into optical remote sensing images has become a research hotspot in recent years to enhance the interpretability of SAR images. This paper proposes a deep learning-based method for SAR-to-optical remote sensing image translation. The network comprises three parts: a global representor, a generator with cascaded multi-head attention, and a multi-scale discriminator. The global representor, built upon InternImage with deformable convolution v3 (DCNv3) as its core operator, leverages its global receptive field and adaptive spatial aggregation capabilities to extract global semantic features from SAR images. The generator follows the classic “encoder-bottleneck-decoder” structure, where the encoder focuses on extracting local detail features from SAR images. The cascaded multi-head attention module within the bottleneck layer optimizes local detail features and facilitates feature interaction between global semantics and local details. The discriminator adopts a multi-scale structure based on the local receptive field PatchGAN, enabling joint global and local discrimination. Furthermore, for the first time in SAR image translation tasks, structural similarity index metric (SSIM) loss is combined with adversarial loss, perceptual loss, and feature matching loss as the loss function. A series of experiments demonstrate the effectiveness and reliability of the proposed method. Compared to mainstream image translation methods, our method ultimately generates higher-quality optical remote sensing images that are semantically consistent, texturally authentic, clearly detailed, and visually reasonable appearances.&lt;/p&gt;</content:encoded></item><item><title>Attention-driven Contrastive Learning for Cross-Modal Hashing with Prototypical Separation</title><link>https://doi.org/10.1016/j.inffus.2025.104078</link><guid>10.1016/j.inffus.2025.104078</guid><pubDate>Thu, 25 Dec 2025 16:05:29 +0000</pubDate><dc:creator>Zhipeng He</dc:creator><dc:creator>Wenzhe Liu</dc:creator><dc:creator>Lian Wu</dc:creator><dc:creator>Jinrong Cui</dc:creator><dc:creator>Jie Wen</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104078</prism:doi><description>Effective retrieval and structuring of heterogeneous data have grown more difficult due to the exponential development of multimedia data. The surge in data volume emphasizes the importance of efficient cross-modal hashing techniques, known for their rapid retrieval speed and minimal storage requirements, which have garnered attention recently. However, existing unsupervised cross-modal hashing methods often fail to capture latent semantic structures and meaningful modality interactions, which limits their retrieval performance. To address these challenges, we propose Attention-driven Contrastive Learning for Cross-Modal Hashing via Prototypical Separation (ACoPSe). The method introduces a modality-aware fusion mechanism to enhance cross-modal feature interaction and a prototype alignment strategy that reduces heterogeneity at the cluster level by leveraging pseudo-labels derived from clustering. Extensive experiments demonstrate that our method achieves comparable performance to state-of-the-art approaches.
Published: 2025-12-25T16:05:29+00:00
Venue: Information Fusion
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhipeng He; Wenzhe Liu; Lian Wu; Jinrong Cui; Jie Wen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104078"&gt;10.1016/j.inffus.2025.104078&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Effective retrieval and structuring of heterogeneous data have grown more difficult due to the exponential development of multimedia data. The surge in data volume emphasizes the importance of efficient cross-modal hashing techniques, known for their rapid retrieval speed and minimal storage requirements, which have garnered attention recently. However, existing unsupervised cross-modal hashing methods often fail to capture latent semantic structures and meaningful modality interactions, which limits their retrieval performance. To address these challenges, we propose Attention-driven Contrastive Learning for Cross-Modal Hashing via Prototypical Separation (ACoPSe). The method introduces a modality-aware fusion mechanism to enhance cross-modal feature interaction and a prototype alignment strategy that reduces heterogeneity at the cluster level by leveraging pseudo-labels derived from clustering. Extensive experiments demonstrate that our method achieves comparable performance to state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</title><link>https://arxiv.org/abs/2512.20217v1</link><guid>http://arxiv.org/abs/2512.20217v1</guid><pubDate>Tue, 23 Dec 2025 10:16:33 +0000</pubDate><dc:creator>Xiangxuan Ren</dc:creator><dc:creator>Zhongdao Wang</dc:creator><dc:creator>Pin Tang</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Jilai Zheng</dc:creator><dc:creator>Chao Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.
Published: 2025-12-23T10:16:33+00:00
Venue: arXiv
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangxuan Ren; Zhongdao Wang; Pin Tang; Guoqing Wang; Jilai Zheng; Chao Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.&lt;/p&gt;</content:encoded></item><item><title>Enhanced Geometry and Semantics for Camera-based 3D Semantic Scene Completion</title><link>https://doi.org/10.1109/tip.2025.3635475</link><guid>10.1109/tip.2025.3635475</guid><pubDate>Wed, 24 Dec 2025 18:48:18 +0000</pubDate><dc:creator>Haihong Xiao</dc:creator><dc:creator>Wenxiong Kang</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Hao Liu</dc:creator><dc:creator>Ying He</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3635475</prism:doi><description>Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.
Published: 2025-12-24T18:48:18+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haihong Xiao; Wenxiong Kang; Yulan Guo; Hao Liu; Ying He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3635475"&gt;10.1109/tip.2025.3635475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Giving machines the ability to infer the complete 3D geometry and semantics of complex scenes is crucial for many downstream tasks, such as decision-making and planning. Vision-centric Semantic Scene Completion (SSC) has emerged as a trendy 3D perception paradigm due to its compatibility with task properties, low cost, and rich visual cues. Despite impressive results, current approaches inevitably suffer from problems such as depth errors or depth ambiguities during the 2D-to-3D transformation process. To overcome these limitations, in this paper, we first introduce an Optical Flow-Guided (OFG) Depth-Net that leverages the strengths of pretrained depth estimation models, while incorporating optical flow images to improve depth prediction accuracy in regions with significant depth changes. Then, we propose a depth ambiguity-mitigated feature lifting strategy that implements deformable cross-attention in 3D pixel space to avoid depth ambiguities caused by the projection process from 3D to 2D and further enhances the effectiveness of feature updating through the utilization of prior mask indices. Moreover, we customize two subnetworks: a residual voxel network and a sparse UNet, to enhance the network’s geometric prediction capabilities and ensure consistent semantic reasoning across varying scales. By doing so, our method achieves performance improvements over state-of-the-art methods on the SemanticKITTI, SSCBench-KITTI-360 and Occ3D-nuScene benchmarks.&lt;/p&gt;</content:encoded></item><item><title>Forget Me Not: Fighting Local Overfitting With Knowledge Fusion and Distillation</title><link>https://doi.org/10.1109/tpami.2025.3647862</link><guid>10.1109/tpami.2025.3647862</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Uri Stern</dc:creator><dc:creator>Eli Corn</dc:creator><dc:creator>Daphna Weinshall</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647862</prism:doi><description>Overfitting in deep neural networks occurs less frequently than expected. This is a puzzling observation, as theory predicts that greater model capacity should eventually lead to overfitting – yet this is rarely seen in practice. But what if overfitting does occur, not globally, but in specific sub-regions of the data space? In this work, we introduce a novel score that measures the forgetting rate of deep models on validation data, capturing what we term local overfitting: a performance degradation confined to certain regions of the input space. We demonstrate that local overfitting can arise even without conventional overfitting, and is closely linked to the double descent phenomenon. Building on these insights, we introduce a two-stage approach that leverages the training history of a single model to recover and retain forgotten knowledge: first, by aggregating checkpoints into an ensemble, and then by distilling it into a single model of the original size, thus enhancing performance without added inference cost. Extensive experiments across multiple datasets, modern architectures, and training regimes validate the effectiveness of our approach. Notably, in the presence of label noise, our method – Knowledge Fusion followed by Knowledge Distillation – outperforms both the original model and independently trained ensembles, achieving a rare win-win scenario: reduced training and inference complexity.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Uri Stern; Eli Corn; Daphna Weinshall&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647862"&gt;10.1109/tpami.2025.3647862&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Overfitting in deep neural networks occurs less frequently than expected. This is a puzzling observation, as theory predicts that greater model capacity should eventually lead to overfitting – yet this is rarely seen in practice. But what if overfitting does occur, not globally, but in specific sub-regions of the data space? In this work, we introduce a novel score that measures the forgetting rate of deep models on validation data, capturing what we term local overfitting: a performance degradation confined to certain regions of the input space. We demonstrate that local overfitting can arise even without conventional overfitting, and is closely linked to the double descent phenomenon. Building on these insights, we introduce a two-stage approach that leverages the training history of a single model to recover and retain forgotten knowledge: first, by aggregating checkpoints into an ensemble, and then by distilling it into a single model of the original size, thus enhancing performance without added inference cost. Extensive experiments across multiple datasets, modern architectures, and training regimes validate the effectiveness of our approach. Notably, in the presence of label noise, our method – Knowledge Fusion followed by Knowledge Distillation – outperforms both the original model and independently trained ensembles, achieving a rare win-win scenario: reduced training and inference complexity.&lt;/p&gt;</content:encoded></item><item><title>Dual Prompts Aware Cross-modal Semantic Interaction and Fusion Network for Remote Sensing Image Captioning</title><link>https://doi.org/10.1109/tgrs.2025.3648057</link><guid>10.1109/tgrs.2025.3648057</guid><pubDate>Wed, 24 Dec 2025 18:45:47 +0000</pubDate><dc:creator>Lanxiao Wang</dc:creator><dc:creator>Heqian Qiu</dc:creator><dc:creator>Minjian Zhang</dc:creator><dc:creator>Fanman Meng</dc:creator><dc:creator>Qingbo Wu</dc:creator><dc:creator>Hongliang Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648057</prism:doi><description>Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.
Published: 2025-12-24T18:45:47+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lanxiao Wang; Heqian Qiu; Minjian Zhang; Fanman Meng; Qingbo Wu; Hongliang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648057"&gt;10.1109/tgrs.2025.3648057&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, remote sensing image captioning (RSIC) has become an emerging research hot spot that requires models to understand and describe remote sensing images. However, the huge modal gap between vision and text makes that it is difficult to achieve accurate cross-modal transformation for RSIC. Existing methods usually directly transform the vision modal into the text modal based on the multi-task learning strategy or visual attention mechanism, which do not make full use of existing prior information to build explicit cross-modal knowledge for vision and text transformation. Considering to utilize the ability of cross-modal alignment in the vision-language model (VLM), we propose a novel dual prompts aware cross-modal semantic interaction and fusion network for RSIC. It can explicitly dig out potential entity concepts and predict scene class in the images. And it further builds dual prompts to achieve cross-modal interaction and fusion, which can build cross-modal common semantic space to provide prior information for caption generation. Specifically, we first introduce an entity-concept exporter to obtain explicit entity concepts in the image based on pre-setting entity space. Next, we design a multi-scale scene predictor to obtain fine-grained visual semantic features and scene class. Then, we propose a prompt aware cross-modal interaction module to build cross-modal common semantic space as intermediate connection for caption generation. Finally, we further design a prompt aware attention fusion module for the transformer decoder, which can utilize cross-modal prompt features to generate accurate captions. We conduct extensive experiments on three challenging datasets, including UCM-Captions, RSICD and NWPU-Captions, and our method achieves SoTA performance. In the typical remote sensing image captioning dataset RSICD, our method achieves 3.3% and 20.0% improvement in BLEU@4 and CIDEr respectively, which show the effectiveness of our method.&lt;/p&gt;</content:encoded></item><item><title>HUGSIM: A Real-Time, Photo-Realistic and Closed-Loop Simulator for Autonomous Driving</title><link>https://doi.org/10.1109/tpami.2025.3647952</link><guid>10.1109/tpami.2025.3647952</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Hongyu Zhou</dc:creator><dc:creator>Longzhong Lin</dc:creator><dc:creator>Jiabao Wang</dc:creator><dc:creator>Yichong Lu</dc:creator><dc:creator>Dongfeng Bai</dc:creator><dc:creator>Bingbing Liu</dc:creator><dc:creator>Yue Wang</dc:creator><dc:creator>Andreas Geiger</dc:creator><dc:creator>Yiyi Liao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647952</prism:doi><description>In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, we tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongyu Zhou; Longzhong Lin; Jiabao Wang; Yichong Lu; Dongfeng Bai; Bingbing Liu; Yue Wang; Andreas Geiger; Yiyi Liao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647952"&gt;10.1109/tpami.2025.3647952&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;In the past few decades, autonomous driving algorithms have made significant progress in perception, planning, and control. However, evaluating individual components does not fully reflect the performance of entire systems, highlighting the need for more holistic assessment methods. This motivates the development of HUGSIM, a closed-loop, photo-realistic, and real-time simulator for evaluating autonomous driving algorithms. We achieve this by lifting captured 2D RGB images into the 3D space via 3D Gaussian Splatting, improving the rendering quality for closed-loop scenarios, and building the closed-loop environment. In terms of rendering, we tackle challenges of novel view synthesis in closed-loop scenarios, including viewpoint extrapolation and 360-degree vehicle rendering. Beyond novel view synthesis, HUGSIM further enables the full closed simulation loop, dynamically updating the ego and actor states and observations based on control commands. Moreover, HUGSIM offers a comprehensive benchmark across more than 70 sequences from KITTI-360, Waymo, nuScenes, and PandaSet, along with over 400 varying scenarios, providing a fair and realistic evaluation platform for existing autonomous driving algorithms. HUGSIM not only serves as an intuitive evaluation benchmark but also unlocks the potential for fine-tuning autonomous driving algorithms in a photorealistic closed-loop setting.&lt;/p&gt;</content:encoded></item><item><title>RSS-Net: A Mamba-Based Network for SAR Image Denoising</title><link>https://doi.org/10.1109/jstars.2025.3647971</link><guid>10.1109/jstars.2025.3647971</guid><pubDate>Wed, 24 Dec 2025 18:46:08 +0000</pubDate><dc:creator>Min Huang</dc:creator><dc:creator>Yunzhao Yang</dc:creator><dc:creator>Qiuhong Sun</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647971</prism:doi><description>Synthetic Aperture Radar (SAR), with its outstanding features, has developed into a vital technical method for the global detection of maritime and terrestrial targets. However, the unique imaging mechanism intrinsic to SAR inherently produces coherent speckle noise. This noise significantly deteriorates the quality of the image, thereby adversely affecting downstream applications like target recognition and classification. Therefore, research on SAR image denoising holds significant practical and theoretical value. However, due to the limitations of the local receptive field of CNN, it is difficult for it to capture global spatial features. Although Transformer can achieve global spatial modeling, its quadratic complexity results in a large amount of computational overhead. To tackle these issues, this paper introduces RSS-Net, a new denoising network for SAR images, built on an encoder-decoder architecture. RSS-Net demonstrates significant improvements in SAR image denoising performance, due to its ability to extract multi-scale feature information. The network incorporates the Residue State-Space Block (RSSB), which fuses Mamba's Vision State Space Module (VSSM) and a CNN-based Channel Attention Block (CAB). VSSM leverages 2D-Selective Scan (SS2D) better acquires spatial information features in SAR images by scanning in four directions. RSSB efficiently merges Mamba's ability to model long-range dependencies and conventional convolution's strengths in extracting local features. This integration effectively alleviates the common detail blurring problem in existing SAR denoising methods and strengthens the restoration of high-frequency image details. The novel application of the Mamba to SAR image denoising enables the proposed method to attain both long-range contextual dependency modeling and linear computational cost, thus resolving the balance between global modeling capacity and efficiency in computation, while also pointing to new avenues for future research....
Published: 2025-12-24T18:46:08+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Min Huang; Yunzhao Yang; Qiuhong Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647971"&gt;10.1109/jstars.2025.3647971&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR), with its outstanding features, has developed into a vital technical method for the global detection of maritime and terrestrial targets. However, the unique imaging mechanism intrinsic to SAR inherently produces coherent speckle noise. This noise significantly deteriorates the quality of the image, thereby adversely affecting downstream applications like target recognition and classification. Therefore, research on SAR image denoising holds significant practical and theoretical value. However, due to the limitations of the local receptive field of CNN, it is difficult for it to capture global spatial features. Although Transformer can achieve global spatial modeling, its quadratic complexity results in a large amount of computational overhead. To tackle these issues, this paper introduces RSS-Net, a new denoising network for SAR images, built on an encoder-decoder architecture. RSS-Net demonstrates significant improvements in SAR image denoising performance, due to its ability to extract multi-scale feature information. The network incorporates the Residue State-Space Block (RSSB), which fuses Mamba&amp;#x27;s Vision State Space Module (VSSM) and a CNN-based Channel Attention Block (CAB). VSSM leverages 2D-Selective Scan (SS2D) better acquires spatial information features in SAR images by scanning in four directions. RSSB efficiently merges Mamba&amp;#x27;s ability to model long-range dependencies and conventional convolution&amp;#x27;s strengths in extracting local features. This integration effectively alleviates the common detail blurring problem in existing SAR denoising methods and strengthens the restoration of high-frequency image details. The novel application of the Mamba to SAR image denoising enables the proposed method to attain both long-range contextual dependency modeling and linear computational cost, thus resolving the balance between global modeling capacity and efficiency in computation, while also pointing to new avenues for future research....&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Benchmark of Spatial Encoding Methods for Tabular Data with Deep Neural Networks</title><link>https://doi.org/10.1016/j.inffus.2025.104088</link><guid>10.1016/j.inffus.2025.104088</guid><pubDate>Thu, 25 Dec 2025 16:05:27 +0000</pubDate><dc:creator>Jiayun Liu</dc:creator><dc:creator>Manuel Castillo-Cara</dc:creator><dc:creator>Raúl García-Castro</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104088</prism:doi><description>Despite the success of deep neural networks on perceptual data, their performance on tabular data remains limited, where traditional models still outperform them. A promising alternative is to transform tabular data into synthetic images, enabling the use of vision architectures such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, the literature lacks a large-scale, standardized benchmark evaluating these transformation techniques. This work presents the first comprehensive evaluation of nine spatial encoding methods across 24 diverse regression and classification datasets. We assess performance, scalability, and computational trade-offs under a unified framework with rigorous hyperparameter optimization. Our results reveal a performance landscape structured by data regimes, defined by sample size ( N ) and dimensionality ( d ), and show that the transformation method exerts a significantly stronger influence on predictive performance than the chosen vision architecture. In particular, REFINED emerges as the most robust transformation across tasks and datasets. Hybrid models (CNN+MLP, ViT+MLP) consistently reduce predictive variance, offering advantages especially in smaller datasets, yet play a secondary role. These findings suggest that transforming tabular data into synthetic images is a powerful, yet data-dependent, strategy. This benchmark provides clear guidance for researchers and practitioners, offering key insights into scalability, transformation behavior, and architectural interplay, establishing a comprehensive reference for future research on spatial encodings for tabular data.
Published: 2025-12-25T16:05:27+00:00
Venue: Information Fusion
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayun Liu; Manuel Castillo-Cara; Raúl García-Castro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104088"&gt;10.1016/j.inffus.2025.104088&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Despite the success of deep neural networks on perceptual data, their performance on tabular data remains limited, where traditional models still outperform them. A promising alternative is to transform tabular data into synthetic images, enabling the use of vision architectures such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, the literature lacks a large-scale, standardized benchmark evaluating these transformation techniques. This work presents the first comprehensive evaluation of nine spatial encoding methods across 24 diverse regression and classification datasets. We assess performance, scalability, and computational trade-offs under a unified framework with rigorous hyperparameter optimization. Our results reveal a performance landscape structured by data regimes, defined by sample size ( N ) and dimensionality ( d ), and show that the transformation method exerts a significantly stronger influence on predictive performance than the chosen vision architecture. In particular, REFINED emerges as the most robust transformation across tasks and datasets. Hybrid models (CNN+MLP, ViT+MLP) consistently reduce predictive variance, offering advantages especially in smaller datasets, yet play a secondary role. These findings suggest that transforming tabular data into synthetic images is a powerful, yet data-dependent, strategy. This benchmark provides clear guidance for researchers and practitioners, offering key insights into scalability, transformation behavior, and architectural interplay, establishing a comprehensive reference for future research on spatial encodings for tabular data.&lt;/p&gt;</content:encoded></item><item><title>BEVTrack: Multi-View Multi-Human Registration and Tracking in the Bird's Eye View</title><link>https://doi.org/10.1109/tpami.2025.3647707</link><guid>10.1109/tpami.2025.3647707</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Zekun Qian</dc:creator><dc:creator>Wei Feng</dc:creator><dc:creator>Feifan Wang</dc:creator><dc:creator>Ruize Han</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647707</prism:doi><description>We handle a new problem of multi-view multi-human tracking in the bird's eye view (BEV). Different from previous works, we require neither the calibration among the multi-view cameras nor the actually captured BEV video. This makes the studied problem closer to real-world applications, however, more challenging. For this purpose, in this work, we propose a novel BEVTrack scheme. Specifically, given multi-view videos, we first use a virtual BEV transform module to obtain the BEV for each view. Then, we propose a unified BEV alignment module to fuse the respectively generated BEVs, in which we specifically design the self-supervised losses by considering both the spatial consistency and the temporal continuity. During the inference, we design the camera-subject collaborative registration and tracking strategy to make use of the mutual dependence between the multi-view cameras and the multiple targets, to achieve the desired BEV tracking. We also build a new benchmark for training and evaluation, the experimental results on which have verified the rationality of the problem and the effectiveness of our method.
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zekun Qian; Wei Feng; Feifan Wang; Ruize Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647707"&gt;10.1109/tpami.2025.3647707&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;We handle a new problem of multi-view multi-human tracking in the bird&amp;#x27;s eye view (BEV). Different from previous works, we require neither the calibration among the multi-view cameras nor the actually captured BEV video. This makes the studied problem closer to real-world applications, however, more challenging. For this purpose, in this work, we propose a novel BEVTrack scheme. Specifically, given multi-view videos, we first use a virtual BEV transform module to obtain the BEV for each view. Then, we propose a unified BEV alignment module to fuse the respectively generated BEVs, in which we specifically design the self-supervised losses by considering both the spatial consistency and the temporal continuity. During the inference, we design the camera-subject collaborative registration and tracking strategy to make use of the mutual dependence between the multi-view cameras and the multiple targets, to achieve the desired BEV tracking. We also build a new benchmark for training and evaluation, the experimental results on which have verified the rationality of the problem and the effectiveness of our method.&lt;/p&gt;</content:encoded></item><item><title>Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation</title><link>https://doi.org/10.1109/tpami.2025.3647829</link><guid>10.1109/tpami.2025.3647829</guid><pubDate>Wed, 24 Dec 2025 18:45:42 +0000</pubDate><dc:creator>Yifei Shi</dc:creator><dc:creator>Boyan Wan</dc:creator><dc:creator>Xin Xu</dc:creator><dc:creator>Kai Xu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647829</prism:doi><description>Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...
Published: 2025-12-24T18:45:42+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifei Shi; Boyan Wan; Xin Xu; Kai Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647829"&gt;10.1109/tpami.2025.3647829&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object&amp;#x27;s canonical space - including unobserved regions in camera space - significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model&amp;#x27;s generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The strategy dynamically determines sampling locations based on the input, thereby boosting the network&amp;#x27;s accuracy and training efficiency. The strategy is implemented with a estimation network which generates sparse sample points with distinctive features capable of determining all object pose DoFs with high certainty. To collect the training data of the estimation network, we propose to automatically generate the pseudo ground-truth with a teacher model. Our method outperforms the state-of-the-art on three pose estimation datasets. It achieves 0.63 in the 5^{\circ }2 5^{\circ }2 cm metric ...&lt;/p&gt;</content:encoded></item><item><title>S
                    &lt;sup&gt;2&lt;/sup&gt;
                    EDL: Selective Semantic Efficient Distillation Learning for Large-Scale Remote Sensing Representation</title><link>https://doi.org/10.1109/jstars.2025.3647928</link><guid>10.1109/jstars.2025.3647928</guid><pubDate>Wed, 24 Dec 2025 18:46:08 +0000</pubDate><dc:creator>Wu Wen</dc:creator><dc:creator>Jinghui Luo</dc:creator><dc:creator>Lizhuang Tan</dc:creator><dc:creator>Konstantin Igorevich Kostromitin</dc:creator><dc:creator>Jian Wang</dc:creator><dc:creator>Peiying Zhang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3647928</prism:doi><description>Self-Supervised Learning (SSL) has gained widespread attention in remote sensing and Earth observation. SSL can extract general-purpose visual representations from large-scale remote sensing data without requiring extensive manual annotations. However, current mainstream paradigms, such as Contrastive Learning (CL) and Masked Image Modeling (MIM), have their own disadvantages. CL excels at learning globally separable representations but often overlooks local details. MIM captures local spatial awareness effectively but lacks global consistency and computational efficiency. To address these challenges, this paper proposes a novel SSL framework named Selective Semantic Efficient Distillation Learning (S2EDL). The S2EDL is built upon a teacher-student knowledge distillation architecture, where the teacher network encodes the complete, augmented image and provides multi-level semantic supervision signals to the student network. Through the selective semantic MIM strategy of the student network, the model can dynamically identify and focus on reconstructing and calculating the loss for the masked regions with the highest informational value. S2EDL enhances fine-grained perception of local spatial patterns through an efficient MIM branch. Combined with a CL branch, it strengthens both global separability and local discriminability of learned features. Comprehensive experimental evaluations on multiple downstream tasks demonstrate that the model pre-trained with S2EDL exhibits superior performance compared to other mainstream SSL methods, thereby validating its effectiveness in learning high-quality and comprehensive remote sensing representations.
Published: 2025-12-24T18:46:08+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wu Wen; Jinghui Luo; Lizhuang Tan; Konstantin Igorevich Kostromitin; Jian Wang; Peiying Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3647928"&gt;10.1109/jstars.2025.3647928&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Self-Supervised Learning (SSL) has gained widespread attention in remote sensing and Earth observation. SSL can extract general-purpose visual representations from large-scale remote sensing data without requiring extensive manual annotations. However, current mainstream paradigms, such as Contrastive Learning (CL) and Masked Image Modeling (MIM), have their own disadvantages. CL excels at learning globally separable representations but often overlooks local details. MIM captures local spatial awareness effectively but lacks global consistency and computational efficiency. To address these challenges, this paper proposes a novel SSL framework named Selective Semantic Efficient Distillation Learning (S2EDL). The S2EDL is built upon a teacher-student knowledge distillation architecture, where the teacher network encodes the complete, augmented image and provides multi-level semantic supervision signals to the student network. Through the selective semantic MIM strategy of the student network, the model can dynamically identify and focus on reconstructing and calculating the loss for the masked regions with the highest informational value. S2EDL enhances fine-grained perception of local spatial patterns through an efficient MIM branch. Combined with a CL branch, it strengthens both global separability and local discriminability of learned features. Comprehensive experimental evaluations on multiple downstream tasks demonstrate that the model pre-trained with S2EDL exhibits superior performance compared to other mainstream SSL methods, thereby validating its effectiveness in learning high-quality and comprehensive remote sensing representations.&lt;/p&gt;</content:encoded></item><item><title>DAWDet: A Dynamic Content-Aware Multi-Branch Framework with Adaptive Wavelet Boosting for Small Object Detection</title><link>https://doi.org/10.1016/j.patcog.2025.112979</link><guid>10.1016/j.patcog.2025.112979</guid><pubDate>Thu, 25 Dec 2025 15:57:06 +0000</pubDate><dc:creator>Yuting Wu</dc:creator><dc:creator>Shaolei Liu</dc:creator><dc:creator>Dongchen Zhu</dc:creator><dc:creator>Lei Wang</dc:creator><dc:creator>Jiamao Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112979</prism:doi><description>Small Object Detection (SOD) aims to classify and localize objects with limited regions, playing a pivotal role in surveillance systems, intelligent transportation, and aerial inspection applications. Compared to general object detection, SOD confronts three fundamental limitations: inadequate discriminative feature representation, scarcity of high-quality training samples, and severe information loss. To address these problems, we propose DAWDet, a novel framework tailored for SOD tasks. First, we design a Dynamic content-aware Multi-branch Feature Pyramid Network (DMFPN) based on adaptive content-aware grid sampling and refined network topology, to obtain richer location information and semantic representation of small objects. Second, we develop an Adaptive Label Assignment Strategy (ALAS) to increase the quantity of high-quality positive samples, which optimizes the regression branch for high-quality small object samples via a designed overlap transformation function. Third, to mitigate information loss, we incorporate lightweight Haar Wavelet transform Downsampling (HWD) modules into the feature fusion process, effectively preserving crucial high-frequency details during resolution reduction. Comprehensive evaluations on standard SOD benchmarks demonstrate our framework achieves state-of-the-art performance while maintaining computational efficiency.
Published: 2025-12-25T15:57:06+00:00
Venue: Pattern Recognition
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuting Wu; Shaolei Liu; Dongchen Zhu; Lei Wang; Jiamao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112979"&gt;10.1016/j.patcog.2025.112979&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Small Object Detection (SOD) aims to classify and localize objects with limited regions, playing a pivotal role in surveillance systems, intelligent transportation, and aerial inspection applications. Compared to general object detection, SOD confronts three fundamental limitations: inadequate discriminative feature representation, scarcity of high-quality training samples, and severe information loss. To address these problems, we propose DAWDet, a novel framework tailored for SOD tasks. First, we design a Dynamic content-aware Multi-branch Feature Pyramid Network (DMFPN) based on adaptive content-aware grid sampling and refined network topology, to obtain richer location information and semantic representation of small objects. Second, we develop an Adaptive Label Assignment Strategy (ALAS) to increase the quantity of high-quality positive samples, which optimizes the regression branch for high-quality small object samples via a designed overlap transformation function. Third, to mitigate information loss, we incorporate lightweight Haar Wavelet transform Downsampling (HWD) modules into the feature fusion process, effectively preserving crucial high-frequency details during resolution reduction. Comprehensive evaluations on standard SOD benchmarks demonstrate our framework achieves state-of-the-art performance while maintaining computational efficiency.&lt;/p&gt;</content:encoded></item><item><title>MFF-MTT: A Multi-feature Fusion-based Deep Learning Algorithm for Maneuvering Target Tracking</title><link>https://doi.org/10.1016/j.inffus.2025.104093</link><guid>10.1016/j.inffus.2025.104093</guid><pubDate>Thu, 25 Dec 2025 00:16:04 +0000</pubDate><dc:creator>Xiaoqing Hu</dc:creator><dc:creator>Hongyan Zhu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104093</prism:doi><description>In target tracking applications, traditional model-driven algorithms suffer from the model mismatch due to the lack of prior knowledge. Recently, some data-driven algorithms have been showing increasing potential in dealing with uncertain target maneuvering behaviors. To further enhance robustness to high maneuverability, we propose a multi-feature fusion-based deep learning algorithm for maneuvering target tracking (MFF-MTT) by combining the convolution and transformer network. Thereinto, the convolution network extracts the local information to capture the transition law of rapidly changing states. The Multi-Head Self-Attention (MHSA) in transformer network enables MFF-MTT to exploit the global information by weighting different parts of input sequence and integrating diverse subspace representations of queries, keys, and values. The local and global features are then fused in two forms of merge and cross to capture the short-term maneuvers and long-term trends of the trajectory jointly. Moreover, we also develop a novel encoder-decoder framework that decodes the fused features by Bi-directional Long Short-Term Memory (Bi-LSTM). In this way, a comprehensive understanding about the inherent structure of the data can be obtained to facilitate the high-accuracy state estimation. Extensive simulation results demonstrate that the proposed MFF-MTT outperforms other comparative methods on estimation precision and robustness in maneuvering target tracking scenarios.
Published: 2025-12-25T00:16:04+00:00
Venue: Information Fusion
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoqing Hu; Hongyan Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104093"&gt;10.1016/j.inffus.2025.104093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;In target tracking applications, traditional model-driven algorithms suffer from the model mismatch due to the lack of prior knowledge. Recently, some data-driven algorithms have been showing increasing potential in dealing with uncertain target maneuvering behaviors. To further enhance robustness to high maneuverability, we propose a multi-feature fusion-based deep learning algorithm for maneuvering target tracking (MFF-MTT) by combining the convolution and transformer network. Thereinto, the convolution network extracts the local information to capture the transition law of rapidly changing states. The Multi-Head Self-Attention (MHSA) in transformer network enables MFF-MTT to exploit the global information by weighting different parts of input sequence and integrating diverse subspace representations of queries, keys, and values. The local and global features are then fused in two forms of merge and cross to capture the short-term maneuvers and long-term trends of the trajectory jointly. Moreover, we also develop a novel encoder-decoder framework that decodes the fused features by Bi-directional Long Short-Term Memory (Bi-LSTM). In this way, a comprehensive understanding about the inherent structure of the data can be obtained to facilitate the high-accuracy state estimation. Extensive simulation results demonstrate that the proposed MFF-MTT outperforms other comparative methods on estimation precision and robustness in maneuvering target tracking scenarios.&lt;/p&gt;</content:encoded></item><item><title>PFI-Net: A Parallel Feature Interaction Network for Infrared and Visible Target Detection</title><link>https://doi.org/10.1016/j.patcog.2025.113003</link><guid>10.1016/j.patcog.2025.113003</guid><pubDate>Thu, 25 Dec 2025 00:03:38 +0000</pubDate><dc:creator>Xiaoxia Wang</dc:creator><dc:creator>Jiangtao Xi</dc:creator><dc:creator>Fengbao Yang</dc:creator><dc:creator>Yunjia Yang</dc:creator><dc:creator>Minglu Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113003</prism:doi><description>Detection networks based on deep learning mainly adopt a single feature interaction mechanism to capture the deep features of targets. As the quality of infrared or visible images deteriorates moderately or severely, this often results in the insignificance of deep feature. To surmount this deficiency, we present a parallel feature interaction network, termed PFI-Net. This architecture involves dual-branch feature extraction and enhancement, parallel feature interaction and decision fusion detector. With dual-branch feature extraction and enhancement as the premise, we construct a parallel feature interaction module with different interaction mode to avoid mutual interference between features of infrared and visible image. This parallel feature interaction module can ensure the features of infrared and visible are guided into two separate independent channels. Additionally, we devise a weighted detection boxes fusion module to achieve the integration of the parallel detection results. This module integrates the advantages of detection results from different channels to promote detection accuracy and stability. Finally, comprehensive experiments on multiple benchmark models demonstrate that the proposed PFI-Net delivers promising detection performance, outperforming other advanced alternatives.
Published: 2025-12-25T00:03:38+00:00
Venue: Pattern Recognition
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxia Wang; Jiangtao Xi; Fengbao Yang; Yunjia Yang; Minglu Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113003"&gt;10.1016/j.patcog.2025.113003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Detection networks based on deep learning mainly adopt a single feature interaction mechanism to capture the deep features of targets. As the quality of infrared or visible images deteriorates moderately or severely, this often results in the insignificance of deep feature. To surmount this deficiency, we present a parallel feature interaction network, termed PFI-Net. This architecture involves dual-branch feature extraction and enhancement, parallel feature interaction and decision fusion detector. With dual-branch feature extraction and enhancement as the premise, we construct a parallel feature interaction module with different interaction mode to avoid mutual interference between features of infrared and visible image. This parallel feature interaction module can ensure the features of infrared and visible are guided into two separate independent channels. Additionally, we devise a weighted detection boxes fusion module to achieve the integration of the parallel detection results. This module integrates the advantages of detection results from different channels to promote detection accuracy and stability. Finally, comprehensive experiments on multiple benchmark models demonstrate that the proposed PFI-Net delivers promising detection performance, outperforming other advanced alternatives.&lt;/p&gt;</content:encoded></item><item><title>Enhancing diffusion models with Gaussianization preprocessing</title><link>https://arxiv.org/abs/2512.21020v1</link><guid>http://arxiv.org/abs/2512.21020v1</guid><pubDate>Wed, 24 Dec 2025 07:34:20 +0000</pubDate><dc:creator>Li Cunzhi</dc:creator><dc:creator>Louis Kang</dc:creator><dc:creator>Hideaki Shimazaki</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion models are a class of generative models that have demonstrated remarkable success in tasks such as image generation. However, one of the bottlenecks of these models is slow sampling due to the delay before the onset of trajectory bifurcation, at which point substantial reconstruction begins. This issue degrades generation quality, especially in the early stages. Our primary objective is to mitigate bifurcation-related issues by preprocessing the training data to enhance reconstruction quality, particularly for small-scale network architectures. Specifically, we propose applying Gaussianization preprocessing to the training data to make the target distribution more closely resemble an independent Gaussian distribution, which serves as the initial density of the reconstruction process. This preprocessing step simplifies the model's task of learning the target distribution, thereby improving generation quality even in the early stages of reconstruction with small networks. The proposed method is, in principle, applicable to a broad range of generative tasks, enabling more stable and efficient sampling processes.
Published: 2025-12-24T07:34:20+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Li Cunzhi; Louis Kang; Hideaki Shimazaki&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models are a class of generative models that have demonstrated remarkable success in tasks such as image generation. However, one of the bottlenecks of these models is slow sampling due to the delay before the onset of trajectory bifurcation, at which point substantial reconstruction begins. This issue degrades generation quality, especially in the early stages. Our primary objective is to mitigate bifurcation-related issues by preprocessing the training data to enhance reconstruction quality, particularly for small-scale network architectures. Specifically, we propose applying Gaussianization preprocessing to the training data to make the target distribution more closely resemble an independent Gaussian distribution, which serves as the initial density of the reconstruction process. This preprocessing step simplifies the model&amp;#x27;s task of learning the target distribution, thereby improving generation quality even in the early stages of reconstruction with small networks. The proposed method is, in principle, applicable to a broad range of generative tasks, enabling more stable and efficient sampling processes.&lt;/p&gt;</content:encoded></item><item><title>DiFaReli++: Diffusion Face Relighting with Consistent Cast Shadows</title><link>https://doi.org/10.1109/tpami.2025.3648667</link><guid>10.1109/tpami.2025.3648667</guid><pubDate>Thu, 25 Dec 2025 18:26:04 +0000</pubDate><dc:creator>Puntawat Ponglertnapakorn</dc:creator><dc:creator>Nontawat Tritrong</dc:creator><dc:creator>Supasorn Suwajanakorn</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648667</prism:doi><description>We introduce a novel approach to single-view face relighting in the wild, addressing challenges such as global illumination and cast shadows. A common scheme in recent methods involves intrinsically decomposing an input image into 3D shape, albedo, and lighting, then recomposing it with the target lighting. However, estimating these components is errorprone and requires many training examples with ground-truth lighting to generalize well. Our work bypasses the need for accurate intrinsic estimation and can be trained solely on 2D images without any light stage data, relit pairs, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We propose a novel conditioning technique that simplifies modeling the complex interaction between light and geometry. It uses a rendered shading reference along with a shadow map, inferred using a simple and effective technique, to spatially modulate the DDIM. Moreover, we propose a single-shot relighting framework that requires just one network pass, given pre-processed data, and even outperforms the teacher model across all metrics. Our method realistically relights in-the-wild images with temporally consistent cast shadows under varying lighting conditions. We achieve state-of-the-art performance on the standard benchmark Multi-PIE and rank highest in user studies.
Published: 2025-12-25T18:26:04+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Puntawat Ponglertnapakorn; Nontawat Tritrong; Supasorn Suwajanakorn&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648667"&gt;10.1109/tpami.2025.3648667&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce a novel approach to single-view face relighting in the wild, addressing challenges such as global illumination and cast shadows. A common scheme in recent methods involves intrinsically decomposing an input image into 3D shape, albedo, and lighting, then recomposing it with the target lighting. However, estimating these components is errorprone and requires many training examples with ground-truth lighting to generalize well. Our work bypasses the need for accurate intrinsic estimation and can be trained solely on 2D images without any light stage data, relit pairs, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We propose a novel conditioning technique that simplifies modeling the complex interaction between light and geometry. It uses a rendered shading reference along with a shadow map, inferred using a simple and effective technique, to spatially modulate the DDIM. Moreover, we propose a single-shot relighting framework that requires just one network pass, given pre-processed data, and even outperforms the teacher model across all metrics. Our method realistically relights in-the-wild images with temporally consistent cast shadows under varying lighting conditions. We achieve state-of-the-art performance on the standard benchmark Multi-PIE and rank highest in user studies.&lt;/p&gt;</content:encoded></item><item><title>IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection</title><link>https://doi.org/10.1016/j.inffus.2025.104097</link><guid>10.1016/j.inffus.2025.104097</guid><pubDate>Thu, 25 Dec 2025 16:05:23 +0000</pubDate><dc:creator>Xuanming Cao</dc:creator><dc:creator>Chengyu Tao</dc:creator><dc:creator>Yifeng Cheng</dc:creator><dc:creator>Juan Du</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104097</prism:doi><description>Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.
Published: 2025-12-25T16:05:23+00:00
Venue: Information Fusion
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuanming Cao; Chengyu Tao; Yifeng Cheng; Juan Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104097"&gt;10.1016/j.inffus.2025.104097&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.&lt;/p&gt;</content:encoded></item><item><title>Towards Arbitrary-Scale Spacecraft Image Super-Resolution via Salient Region-Guidance</title><link>https://doi.org/10.1016/j.patcog.2025.112973</link><guid>10.1016/j.patcog.2025.112973</guid><pubDate>Thu, 25 Dec 2025 15:56:50 +0000</pubDate><dc:creator>Jingfan Yang</dc:creator><dc:creator>Hu Gao</dc:creator><dc:creator>Ying Zhang</dc:creator><dc:creator>Depeng Dang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112973</prism:doi><description>Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose an efficient salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results on spacecraft radar image dataset and optical image dataset demonstrate that the proposed SGSASR outperforms state-of-the-art approaches. The codes are available at: https://github.com/shenduke/SGSASR .
Published: 2025-12-25T15:56:50+00:00
Venue: Pattern Recognition
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingfan Yang; Hu Gao; Ying Zhang; Depeng Dang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112973"&gt;10.1016/j.patcog.2025.112973&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose an efficient salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results on spacecraft radar image dataset and optical image dataset demonstrate that the proposed SGSASR outperforms state-of-the-art approaches. The codes are available at: https://github.com/shenduke/SGSASR .&lt;/p&gt;</content:encoded></item><item><title>TPIN: Text-based Parallel Interaction Network with Modality-Common and Modality-Specific for Multimodal Sentiment Analysis</title><link>https://doi.org/10.1016/j.inffus.2025.104087</link><guid>10.1016/j.inffus.2025.104087</guid><pubDate>Thu, 25 Dec 2025 00:16:18 +0000</pubDate><dc:creator>Changbin Wang</dc:creator><dc:creator>Fengrui Ji</dc:creator><dc:creator>Baolin Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104087</prism:doi><description>Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.
Published: 2025-12-25T00:16:18+00:00
Venue: Information Fusion
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changbin Wang; Fengrui Ji; Baolin Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104087"&gt;10.1016/j.inffus.2025.104087&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.&lt;/p&gt;</content:encoded></item></channel></rss>