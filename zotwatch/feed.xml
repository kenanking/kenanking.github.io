<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Thu, 15 Jan 2026 02:48:55 +0000</lastBuildDate><item><title>IRPNet: Infrared Small Target Detection via RGB Prior Guidance and Physics Feature Fusion</title><link>https://doi.org/10.1109/tgrs.2026.3653784</link><guid>10.1109/tgrs.2026.3653784</guid><pubDate>Tue, 13 Jan 2026 20:58:44 +0000</pubDate><dc:creator>Rui Yao</dc:creator><dc:creator>Nana Guo</dc:creator><dc:creator>Hancheng Zhu</dc:creator><dc:creator>Kunyang Sun</dc:creator><dc:creator>Fuyuan Hu</dc:creator><dc:creator>Xixi Li</dc:creator><dc:creator>Jiaqi Zhao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3653784</prism:doi><description>Infrared small target detection (IRSTD) aims to tackle the tough issue of identifying dim and tiny targets in low signal-to-noise ratio (SNR) images, which is crucial for real-world applications such as surveillance, military, and marine rescue. Recent breakthroughs in deep learning have driven IRSTD advancements. However, these methods treat it as a pure black-box paradigm, neglecting the inherent physics of infrared images. This issue significantly affects the model’s accuracy, resulting in lower performance and increased false alarms. Furthermore, existing datasets are limited to about 1,000 samples per benchmark, whereas RGB datasets offer richer samples and information. Inspired by the fact that low SNR and the scarcity of datasets greatly challenge performance, we propose a novel IRSTD network with RGB prior guidance and physics feature fusion (IRPNet). Specifically, an image encoder of CLIP including rich RGB prior information is first employed to integrate RGB knowledge into IRSTD to enrich the model’s representational capacity. Subsequently, infrared-specific visual features are extracted using convolutional block attention mechanisms to capture complex pixel-level relationships. In parallel, a dedicated physical feature extraction (PFE) block is applied to capture the essential properties of infrared images, complementing purely data-driven approaches. Finally, these features are progressively fused through a physical-visual feature fusion (PVFF) block and a multi-scale feature fusion (MSFF) module to enhance representations. Extensive experiments on NUAA-SIRST, IRSTD-1k, NUDT-SIRST and SIRST-Aug demonstrate that IRPNet outperforms existing state-of-the-art (SOTA) methods, providing a more robust and accurate solution. The source code is available at https://github.com/rayyao/IRPNet.
Published: 2026-01-13T20:58:44+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.833 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Yao; Nana Guo; Hancheng Zhu; Kunyang Sun; Fuyuan Hu; Xixi Li; Jiaqi Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3653784"&gt;10.1109/tgrs.2026.3653784&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.833 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) aims to tackle the tough issue of identifying dim and tiny targets in low signal-to-noise ratio (SNR) images, which is crucial for real-world applications such as surveillance, military, and marine rescue. Recent breakthroughs in deep learning have driven IRSTD advancements. However, these methods treat it as a pure black-box paradigm, neglecting the inherent physics of infrared images. This issue significantly affects the model’s accuracy, resulting in lower performance and increased false alarms. Furthermore, existing datasets are limited to about 1,000 samples per benchmark, whereas RGB datasets offer richer samples and information. Inspired by the fact that low SNR and the scarcity of datasets greatly challenge performance, we propose a novel IRSTD network with RGB prior guidance and physics feature fusion (IRPNet). Specifically, an image encoder of CLIP including rich RGB prior information is first employed to integrate RGB knowledge into IRSTD to enrich the model’s representational capacity. Subsequently, infrared-specific visual features are extracted using convolutional block attention mechanisms to capture complex pixel-level relationships. In parallel, a dedicated physical feature extraction (PFE) block is applied to capture the essential properties of infrared images, complementing purely data-driven approaches. Finally, these features are progressively fused through a physical-visual feature fusion (PVFF) block and a multi-scale feature fusion (MSFF) module to enhance representations. Extensive experiments on NUAA-SIRST, IRSTD-1k, NUDT-SIRST and SIRST-Aug demonstrate that IRPNet outperforms existing state-of-the-art (SOTA) methods, providing a more robust and accurate solution. The source code is available at https://github.com/rayyao/IRPNet.&lt;/p&gt;</content:encoded></item><item><title>Cross-Scene Hyperspectral Image Classification via Bidirectional Mamba and Domain Mixing Network</title><link>https://doi.org/10.1109/tnnls.2026.3651563</link><guid>10.1109/tnnls.2026.3651563</guid><pubDate>Tue, 13 Jan 2026 20:59:32 +0000</pubDate><dc:creator>Junzhe Dang</dc:creator><dc:creator>Chengwang Guo</dc:creator><dc:creator>Mengmeng Zhang</dc:creator><dc:creator>Yuxiang Zhang</dc:creator><dc:creator>Wen Jia</dc:creator><dc:creator>Wei Li</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2026.3651563</prism:doi><description>To overcome the challenges posed by domain shift in hyperspectral image (HSI) classification, methods based on domain adaptation (DA) have been widely used. Currently, most HSI DA methods focus on designing complex strategies to align the distributions of the source domain (SD) and the target domain (TD) in the feature space after feature extraction, yielding promising results. However, when there exists a large domain shift between SD and TD, it becomes challenging to map them into the same feature space. In this article, we propose the bidirectional mamba and domain mixing network (BMDMnet). Since pure CNN architectures are constrained in local feature extraction, while transformer-based models improve global feature capturing capability at the cost of high computational complexity, we propose the bidirectional mamba module (BMM) as an efficient solution for capturing long-range dependencies. In addition, a self-distillation strategy is employed during training. By utilizing a more stable teacher model, reliable predictions can be obtained in the TD. Subsequently, a domain mixing supervised learning (DMSL) module is designed, which creates a mixed domain by selecting low-entropy sample-pseudo-label pairs from the TD and randomly combining them with sample-label pairs from the SD. DMSL aims to introduce mixed domain to mitigate the inter-domain gap in the data space, thereby enabling the model to learn TD representations more effectively. Experiments demonstrate that BMDMnet outperforms state-of-the-art algorithms across three cross-scene datasets.
Published: 2026-01-13T20:59:32+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junzhe Dang; Chengwang Guo; Mengmeng Zhang; Yuxiang Zhang; Wen Jia; Wei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2026.3651563"&gt;10.1109/tnnls.2026.3651563&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;To overcome the challenges posed by domain shift in hyperspectral image (HSI) classification, methods based on domain adaptation (DA) have been widely used. Currently, most HSI DA methods focus on designing complex strategies to align the distributions of the source domain (SD) and the target domain (TD) in the feature space after feature extraction, yielding promising results. However, when there exists a large domain shift between SD and TD, it becomes challenging to map them into the same feature space. In this article, we propose the bidirectional mamba and domain mixing network (BMDMnet). Since pure CNN architectures are constrained in local feature extraction, while transformer-based models improve global feature capturing capability at the cost of high computational complexity, we propose the bidirectional mamba module (BMM) as an efficient solution for capturing long-range dependencies. In addition, a self-distillation strategy is employed during training. By utilizing a more stable teacher model, reliable predictions can be obtained in the TD. Subsequently, a domain mixing supervised learning (DMSL) module is designed, which creates a mixed domain by selecting low-entropy sample-pseudo-label pairs from the TD and randomly combining them with sample-label pairs from the SD. DMSL aims to introduce mixed domain to mitigate the inter-domain gap in the data space, thereby enabling the model to learn TD representations more effectively. Experiments demonstrate that BMDMnet outperforms state-of-the-art algorithms across three cross-scene datasets.&lt;/p&gt;</content:encoded></item><item><title>OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation</title><link>https://arxiv.org/abs/2601.06835v1</link><guid>http://arxiv.org/abs/2601.06835v1</guid><pubDate>Sun, 11 Jan 2026 09:57:04 +0000</pubDate><dc:creator>Hyunseo Lee</dc:creator><dc:creator>Sang Min Kim</dc:creator><dc:creator>Ho Kyung Shin</dc:creator><dc:creator>Taeheon Kim</dc:creator><dc:creator>Woo-Jeoung Nam</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.
Published: 2026-01-11T09:57:04+00:00
Venue: arXiv
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyunseo Lee; Sang Min Kim; Ho Kyung Shin; Taeheon Kim; Woo-Jeoung Nam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>Multi-Stage Knowledge Integration of Vision-Language Models for Continual Learning</title><link>https://doi.org/10.1109/tip.2026.3652014</link><guid>10.1109/tip.2026.3652014</guid><pubDate>Tue, 13 Jan 2026 21:01:14 +0000</pubDate><dc:creator>Hongsheng Zhang</dc:creator><dc:creator>Zhong Ji</dc:creator><dc:creator>Jingren Liu</dc:creator><dc:creator>Yanwei Pang</dc:creator><dc:creator>Jungong Han</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3652014</prism:doi><description>Vision Language Models (VLMs), pre-trained on large-scale image-text datasets, enable zero-shot predictions for unseen data but may underperform on specific unseen tasks. Continual learning (CL) can help VLMs effectively adapt to new data distributions without joint training, but faces challenges of catastrophic forgetting and generalization forgetting. Although significant progress has been achieved by distillation-based methods, they exhibit two severe limitations. One is the popularly adopted single-teacher paradigm fails to impart comprehensive knowledge, The other is the existing methods inadequately leverage the multimodal information in the original training dataset, instead they rely on additional data for distillation, which increases computational and storage overhead. To mitigate both limitations, by drawing on Knowledge Integration Theory (KIT), we propose a Multi-Stage Knowledge Integration network (MulKI) to emulate the human learning process in distillation methods. MulKI achieves this through four stages, including Eliciting Ideas, Adding New Ideas, Distinguishing Ideas, and Making Connections. During the four stages, we first leverage prototypes to align across modalities, eliciting cross-modal knowledge, then adding new knowledge by constructing fine-grained intra- and inter-modality relationships with prototypes. After that, knowledge from two teacher models is adaptively distinguished and re-weighted. Finally, we connect between models from intra- and inter-task, integrating preceding and new knowledge. Our method demonstrates significant improvements in maintaining zero-shot capabilities while supporting continual learning across diverse downstream tasks, showcasing its potential in adapting VLMs to evolving data distributions.
Published: 2026-01-13T21:01:14+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongsheng Zhang; Zhong Ji; Jingren Liu; Yanwei Pang; Jungong Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3652014"&gt;10.1109/tip.2026.3652014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Language Models (VLMs), pre-trained on large-scale image-text datasets, enable zero-shot predictions for unseen data but may underperform on specific unseen tasks. Continual learning (CL) can help VLMs effectively adapt to new data distributions without joint training, but faces challenges of catastrophic forgetting and generalization forgetting. Although significant progress has been achieved by distillation-based methods, they exhibit two severe limitations. One is the popularly adopted single-teacher paradigm fails to impart comprehensive knowledge, The other is the existing methods inadequately leverage the multimodal information in the original training dataset, instead they rely on additional data for distillation, which increases computational and storage overhead. To mitigate both limitations, by drawing on Knowledge Integration Theory (KIT), we propose a Multi-Stage Knowledge Integration network (MulKI) to emulate the human learning process in distillation methods. MulKI achieves this through four stages, including Eliciting Ideas, Adding New Ideas, Distinguishing Ideas, and Making Connections. During the four stages, we first leverage prototypes to align across modalities, eliciting cross-modal knowledge, then adding new knowledge by constructing fine-grained intra- and inter-modality relationships with prototypes. After that, knowledge from two teacher models is adaptively distinguished and re-weighted. Finally, we connect between models from intra- and inter-task, integrating preceding and new knowledge. Our method demonstrates significant improvements in maintaining zero-shot capabilities while supporting continual learning across diverse downstream tasks, showcasing its potential in adapting VLMs to evolving data distributions.&lt;/p&gt;</content:encoded></item><item><title>SAMURAI: Motion-Aware Memory for Training-Free Visual Object Tracking with SAM 2</title><link>https://doi.org/10.1109/tip.2026.3651835</link><guid>10.1109/tip.2026.3651835</guid><pubDate>Tue, 13 Jan 2026 21:01:14 +0000</pubDate><dc:creator>Cheng-Yeng Yang</dc:creator><dc:creator>Hsiang-Wei Huang</dc:creator><dc:creator>Zhongyu Jiang</dc:creator><dc:creator>Wenhao Chai</dc:creator><dc:creator>Jenq-Neng Hwang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3651835</prism:doi><description>The Segment Anything Model 2 (SAM 2) has demonstrated exceptional performance in object segmentation tasks but encounters challenges in visual object tracking, particularly in handling crowded scenes with fast-moving or self-occluding objects. Additionally, its fixed-window memory mechanism indiscriminately retains past frames, leading to error accumulation. This issue results in incorrect memory retention during occlusions, causing the model to condition future predictions on unreliable features and leading to identity switches or drift in crowded scenes. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 that integrates temporal motion cues with a novel motion-aware memory selection strategy. SAMURAI effectively predicts object motion and refines mask selection, achieving robust and precise tracking without requiring retraining or fine-tuning. It demonstrates strong training-free performance across multiple VOT benchmark datasets, underscoring its generalization capability. SAMURAI achieves state-of-the-art performance on LaSOText, GOT-10k, and TrackingNet, while also delivering competitive results on LaSOT, VOT2020-ST, VOT2022-ST, and VOS benchmarks such as SA-V. These results highlight SAMURAI’s robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments with an optimized memory selection mechanism. Code and results are available at https://github.com/yangchris11/samurai.
Published: 2026-01-13T21:01:14+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheng-Yeng Yang; Hsiang-Wei Huang; Zhongyu Jiang; Wenhao Chai; Jenq-Neng Hwang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3651835"&gt;10.1109/tip.2026.3651835&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;The Segment Anything Model 2 (SAM 2) has demonstrated exceptional performance in object segmentation tasks but encounters challenges in visual object tracking, particularly in handling crowded scenes with fast-moving or self-occluding objects. Additionally, its fixed-window memory mechanism indiscriminately retains past frames, leading to error accumulation. This issue results in incorrect memory retention during occlusions, causing the model to condition future predictions on unreliable features and leading to identity switches or drift in crowded scenes. This paper introduces SAMURAI, an enhanced adaptation of SAM 2 that integrates temporal motion cues with a novel motion-aware memory selection strategy. SAMURAI effectively predicts object motion and refines mask selection, achieving robust and precise tracking without requiring retraining or fine-tuning. It demonstrates strong training-free performance across multiple VOT benchmark datasets, underscoring its generalization capability. SAMURAI achieves state-of-the-art performance on LaSOText, GOT-10k, and TrackingNet, while also delivering competitive results on LaSOT, VOT2020-ST, VOT2022-ST, and VOS benchmarks such as SA-V. These results highlight SAMURAI’s robustness in complex tracking scenarios and its potential for real-world applications in dynamic environments with an optimized memory selection mechanism. Code and results are available at https://github.com/yangchris11/samurai.&lt;/p&gt;</content:encoded></item><item><title>Progressive Feature Encoding with Background Perturbation Learning for Ultra-Fine-Grained Visual Categorization</title><link>https://doi.org/10.1109/tip.2026.3651956</link><guid>10.1109/tip.2026.3651956</guid><pubDate>Tue, 13 Jan 2026 21:01:14 +0000</pubDate><dc:creator>Xin Jiang</dc:creator><dc:creator>Ziye Fang</dc:creator><dc:creator>Fei Shen</dc:creator><dc:creator>Junyao Gao</dc:creator><dc:creator>Zechao Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3651956</prism:doi><description>Ultra-Fine-Grained Visual Categorization (Ultra-FGVC) aims to classify objects into sub-granular categories, presenting the challenge of distinguishing visually similar objects with limited data. Existing methods primarily address sample scarcity but often overlook the importance of leveraging intrinsic object features to construct highly discriminative representations. This limitation significantly constrains their effectiveness in Ultra-FGVC tasks. To address these challenges, we propose SV-Transformer that progressively encodes object features while incorporating background perturbation modeling to generate robust and discriminative representations. At the core of our approach is a progressive feature encoder, which hierarchically extracts global semantic structures and local discriminative details from backbone-generated representations. This design enhances inter-class separability while ensuring resilience to intra-class variations. Furthermore, our background perturbation learning mechanism introduces controlled variations in the feature space, effectively mitigating the impact of sample limitations and improving the model’s capacity to capture fine-grained distinctions. Comprehensive experiments demonstrate that SV-Transformer achieves state-of-the-art performance on benchmark Ultra-FGVC datasets, showcasing its efficacy in addressing the challenges of Ultra-FGVC task.
Published: 2026-01-13T21:01:14+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Jiang; Ziye Fang; Fei Shen; Junyao Gao; Zechao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3651956"&gt;10.1109/tip.2026.3651956&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Ultra-Fine-Grained Visual Categorization (Ultra-FGVC) aims to classify objects into sub-granular categories, presenting the challenge of distinguishing visually similar objects with limited data. Existing methods primarily address sample scarcity but often overlook the importance of leveraging intrinsic object features to construct highly discriminative representations. This limitation significantly constrains their effectiveness in Ultra-FGVC tasks. To address these challenges, we propose SV-Transformer that progressively encodes object features while incorporating background perturbation modeling to generate robust and discriminative representations. At the core of our approach is a progressive feature encoder, which hierarchically extracts global semantic structures and local discriminative details from backbone-generated representations. This design enhances inter-class separability while ensuring resilience to intra-class variations. Furthermore, our background perturbation learning mechanism introduces controlled variations in the feature space, effectively mitigating the impact of sample limitations and improving the model’s capacity to capture fine-grained distinctions. Comprehensive experiments demonstrate that SV-Transformer achieves state-of-the-art performance on benchmark Ultra-FGVC datasets, showcasing its efficacy in addressing the challenges of Ultra-FGVC task.&lt;/p&gt;</content:encoded></item><item><title>Physics-Driven Feature Decoupling for Infrared Small Targets: A Dual Geometry-Guided Experts Network</title><link>https://doi.org/10.1016/j.knosys.2026.115268</link><guid>10.1016/j.knosys.2026.115268</guid><pubDate>Tue, 13 Jan 2026 00:40:52 +0000</pubDate><dc:creator>Yubing Lu</dc:creator><dc:creator>Pingping Liu</dc:creator><dc:creator>Tongshun Zhang</dc:creator><dc:creator>Aohua Li</dc:creator><dc:creator>Qiuzhan Zhou</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115268</prism:doi><description>Infrared small target detection (ISTD) presents a critical state estimation challenge for autonomous systems, where the core challenge lies in the reliable identification of weak, low-contrast targets obscured by complex background clutter. Conventional deep learning methods suffer from feature entanglement between targets and backgrounds, limiting detection efficacy and stability. To address this, we propose the Dual Geometry-Guided Experts Network (DGGENet), a novel architecture built upon the Mixture-of-Experts (MoE) paradigm. DGGENet’s innovation centers on its Target-Background Decoupling Module (TBDM). TBDM employs a geometry-guided dynamic routing mechanism functioning as an adaptive gating network. This gate continuously analyzes input features to dynamically establish two specialized expert groups: one dedicated to target channels and another to background channels. Each group functions as a sub-network specialized for its assigned feature subspace. Guided by Robust Principal Component Analysis (RPCA) principles, the inter-group expert iteration promotes low-rank background and sparse target representations during collaborative feature refinement. Subsequently, a cross fusion module acts as state feedback, enabling semantically consistent interaction between the optimized representations. This closed-loop interaction explicitly models target-background correlations, ensuring global consistency and stability in the final output and yielding effectively disentangled feature representations. Comprehensive evaluation demonstrates that integrating TBDM into a U-Net architecture (as DGGENet) achieves superior performance on established benchmarks, attaining remarkable mIoU scores of 96.10% on the NUDT-SIRST dataset and 69.39% on the IRSTD-1K dataset. Furthermore, TBDM proves to be a versatile plug-and-play component, consistently enhancing diverse ISTD frameworks and underscoring its broad applicability across detection paradigms.
Published: 2026-01-13T00:40:52+00:00
Venue: Knowledge-Based Systems
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yubing Lu; Pingping Liu; Tongshun Zhang; Aohua Li; Qiuzhan Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115268"&gt;10.1016/j.knosys.2026.115268&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) presents a critical state estimation challenge for autonomous systems, where the core challenge lies in the reliable identification of weak, low-contrast targets obscured by complex background clutter. Conventional deep learning methods suffer from feature entanglement between targets and backgrounds, limiting detection efficacy and stability. To address this, we propose the Dual Geometry-Guided Experts Network (DGGENet), a novel architecture built upon the Mixture-of-Experts (MoE) paradigm. DGGENet’s innovation centers on its Target-Background Decoupling Module (TBDM). TBDM employs a geometry-guided dynamic routing mechanism functioning as an adaptive gating network. This gate continuously analyzes input features to dynamically establish two specialized expert groups: one dedicated to target channels and another to background channels. Each group functions as a sub-network specialized for its assigned feature subspace. Guided by Robust Principal Component Analysis (RPCA) principles, the inter-group expert iteration promotes low-rank background and sparse target representations during collaborative feature refinement. Subsequently, a cross fusion module acts as state feedback, enabling semantically consistent interaction between the optimized representations. This closed-loop interaction explicitly models target-background correlations, ensuring global consistency and stability in the final output and yielding effectively disentangled feature representations. Comprehensive evaluation demonstrates that integrating TBDM into a U-Net architecture (as DGGENet) achieves superior performance on established benchmarks, attaining remarkable mIoU scores of 96.10% on the NUDT-SIRST dataset and 69.39% on the IRSTD-1K dataset. Furthermore, TBDM proves to be a versatile plug-and-play component, consistently enhancing diverse ISTD frameworks and underscoring its broad applicability across detection paradigms.&lt;/p&gt;</content:encoded></item><item><title>AtomThink: Multimodal Slow Thinking With Atomic Step Reasoning</title><link>https://doi.org/10.1109/tpami.2026.3653573</link><guid>10.1109/tpami.2026.3653573</guid><pubDate>Tue, 13 Jan 2026 20:58:38 +0000</pubDate><dc:creator>Kun Xiang</dc:creator><dc:creator>Zhili Liu</dc:creator><dc:creator>Terry Jingchen Zhang</dc:creator><dc:creator>Yinya Huang</dc:creator><dc:creator>Yunshuang Nie</dc:creator><dc:creator>Kaixin Cai</dc:creator><dc:creator>Yiyang Yin</dc:creator><dc:creator>Runhui Huang</dc:creator><dc:creator>Hanhui Li</dc:creator><dc:creator>Yihan Zeng</dc:creator><dc:creator>Yu-Jie Yuan</dc:creator><dc:creator>Jianhua Han</dc:creator><dc:creator>Lanqing Hong</dc:creator><dc:creator>Hang Xu</dc:creator><dc:creator>Xiaodan Liang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653573</prism:doi><description>In this paper, we address the challenging task of multimodal reasoning by incorporating the notion of “slow thinking” into multimodal large language models (MLLMs). Our core idea is that models can learn to adaptively use different levels of reasoning to tackle questions of varying complexity. We propose a novel paradigm of Self-structured Chain of Thought (SCoT), which consists of minimal semantic atomic steps. Unlike existing methods that rely on structured templates or free-form paradigms, our method not only generates flexible CoT structures for various complex tasks but also mitigates the phenomenon of overthinking for easier tasks. To introduce structured reasoning into visual cognition, we design a novel AtomThink framework with four key modules: (i) a data engine to generate high-quality multimodal reasoning paths; (ii) a supervised fine-tuning (SFT) process with serialized inference data; (iii) a policy-guided multi-turn inference method; and (iv) an atomic capability metric to evaluate the single-step utilization rate. Extensive experiments demonstrate that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving more than 10% average accuracy gains on MathVista and MathVerse. Compared to state-of-the-art structured CoT approaches, our method not only achieves higher accuracy but also improves data utilization by 5 × and boosts inference efficiency by 85.3%. Our code is publicly available at https://github.com/Kun-Xiang/AtomThink.
Published: 2026-01-13T20:58:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kun Xiang; Zhili Liu; Terry Jingchen Zhang; Yinya Huang; Yunshuang Nie; Kaixin Cai; Yiyang Yin; Runhui Huang; Hanhui Li; Yihan Zeng; Yu-Jie Yuan; Jianhua Han; Lanqing Hong; Hang Xu; Xiaodan Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653573"&gt;10.1109/tpami.2026.3653573&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we address the challenging task of multimodal reasoning by incorporating the notion of “slow thinking” into multimodal large language models (MLLMs). Our core idea is that models can learn to adaptively use different levels of reasoning to tackle questions of varying complexity. We propose a novel paradigm of Self-structured Chain of Thought (SCoT), which consists of minimal semantic atomic steps. Unlike existing methods that rely on structured templates or free-form paradigms, our method not only generates flexible CoT structures for various complex tasks but also mitigates the phenomenon of overthinking for easier tasks. To introduce structured reasoning into visual cognition, we design a novel AtomThink framework with four key modules: (i) a data engine to generate high-quality multimodal reasoning paths; (ii) a supervised fine-tuning (SFT) process with serialized inference data; (iii) a policy-guided multi-turn inference method; and (iv) an atomic capability metric to evaluate the single-step utilization rate. Extensive experiments demonstrate that the proposed AtomThink significantly improves the performance of baseline MLLMs, achieving more than 10% average accuracy gains on MathVista and MathVerse. Compared to state-of-the-art structured CoT approaches, our method not only achieves higher accuracy but also improves data utilization by 5 × and boosts inference efficiency by 85.3%. Our code is publicly available at https://github.com/Kun-Xiang/AtomThink.&lt;/p&gt;</content:encoded></item><item><title>GeoCraft: A Diffusion Model-Based 3D Reconstruction Method Driven by Image and Point Cloud Fusion</title><link>https://doi.org/10.1016/j.inffus.2026.104149</link><guid>10.1016/j.inffus.2026.104149</guid><pubDate>Wed, 14 Jan 2026 00:29:37 +0000</pubDate><dc:creator>Weixuan Ma</dc:creator><dc:creator>Yamin Li</dc:creator><dc:creator>Chujin Liu</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Jie Li</dc:creator><dc:creator>Kansong Chen</dc:creator><dc:creator>Weixuan Gao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104149</prism:doi><description>With the rapid development of technologies like virtual reality (VR), autonomous driving, and digital twins, the demand for high-precision and realistic multimodal 3D reconstruction has surged. This technology has become a core research focus in computer vision and graphics due to its ability to integrate multi-source data, such as 2D images and point clouds. However, existing methods face challenges such as geometric inconsistency in single-view reconstruction, poor point cloud-to-mesh conversion, and insufficient multimodal feature fusion, limiting their practical application. To address these issues, this paper proposes GeoCraft, a multimodal 3D reconstruction method that generates high-precision 3D models from 2D images through three collaborative stages: Diff2DPoint, Point2DMesh, and Vision3DGen. Specifically, Diff2DPoint generates an initial point cloud with geometric alignment using a diffusion model and projection feature fusion; Point2DMesh converts the point cloud into a high-quality mesh using an autoregressive decoder-only Transformer and Direct Preference Optimization (DPO); Vision3DGen creates high-fidelity 3D objects through multimodal feature alignment. Experiments on the Google Scanned Objects (GSO) and Pix3D datasets show that GeoCraft excels in key metrics. On the GSO dataset, its CMMD is 2.810 and FID CLIP is 26.420; on Pix3D, CMMD is 3.020 and FID CLIP is 27.030. GeoCraft significantly outperforms existing 3D reconstruction methods and also demonstrates advantages in computational efficiency, effectively solving key challenges in 3D reconstruction.The code is available at https://github.com/weixuanma/GeoCraft .
Published: 2026-01-14T00:29:37+00:00
Venue: Information Fusion
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weixuan Ma; Yamin Li; Chujin Liu; Hao Zhang; Jie Li; Kansong Chen; Weixuan Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104149"&gt;10.1016/j.inffus.2026.104149&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid development of technologies like virtual reality (VR), autonomous driving, and digital twins, the demand for high-precision and realistic multimodal 3D reconstruction has surged. This technology has become a core research focus in computer vision and graphics due to its ability to integrate multi-source data, such as 2D images and point clouds. However, existing methods face challenges such as geometric inconsistency in single-view reconstruction, poor point cloud-to-mesh conversion, and insufficient multimodal feature fusion, limiting their practical application. To address these issues, this paper proposes GeoCraft, a multimodal 3D reconstruction method that generates high-precision 3D models from 2D images through three collaborative stages: Diff2DPoint, Point2DMesh, and Vision3DGen. Specifically, Diff2DPoint generates an initial point cloud with geometric alignment using a diffusion model and projection feature fusion; Point2DMesh converts the point cloud into a high-quality mesh using an autoregressive decoder-only Transformer and Direct Preference Optimization (DPO); Vision3DGen creates high-fidelity 3D objects through multimodal feature alignment. Experiments on the Google Scanned Objects (GSO) and Pix3D datasets show that GeoCraft excels in key metrics. On the GSO dataset, its CMMD is 2.810 and FID CLIP is 26.420; on Pix3D, CMMD is 3.020 and FID CLIP is 27.030. GeoCraft significantly outperforms existing 3D reconstruction methods and also demonstrates advantages in computational efficiency, effectively solving key challenges in 3D reconstruction.The code is available at https://github.com/weixuanma/GeoCraft .&lt;/p&gt;</content:encoded></item><item><title>A Multi-Modal Approach for Robust Oriented Ship Detection: Dataset and Methodology</title><link>https://doi.org/10.3390/rs18020274</link><guid>10.3390/rs18020274</guid><pubDate>Wed, 14 Jan 2026 15:12:04 +0000</pubDate><dc:creator>Jianing You</dc:creator><dc:creator>Yixuan Lv</dc:creator><dc:creator>Shengyang Li</dc:creator><dc:creator>Silei Liu</dc:creator><dc:creator>Kailun Zhang</dc:creator><dc:creator>Yuxuan Liu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020274</prism:doi><description>Maritime ship detection is a critical task for security and traffic management. To advance research in this area, we constructed a new high-resolution, spatially aligned optical-SAR dataset, named MOS-Ship. Building on this, we propose MOS-DETR, a novel query-based framework. This model incorporates an innovative multi-modal Swin Transformer backbone to extract unified feature pyramids from both RGB and SAR images. This design allows the model to jointly exploit optical textures and SAR scattering signatures for precise, oriented bounding box prediction. We also introduce an adaptive probabilistic fusion mechanism. This post-processing module dynamically integrates the detection results generated by our model from the optical and SAR inputs, synergistically combining their complementary strengths. Experiments validate that MOS-DETR achieves highly competitive accuracy and significantly outperforms unimodal baselines, demonstrating superior robustness across diverse conditions. This work provides a robust framework and methodology for advancing multimodal maritime surveillance.
Published: 2026-01-14T15:12:04+00:00
Venue: Remote Sensing
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianing You; Yixuan Lv; Shengyang Li; Silei Liu; Kailun Zhang; Yuxuan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020274"&gt;10.3390/rs18020274&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Maritime ship detection is a critical task for security and traffic management. To advance research in this area, we constructed a new high-resolution, spatially aligned optical-SAR dataset, named MOS-Ship. Building on this, we propose MOS-DETR, a novel query-based framework. This model incorporates an innovative multi-modal Swin Transformer backbone to extract unified feature pyramids from both RGB and SAR images. This design allows the model to jointly exploit optical textures and SAR scattering signatures for precise, oriented bounding box prediction. We also introduce an adaptive probabilistic fusion mechanism. This post-processing module dynamically integrates the detection results generated by our model from the optical and SAR inputs, synergistically combining their complementary strengths. Experiments validate that MOS-DETR achieves highly competitive accuracy and significantly outperforms unimodal baselines, demonstrating superior robustness across diverse conditions. This work provides a robust framework and methodology for advancing multimodal maritime surveillance.&lt;/p&gt;</content:encoded></item><item><title>SCRTN: Enhancing Multi-modal 3D Object Detection in Complex Environments</title><link>https://doi.org/10.1016/j.patcog.2026.113068</link><guid>10.1016/j.patcog.2026.113068</guid><pubDate>Tue, 13 Jan 2026 17:04:41 +0000</pubDate><dc:creator>Xiufeng Zhu</dc:creator><dc:creator>Qing Shen</dc:creator><dc:creator>Zhenfang Liu</dc:creator><dc:creator>Kang Zhao</dc:creator><dc:creator>Jungang Lou</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113068</prism:doi><description>In complex application scenarios, noise and environmental interference significantly challenge the accurate association of multi-modal features for 3D object detection. To tackle this issue, this study introduces an advanced multi-modal framework, the Sparse Convolutional Residual Network. The framework integrates two key innovations: first, a region-of-interest feature fusion module called ResTransfusion, which enhances global feature associations between voxel point clouds and augmented color-based point clouds; second, a distant voxel retention sampling strategy that strategically reduces voxel count while maintaining key spatial information, thereby improving computational efficiency. Extensive experiments on the KITTI, NuScenes, and Waymo Open datasets demonstrate the effectiveness of the proposed approach. Notably, it achieves a state-of-the-art mean average precision (mAP) of 89.67% on the KITTI Hard benchmark and delivers competitive performance on NuScenes and Waymo, particularly in noisy and occluded real-world settings where it surpasses existing methods. Our project page is available at https://github.com/zhuxzhuif/SCRTN .
Published: 2026-01-13T17:04:41+00:00
Venue: Pattern Recognition
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiufeng Zhu; Qing Shen; Zhenfang Liu; Kang Zhao; Jungang Lou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113068"&gt;10.1016/j.patcog.2026.113068&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;In complex application scenarios, noise and environmental interference significantly challenge the accurate association of multi-modal features for 3D object detection. To tackle this issue, this study introduces an advanced multi-modal framework, the Sparse Convolutional Residual Network. The framework integrates two key innovations: first, a region-of-interest feature fusion module called ResTransfusion, which enhances global feature associations between voxel point clouds and augmented color-based point clouds; second, a distant voxel retention sampling strategy that strategically reduces voxel count while maintaining key spatial information, thereby improving computational efficiency. Extensive experiments on the KITTI, NuScenes, and Waymo Open datasets demonstrate the effectiveness of the proposed approach. Notably, it achieves a state-of-the-art mean average precision (mAP) of 89.67% on the KITTI Hard benchmark and delivers competitive performance on NuScenes and Waymo, particularly in noisy and occluded real-world settings where it surpasses existing methods. Our project page is available at https://github.com/zhuxzhuif/SCRTN .&lt;/p&gt;</content:encoded></item><item><title>Adapt, Generate, and Supervise: Geometry-Aware Diffusion-Guided SAM Framework for Remote Sensing Semantic Segmentation</title><link>https://doi.org/10.1109/tgrs.2026.3653952</link><guid>10.1109/tgrs.2026.3653952</guid><pubDate>Tue, 13 Jan 2026 20:58:44 +0000</pubDate><dc:creator>Wujie Zhou</dc:creator><dc:creator>Jin Xie</dc:creator><dc:creator>Caie Xu</dc:creator><dc:creator>Yuanyuan Liu</dc:creator><dc:creator>Yunchao Wang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3653952</prism:doi><description>Foundation models such as the segment anything model (SAM) have remarkable generalization capabilities in natural image segmentation. However, their application to remote sensing (RS) semantic segmentation faces significant challenges. The single-modal architecture of SAM cannot effectively utilize multi-modal RS data, its vision Transformer encoder lacks sensitivity to multi-scale spatial structures that are characteristic of RS imagery, and its dependence on manual prompts hinders large-scale automation. To address these challenges, we propose a geometry-aware diffusion-guided SAM framework (GeoSAM) that transforms SAM into fully automated multi-modal semantic segmentation through three synergistic innovations. First, we introduce a multi-scale geometric-aware adaptation module (GeoAdapter) that hierarchically integrates RGB images with normalized digital surface model data within the encoder. GeoAdapter incorporates a novel class-prior generator that combines geometric convolution, prototype similarity matching, and statistical modeling to produce structure-aware guidance for semantic–geometric feature fusion. Second, we present a diffusion prompt module that pioneers the use of conditional diffusion models for automatic prompt generation in RS applications, eliminating manual interactions through semantic-feature-guided denoising diffusion implicit model sampling. Third, we propose a prompt-level supervision strategy that mitigates training–inference distribution discrepancies through constraints on semantic consistency and structural alignment, ensuring robust prompt generation across different phases. Extensive experiments demonstrate state-of-the-art performance: 90.92% mean accuracy (mAcc) and 82.71% mean intersection over union (mIoU) on Vaihingen, and 85.28% mAcc and 75.49% mIoU on Potsdam, with improvements of 16.52% mAcc and 16.29% mIoU over original SAM on Vaihingen. The source code is available at https://github.com/110-011/GeoSAM.
Published: 2026-01-13T20:58:44+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wujie Zhou; Jin Xie; Caie Xu; Yuanyuan Liu; Yunchao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3653952"&gt;10.1109/tgrs.2026.3653952&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Foundation models such as the segment anything model (SAM) have remarkable generalization capabilities in natural image segmentation. However, their application to remote sensing (RS) semantic segmentation faces significant challenges. The single-modal architecture of SAM cannot effectively utilize multi-modal RS data, its vision Transformer encoder lacks sensitivity to multi-scale spatial structures that are characteristic of RS imagery, and its dependence on manual prompts hinders large-scale automation. To address these challenges, we propose a geometry-aware diffusion-guided SAM framework (GeoSAM) that transforms SAM into fully automated multi-modal semantic segmentation through three synergistic innovations. First, we introduce a multi-scale geometric-aware adaptation module (GeoAdapter) that hierarchically integrates RGB images with normalized digital surface model data within the encoder. GeoAdapter incorporates a novel class-prior generator that combines geometric convolution, prototype similarity matching, and statistical modeling to produce structure-aware guidance for semantic–geometric feature fusion. Second, we present a diffusion prompt module that pioneers the use of conditional diffusion models for automatic prompt generation in RS applications, eliminating manual interactions through semantic-feature-guided denoising diffusion implicit model sampling. Third, we propose a prompt-level supervision strategy that mitigates training–inference distribution discrepancies through constraints on semantic consistency and structural alignment, ensuring robust prompt generation across different phases. Extensive experiments demonstrate state-of-the-art performance: 90.92% mean accuracy (mAcc) and 82.71% mean intersection over union (mIoU) on Vaihingen, and 85.28% mAcc and 75.49% mIoU on Potsdam, with improvements of 16.52% mAcc and 16.29% mIoU over original SAM on Vaihingen. The source code is available at https://github.com/110-011/GeoSAM.&lt;/p&gt;</content:encoded></item><item><title>Reconstruction Guided Few-shot Network For Remote Sensing Image Classification</title><link>https://arxiv.org/abs/2601.07335v1</link><guid>http://arxiv.org/abs/2601.07335v1</guid><pubDate>Mon, 12 Jan 2026 09:02:30 +0000</pubDate><dc:creator>Mohit Jaiswal</dc:creator><dc:creator>Naman Jain</dc:creator><dc:creator>Shivani Pathak</dc:creator><dc:creator>Mainak Singha</dc:creator><dc:creator>Nikunja Bihari Kar</dc:creator><dc:creator>Ankit Jha</dc:creator><dc:creator>Biplab Banerjee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.
Published: 2026-01-12T09:02:30+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mohit Jaiswal; Naman Jain; Shivani Pathak; Mainak Singha; Nikunja Bihari Kar; Ankit Jha; Biplab Banerjee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.&lt;/p&gt;</content:encoded></item><item><title>Disentangle Object and Non-object Infrared Features via Language Guidance</title><link>https://arxiv.org/abs/2601.09228v1</link><guid>http://arxiv.org/abs/2601.09228v1</guid><pubDate>Wed, 14 Jan 2026 06:59:54 +0000</pubDate><dc:creator>Fan Liu</dc:creator><dc:creator>Ting Wu</dc:creator><dc:creator>Chuanyi Zhang</dc:creator><dc:creator>Liang Yao</dc:creator><dc:creator>Xing Ma</dc:creator><dc:creator>Yuhui Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.
Published: 2026-01-14T06:59:54+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fan Liu; Ting Wu; Chuanyi Zhang; Liang Yao; Xing Ma; Yuhui Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.&lt;/p&gt;</content:encoded></item><item><title>Benchmarking large language models on safety risks in scientific laboratories</title><link>https://doi.org/10.1038/s42256-025-01152-1</link><guid>10.1038/s42256-025-01152-1</guid><pubDate>Wed, 14 Jan 2026 10:01:54 +0000</pubDate><dc:creator>Yujun Zhou</dc:creator><dc:creator>Jingdong Yang</dc:creator><dc:creator>Yue Huang</dc:creator><dc:creator>Kehan Guo</dc:creator><dc:creator>Zoe Emory</dc:creator><dc:creator>Bikram Ghosh</dc:creator><dc:creator>Amita Bedar</dc:creator><dc:creator>Sujay Shekar</dc:creator><dc:creator>Zhenwen Liang</dc:creator><dc:creator>Pin-Yu Chen</dc:creator><dc:creator>Tian Gao</dc:creator><dc:creator>Werner Geyer</dc:creator><dc:creator>Nuno Moniz</dc:creator><dc:creator>Nitesh V. Chawla</dc:creator><dc:creator>Xiangliang Zhang</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01152-1</prism:doi><description>Artificial intelligence is revolutionizing scientific research, yet its growing integration into laboratory environments presents critical safety challenges. Large language models and vision language models now assist in experiment design and procedural guidance, yet their ‘illusion of understanding’ may lead researchers to overtrust unsafe outputs. Here we show that current models remain far from meeting the reliability needed for safe laboratory operation. We introduce LabSafety Bench, a comprehensive benchmark that evaluates models on hazard identification, risk assessment and consequence prediction across 765 multiple-choice questions and 404 realistic laboratory scenarios, encompassing 3,128 open-ended tasks. Evaluations on 19 advanced large language models and vision language models show that no model evaluated on hazard identification surpasses 70% accuracy. While proprietary models perform well on structured assessments, they do not show a clear advantage in open-ended reasoning. These results underscore the urgent need for specialized safety evaluation frameworks before deploying artificial intelligence systems in real laboratory settings. Large language models are starting to be used in safety-critical tasks such as controlling robots. Zhou et al. present LabSafety Bench, a benchmark evaluating the ability of large language models to identify hazards and assess laboratory risks.
Published: 2026-01-14T10:01:54+00:00
Venue: Nature Machine Intelligence
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yujun Zhou; Jingdong Yang; Yue Huang; Kehan Guo; Zoe Emory; Bikram Ghosh; Amita Bedar; Sujay Shekar; Zhenwen Liang; Pin-Yu Chen; Tian Gao; Werner Geyer; Nuno Moniz; Nitesh V. Chawla; Xiangliang Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01152-1"&gt;10.1038/s42256-025-01152-1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Artificial intelligence is revolutionizing scientific research, yet its growing integration into laboratory environments presents critical safety challenges. Large language models and vision language models now assist in experiment design and procedural guidance, yet their ‘illusion of understanding’ may lead researchers to overtrust unsafe outputs. Here we show that current models remain far from meeting the reliability needed for safe laboratory operation. We introduce LabSafety Bench, a comprehensive benchmark that evaluates models on hazard identification, risk assessment and consequence prediction across 765 multiple-choice questions and 404 realistic laboratory scenarios, encompassing 3,128 open-ended tasks. Evaluations on 19 advanced large language models and vision language models show that no model evaluated on hazard identification surpasses 70% accuracy. While proprietary models perform well on structured assessments, they do not show a clear advantage in open-ended reasoning. These results underscore the urgent need for specialized safety evaluation frameworks before deploying artificial intelligence systems in real laboratory settings. Large language models are starting to be used in safety-critical tasks such as controlling robots. Zhou et al. present LabSafety Bench, a benchmark evaluating the ability of large language models to identify hazards and assess laboratory risks.&lt;/p&gt;</content:encoded></item><item><title>Revisiting 360 Depth Estimation With PanoGabor: A New Fusion Perspective</title><link>https://doi.org/10.1109/tpami.2026.3653796</link><guid>10.1109/tpami.2026.3653796</guid><pubDate>Tue, 13 Jan 2026 20:58:38 +0000</pubDate><dc:creator>Zhijie Shen</dc:creator><dc:creator>Chunyu Lin</dc:creator><dc:creator>Lang Nie</dc:creator><dc:creator>Kang Liao</dc:creator><dc:creator>Weisi Lin</dc:creator><dc:creator>Yao Zhao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653796</prism:doi><description>Depth estimation from a monocular 360 image is important to the perception of the entire 3D environment. However, the inherent distortion and large field of view (FoV) in 360 images pose great challenges for this task. To this end, existing mainstream solutions typically introduce additional perspective-based 360 representations (e.g., Cubemap) to achieve effective feature extraction. Nevertheless, regardless of the introduced representations, they eventually need to be unified into the equirectangular projection (ERP) format for the subsequent depth estimation, which inevitably reintroduces additional distortions. In this work, we propose an oriented-distortion-aware Gabor Fusion framework (PGFuse) to address the above challenges. First, we introduce Gabor filters that analyze texture in the frequency domain, extending the receptive fields and enhancing depth cues. To address the reintroduced distortions, we design a latitude-aware distortion representation to generate customized, distortion-aware Gabor filters (PanoGabor filters). Furthermore, we design a channel- wise and spatial- wise unidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor filters to unify other representations into the ERP format, delivering effective and distortion-aware features. Considering the orientation sensitivity of the Gabor transform, we further introduce a spherical gradient constraint to stabilize this sensitivity. Experimental results on three popular indoor 360 benchmarks demonstrate the superiority of the proposed PGFuse to existing state-of-the-art solutions. Code and models will be available at https://github.com/zhijieshen-bjtu/PGFuse.
Published: 2026-01-13T20:58:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhijie Shen; Chunyu Lin; Lang Nie; Kang Liao; Weisi Lin; Yao Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653796"&gt;10.1109/tpami.2026.3653796&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Depth estimation from a monocular 360 image is important to the perception of the entire 3D environment. However, the inherent distortion and large field of view (FoV) in 360 images pose great challenges for this task. To this end, existing mainstream solutions typically introduce additional perspective-based 360 representations (e.g., Cubemap) to achieve effective feature extraction. Nevertheless, regardless of the introduced representations, they eventually need to be unified into the equirectangular projection (ERP) format for the subsequent depth estimation, which inevitably reintroduces additional distortions. In this work, we propose an oriented-distortion-aware Gabor Fusion framework (PGFuse) to address the above challenges. First, we introduce Gabor filters that analyze texture in the frequency domain, extending the receptive fields and enhancing depth cues. To address the reintroduced distortions, we design a latitude-aware distortion representation to generate customized, distortion-aware Gabor filters (PanoGabor filters). Furthermore, we design a channel- wise and spatial- wise unidirectional fusion module (CS-UFM) that integrates the proposed PanoGabor filters to unify other representations into the ERP format, delivering effective and distortion-aware features. Considering the orientation sensitivity of the Gabor transform, we further introduce a spherical gradient constraint to stabilize this sensitivity. Experimental results on three popular indoor 360 benchmarks demonstrate the superiority of the proposed PGFuse to existing state-of-the-art solutions. Code and models will be available at https://github.com/zhijieshen-bjtu/PGFuse.&lt;/p&gt;</content:encoded></item><item><title>Electromagnetic Scattering Characteristic-Enhanced Dual-Branch Network with Simulated Image Guidance for SAR Ship Classification</title><link>https://doi.org/10.3390/rs18020252</link><guid>10.3390/rs18020252</guid><pubDate>Tue, 13 Jan 2026 15:52:57 +0000</pubDate><dc:creator>Yanlin Feng</dc:creator><dc:creator>Xikai Fu</dc:creator><dc:creator>Shangchen Feng</dc:creator><dc:creator>Xiaolei Lv</dc:creator><dc:creator>Yiyi Wang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020252</prism:doi><description>Synthetic aperture radar (SAR), with its unique imaging principle and technical characteristics, has significant advantages in surface observation and thus has been widely applied in tasks such as object detection and target classification. However, limited by the lack of labeled SAR image datasets, the accuracy and generalization ability of the existing models in practical applications still need to be improved. In order to solve this problem, this paper proposes a spaceborne SAR image simulation technology and innovatively introduces the concept of bounce number map (BNM), establishing a high-resolution, parameterized simulated data support system for target recognition and classification tasks. In addition, an electromagnetic scattering characteristic-enhanced dual-branch network with simulated image guidance for SAR ship classification (SeDSG) was designed in this paper. It adopts a multi-source data utilization strategy, taking SAR images as the main branch input to capture the global features of real scenes, and using simulated data as the auxiliary branch input to excavate the electromagnetic scattering characteristics and detailed structural features. Through feature fusion, the advantages of the two branches are integrated to improve the adaptability and stability of the model to complex scenes. Experimental results show that the classification accuracy of the proposed network is improved on the OpenSARShip and FUSAR-Ship datasets. Meanwhile, the transfer learning classification results based on the SRSDD dataset verify the enhanced generalization and adaptive capabilities of the network, providing a new approach for data classification tasks with an insufficient number of samples.
Published: 2026-01-13T15:52:57+00:00
Venue: Remote Sensing
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanlin Feng; Xikai Fu; Shangchen Feng; Xiaolei Lv; Yiyi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020252"&gt;10.3390/rs18020252&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR), with its unique imaging principle and technical characteristics, has significant advantages in surface observation and thus has been widely applied in tasks such as object detection and target classification. However, limited by the lack of labeled SAR image datasets, the accuracy and generalization ability of the existing models in practical applications still need to be improved. In order to solve this problem, this paper proposes a spaceborne SAR image simulation technology and innovatively introduces the concept of bounce number map (BNM), establishing a high-resolution, parameterized simulated data support system for target recognition and classification tasks. In addition, an electromagnetic scattering characteristic-enhanced dual-branch network with simulated image guidance for SAR ship classification (SeDSG) was designed in this paper. It adopts a multi-source data utilization strategy, taking SAR images as the main branch input to capture the global features of real scenes, and using simulated data as the auxiliary branch input to excavate the electromagnetic scattering characteristics and detailed structural features. Through feature fusion, the advantages of the two branches are integrated to improve the adaptability and stability of the model to complex scenes. Experimental results show that the classification accuracy of the proposed network is improved on the OpenSARShip and FUSAR-Ship datasets. Meanwhile, the transfer learning classification results based on the SRSDD dataset verify the enhanced generalization and adaptive capabilities of the network, providing a new approach for data classification tasks with an insufficient number of samples.&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation</title><link>https://arxiv.org/abs/2601.06882v1</link><guid>http://arxiv.org/abs/2601.06882v1</guid><pubDate>Sun, 11 Jan 2026 12:10:56 +0000</pubDate><dc:creator>Dillan Imans</dc:creator><dc:creator>Phuoc-Nguyen Bui</dc:creator><dc:creator>Duc-Tai Le</dc:creator><dc:creator>Hyunseung Choo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation
Published: 2026-01-11T12:10:56+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dillan Imans; Phuoc-Nguyen Bui; Duc-Tai Le; Hyunseung Choo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation&lt;/p&gt;</content:encoded></item><item><title>Blind Inversion using Latent Diffusion Priors</title><link>https://doi.org/10.1109/tip.2026.3651963</link><guid>10.1109/tip.2026.3651963</guid><pubDate>Tue, 13 Jan 2026 21:01:14 +0000</pubDate><dc:creator>Weimin Bai</dc:creator><dc:creator>Siyi Chen</dc:creator><dc:creator>Wenzheng Chen</dc:creator><dc:creator>He Sun</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3651963</prism:doi><description>Diffusion models have emerged as powerful tools for solving inverse problems due to their exceptional ability to model complex prior distributions. However, existing methods predominantly assume known forward operators (i.e., non-blind), limiting their applicability in practical settings where acquiring such operators is costly. Additionally, many current approaches rely on pixel-space diffusion models, leaving the potential of more powerful latent diffusion models (LDMs) underexplored. In this paper, we introduce LatentDEM, an innovative technique that addresses more challenging blind inverse problems using latent diffusion priors. At the core of our method is solving blind inverse problems within an iterative Expectation-Maximization (EM) framework: (1) the E-step recovers clean images from corrupted observations using LDM priors and a known forward model, and (2) the M-step estimates the forward operator based on the recovered images. Additionally, we propose two novel optimization techniques tailored for LDM priors and EM frameworks, yielding more accurate and efficient blind inversion results. As a general framework, LatentDEM supports both linear and non-linear inverse problems. Beyond common 2D image restoration tasks, it enables new capabilities in non-linear 3D inverse rendering problems. We validate LatentDEM’s performance on representative 2D blind deblurring and 3D pose-free sparse-view reconstruction tasks, demonstrating its superior efficacy over prior arts. The project page can be found at https://ai4imaging.github.io/latentdem/.
Published: 2026-01-13T21:01:14+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weimin Bai; Siyi Chen; Wenzheng Chen; He Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3651963"&gt;10.1109/tip.2026.3651963&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion models have emerged as powerful tools for solving inverse problems due to their exceptional ability to model complex prior distributions. However, existing methods predominantly assume known forward operators (i.e., non-blind), limiting their applicability in practical settings where acquiring such operators is costly. Additionally, many current approaches rely on pixel-space diffusion models, leaving the potential of more powerful latent diffusion models (LDMs) underexplored. In this paper, we introduce LatentDEM, an innovative technique that addresses more challenging blind inverse problems using latent diffusion priors. At the core of our method is solving blind inverse problems within an iterative Expectation-Maximization (EM) framework: (1) the E-step recovers clean images from corrupted observations using LDM priors and a known forward model, and (2) the M-step estimates the forward operator based on the recovered images. Additionally, we propose two novel optimization techniques tailored for LDM priors and EM frameworks, yielding more accurate and efficient blind inversion results. As a general framework, LatentDEM supports both linear and non-linear inverse problems. Beyond common 2D image restoration tasks, it enables new capabilities in non-linear 3D inverse rendering problems. We validate LatentDEM’s performance on representative 2D blind deblurring and 3D pose-free sparse-view reconstruction tasks, demonstrating its superior efficacy over prior arts. The project page can be found at https://ai4imaging.github.io/latentdem/.&lt;/p&gt;</content:encoded></item><item><title>A symmetrical attention-assisted multi-modal fusion network under uncertain absent modalities</title><link>https://doi.org/10.1016/j.eswa.2026.131179</link><guid>10.1016/j.eswa.2026.131179</guid><pubDate>Tue, 13 Jan 2026 16:10:54 +0000</pubDate><dc:creator>Jiayao Li</dc:creator><dc:creator>Saihua Cai</dc:creator><dc:creator>Kaiyi Zhao</dc:creator><dc:creator>Ruizhi Sun</dc:creator><dc:creator>Gang Yuan</dc:creator><dc:creator>Zeqiu Chen</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131179</prism:doi><description>The research of absent modalities learning is an important challenge in the field of multi-modal learning. A series of methods have been proposed to address the degraded prediction performance due to missing modalities. However, existing studies ignore the deep symmetric information of different modalities and lack considering the uncertainty of missing modalities. To address these two challenges, we propose a S ymmetrical attention-assisted M ulti-modal F usion N etwork called SMFN under uncertain absent modalities. First, the modality representation network is utilized to represent different modalities in order to obtain the semantic features of different modalities; Then, the symmetrical attention-assisted module is designed for symmetrically exploring both intra- and inter-modal feature information for tackling the first challenge; Next, a multi-modal fusion module is introduced to map the feature information of own modality and different modalities into the same common space, thereby enhancing the robustness of uncertain missing modalities; Finally, the dependency between modalities is learned using the transformer to accomplish prediction is accomplished. In addition, a pre-training model is also designed to train the complete modalities for improving the prediction accuracy. Extensive experimental results on benchmark datasets demonstrate that compared to the state-of-the-art models, the SMFN improves the accuracy for 6.52% and the F1-score for 11.59% on average, it also exhibits good robustness and convergence.
Published: 2026-01-13T16:10:54+00:00
Venue: Expert Systems with Applications
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayao Li; Saihua Cai; Kaiyi Zhao; Ruizhi Sun; Gang Yuan; Zeqiu Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131179"&gt;10.1016/j.eswa.2026.131179&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;The research of absent modalities learning is an important challenge in the field of multi-modal learning. A series of methods have been proposed to address the degraded prediction performance due to missing modalities. However, existing studies ignore the deep symmetric information of different modalities and lack considering the uncertainty of missing modalities. To address these two challenges, we propose a S ymmetrical attention-assisted M ulti-modal F usion N etwork called SMFN under uncertain absent modalities. First, the modality representation network is utilized to represent different modalities in order to obtain the semantic features of different modalities; Then, the symmetrical attention-assisted module is designed for symmetrically exploring both intra- and inter-modal feature information for tackling the first challenge; Next, a multi-modal fusion module is introduced to map the feature information of own modality and different modalities into the same common space, thereby enhancing the robustness of uncertain missing modalities; Finally, the dependency between modalities is learned using the transformer to accomplish prediction is accomplished. In addition, a pre-training model is also designed to train the complete modalities for improving the prediction accuracy. Extensive experimental results on benchmark datasets demonstrate that compared to the state-of-the-art models, the SMFN improves the accuracy for 6.52% and the F1-score for 11.59% on average, it also exhibits good robustness and convergence.&lt;/p&gt;</content:encoded></item><item><title>LiteEmbed: Adapting CLIP to Rare Classes</title><link>https://arxiv.org/abs/2601.09661v1</link><guid>http://arxiv.org/abs/2601.09661v1</guid><pubDate>Wed, 14 Jan 2026 17:53:11 +0000</pubDate><dc:creator>Aishwarya Agarwal</dc:creator><dc:creator>Srikrishna Karanam</dc:creator><dc:creator>Vineet Gandhi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.
Published: 2026-01-14T17:53:11+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aishwarya Agarwal; Srikrishna Karanam; Vineet Gandhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&amp;#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&amp;#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.&lt;/p&gt;</content:encoded></item><item><title>GAFF: Global Attention Feature Flow Network for Optical and SAR Image Registration Under Geometric Transformations</title><link>https://doi.org/10.1109/jstars.2026.3653492</link><guid>10.1109/jstars.2026.3653492</guid><pubDate>Tue, 13 Jan 2026 20:59:09 +0000</pubDate><dc:creator>Xuecong Liu</dc:creator><dc:creator>Zixuan Sun</dc:creator><dc:creator>Hongwei Ding</dc:creator><dc:creator>Xin Song</dc:creator><dc:creator>Shuaiying Zhang</dc:creator><dc:creator>Yongsheng Sun</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3653492</prism:doi><description>The registration of optical and synthetic aperture radar (SAR) images under geometric distortions is critical for remote sensing applications such as image fusion and visual navigation. However, this task faces significant challenges due to inherent radiometric and geometric differences, diverse terrain conditions, and distinct noise patterns between the two modalities, which compromises the accuracy, robustness, and generalization of existing registration methods. To address these limitations, this paper introduces a novel global attention feature flow (GAFF) network, which synergistically combines model-driven and data-driven methodologies. Specifically, the framework leverages a hybrid CNN-Transformer architecture, integrating modality independent region descriptors (MIRD) for feature extraction, to achieve highly generalizable and consistent feature representations across modalities. GAFF employs a global attention mechanism to establish robust feature correspondences and generate the initial feature flow. Further, we introduce a hierarchical feature flow refinement module (HFRM) and a combined weight loss function to optimize feature flow generation. Experimental results demonstrate that GAFF delivers highly accurate, robust, and generalizable optical-SAR registration under varying geometric conditions, maintaining strong generalization capabilities across OS, WHU-OPT-SAR, and OSTerrain dataset.
Published: 2026-01-13T20:59:09+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuecong Liu; Zixuan Sun; Hongwei Ding; Xin Song; Shuaiying Zhang; Yongsheng Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3653492"&gt;10.1109/jstars.2026.3653492&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;The registration of optical and synthetic aperture radar (SAR) images under geometric distortions is critical for remote sensing applications such as image fusion and visual navigation. However, this task faces significant challenges due to inherent radiometric and geometric differences, diverse terrain conditions, and distinct noise patterns between the two modalities, which compromises the accuracy, robustness, and generalization of existing registration methods. To address these limitations, this paper introduces a novel global attention feature flow (GAFF) network, which synergistically combines model-driven and data-driven methodologies. Specifically, the framework leverages a hybrid CNN-Transformer architecture, integrating modality independent region descriptors (MIRD) for feature extraction, to achieve highly generalizable and consistent feature representations across modalities. GAFF employs a global attention mechanism to establish robust feature correspondences and generate the initial feature flow. Further, we introduce a hierarchical feature flow refinement module (HFRM) and a combined weight loss function to optimize feature flow generation. Experimental results demonstrate that GAFF delivers highly accurate, robust, and generalizable optical-SAR registration under varying geometric conditions, maintaining strong generalization capabilities across OS, WHU-OPT-SAR, and OSTerrain dataset.&lt;/p&gt;</content:encoded></item><item><title>Crowd detection using Very-Fine-Resolution satellite imagery</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.001</link><guid>10.1016/j.isprsjprs.2026.01.001</guid><pubDate>Tue, 13 Jan 2026 04:48:48 +0000</pubDate><dc:creator>Tong Xiao</dc:creator><dc:creator>Qunming Wang</dc:creator><dc:creator>Ping Lu</dc:creator><dc:creator>Tenghai Huang</dc:creator><dc:creator>Xiaohua Tong</dc:creator><dc:creator>Peter M. Atkinson</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.001</prism:doi><description>Accurate crowd detection (CD) is critical for public safety and historical pattern analysis, yet existing methods relying on ground and aerial imagery suffer from limited spatio-temporal coverage. The development of very-fine-resolution (VFR) satellite sensor imagery (e.g., ∼0.3 m spatial resolution) provides unprecedented opportunities for large-scale crowd activity analysis, but it has never been considered for this task. To address this gap, we proposed CrowdSat-Net, a novel point-based convolutional neural network, which features two innovative components: Dual-Context Progressive Attention Network (DCPAN) to improve feature representation of individuals by aggregating scene context and local individual characteristics, and High-Frequency Guided Deformable Upsampler (HFGDU) that recovers high-frequency information during upsampling through frequency-domain guided deformable convolutions. To validate the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR satellite imagery dataset designed specifically for CD tasks, comprising over 120 k manually labeled individuals from multi-source satellite platforms (Beijing-3 N, Jilin-1 Gaofen-04A and Google Earth) across China. In the experiments, CrowdSat-Net was compared with eight state-of-the-art point-based CD methods (originally designed for ground or aerial imagery and satellite-based animal detection) using CrowdSat and achieved the largest F1-score of 66.12 % and Precision of 73.23 %, surpassing the second-best method by 0.80 % and 6.83 %, respectively. Moreover, extensive ablation experiments validated the importance of the DCPAN and HFGDU modules. Furthermore, cross-regional evaluation further demonstrated the spatial generalizability of CrowdSat-Net. This research advances CD capability by providing both a newly developed network architecture for CD and a pioneering benchmark dataset to facilitate future CD development. The source code is available at https://github.com/Tong-777777/CrowdSat-Net.
Published: 2026-01-13T04:48:48+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tong Xiao; Qunming Wang; Ping Lu; Tenghai Huang; Xiaohua Tong; Peter M. Atkinson&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.001"&gt;10.1016/j.isprsjprs.2026.01.001&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate crowd detection (CD) is critical for public safety and historical pattern analysis, yet existing methods relying on ground and aerial imagery suffer from limited spatio-temporal coverage. The development of very-fine-resolution (VFR) satellite sensor imagery (e.g., ∼0.3 m spatial resolution) provides unprecedented opportunities for large-scale crowd activity analysis, but it has never been considered for this task. To address this gap, we proposed CrowdSat-Net, a novel point-based convolutional neural network, which features two innovative components: Dual-Context Progressive Attention Network (DCPAN) to improve feature representation of individuals by aggregating scene context and local individual characteristics, and High-Frequency Guided Deformable Upsampler (HFGDU) that recovers high-frequency information during upsampling through frequency-domain guided deformable convolutions. To validate the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR satellite imagery dataset designed specifically for CD tasks, comprising over 120 k manually labeled individuals from multi-source satellite platforms (Beijing-3 N, Jilin-1 Gaofen-04A and Google Earth) across China. In the experiments, CrowdSat-Net was compared with eight state-of-the-art point-based CD methods (originally designed for ground or aerial imagery and satellite-based animal detection) using CrowdSat and achieved the largest F1-score of 66.12 % and Precision of 73.23 %, surpassing the second-best method by 0.80 % and 6.83 %, respectively. Moreover, extensive ablation experiments validated the importance of the DCPAN and HFGDU modules. Furthermore, cross-regional evaluation further demonstrated the spatial generalizability of CrowdSat-Net. This research advances CD capability by providing both a newly developed network architecture for CD and a pioneering benchmark dataset to facilitate future CD development. The source code is available at https://github.com/Tong-777777/CrowdSat-Net.&lt;/p&gt;</content:encoded></item><item><title>Advancing Multinational License Plate Recognition Through Synthetic and Real Data Fusion: A Comprehensive Evaluation</title><link>https://arxiv.org/abs/2601.07671v1</link><guid>http://arxiv.org/abs/2601.07671v1</guid><pubDate>Mon, 12 Jan 2026 15:52:52 +0000</pubDate><dc:creator>Rayson Laroca</dc:creator><dc:creator>Valter Estevam</dc:creator><dc:creator>Gladston J. P. Moreira</dc:creator><dc:creator>Rodrigo Minetto</dc:creator><dc:creator>David Menotti</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1049/itr2.70086</prism:doi><description>Automatic License Plate Recognition is a frequent research topic due to its wide-ranging practical applications. While recent studies use synthetic images to improve License Plate Recognition (LPR) results, there remain several limitations in these efforts. This work addresses these constraints by comprehensively exploring the integration of real and synthetic data to enhance LPR performance. We subject 16 Optical Character Recognition (OCR) models to a benchmarking process involving 12 public datasets acquired from various regions. Several key findings emerge from our investigation. Primarily, the massive incorporation of synthetic data substantially boosts model performance in both intra- and cross-dataset scenarios. We examine three distinct methodologies for generating synthetic data: template-based generation, character permutation, and utilizing a Generative Adversarial Network (GAN) model, each contributing significantly to performance enhancement. The combined use of these methodologies demonstrates a notable synergistic effect, leading to end-to-end results that surpass those reached by state-of-the-art methods and established commercial systems. Our experiments also underscore the efficacy of synthetic data in mitigating challenges posed by limited training data, enabling remarkable results to be achieved even with small fractions of the original training data. Finally, we investigate the trade-off between accuracy and speed among different models, identifying those that strike the optimal balance in each intra-dataset and cross-dataset settings.
Published: 2026-01-12T15:52:52+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rayson Laroca; Valter Estevam; Gladston J. P. Moreira; Rodrigo Minetto; David Menotti&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1049/itr2.70086"&gt;10.1049/itr2.70086&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Automatic License Plate Recognition is a frequent research topic due to its wide-ranging practical applications. While recent studies use synthetic images to improve License Plate Recognition (LPR) results, there remain several limitations in these efforts. This work addresses these constraints by comprehensively exploring the integration of real and synthetic data to enhance LPR performance. We subject 16 Optical Character Recognition (OCR) models to a benchmarking process involving 12 public datasets acquired from various regions. Several key findings emerge from our investigation. Primarily, the massive incorporation of synthetic data substantially boosts model performance in both intra- and cross-dataset scenarios. We examine three distinct methodologies for generating synthetic data: template-based generation, character permutation, and utilizing a Generative Adversarial Network (GAN) model, each contributing significantly to performance enhancement. The combined use of these methodologies demonstrates a notable synergistic effect, leading to end-to-end results that surpass those reached by state-of-the-art methods and established commercial systems. Our experiments also underscore the efficacy of synthetic data in mitigating challenges posed by limited training data, enabling remarkable results to be achieved even with small fractions of the original training data. Finally, we investigate the trade-off between accuracy and speed among different models, identifying those that strike the optimal balance in each intra-dataset and cross-dataset settings.&lt;/p&gt;</content:encoded></item><item><title>LLMDNet: An Aautonomous mining truck object detection network in low-light conditions</title><link>https://doi.org/10.1016/j.knosys.2026.115286</link><guid>10.1016/j.knosys.2026.115286</guid><pubDate>Wed, 14 Jan 2026 13:35:46 +0000</pubDate><dc:creator>Feixiang Xu</dc:creator><dc:creator>Rui Zhang</dc:creator><dc:creator>Yafei Wang</dc:creator><dc:creator>He Jiang</dc:creator><dc:creator>Deqiang Cheng</dc:creator><dc:creator>Jiansheng Qian</dc:creator><dc:creator>Fengqian Sun</dc:creator><dc:creator>Lige Xue</dc:creator><dc:creator>Chen Zhou</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115286</prism:doi><description>Accurate and reliable object detection is a crucial component of the perception system for autonomous mining trucks. However, low-light environment is a common working scenario in surface mines, where interference from low-light backgrounds and loss of object information pose significant challenges to object detection. Most existing end-to-end low-light object detection methods based on YOLO series are built upon hierarchical interactions for cross-layer fusion, in which information is often degraded during transmission, thereby hindering effective multi-scale feature integration. To this end, a Low-Light Modulation Detection Network (LLMDNet) is proposed to enhance object representation and detection robustness under such conditions. It consists of a robust feature fusion pathway, which is combination of Low-Light Modulation Network (LLMN) and Multi-level Feature Balancing Strategy (MFBS). Three key components are integrated in LLMN to enhance object representation in a progressive manner. Firstly, the Low-Light Information Filter (LLIF) conducts cross-scale differential operations to mitigate background interference and emphasize edge details. Following this, the Information Injection Module (IIM) is applied to facilitate dynamic fusion between deep and shallow features, enabling rich semantic interaction. Subsequently, the Directional Attention Mechanism (DAM) captures spatial structural cues along horizontal, vertical, and channel dimensions to enhance structural perception. To further refine the features processed by DAM, IIM is reintroduced to ensure deeper interaction across scales, boosting the representation capacity before detection. And to ensure effective detection, MFBS is utilized to integrate features across multiple scales in a coordinated manner before feeding them into the detection head. Finally, a custom dataset Low-light Auto-Mine (LAM) is constructed to realize object detection of autonomous mining trucks in low-light conditions. And extensive experiments are conducted on both LAM and Exdark datasets. LLMDNet achieves the mean Average Precision@50 (mAP 50 ) of 86.1% and 81.7% on the LAM and Exdark datasets, respectively. Compared to the state-of-the-art YOLA, the mAP 50 with the proposed LLMDNet is increased by 1.9% on LAM. Moreover, compared to YOLOv8, there is a significant improvement of 4.1% and a 3.3% increase in the mAP 50 , respectively. The results further demonstrate that our model can effectively improve detection accuracy.
Published: 2026-01-14T13:35:46+00:00
Venue: Knowledge-Based Systems
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feixiang Xu; Rui Zhang; Yafei Wang; He Jiang; Deqiang Cheng; Jiansheng Qian; Fengqian Sun; Lige Xue; Chen Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115286"&gt;10.1016/j.knosys.2026.115286&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate and reliable object detection is a crucial component of the perception system for autonomous mining trucks. However, low-light environment is a common working scenario in surface mines, where interference from low-light backgrounds and loss of object information pose significant challenges to object detection. Most existing end-to-end low-light object detection methods based on YOLO series are built upon hierarchical interactions for cross-layer fusion, in which information is often degraded during transmission, thereby hindering effective multi-scale feature integration. To this end, a Low-Light Modulation Detection Network (LLMDNet) is proposed to enhance object representation and detection robustness under such conditions. It consists of a robust feature fusion pathway, which is combination of Low-Light Modulation Network (LLMN) and Multi-level Feature Balancing Strategy (MFBS). Three key components are integrated in LLMN to enhance object representation in a progressive manner. Firstly, the Low-Light Information Filter (LLIF) conducts cross-scale differential operations to mitigate background interference and emphasize edge details. Following this, the Information Injection Module (IIM) is applied to facilitate dynamic fusion between deep and shallow features, enabling rich semantic interaction. Subsequently, the Directional Attention Mechanism (DAM) captures spatial structural cues along horizontal, vertical, and channel dimensions to enhance structural perception. To further refine the features processed by DAM, IIM is reintroduced to ensure deeper interaction across scales, boosting the representation capacity before detection. And to ensure effective detection, MFBS is utilized to integrate features across multiple scales in a coordinated manner before feeding them into the detection head. Finally, a custom dataset Low-light Auto-Mine (LAM) is constructed to realize object detection of autonomous mining trucks in low-light conditions. And extensive experiments are conducted on both LAM and Exdark datasets. LLMDNet achieves the mean Average Precision@50 (mAP 50 ) of 86.1% and 81.7% on the LAM and Exdark datasets, respectively. Compared to the state-of-the-art YOLA, the mAP 50 with the proposed LLMDNet is increased by 1.9% on LAM. Moreover, compared to YOLOv8, there is a significant improvement of 4.1% and a 3.3% increase in the mAP 50 , respectively. The results further demonstrate that our model can effectively improve detection accuracy.&lt;/p&gt;</content:encoded></item><item><title>AMFC-DEIM: Improved DEIM With Adaptive Matching and Focal Convolution for Remote Sensing Small Object Detection</title><link>https://doi.org/10.1109/jstars.2026.3653626</link><guid>10.1109/jstars.2026.3653626</guid><pubDate>Tue, 13 Jan 2026 20:59:09 +0000</pubDate><dc:creator>Xiaole Lin</dc:creator><dc:creator>Guangping Li</dc:creator><dc:creator>Jiahua Xie</dc:creator><dc:creator>Zhuokun Zhi</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3653626</prism:doi><description>While Convolutional Neural Network (CNN)-based methods for small object detection in remote sensing imagery have advanced considerably, substantial challenges remain unresolved, primarily stemming from complex backgrounds and insufficient feature representation. To address these issues, we propose a novel architecture specifically designed to accommodate the unique demands of small objects, termed AMFC-DEIM. This framework introduces three key innovations: (1) the Adaptive One-to-One (O2O) matching mechanism, which enhances Dense O2O matching by adaptively adjusting the matching grid configuration to the object distribution, thereby preserving the resolution of small objects throughout training; (2) the Focal Convolution Module, engineered to explicitly align with the spatial characteristics of small objects for extracting fine-grained features; and (3) the Enhanced Normalized Wasserstein Distance, which stabilizes the training process and bolsters performance on small targets. Comprehensive experiments conducted on three benchmark remote sensing small object detection datasets: RSOD, LEVIR-SHIP and NWPU VHR-10, demonstrate that AMFC-DEIM achieves remarkable performance, attaining AP 50 _{50} scores of 96.2%, 86.2%, and 95.1%, respectively, while maintaining only 5.27M parameters. These results substantially outperform several established benchmark models and state-of-the-art methods.
Published: 2026-01-13T20:59:09+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaole Lin; Guangping Li; Jiahua Xie; Zhuokun Zhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3653626"&gt;10.1109/jstars.2026.3653626&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;While Convolutional Neural Network (CNN)-based methods for small object detection in remote sensing imagery have advanced considerably, substantial challenges remain unresolved, primarily stemming from complex backgrounds and insufficient feature representation. To address these issues, we propose a novel architecture specifically designed to accommodate the unique demands of small objects, termed AMFC-DEIM. This framework introduces three key innovations: (1) the Adaptive One-to-One (O2O) matching mechanism, which enhances Dense O2O matching by adaptively adjusting the matching grid configuration to the object distribution, thereby preserving the resolution of small objects throughout training; (2) the Focal Convolution Module, engineered to explicitly align with the spatial characteristics of small objects for extracting fine-grained features; and (3) the Enhanced Normalized Wasserstein Distance, which stabilizes the training process and bolsters performance on small targets. Comprehensive experiments conducted on three benchmark remote sensing small object detection datasets: RSOD, LEVIR-SHIP and NWPU VHR-10, demonstrate that AMFC-DEIM achieves remarkable performance, attaining AP 50 _{50} scores of 96.2%, 86.2%, and 95.1%, respectively, while maintaining only 5.27M parameters. These results substantially outperform several established benchmark models and state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>OceanSAR-2: A Universal Feature Extractor for SAR Ocean Observation</title><link>https://arxiv.org/abs/2601.07392v1</link><guid>http://arxiv.org/abs/2601.07392v1</guid><pubDate>Mon, 12 Jan 2026 10:20:43 +0000</pubDate><dc:creator>Alexandre Tuel</dc:creator><dc:creator>Thomas Kerdreux</dc:creator><dc:creator>Quentin Febvre</dc:creator><dc:creator>Alexis Mouche</dc:creator><dc:creator>Antoine Grouazel</dc:creator><dc:creator>Jean-Renaud Miadana</dc:creator><dc:creator>Antoine Audras</dc:creator><dc:creator>Chen Wang</dc:creator><dc:creator>Bertrand Chapron</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.
Published: 2026-01-12T10:20:43+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Alexandre Tuel; Thomas Kerdreux; Quentin Febvre; Alexis Mouche; Antoine Grouazel; Jean-Renaud Miadana; Antoine Audras; Chen Wang; Bertrand Chapron&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;We present OceanSAR-2, the second generation of our foundation model for SAR-based ocean observation. Building on our earlier release, which pioneered self-supervised learning on Sentinel-1 Wave Mode data, OceanSAR-2 relies on improved SSL training and dynamic data curation strategies, which enhances performance while reducing training cost. OceanSAR-2 demonstrates strong transfer performance across downstream tasks, including geophysical pattern classification, ocean surface wind vector and significant wave height estimation, and iceberg detection. We release standardized benchmark datasets, providing a foundation for systematic evaluation and advancement of SAR models for ocean applications.&lt;/p&gt;</content:encoded></item><item><title>Two-Stage Fine-Tuning of Large Vision-Language Models with Hierarchical Prompting for Few-Shot Object Detection in Remote Sensing Images</title><link>https://doi.org/10.3390/rs18020266</link><guid>10.3390/rs18020266</guid><pubDate>Wed, 14 Jan 2026 15:12:04 +0000</pubDate><dc:creator>Yongqi Shi</dc:creator><dc:creator>Ruopeng Yang</dc:creator><dc:creator>Changsheng Yin</dc:creator><dc:creator>Yiwei Lu</dc:creator><dc:creator>Bo Huang</dc:creator><dc:creator>Yu Tao</dc:creator><dc:creator>Yihao Zhong</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020266</prism:doi><description>Few-shot object detection (FSOD) in high-resolution remote sensing (RS) imagery remains challenging due to scarce annotations, large intra-class variability, and high visual similarity between categories, which together limit the generalization ability of convolutional neural network (CNN)-based detectors. To address this issue, we explore leveraging large vision-language models (LVLMs) for FSOD in RS. We propose a two-stage, parameter-efficient fine-tuning framework with hierarchical prompting that adapts Qwen3-VL for object detection. In the first stage, low-rank adaptation (LoRA) modules are inserted into the vision and text encoders and trained jointly with a Detection Transformer (DETR)-style detection head on fully annotated base classes under three-level hierarchical prompts. In the second stage, the vision LoRA parameters are frozen, the text encoder is updated using K-shot novel-class samples, and the detection head is partially frozen, with selected components refined using the same three-level hierarchical prompting scheme. To preserve base-class performance and reduce class confusion, we further introduce knowledge distillation and semantic consistency losses. Experiments on the DIOR and NWPU VHR-10.v2 datasets show that the proposed method consistently improves novel-class performance while maintaining competitive base-class accuracy and surpasses existing baselines, demonstrating the effectiveness of integrating hierarchical semantic reasoning into LVLM-based FSOD for RS imagery.
Published: 2026-01-14T15:12:04+00:00
Venue: Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yongqi Shi; Ruopeng Yang; Changsheng Yin; Yiwei Lu; Bo Huang; Yu Tao; Yihao Zhong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020266"&gt;10.3390/rs18020266&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot object detection (FSOD) in high-resolution remote sensing (RS) imagery remains challenging due to scarce annotations, large intra-class variability, and high visual similarity between categories, which together limit the generalization ability of convolutional neural network (CNN)-based detectors. To address this issue, we explore leveraging large vision-language models (LVLMs) for FSOD in RS. We propose a two-stage, parameter-efficient fine-tuning framework with hierarchical prompting that adapts Qwen3-VL for object detection. In the first stage, low-rank adaptation (LoRA) modules are inserted into the vision and text encoders and trained jointly with a Detection Transformer (DETR)-style detection head on fully annotated base classes under three-level hierarchical prompts. In the second stage, the vision LoRA parameters are frozen, the text encoder is updated using K-shot novel-class samples, and the detection head is partially frozen, with selected components refined using the same three-level hierarchical prompting scheme. To preserve base-class performance and reduce class confusion, we further introduce knowledge distillation and semantic consistency losses. Experiments on the DIOR and NWPU VHR-10.v2 datasets show that the proposed method consistently improves novel-class performance while maintaining competitive base-class accuracy and surpasses existing baselines, demonstrating the effectiveness of integrating hierarchical semantic reasoning into LVLM-based FSOD for RS imagery.&lt;/p&gt;</content:encoded></item><item><title>Learn to Enhance Sparse Spike Streams</title><link>https://doi.org/10.1109/tpami.2026.3653768</link><guid>10.1109/tpami.2026.3653768</guid><pubDate>Tue, 13 Jan 2026 20:58:38 +0000</pubDate><dc:creator>Liwen Hu</dc:creator><dc:creator>Yijia Guo</dc:creator><dc:creator>Mianzhi Liu</dc:creator><dc:creator>Yiming Fan</dc:creator><dc:creator>Rui Ma</dc:creator><dc:creator>Shengbo Chen</dc:creator><dc:creator>Lei Ma</dc:creator><dc:creator>Tiejun Huang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3653768</prism:doi><description>High-speed vision tasks have long been a challenge in computer vision. Recently, the spike camera has shown great potential in these tasks due to its high temporal resolution. Unlike traditional cameras, it emits asynchronous spike signals to capture visual information. However, under low-light conditions, spike signals becomehighly sparse, and the sparse spike streamseverely hinders theeffectiveness of existing spike-based methods in high-speed scenarios. To address this challenge,we introduce SS2DS, the first deep learning framework that enhances sparse spike streams into dense spike streams. SS2DS first estimates the spike firing frequency within sparse streams. Subsequently, the spike firing frequency is enhanced by a neural network. Finally, SS2DS decodes the enhanced spike stream from the enhanced spike firing frequency sequence. SS2DS can adjust the temporal distribution of sparse spike streams and improve the performance degradation of existing methods in low-light and high-speed scenarios. In order to evaluate sparse spikestream enhancement,we construct both synthetic and real sparse spike stream datasets. The real dataset iscollected in dynamic scenarios using the third-generation spike camera.By comparing the reconstruction results, enhanced spike streams achieve an average improvement of +0.78 MA, -18.42 BRISQUE, and -1.42 NIQE over sparse spike streams. Moreover, the enhanced spike streams also benefit other spike-based vision tasks, such as 3D reconstruction (+1.325 dB PSNR, +0.005 SSIM, and -0.01 LPIPS) and super-resolution (+0.63 MA, -13.67 BRISQUE, and -1.28 NIQE). Code and datasets will be released after publication.
Published: 2026-01-13T20:58:38+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liwen Hu; Yijia Guo; Mianzhi Liu; Yiming Fan; Rui Ma; Shengbo Chen; Lei Ma; Tiejun Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3653768"&gt;10.1109/tpami.2026.3653768&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;High-speed vision tasks have long been a challenge in computer vision. Recently, the spike camera has shown great potential in these tasks due to its high temporal resolution. Unlike traditional cameras, it emits asynchronous spike signals to capture visual information. However, under low-light conditions, spike signals becomehighly sparse, and the sparse spike streamseverely hinders theeffectiveness of existing spike-based methods in high-speed scenarios. To address this challenge,we introduce SS2DS, the first deep learning framework that enhances sparse spike streams into dense spike streams. SS2DS first estimates the spike firing frequency within sparse streams. Subsequently, the spike firing frequency is enhanced by a neural network. Finally, SS2DS decodes the enhanced spike stream from the enhanced spike firing frequency sequence. SS2DS can adjust the temporal distribution of sparse spike streams and improve the performance degradation of existing methods in low-light and high-speed scenarios. In order to evaluate sparse spikestream enhancement,we construct both synthetic and real sparse spike stream datasets. The real dataset iscollected in dynamic scenarios using the third-generation spike camera.By comparing the reconstruction results, enhanced spike streams achieve an average improvement of +0.78 MA, -18.42 BRISQUE, and -1.42 NIQE over sparse spike streams. Moreover, the enhanced spike streams also benefit other spike-based vision tasks, such as 3D reconstruction (+1.325 dB PSNR, +0.005 SSIM, and -0.01 LPIPS) and super-resolution (+0.63 MA, -13.67 BRISQUE, and -1.28 NIQE). Code and datasets will be released after publication.&lt;/p&gt;</content:encoded></item><item><title>GenDet: Painting Colored Bounding Boxes on Images via Diffusion Model for Object Detection</title><link>https://arxiv.org/abs/2601.07273v1</link><guid>http://arxiv.org/abs/2601.07273v1</guid><pubDate>Mon, 12 Jan 2026 07:29:59 +0000</pubDate><dc:creator>Chen Min</dc:creator><dc:creator>Chengyang Li</dc:creator><dc:creator>Fanjie Kong</dc:creator><dc:creator>Qi Zhu</dc:creator><dc:creator>Dawei Zhao</dc:creator><dc:creator>Liang Xiao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper presents GenDet, a novel framework that redefines object detection as an image generation task. In contrast to traditional approaches, GenDet adopts a pioneering approach by leveraging generative modeling: it conditions on the input image and directly generates bounding boxes with semantic annotations in the original image space. GenDet establishes a conditional generation architecture built upon the large-scale pre-trained Stable Diffusion model, formulating the detection task as semantic constraints within the latent space. It enables precise control over bounding box positions and category attributes, while preserving the flexibility of the generative model. This novel methodology effectively bridges the gap between generative models and discriminative tasks, providing a fresh perspective for constructing unified visual understanding systems. Systematic experiments demonstrate that GenDet achieves competitive accuracy compared to discriminative detectors, while retaining the flexibility characteristic of generative methods.
Published: 2026-01-12T07:29:59+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Min; Chengyang Li; Fanjie Kong; Qi Zhu; Dawei Zhao; Liang Xiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;This paper presents GenDet, a novel framework that redefines object detection as an image generation task. In contrast to traditional approaches, GenDet adopts a pioneering approach by leveraging generative modeling: it conditions on the input image and directly generates bounding boxes with semantic annotations in the original image space. GenDet establishes a conditional generation architecture built upon the large-scale pre-trained Stable Diffusion model, formulating the detection task as semantic constraints within the latent space. It enables precise control over bounding box positions and category attributes, while preserving the flexibility of the generative model. This novel methodology effectively bridges the gap between generative models and discriminative tasks, providing a fresh perspective for constructing unified visual understanding systems. Systematic experiments demonstrate that GenDet achieves competitive accuracy compared to discriminative detectors, while retaining the flexibility characteristic of generative methods.&lt;/p&gt;</content:encoded></item></channel></rss>