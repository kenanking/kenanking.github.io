<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 06 Dec 2025 02:41:48 +0000</lastBuildDate><item><title>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</title><link>https://doi.org/10.1109/tpami.2025.3640589</link><guid>10.1109/tpami.2025.3640589</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Xiang Xu</dc:creator><dc:creator>Lingdong Kong</dc:creator><dc:creator>Hui Shuai</dc:creator><dc:creator>Wenwei Zhang</dc:creator><dc:creator>Liang Pan</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Ziwei Liu</dc:creator><dc:creator>Qingshan Liu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640589</prism:doi><description>LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.870 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Xu; Lingdong Kong; Hui Shuai; Wenwei Zhang; Liang Pan; Kai Chen; Ziwei Liu; Qingshan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640589"&gt;10.1109/tpami.2025.3640589&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.870 (must_read)&lt;/p&gt;
&lt;p&gt;LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow.&lt;/p&gt;</content:encoded></item><item><title>Generative models for SAR–optical image translation: A systematic review</title><link>https://doi.org/10.1016/j.jag.2025.105009</link><guid>10.1016/j.jag.2025.105009</guid><pubDate>Thu, 04 Dec 2025 22:45:43 +0000</pubDate><dc:creator>Zhao Wang</dc:creator><dc:creator>Zheng Zhang</dc:creator><dc:creator>Xiaojun Shan</dc:creator><dc:creator>Hong-an Wei</dc:creator><dc:creator>Ping Tang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105009</prism:doi><description>Growing demands in sustainable development and resource management are driving increasing reliance on remote sensing-based Earth observation and image interpretation. In parallel, multimodal collaborative processing is attracting research attention. Synthetic aperture radar (SAR) and optical images offer complementary advantages but pose challenges for simultaneous use due to platform constraints and environmental conditions, often leaving only one modality available and impeding joint analysis. Generative models, particularly generative adversarial networks (GANs) and diffusion models (DMs), address this by learning cross-modal mappings. Translated images preserve structure and semantics while adopting target characteristics, thereby facilitating collaborative use. This review systematically categorizes translation frameworks spanning GANs, DMs, and other generative models. It then details downstream tasks supported by SAR–optical translation, including cloud removal, change detection, semantic segmentation, registration, and object detection, highlighting how translation bridges data gaps and enhances interpretation robustness. Furthermore, we provide open-source code and public datasets, discuss current challenges, and outline future research directions.
Published: 2025-12-04T22:45:43+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.856 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Wang; Zheng Zhang; Xiaojun Shan; Hong-an Wei; Ping Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105009"&gt;10.1016/j.jag.2025.105009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.856 (must_read)&lt;/p&gt;
&lt;p&gt;Growing demands in sustainable development and resource management are driving increasing reliance on remote sensing-based Earth observation and image interpretation. In parallel, multimodal collaborative processing is attracting research attention. Synthetic aperture radar (SAR) and optical images offer complementary advantages but pose challenges for simultaneous use due to platform constraints and environmental conditions, often leaving only one modality available and impeding joint analysis. Generative models, particularly generative adversarial networks (GANs) and diffusion models (DMs), address this by learning cross-modal mappings. Translated images preserve structure and semantics while adopting target characteristics, thereby facilitating collaborative use. This review systematically categorizes translation frameworks spanning GANs, DMs, and other generative models. It then details downstream tasks supported by SAR–optical translation, including cloud removal, change detection, semantic segmentation, registration, and object detection, highlighting how translation bridges data gaps and enhances interpretation robustness. Furthermore, we provide open-source code and public datasets, discuss current challenges, and outline future research directions.&lt;/p&gt;</content:encoded></item><item><title>AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views</title><link>https://doi.org/10.1145/3763326</link><guid>10.1145/3763326</guid><pubDate>Thu, 04 Dec 2025 17:15:39 +0000</pubDate><dc:creator>Lihan Jiang</dc:creator><dc:creator>Yucheng Mao</dc:creator><dc:creator>Linning Xu</dc:creator><dc:creator>Tao Lu</dc:creator><dc:creator>Kerui Ren</dc:creator><dc:creator>Yichen Jin</dc:creator><dc:creator>Xudong Xu</dc:creator><dc:creator>Mulin Yu</dc:creator><dc:creator>Jiangmiao Pang</dc:creator><dc:creator>Feng Zhao</dc:creator><dc:creator>Dahua Lin</dc:creator><dc:creator>Bo Dai</dc:creator><prism:publicationName>ACM Transactions on Graphics</prism:publicationName><prism:doi>10.1145/3763326</prism:doi><description>We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.
Published: 2025-12-04T17:15:39+00:00
Venue: ACM Transactions on Graphics
Score: 0.837 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lihan Jiang; Yucheng Mao; Linning Xu; Tao Lu; Kerui Ren; Yichen Jin; Xudong Xu; Mulin Yu; Jiangmiao Pang; Feng Zhao; Dahua Lin; Bo Dai&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ACM Transactions on Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1145/3763326"&gt;10.1145/3763326&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.837 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce AnySplat, a feed-forward network for novel-view synthesis from uncalibrated image collections. In contrast to traditional neural-rendering pipelines that demand known camera poses and per-scene optimization, or recent feed-forward methods that buckle under the computational weight of dense views—our model predicts everything in one shot. A single forward pass yields a set of 3D Gaussian primitives encoding both scene geometry and appearance, and the corresponding camera intrinsics and extrinsics for each input image. This unified design scales effortlessly to casually captured, multi-view datasets without any pose annotations. In extensive zero-shot evaluations, AnySplat matches the quality of pose-aware baselines in both sparse- and dense-view scenarios while surpassing existing pose-free approaches. Moreover, it greatly reduces rendering latency compared to optimization-based neural fields, bringing real-time novel-view synthesis within reach for unconstrained capture settings. Project page: https://city-super.github.io/anysplat/.&lt;/p&gt;</content:encoded></item><item><title>AttBEV: Enhancing Multi-Modal 3D Object Detection with CBAM Attention in BEVFusion for Autonomous Driving</title><link>https://doi.org/10.1109/lra.2025.3641130</link><guid>10.1109/lra.2025.3641130</guid><pubDate>Fri, 05 Dec 2025 18:40:34 +0000</pubDate><dc:creator>Na Zhang</dc:creator><dc:creator>Edmundo Guerra</dc:creator><dc:creator>Antoni Grau</dc:creator><prism:publicationName>IEEE Robotics and Automation Letters</prism:publicationName><prism:doi>10.1109/lra.2025.3641130</prism:doi><description>Multimodal fusion has an important research value in environmental perception for autonomous driving. Among them, BEVFusion has become one of the mainstream framework for LiDAR camera fusion by unifying multimodal features in the bird's-eye view (BEV) space. However, its performance is limited by inefficient cross-modal interaction and information loss during BEV projection, especially for dynamic objects and edge cases. To address these limitations, we propose AttBEV, an advanced fusion architecture that introduces a CBAM at the feature fusion layer: a lightweight attention mechanism that improves the model's ability to capture key information through dynamic feature calibration of channel and spatial dimensions. Extensive experiments on the nuScenes dataset demonstrate that AttBEV achieves superior performance compared to BEVFusion on most evaluation metrics. NDS reaches 0.6795, which is 2.63% higher than BEVFusion's 0.6532, and mAP reaches 0.6426, which is 1.79% higher than BEVFusion's 0.6247. In general, AttBEV outperforms existing methods in both model accuracy and generalization ability and significantly improves the performance of 3D object detection in autonomous driving scenarios.
Published: 2025-12-05T18:40:34+00:00
Venue: IEEE Robotics and Automation Letters
Score: 0.833 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Na Zhang; Edmundo Guerra; Antoni Grau&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Robotics and Automation Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lra.2025.3641130"&gt;10.1109/lra.2025.3641130&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.833 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal fusion has an important research value in environmental perception for autonomous driving. Among them, BEVFusion has become one of the mainstream framework for LiDAR camera fusion by unifying multimodal features in the bird&amp;#x27;s-eye view (BEV) space. However, its performance is limited by inefficient cross-modal interaction and information loss during BEV projection, especially for dynamic objects and edge cases. To address these limitations, we propose AttBEV, an advanced fusion architecture that introduces a CBAM at the feature fusion layer: a lightweight attention mechanism that improves the model&amp;#x27;s ability to capture key information through dynamic feature calibration of channel and spatial dimensions. Extensive experiments on the nuScenes dataset demonstrate that AttBEV achieves superior performance compared to BEVFusion on most evaluation metrics. NDS reaches 0.6795, which is 2.63% higher than BEVFusion&amp;#x27;s 0.6532, and mAP reaches 0.6426, which is 1.79% higher than BEVFusion&amp;#x27;s 0.6247. In general, AttBEV outperforms existing methods in both model accuracy and generalization ability and significantly improves the performance of 3D object detection in autonomous driving scenarios.&lt;/p&gt;</content:encoded></item><item><title>Harnessing Lightweight Transformer With Contextual Synergic Enhancement for Efficient 3D Medical Image Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3640233</link><guid>10.1109/tpami.2025.3640233</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Xinyu Liu</dc:creator><dc:creator>Zhen Chen</dc:creator><dc:creator>Wuyang Li</dc:creator><dc:creator>Chenxin Li</dc:creator><dc:creator>Yixuan Yuan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640233</prism:doi><description>Transformers have shown remarkable performance in 3D medical image segmentation, but their high computational requirements and need for large amounts of labeled data limit their applicability. To address these challenges, we consider two crucial aspects: model efficiency and data efficiency. Specifically, we propose Light-UNETR, a lightweight transformer designed to achieve model efficiency. Light-UNETR features a Lightweight Dimension Reductive Attention (LIDR) module, which reduces spatial and channel dimensions while capturing both global and local features via multi-branch attention. Additionally, we introduce a Compact Gated Linear Unit (CGLU) to selectively control channel interaction with minimal parameters. Furthermore, we introduce a Contextual Synergic Enhancement (CSE) learning strategy, which aims to boost the data efficiency of Transformers. It first leverages the extrinsic contextual information to support the learning of unlabeled data with Attention-Guided Replacement, then applies Spatial Masking Consistency that utilizes intrinsic contextual information to enhance the spatial context reasoning for unlabeled data. Extensive experiments on various benchmarks demonstrate the superiority of our approach in both performance and efficiency. For example, with only 10% labeled data on the Left Atrial Segmentation dataset, our method surpasses BCP by 1.43% Jaccard while drastically reducing the FLOPs by 90.8% and parameters by 85.8%. Code is released at https://github.com/CUHK-AIM-Group/Light-UNETR.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.830 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Liu; Zhen Chen; Wuyang Li; Chenxin Li; Yixuan Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640233"&gt;10.1109/tpami.2025.3640233&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.830 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers have shown remarkable performance in 3D medical image segmentation, but their high computational requirements and need for large amounts of labeled data limit their applicability. To address these challenges, we consider two crucial aspects: model efficiency and data efficiency. Specifically, we propose Light-UNETR, a lightweight transformer designed to achieve model efficiency. Light-UNETR features a Lightweight Dimension Reductive Attention (LIDR) module, which reduces spatial and channel dimensions while capturing both global and local features via multi-branch attention. Additionally, we introduce a Compact Gated Linear Unit (CGLU) to selectively control channel interaction with minimal parameters. Furthermore, we introduce a Contextual Synergic Enhancement (CSE) learning strategy, which aims to boost the data efficiency of Transformers. It first leverages the extrinsic contextual information to support the learning of unlabeled data with Attention-Guided Replacement, then applies Spatial Masking Consistency that utilizes intrinsic contextual information to enhance the spatial context reasoning for unlabeled data. Extensive experiments on various benchmarks demonstrate the superiority of our approach in both performance and efficiency. For example, with only 10% labeled data on the Left Atrial Segmentation dataset, our method surpasses BCP by 1.43% Jaccard while drastically reducing the FLOPs by 90.8% and parameters by 85.8%. Code is released at https://github.com/CUHK-AIM-Group/Light-UNETR.&lt;/p&gt;</content:encoded></item><item><title>BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection</title><link>https://arxiv.org/abs/2512.02972v1</link><guid>http://arxiv.org/abs/2512.02972v1</guid><pubDate>Tue, 02 Dec 2025 17:50:33 +0000</pubDate><dc:creator>Guowen Zhang</dc:creator><dc:creator>Chenhang He</dc:creator><dc:creator>Liyi Chen</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.
Published: 2025-12-02T17:50:33+00:00
Venue: arXiv
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guowen Zhang; Chenhang He; Liyi Chen; Lei Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;Integrating LiDAR and camera information in the bird&amp;#x27;s eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.&lt;/p&gt;</content:encoded></item><item><title>Optical Context Compression Is Just (Bad) Autoencoding</title><link>https://arxiv.org/abs/2512.03643v1</link><guid>http://arxiv.org/abs/2512.03643v1</guid><pubDate>Wed, 03 Dec 2025 10:27:27 +0000</pubDate><dc:creator>Ivan Yee Lee</dc:creator><dc:creator>Cheng Yang</dc:creator><dc:creator>Taylor Berg-Kirkpatrick</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding
Published: 2025-12-03T10:27:27+00:00
Venue: arXiv
Score: 0.828 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ivan Yee Lee; Cheng Yang; Taylor Berg-Kirkpatrick&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.828 (must_read)&lt;/p&gt;
&lt;p&gt;DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR&amp;#x27;s reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding&lt;/p&gt;</content:encoded></item><item><title>Extrapolate azimuth angles: Text and edge guided ISAR image generation based on foundation model</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.002</link><guid>10.1016/j.isprsjprs.2025.12.002</guid><pubDate>Thu, 04 Dec 2025 17:17:35 +0000</pubDate><dc:creator>Jiawei Zhang</dc:creator><dc:creator>Xiaolin Zhou</dc:creator><dc:creator>Weidong Jiang</dc:creator><dc:creator>Xiaolong Su</dc:creator><dc:creator>Zhen Liu</dc:creator><dc:creator>Li Liu</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.002</prism:doi><description>Inverse Synthetic Aperture Radar (ISAR) has been widely applied in remote sensing and space target monitoring. Automatic Target Recognition (ATR) based on ISAR imagery plays a critical role in target interpretation and pose estimation. With the growing adoption of intelligent methods in the ATR domain, the quantity and quality of ISAR data have become decisive factors influencing algorithm performance. However, due to the complexity of ISAR imaging algorithms and the high cost of data acquisition, high-quality ISAR image datasets remain extremely scarce. As a result, learning the underlying characteristics of existing ISAR data to generate large-scale usable samples has become a pressing research focus. Although some preliminary studies have explored ISAR image data augmentation, most of them rely on image sequence interpolation or conditional generation, both of which exhibit critical limitations: the former requires densely sampled image sequences with small angular intervals, while the latter can only model the mapping between limited azimuth conditions and ISAR images. Neither approach is capable of generating images of new targets under unseen azimuth conditions, resulting in poor generalization and leaving substantial room for further exploration. To address these limitations, we formally define a novel research problem, termed ISAR azimuth angle extrapolation. This task fundamentally involves high-dimensional, structured, cross-view image synthesis, requiring the restoration of visual details while ensuring physical consistency and structural stability. To address this problem, we propose ISAR-ExtraNet, a foundation-model-based framework for ISAR azimuth angle extrapolation. ISAR-ExtraNet leverages the strong representation, modeling, and generalization capabilities of pretrained foundation models to generate ISAR images of new targets under novel azimuth conditions. Specifically, the model employs a two-stage coarse-to-fine fine-tuning strategy, incorporating optical image contours and scattering center distribution constraints to guide the generation process. This design enhances both semantic alignment and structural fidelity in the generated ISAR images. Comprehensive experiments demonstrate that ISAR-ExtraNet significantly outperforms baseline methods and fine-tuned foundation models, achieving 28.76 dB in PSNR and 0.80 in SSIM. We hope that the training paradigm introduced in ISAR-ExtraNet will inspire further exploration of the ISAR azimuth extrapolation problem and foster progress in this emerging research area.
Published: 2025-12-04T17:17:35+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.825 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Zhang; Xiaolin Zhou; Weidong Jiang; Xiaolong Su; Zhen Liu; Li Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.002"&gt;10.1016/j.isprsjprs.2025.12.002&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.825 (must_read)&lt;/p&gt;
&lt;p&gt;Inverse Synthetic Aperture Radar (ISAR) has been widely applied in remote sensing and space target monitoring. Automatic Target Recognition (ATR) based on ISAR imagery plays a critical role in target interpretation and pose estimation. With the growing adoption of intelligent methods in the ATR domain, the quantity and quality of ISAR data have become decisive factors influencing algorithm performance. However, due to the complexity of ISAR imaging algorithms and the high cost of data acquisition, high-quality ISAR image datasets remain extremely scarce. As a result, learning the underlying characteristics of existing ISAR data to generate large-scale usable samples has become a pressing research focus. Although some preliminary studies have explored ISAR image data augmentation, most of them rely on image sequence interpolation or conditional generation, both of which exhibit critical limitations: the former requires densely sampled image sequences with small angular intervals, while the latter can only model the mapping between limited azimuth conditions and ISAR images. Neither approach is capable of generating images of new targets under unseen azimuth conditions, resulting in poor generalization and leaving substantial room for further exploration. To address these limitations, we formally define a novel research problem, termed ISAR azimuth angle extrapolation. This task fundamentally involves high-dimensional, structured, cross-view image synthesis, requiring the restoration of visual details while ensuring physical consistency and structural stability. To address this problem, we propose ISAR-ExtraNet, a foundation-model-based framework for ISAR azimuth angle extrapolation. ISAR-ExtraNet leverages the strong representation, modeling, and generalization capabilities of pretrained foundation models to generate ISAR images of new targets under novel azimuth conditions. Specifically, the model employs a two-stage coarse-to-fine fine-tuning strategy, incorporating optical image contours and scattering center distribution constraints to guide the generation process. This design enhances both semantic alignment and structural fidelity in the generated ISAR images. Comprehensive experiments demonstrate that ISAR-ExtraNet significantly outperforms baseline methods and fine-tuned foundation models, achieving 28.76 dB in PSNR and 0.80 in SSIM. We hope that the training paradigm introduced in ISAR-ExtraNet will inspire further exploration of the ISAR azimuth extrapolation problem and foster progress in this emerging research area.&lt;/p&gt;</content:encoded></item><item><title>WMRNet: Wavelet Mamba with Reversible Structure for Infrared Small Target Detection</title><link>https://doi.org/10.1109/tip.2025.3637729</link><guid>10.1109/tip.2025.3637729</guid><pubDate>Fri, 05 Dec 2025 18:41:09 +0000</pubDate><dc:creator>Mingjin Zhang</dc:creator><dc:creator>Xiaolong Li</dc:creator><dc:creator>Jie Guo</dc:creator><dc:creator>Yunsong Li</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3637729</prism:doi><description>Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods.
Published: 2025-12-05T18:41:09+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.821 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mingjin Zhang; Xiaolong Li; Jie Guo; Yunsong Li; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3637729"&gt;10.1109/tip.2025.3637729&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.821 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is of great practical significance in many real-world applications, such as maritime rescue and early warning systems, benefiting from the unique and excellent infrared imaging ability in adverse weather and low-light conditions. Nevertheless, segmenting small targets from the background remains a challenge. When the subsampling frequency during image processing does not satisfy the Nyquist criterion, the aliasing effect occurs, which makes it extremely difficult to identify small targets. To address this challenge, we propose a novel Wavelet Mamba with Reversible Structure Network (WMRNet) for infrared small target detection in this paper. Specifically, WMRNet consists of a Discrete Wavelet Mamba (DW-Mamba) module and a Third-order Difference Equation guided Reversible (TDE-Rev) structure. DW-Mamba employs the Discrete Wavelet Transform to decompose images into multiple subbands, integrating this information into the state equations of a state space model. This method minimizes frequency interference while preserving a global perspective, thereby effectively reducing background aliasing. The TDE-Rev aims to suppress edge aliasing effects by refining the target edges, which first processes features with an explicit neural structure derived from the second-order difference equations and then promotes feature interactions through a reversible structure. Extensive experiments on the public IRSTD-1k and SIRST datasets demonstrate that the proposed WMRNet outperforms the state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Cross-modal Guiding Attention for RGBT Tracking</title><link>https://doi.org/10.1016/j.inffus.2025.104008</link><guid>10.1016/j.inffus.2025.104008</guid><pubDate>Thu, 04 Dec 2025 08:09:02 +0000</pubDate><dc:creator>Yun Xiao</dc:creator><dc:creator>Qi Li</dc:creator><dc:creator>Lei Liu</dc:creator><dc:creator>Chenglong Li</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104008</prism:doi><description>RGBT tracking aims to achieve robust performance in various scenarios by fully integrating complementary information from RGB and thermal infrared modalities. Existing transformer RGBT trackers usually use a self-attention scheme to enhance the intra-modal features and cross-attention to perform cross-modal information interaction. Some methods eliminate cross-attention computation by performing the calculation of self-attention only for the concatenated multi-modal token vectors or correlation vectors, which greatly improves the efficiency of tracking. However, such interaction between different modes is easily affected by low-quality representations (e.g., noise-corrupted tokens and ambiguous correlations), which limits the tracking performance. To address this issue, we propose an effective and efficient RGBT tracker based on the novel Cross-modal Guiding Attention (CGA) mechanism, which performs bidirectional information guidance to mitigate the effect of low-quality representations in both attention weights computation and cross-modal feature interaction. In particular, we replace the vanilla Multi-Head Attention (MHA) block in Vision Transformer (ViT) with our novel CGA block, which incorporates Bidirectional Weight Guiding Module (BiWGM) and Bidirectional Feature Guiding Module (BiFGM). The BiWGM is designed to enhance consistency in multi-modal target relational modeling by enabling global semantic-level reallocation of attention weights, thus preventing indiscriminate cross-modal fusion of low-quality representations. Furthermore, we introduce the BiFGM to perform fine-grained feature token enhancement based on global semantic information by jointly leveraging intra-modal feature self-reinforcement and inter-modal complementary feature enhancement. We evaluate our tracker on four benchmark datasets, including RGBT210, RGBT234, LasHeR, and VTUAV. Extensive experiments show the outstanding performance of our tracker against SOTA methods and maintain real-time speed.
Published: 2025-12-04T08:09:02+00:00
Venue: Information Fusion
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yun Xiao; Qi Li; Lei Liu; Chenglong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104008"&gt;10.1016/j.inffus.2025.104008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;RGBT tracking aims to achieve robust performance in various scenarios by fully integrating complementary information from RGB and thermal infrared modalities. Existing transformer RGBT trackers usually use a self-attention scheme to enhance the intra-modal features and cross-attention to perform cross-modal information interaction. Some methods eliminate cross-attention computation by performing the calculation of self-attention only for the concatenated multi-modal token vectors or correlation vectors, which greatly improves the efficiency of tracking. However, such interaction between different modes is easily affected by low-quality representations (e.g., noise-corrupted tokens and ambiguous correlations), which limits the tracking performance. To address this issue, we propose an effective and efficient RGBT tracker based on the novel Cross-modal Guiding Attention (CGA) mechanism, which performs bidirectional information guidance to mitigate the effect of low-quality representations in both attention weights computation and cross-modal feature interaction. In particular, we replace the vanilla Multi-Head Attention (MHA) block in Vision Transformer (ViT) with our novel CGA block, which incorporates Bidirectional Weight Guiding Module (BiWGM) and Bidirectional Feature Guiding Module (BiFGM). The BiWGM is designed to enhance consistency in multi-modal target relational modeling by enabling global semantic-level reallocation of attention weights, thus preventing indiscriminate cross-modal fusion of low-quality representations. Furthermore, we introduce the BiFGM to perform fine-grained feature token enhancement based on global semantic information by jointly leveraging intra-modal feature self-reinforcement and inter-modal complementary feature enhancement. We evaluate our tracker on four benchmark datasets, including RGBT210, RGBT234, LasHeR, and VTUAV. Extensive experiments show the outstanding performance of our tracker against SOTA methods and maintain real-time speed.&lt;/p&gt;</content:encoded></item><item><title>Heatmap Pooling Network for Action Recognition From RGB Videos</title><link>https://doi.org/10.1109/tpami.2025.3640697</link><guid>10.1109/tpami.2025.3640697</guid><pubDate>Fri, 05 Dec 2025 18:37:55 +0000</pubDate><dc:creator>Mengyuan Liu</dc:creator><dc:creator>Jinfu Liu</dc:creator><dc:creator>Yongkang Jiang</dc:creator><dc:creator>Bin He</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3640697</prism:doi><description>Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.
Published: 2025-12-05T18:37:55+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengyuan Liu; Jinfu Liu; Yongkang Jiang; Bin He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3640697"&gt;10.1109/tpami.2025.3640697&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Human action recognition (HAR) in videos has garnered widespread attention due to the rich information in RGB videos. Nevertheless, existing methods for extracting deep features from RGB videos face challenges such as information redundancy, susceptibility to noise and high storage costs. To address these issues and fully harness the useful information in videos, we propose a novel heatmap pooling network (HP-Net) for action recognition from videos, which extracts information-rich, robust and concise pooled features of the human body in videos through a feedback pooling module. The extracted pooled features demonstrate obvious performance advantages over the previously obtained pose data and heatmap features from videos. In addition, we design a spatial-motion co-learning module and a text refinement modulation module to integrate the extracted pooled features with other multimodal data, enabling more robust action recognition. Extensive experiments on several benchmarks namely NTU RGB+D 60, NTU RGB+D 120, Toyota-Smarthome and UAV-Human consistently verify the effectiveness of our HP-Net, which outperforms the existing human action recognition methods. Our code is publicly available at: https://github.com/liujf69/HPNet-Action.&lt;/p&gt;</content:encoded></item><item><title>A Semantic-Aware and Multi-Guided Network for Infrared-Visible Image Fusion</title><link>https://doi.org/10.1109/tmm.2025.3639988</link><guid>10.1109/tmm.2025.3639988</guid><pubDate>Thu, 04 Dec 2025 18:38:10 +0000</pubDate><dc:creator>Xiaoli Zhang</dc:creator><dc:creator>Liying Wang</dc:creator><dc:creator>Libo Zhao</dc:creator><dc:creator>Xiongfei Li</dc:creator><dc:creator>Siwei Ma</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2025.3639988</prism:doi><description>Multi-modality image fusion aims at fusing modality-specific (complementarity) and modality-shared (correlation) information from multiple source images. To tackle the overlooking of inter-feature relationships, high-frequency information loss, and the limited attention to downstream tasks, this paper focuses on efficiently extracting complementary in formation and aggregating multi-guided features. We propose a three-branch encoder-decoder architecture along with corresponding fusion layers as the fusion strategy. Firstly, shallow features from individual modalities are extracted by a depthwise convolution layer combined with the transformer block. In the three parallel branches of the encoder, Cross Attention and Invertible Block (CAI) extracts local features and preserves high frequency texture details. Base Feature Extraction Module (BFE) captures long-range dependencies and enhances modality-shared information. Graph Reasoning Module (GR) is introduced to reason high-level cross-modality relations and simultaneously ex tract low-level detail features as CAI's modality-specific complementary information. Experiments demonstrate the competitive results compared with state-of-the-art methods in visible/infrared image fusion and medical image fusion tasks. Moreover, the proposed algorithm surpasses the state-of-the-art methods in terms of subsequent tasks, averagely scoring 8.27% mAP@0.5 higher in object detection and 5.85% mIoU higher in semantic segmentation.
Published: 2025-12-04T18:38:10+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoli Zhang; Liying Wang; Libo Zhao; Xiongfei Li; Siwei Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2025.3639988"&gt;10.1109/tmm.2025.3639988&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modality image fusion aims at fusing modality-specific (complementarity) and modality-shared (correlation) information from multiple source images. To tackle the overlooking of inter-feature relationships, high-frequency information loss, and the limited attention to downstream tasks, this paper focuses on efficiently extracting complementary in formation and aggregating multi-guided features. We propose a three-branch encoder-decoder architecture along with corresponding fusion layers as the fusion strategy. Firstly, shallow features from individual modalities are extracted by a depthwise convolution layer combined with the transformer block. In the three parallel branches of the encoder, Cross Attention and Invertible Block (CAI) extracts local features and preserves high frequency texture details. Base Feature Extraction Module (BFE) captures long-range dependencies and enhances modality-shared information. Graph Reasoning Module (GR) is introduced to reason high-level cross-modality relations and simultaneously ex tract low-level detail features as CAI&amp;#x27;s modality-specific complementary information. Experiments demonstrate the competitive results compared with state-of-the-art methods in visible/infrared image fusion and medical image fusion tasks. Moreover, the proposed algorithm surpasses the state-of-the-art methods in terms of subsequent tasks, averagely scoring 8.27% mAP@0.5 higher in object detection and 5.85% mIoU higher in semantic segmentation.&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Domain Adaptive Object Detection via Discriminative Instance Teacher</title><link>https://doi.org/10.1016/j.eswa.2025.130656</link><guid>10.1016/j.eswa.2025.130656</guid><pubDate>Fri, 05 Dec 2025 00:36:16 +0000</pubDate><dc:creator>Yiming Ge</dc:creator><dc:creator>Hui Liu</dc:creator><dc:creator>Yanjie Hu</dc:creator><dc:creator>Jie Zhao</dc:creator><dc:creator>Junzhao Du</dc:creator><dc:creator>Ertong Shang</dc:creator><dc:creator>Zhaocheng Niu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2025.130656</prism:doi><description>Domain adaptive object detection (DAOD) poses significant challenges due to pronounced domain shifts. Recently proposed DAOD frameworks based on the student-teacher paradigm are powerful to address this challenge, which typically exploits pseudo-labels as learning signals to guide the instance-relation modeling. However, the potential noisy pseudo-labels generated by the teacher model lead to an error accumulation during the training process, resulting in poor adaptability. Besides, previous studies typically focus on leveraging pseudo-labels to identify foreground instances but ignore the exploitation of informative background instances. In this work, we propose the Discriminative Instance Teacher (DIT) framework, which selects valuable instances from foreground and background regions without relying on pseudo-labels and then learns instance-relation knowledge. Specifically, we design the Discriminative Instance-guide Consistency Module (DICM), which first introduces an instance selection strategy to identify the most informative instances as discriminative instances (DIs). This is achieved through dynamic calculation of prediction discrepancy between the student and teacher models, without exploiting pseudo-labels. Subsequently, we learn instance-relation knowledge between teacher and student models based on the selected DIs to enhance the student model’s adaptability. Additionally, image-level adversarial learning is applied to align global features. Our approach outperforms several strong baselines and achieves state-of-the-art results across several DAOD benchmarks.
Published: 2025-12-05T00:36:16+00:00
Venue: Expert Systems with Applications
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Ge; Hui Liu; Yanjie Hu; Jie Zhao; Junzhao Du; Ertong Shang; Zhaocheng Niu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2025.130656"&gt;10.1016/j.eswa.2025.130656&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Domain adaptive object detection (DAOD) poses significant challenges due to pronounced domain shifts. Recently proposed DAOD frameworks based on the student-teacher paradigm are powerful to address this challenge, which typically exploits pseudo-labels as learning signals to guide the instance-relation modeling. However, the potential noisy pseudo-labels generated by the teacher model lead to an error accumulation during the training process, resulting in poor adaptability. Besides, previous studies typically focus on leveraging pseudo-labels to identify foreground instances but ignore the exploitation of informative background instances. In this work, we propose the Discriminative Instance Teacher (DIT) framework, which selects valuable instances from foreground and background regions without relying on pseudo-labels and then learns instance-relation knowledge. Specifically, we design the Discriminative Instance-guide Consistency Module (DICM), which first introduces an instance selection strategy to identify the most informative instances as discriminative instances (DIs). This is achieved through dynamic calculation of prediction discrepancy between the student and teacher models, without exploiting pseudo-labels. Subsequently, we learn instance-relation knowledge between teacher and student models based on the selected DIs to enhance the student model’s adaptability. Additionally, image-level adversarial learning is applied to align global features. Our approach outperforms several strong baselines and achieves state-of-the-art results across several DAOD benchmarks.&lt;/p&gt;</content:encoded></item><item><title>ASQ &amp;amp; POST: A synergistic framework for adaptive and non-uniform quantization</title><link>https://doi.org/10.1016/j.neucom.2025.132332</link><guid>10.1016/j.neucom.2025.132332</guid><pubDate>Thu, 04 Dec 2025 17:14:07 +0000</pubDate><dc:creator>Wenqiang Zhou</dc:creator><dc:creator>Zhendong Yu</dc:creator><dc:creator>Xinyu Liu</dc:creator><dc:creator>Jiaming Yang</dc:creator><dc:creator>Rong Xiao</dc:creator><dc:creator>Tao Wang</dc:creator><dc:creator>Chenwei Tang</dc:creator><dc:creator>Jiancheng Lv</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132332</prism:doi><description>Quantization-Aware Training (QAT) faces a fundamental paradox: optimizing quantization parameters for the training set often results in rigid models that fail to generalize to the dynamic input distributions encountered during inference. This brittleness poses a critical barrier to the deployment of robust, efficient models in real-world scenarios. In this paper, we resolve this paradox with a novel framework that redefines the quantizer to be both dynamically adaptive and structurally expressive. First, we propose an Adaptive Step-size Quantization ( ASQ ) module to dynamically adjust quantization step-sizes based on input activation statistics, enabling the model to generalize robustly across diverse and unseen data distributions. To fully leverage this dynamic adaptability, we then introduce Power-of-Square-root-of-Two ( POST ), a non-uniform exponential grid to offer finer-grained resolution. POST naturally aligns with the bell-shaped distributions of weights, capturing information more faithfully. This structural refinement is realized efficiently for hardware through a Look-Up Table (LUT)-based implementation. Extensive experiments demonstrate that the synergy between ASQ ’s dynamic adaptation and POST ’s structural precision leads to state-of-the-art performance compared with existing QAT techniques. Strikingly, our 4-bit quantized ResNet-34 on ImageNet not only recovers but surpasses its full-precision counterpart by 1.2 % in top-1 accuracy. Code is available at https://github.com/SENGEL13/ASQ-POST .
Published: 2025-12-04T17:14:07+00:00
Venue: Neurocomputing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenqiang Zhou; Zhendong Yu; Xinyu Liu; Jiaming Yang; Rong Xiao; Tao Wang; Chenwei Tang; Jiancheng Lv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132332"&gt;10.1016/j.neucom.2025.132332&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Quantization-Aware Training (QAT) faces a fundamental paradox: optimizing quantization parameters for the training set often results in rigid models that fail to generalize to the dynamic input distributions encountered during inference. This brittleness poses a critical barrier to the deployment of robust, efficient models in real-world scenarios. In this paper, we resolve this paradox with a novel framework that redefines the quantizer to be both dynamically adaptive and structurally expressive. First, we propose an Adaptive Step-size Quantization ( ASQ ) module to dynamically adjust quantization step-sizes based on input activation statistics, enabling the model to generalize robustly across diverse and unseen data distributions. To fully leverage this dynamic adaptability, we then introduce Power-of-Square-root-of-Two ( POST ), a non-uniform exponential grid to offer finer-grained resolution. POST naturally aligns with the bell-shaped distributions of weights, capturing information more faithfully. This structural refinement is realized efficiently for hardware through a Look-Up Table (LUT)-based implementation. Extensive experiments demonstrate that the synergy between ASQ ’s dynamic adaptation and POST ’s structural precision leads to state-of-the-art performance compared with existing QAT techniques. Strikingly, our 4-bit quantized ResNet-34 on ImageNet not only recovers but surpasses its full-precision counterpart by 1.2 % in top-1 accuracy. Code is available at https://github.com/SENGEL13/ASQ-POST .&lt;/p&gt;</content:encoded></item><item><title>ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers</title><link>https://arxiv.org/abs/2512.03673v1</link><guid>http://arxiv.org/abs/2512.03673v1</guid><pubDate>Wed, 03 Dec 2025 11:02:16 +0000</pubDate><dc:creator>Feice Huang</dc:creator><dc:creator>Zuliang Han</dc:creator><dc:creator>Xing Zhou</dc:creator><dc:creator>Yihuang Chen</dc:creator><dc:creator>Lifei Zhu</dc:creator><dc:creator>Haoqian Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.
Published: 2025-12-03T11:02:16+00:00
Venue: arXiv
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Feice Huang; Zuliang Han; Xing Zhou; Yihuang Chen; Lifei Zhu; Haoqian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.&lt;/p&gt;</content:encoded></item><item><title>Removal Then Selection: A Coarse-to-Fine Fusion Perspective for RGB-Infrared Object Detection</title><link>https://doi.org/10.1109/tits.2025.3638627</link><guid>10.1109/tits.2025.3638627</guid><pubDate>Thu, 04 Dec 2025 18:38:37 +0000</pubDate><dc:creator>Tianyi Zhao</dc:creator><dc:creator>Maoxun Yuan</dc:creator><dc:creator>Feng Jiang</dc:creator><dc:creator>Nan Wang</dc:creator><dc:creator>Xingxing Wei</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2025.3638627</prism:doi><description>In recent years, object detection utilizing both visible (RGB) and thermal infrared (IR) imagery has garnered extensive attention and has been widely implemented across a diverse array of fields. By leveraging the complementary properties between RGB and IR images, the object detection task can achieve reliable and robust object localization across a variety of lighting conditions, from daytime to nighttime environments. While RGB-IR multi-modal data input generally enhances overall detection performance, most existing multi-modal object detection methods fail to fully exploit the complementary potential of these two modalities. We believe that this issue arises not only from the challenges associated with effectively integrating multi-modal information but also from the presence of redundant features in both the RGB and IR modalities. The redundant information of each modality will exacerbate the fusion imprecision problems during propagation. To address this issue, we draw inspiration from the human cognitive mechanisms for processing multi-modal information and propose a novel coarse-to-fine perspective to purify and fuse features from both modalities. Specifically, following this perspective, we design a Redundant Spectrum Removal module to remove interfering information within each modality coarsely and a Dynamic Feature Selection module to finely select the desired features for feature fusion. To verify the effectiveness of the coarse-to-fine fusion strategy, we construct a new object detector called the Removal then Selection Detector (RSDet). Extensive experiments on five RGB-IR object detection datasets verify the superior performance of our method. The source code and results are available at https://github.com/Zhao-Tian-yi/RSDet.git
Published: 2025-12-04T18:38:37+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyi Zhao; Maoxun Yuan; Feng Jiang; Nan Wang; Xingxing Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2025.3638627"&gt;10.1109/tits.2025.3638627&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, object detection utilizing both visible (RGB) and thermal infrared (IR) imagery has garnered extensive attention and has been widely implemented across a diverse array of fields. By leveraging the complementary properties between RGB and IR images, the object detection task can achieve reliable and robust object localization across a variety of lighting conditions, from daytime to nighttime environments. While RGB-IR multi-modal data input generally enhances overall detection performance, most existing multi-modal object detection methods fail to fully exploit the complementary potential of these two modalities. We believe that this issue arises not only from the challenges associated with effectively integrating multi-modal information but also from the presence of redundant features in both the RGB and IR modalities. The redundant information of each modality will exacerbate the fusion imprecision problems during propagation. To address this issue, we draw inspiration from the human cognitive mechanisms for processing multi-modal information and propose a novel coarse-to-fine perspective to purify and fuse features from both modalities. Specifically, following this perspective, we design a Redundant Spectrum Removal module to remove interfering information within each modality coarsely and a Dynamic Feature Selection module to finely select the desired features for feature fusion. To verify the effectiveness of the coarse-to-fine fusion strategy, we construct a new object detector called the Removal then Selection Detector (RSDet). Extensive experiments on five RGB-IR object detection datasets verify the superior performance of our method. The source code and results are available at https://github.com/Zhao-Tian-yi/RSDet.git&lt;/p&gt;</content:encoded></item><item><title>Boundary-Aware Test-Time Adaptation for Zero-Shot Medical Image Segmentation</title><link>https://arxiv.org/abs/2512.04520v1</link><guid>http://arxiv.org/abs/2512.04520v1</guid><pubDate>Thu, 04 Dec 2025 07:08:21 +0000</pubDate><dc:creator>Chenlin Xu</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Lituan Wang</dc:creator><dc:creator>Xinyu Pu</dc:creator><dc:creator>Pengfei Ma</dc:creator><dc:creator>Guangwu Qian</dc:creator><dc:creator>Zizhou Wang</dc:creator><dc:creator>Yan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM's zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.
Published: 2025-12-04T07:08:21+00:00
Venue: arXiv
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenlin Xu; Lei Zhang; Lituan Wang; Xinyu Pu; Pengfei Ma; Guangwu Qian; Zizhou Wang; Yan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the scarcity of annotated data and the substantial computational costs of model, conventional tuning methods in medical image segmentation face critical challenges. Current approaches to adapting pretrained models, including full-parameter and parameter-efficient fine-tuning, still rely heavily on task-specific training on downstream tasks. Therefore, zero-shot segmentation has gained increasing attention, especially with foundation models such as SAM demonstrating promising generalization capabilities. However, SAM still faces notable limitations on medical datasets due to domain shifts, making efficient zero-shot enhancement an urgent research goal. To address these challenges, we propose BA-TTA-SAM, a task-agnostic test-time adaptation framework that significantly enhances the zero-shot segmentation performance of SAM via test-time adaptation. This framework integrates two key mechanisms: (1) The encoder-level Gaussian prompt injection embeds Gaussian-based prompts directly into the image encoder, providing explicit guidance for initial representation learning. (2) The cross-layer boundary-aware attention alignment exploits the hierarchical feature interactions within the ViT backbone, aligning deep semantic responses with shallow boundary cues. Experiments on four datasets, including ISIC, Kvasir, BUSI, and REFUGE, show an average improvement of 12.4\% in the DICE score compared with SAM&amp;#x27;s zero-shot segmentation performance. The results demonstrate that our method consistently outperforms state-of-the-art models in medical image segmentation. Our framework significantly enhances the generalization ability of SAM, without requiring any source-domain training data. Extensive experiments on publicly available medical datasets strongly demonstrate the superiority of our framework. Our code is available at https://github.com/Emilychenlin/BA-TTA-SAM.&lt;/p&gt;</content:encoded></item><item><title>UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking</title><link>https://arxiv.org/abs/2512.02668v1</link><guid>http://arxiv.org/abs/2512.02668v1</guid><pubDate>Tue, 02 Dec 2025 11:47:13 +0000</pubDate><dc:creator>Qionglin Ren</dc:creator><dc:creator>Dawei Zhang</dc:creator><dc:creator>Chunxu Tian</dc:creator><dc:creator>Dan Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.
Published: 2025-12-02T11:47:13+00:00
Venue: arXiv
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qionglin Ren; Dawei Zhang; Chunxu Tian; Dan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.&lt;/p&gt;</content:encoded></item><item><title>Harnessing Diffusion-Yielded Score Priors for Image Restoration</title><link>https://doi.org/10.1145/3763346</link><guid>10.1145/3763346</guid><pubDate>Thu, 04 Dec 2025 17:15:39 +0000</pubDate><dc:creator>Xinqi Lin</dc:creator><dc:creator>Fanghua Yu</dc:creator><dc:creator>Jinfan Hu</dc:creator><dc:creator>Zhiyuan You</dc:creator><dc:creator>Wu Shi</dc:creator><dc:creator>Jimmy S. Ren</dc:creator><dc:creator>Jinjin Gu</dc:creator><dc:creator>Chao Dong</dc:creator><prism:publicationName>ACM Transactions on Graphics</prism:publicationName><prism:doi>10.1145/3763346</prism:doi><description>Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration.
Published: 2025-12-04T17:15:39+00:00
Venue: ACM Transactions on Graphics
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinqi Lin; Fanghua Yu; Jinfan Hu; Zhiyuan You; Wu Shi; Jimmy S. Ren; Jinjin Gu; Chao Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ACM Transactions on Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1145/3763346"&gt;10.1145/3763346&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;Deep image restoration models aim to learn a mapping from degraded image space to natural image space. However, they face several critical challenges: removing degradation, generating realistic details, and ensuring pixel-level consistency. Over time, three major classes of methods have emerged, including MSE-based, GAN-based, and diffusion-based methods. However, they fail to achieve a good balance between restoration quality, fidelity, and speed. We propose a novel method, HYPIR, to address these challenges. Our solution pipeline is straightforward: it involves initializing the image restoration model with a pre-trained diffusion model and then fine-tuning it with adversarial training. This approach does not rely on diffusion loss, iterative sampling, or additional adapters. We theoretically demonstrate that initializing adversarial training from a pre-trained diffusion model positions the initial restoration model very close to the natural image distribution. Consequently, this initialization improves numerical stability, avoids mode collapse, and substantially accelerates the convergence of adversarial training. Moreover, HYPIR inherits the capabilities of diffusion models with rich user control, enabling text-guided restoration and adjustable texture richness. Requiring only a single forward pass, it achieves faster convergence and inference speed than diffusion-based methods. Extensive experiments show that HYPIR outperforms previous state-of-the-art methods, achieving efficient and high-quality image restoration.&lt;/p&gt;</content:encoded></item><item><title>LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging</title><link>https://arxiv.org/abs/2512.04939v1</link><guid>http://arxiv.org/abs/2512.04939v1</guid><pubDate>Thu, 04 Dec 2025 16:07:02 +0000</pubDate><dc:creator>Zhijian Shu</dc:creator><dc:creator>Cheng Lin</dc:creator><dc:creator>Tao Xie</dc:creator><dc:creator>Wei Yin</dc:creator><dc:creator>Ben Li</dc:creator><dc:creator>Zhiyuan Pu</dc:creator><dc:creator>Weize Li</dc:creator><dc:creator>Yao Yao</dc:creator><dc:creator>Xun Cao</dc:creator><dc:creator>Xiaoyang Guo</dc:creator><dc:creator>Xiao-Xiao Long</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/
Published: 2025-12-04T16:07:02+00:00
Venue: arXiv
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhijian Shu; Cheng Lin; Tao Xie; Wei Yin; Ben Li; Zhiyuan Pu; Weize Li; Yao Yao; Xun Cao; Xiaoyang Guo; Xiao-Xiao Long&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token&amp;#x27;s geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT&amp;#x27;s core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT&amp;#x27;s effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/&lt;/p&gt;</content:encoded></item><item><title>Multi-modal Collaborative Learning with Vision Foundation Model Prompt Boosts 3D Semi-supervised Semantic Segmentation</title><link>https://doi.org/10.1016/j.inffus.2025.104019</link><guid>10.1016/j.inffus.2025.104019</guid><pubDate>Fri, 05 Dec 2025 00:44:45 +0000</pubDate><dc:creator>Xiang He</dc:creator><dc:creator>Xu Li</dc:creator><dc:creator>Baidan Li</dc:creator><dc:creator>Zhiyuan Xu</dc:creator><dc:creator>Qimin Xu</dc:creator><dc:creator>Hongwei Lu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104019</prism:doi><description>3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.
Published: 2025-12-05T00:44:45+00:00
Venue: Information Fusion
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang He; Xu Li; Baidan Li; Zhiyuan Xu; Qimin Xu; Hongwei Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104019"&gt;10.1016/j.inffus.2025.104019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;3D semi-supervised semantic segmentation aims to mitigate heavy reliance on large-scale high-quality annotations to achieve accurate fine-grained and stereoscopic perception, and serves as a promising technique for intelligent industries. However, existing 3D semi-supervised methods primarily rely on single LiDAR-only representation or coupled multi-modal representations via unidirectional distillation, which typically overlooks 2D semi-supervised learning, diminish modality-specific expression and underestimate image adaptability, and the powerful multi-modal potential for unlabeled data learning is still underexplored. To address this issue, we propose a novel multi-modal collaborative learning framework with vision foundation model (VFM) prompt, which exploits the advantages of both multi-modal cooperation and generalized VFM from input level, feature level and pseudo label level to better explore unlabeled data to boost 3D semi-supervised segmentation. Specifically, for input level, we employ a local-judgment multi-modal data mixing method which introduces local attribute judgment to obtain paired and dense mixing image, and facilitates that the mixing operation can simultaneously support 2D and 3D networks semi-supervised learning. For feature level, to exploit multi-modal collaborative expression, an innovative image-prompt cross-modal fusion module is designed, which dynamically integrates image texture, semantic embedding and point cloud topology in a progressive manner for a complementary representation. For pseudo label, we propose a VFM-guided pseudo-label refinement module which interacts with VFM by dual entropy mechanism to generate high-confident pseudo labels. Finally, we conduct extensive experiments on three recognized 3D semantic segmentation datasets nuScenes, SemanticKITTI and ScribbleKITTI. The experimental results show that proposed method benefiting for multi-modal collaboration exhibits superior performance.&lt;/p&gt;</content:encoded></item><item><title>MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms</title><link>https://arxiv.org/abs/2512.03640v1</link><guid>http://arxiv.org/abs/2512.03640v1</guid><pubDate>Wed, 03 Dec 2025 10:22:27 +0000</pubDate><dc:creator>Jiahao Zhang</dc:creator><dc:creator>Xiao Zhao</dc:creator><dc:creator>Guangyu Gao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1007/978-981-96-2061-6_29</prism:doi><description>Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network's ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet's superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.
Published: 2025-12-03T10:22:27+00:00
Venue: arXiv
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiahao Zhang; Xiao Zhao; Guangyu Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/978-981-96-2061-6_29"&gt;10.1007/978-981-96-2061-6_29&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network&amp;#x27;s ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet&amp;#x27;s superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.&lt;/p&gt;</content:encoded></item><item><title>GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection</title><link>https://arxiv.org/abs/2512.02991v1</link><guid>http://arxiv.org/abs/2512.02991v1</guid><pubDate>Tue, 02 Dec 2025 18:05:02 +0000</pubDate><dc:creator>Md Sohag Mia</dc:creator><dc:creator>Md Nahid Hasan</dc:creator><dc:creator>Tawhid Ahmed</dc:creator><dc:creator>Muhammad Abdullah Adnan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.
Published: 2025-12-02T18:05:02+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Md Sohag Mia; Md Nahid Hasan; Tawhid Ahmed; Muhammad Abdullah Adnan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.&lt;/p&gt;</content:encoded></item><item><title>Difference Decomposition Networks for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2512.03470v1</link><guid>http://arxiv.org/abs/2512.03470v1</guid><pubDate>Wed, 03 Dec 2025 05:52:06 +0000</pubDate><dc:creator>Chen Hu</dc:creator><dc:creator>Mingyu Zhou</dc:creator><dc:creator>Shuai Yuan</dc:creator><dc:creator>Hongbo Hu</dc:creator><dc:creator>Xiangyu Qiu</dc:creator><dc:creator>Junhai Luo</dc:creator><dc:creator>Tian Pu</dc:creator><dc:creator>Xiyin Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.
Published: 2025-12-03T05:52:06+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Hu; Mingyu Zhou; Shuai Yuan; Hongbo Hu; Xiangyu Qiu; Junhai Luo; Tian Pu; Xiyin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.&lt;/p&gt;</content:encoded></item><item><title>融合小波卷积与频域注意力的小目标检测改进</title><link>https://doi.org/10.11834/jig.250293</link><guid>10.11834/jig.250293</guid><pubDate>Fri, 05 Dec 2025 08:29:30 +0000</pubDate><dc:creator>Liu Xu</dc:creator><dc:creator>Song Peibo</dc:creator><dc:creator>Bao Fangxun</dc:creator><dc:creator>Du Hongwei</dc:creator><prism:publicationName>Journal of Image and Graphics</prism:publicationName><prism:doi>10.11834/jig.250293</prism:doi><description>目的无人机拍摄图像存在小目标数量多，易受恶劣天气等噪声污染的特点，针对无人机拍摄图像的小目标检测技术在军用领域和商用领域都发挥着重要作用。然而，现有的目标检测方法在定位小目标方面仍然存在检测精度低的问题。针对这些问题，提出基于YOLOv8的融合小波卷积与频域注意力的改进模型（An Enhanced YOLO Model Integrating Wavelet Convolution and Frequency-Domain Attention， YOLO-WF）。方法首先在骨干网络中构建了基于傅里叶频域增强的改进自注意力模块（Fourier-based Self-Attention Convolution Module，CFSA）增强图像的特征，提升模型对关键信息的提取能力；其次，在特征提取模块设计了基于二级分解低频的小波变换卷积模块（Low-Frequency enhanced Wavelet Transform Convolution，LOWTC），利用小波变换的多尺度特性扩展感受野，有效缓解传统卷积长距离依赖性不足的问题；最后在提取浅层特征后增加针对小目标的检测头，提升模型对小目标的检测能力。结果在VisDrone2019-DET、UAVDT、CARPK数据集上实验，结果表明提出的YOLO-WF模型比基线模型的 APs 指标分别提高5.5个、3.08个、6.8个百分点，达到19.9%、38.54%和33.3%。 AP50 和 APm 指标也均有提升，以VisDrone2019-DET为例， AP50 、 APm 分别达到47.1%、40.3%，相比基线模型分别提高3.5、3.0个百分点，且参数量下降0.4%。结论YOLO-WF通过频域-小波融合策略，显著提升了中小目标的检测精度，且未引入额外存储负担，可直接迁移至其他航拍检测任务。
Published: 2025-12-05T08:29:30+00:00
Venue: Journal of Image and Graphics
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Liu Xu; Song Peibo; Bao Fangxun; Du Hongwei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Journal of Image and Graphics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.11834/jig.250293"&gt;10.11834/jig.250293&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;目的无人机拍摄图像存在小目标数量多，易受恶劣天气等噪声污染的特点，针对无人机拍摄图像的小目标检测技术在军用领域和商用领域都发挥着重要作用。然而，现有的目标检测方法在定位小目标方面仍然存在检测精度低的问题。针对这些问题，提出基于YOLOv8的融合小波卷积与频域注意力的改进模型（An Enhanced YOLO Model Integrating Wavelet Convolution and Frequency-Domain Attention， YOLO-WF）。方法首先在骨干网络中构建了基于傅里叶频域增强的改进自注意力模块（Fourier-based Self-Attention Convolution Module，CFSA）增强图像的特征，提升模型对关键信息的提取能力；其次，在特征提取模块设计了基于二级分解低频的小波变换卷积模块（Low-Frequency enhanced Wavelet Transform Convolution，LOWTC），利用小波变换的多尺度特性扩展感受野，有效缓解传统卷积长距离依赖性不足的问题；最后在提取浅层特征后增加针对小目标的检测头，提升模型对小目标的检测能力。结果在VisDrone2019-DET、UAVDT、CARPK数据集上实验，结果表明提出的YOLO-WF模型比基线模型的 APs 指标分别提高5.5个、3.08个、6.8个百分点，达到19.9%、38.54%和33.3%。 AP50 和 APm 指标也均有提升，以VisDrone2019-DET为例， AP50 、 APm 分别达到47.1%、40.3%，相比基线模型分别提高3.5、3.0个百分点，且参数量下降0.4%。结论YOLO-WF通过频域-小波融合策略，显著提升了中小目标的检测精度，且未引入额外存储负担，可直接迁移至其他航拍检测任务。&lt;/p&gt;</content:encoded></item><item><title>A review of fake news detection based on transfer learning</title><link>https://doi.org/10.1016/j.inffus.2025.104029</link><guid>10.1016/j.inffus.2025.104029</guid><pubDate>Thu, 04 Dec 2025 00:41:34 +0000</pubDate><dc:creator>Chen Bo Qi</dc:creator><dc:creator>Xiao Hua Li</dc:creator><dc:creator>Xing Yang</dc:creator><dc:creator>Ming Zheng Li</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104029</prism:doi><description>In recent years, transfer learning has made significant strides in fake news detection, but comprehensive investigations remain limited. To address this gap, this paper thoroughly synthesizes the current progress, methodologies, experiments, and challenges associated with transfer learning-based fake news detection. First, we categorize the transfer learning algorithms for fake news detection into single-domain and multi-domain algorithms based on transfer strategies. Subsequently, we further classify these algorithms into cross-domain, domain adaptation, and domain generalization fake news detection algorithms, considering both single-domain and multi-domain scenarios. Additionally, we discuss their working principles and transfer mechanisms, summarizing their strengths and limitations. We then select representative algorithms from each category and conduct comparative experiments to evaluate their domain transfer capabilities. The experimental results demonstrate that transfer learning-based fake news detection algorithms exhibit excellent performance across five benchmark datasets. Finally, we present unresolved challenges and future research directions. This survey not merely systematizes the understanding of domain transfer in fake news detection but further serves as a practical guide for selecting appropriate transfer techniques for implementation in fake news detection.
Published: 2025-12-04T00:41:34+00:00
Venue: Information Fusion
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen Bo Qi; Xiao Hua Li; Xing Yang; Ming Zheng Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104029"&gt;10.1016/j.inffus.2025.104029&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, transfer learning has made significant strides in fake news detection, but comprehensive investigations remain limited. To address this gap, this paper thoroughly synthesizes the current progress, methodologies, experiments, and challenges associated with transfer learning-based fake news detection. First, we categorize the transfer learning algorithms for fake news detection into single-domain and multi-domain algorithms based on transfer strategies. Subsequently, we further classify these algorithms into cross-domain, domain adaptation, and domain generalization fake news detection algorithms, considering both single-domain and multi-domain scenarios. Additionally, we discuss their working principles and transfer mechanisms, summarizing their strengths and limitations. We then select representative algorithms from each category and conduct comparative experiments to evaluate their domain transfer capabilities. The experimental results demonstrate that transfer learning-based fake news detection algorithms exhibit excellent performance across five benchmark datasets. Finally, we present unresolved challenges and future research directions. This survey not merely systematizes the understanding of domain transfer in fake news detection but further serves as a practical guide for selecting appropriate transfer techniques for implementation in fake news detection.&lt;/p&gt;</content:encoded></item><item><title>Efficient and Scalable Point Cloud Generation With Sparse Point-Voxel Diffusion Models</title><link>https://doi.org/10.1109/tnnls.2025.3636409</link><guid>10.1109/tnnls.2025.3636409</guid><pubDate>Thu, 04 Dec 2025 18:38:05 +0000</pubDate><dc:creator>Ioannis Romanelis</dc:creator><dc:creator>Vlassis Fotis</dc:creator><dc:creator>Athanasios Kalogeras</dc:creator><dc:creator>Christos Alexakos</dc:creator><dc:creator>Adrian Munteanu</dc:creator><dc:creator>Konstantinos Moustakas</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2025.3636409</prism:doi><description>We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD
Published: 2025-12-04T18:38:05+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ioannis Romanelis; Vlassis Fotis; Athanasios Kalogeras; Christos Alexakos; Adrian Munteanu; Konstantinos Moustakas&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2025.3636409"&gt;10.1109/tnnls.2025.3636409&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;We propose a novel point cloud U-Net diffusion architecture for 3-D generative modeling capable of generating high-quality and diverse 3-D shapes while maintaining fast generation times. Our network employs a dual-branch architecture, combining the high-resolution representations of points with the computational efficiency of sparse voxels. Our fastest variant outperforms all nondiffusion generative approaches on unconditional shape generation, the most popular benchmark for evaluating point cloud generative models, while our largest model achieves state-of-the-art results among diffusion methods, with a runtime approximately 70% of the previously state-of-the-art point-voxel diffusion (PVD), measured on the same hardware setting. Beyond unconditional generation, we perform extensive evaluations, including conditional generation on all categories of ShapeNet, demonstrating the scalability of our model to larger datasets, and implicit generation, which allows our network to produce high-quality point clouds on fewer timesteps, further decreasing the generation time. Finally, we evaluate the architecture’s performance in point cloud completion and super-resolution. Our model excels in all tasks, establishing it as a state-of-the-art diffusion U-Net for point cloud generative modeling. The code is publicly available at https://github.com/JohnRomanelis/SPVD&lt;/p&gt;</content:encoded></item><item><title>ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection</title><link>https://arxiv.org/abs/2512.02696v1</link><guid>http://arxiv.org/abs/2512.02696v1</guid><pubDate>Tue, 02 Dec 2025 12:28:07 +0000</pubDate><dc:creator>Omid Reza Heidari</dc:creator><dc:creator>Yang Wang</dc:creator><dc:creator>Xinxin Zuo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.
Published: 2025-12-02T12:28:07+00:00
Venue: arXiv
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Omid Reza Heidari; Yang Wang; Xinxin Zuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.&lt;/p&gt;</content:encoded></item><item><title>Infrared UAV Target Tracking with Dynamic Feature Refinement and Global Contextual Attention Knowledge Distillation</title><link>https://arxiv.org/abs/2512.04581v1</link><guid>http://arxiv.org/abs/2512.04581v1</guid><pubDate>Thu, 04 Dec 2025 08:49:23 +0000</pubDate><dc:creator>Houzhang Fang</dc:creator><dc:creator>Chenxing Wu</dc:creator><dc:creator>Kun Bai</dc:creator><dc:creator>Tianqi Chen</dc:creator><dc:creator>Xiaolin Wang</dc:creator><dc:creator>Xiyang Liu</dc:creator><dc:creator>Yi Chang</dc:creator><dc:creator>Luxin Yan</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network's focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.
Published: 2025-12-04T08:49:23+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Houzhang Fang; Chenxing Wu; Kun Bai; Tianqi Chen; Xiaolin Wang; Xiyang Liu; Yi Chang; Luxin Yan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Unmanned aerial vehicle (UAV) target tracking based on thermal infrared imaging has been one of the most important sensing technologies in anti-UAV applications. However, the infrared UAV targets often exhibit weak features and complex backgrounds, posing significant challenges to accurate tracking. To address these problems, we introduce SiamDFF, a novel dynamic feature fusion Siamese network that integrates feature enhancement and global contextual attention knowledge distillation for infrared UAV target (IRUT) tracking. The SiamDFF incorporates a selective target enhancement network (STEN), a dynamic spatial feature aggregation module (DSFAM), and a dynamic channel feature aggregation module (DCFAM). The STEN employs intensity-aware multi-head cross-attention to adaptively enhance important regions for both template and search branches. The DSFAM enhances multi-scale UAV target features by integrating local details with global features, utilizing spatial attention guidance within the search frame. The DCFAM effectively integrates the mixed template generated from STEN in the template branch and original template, avoiding excessive background interference with the template and thereby enhancing the emphasis on UAV target region features within the search frame. Furthermore, to enhance the feature extraction capabilities of the network for IRUT without adding extra computational burden, we propose a novel tracking-specific target-aware contextual attention knowledge distiller. It transfers the target prior from the teacher network to the student model, significantly improving the student network&amp;#x27;s focus on informative regions at each hierarchical level of the backbone network. Extensive experiments on real infrared UAV datasets demonstrate that the proposed approach outperforms state-of-the-art target trackers under complex backgrounds while achieving a real-time tracking speed.&lt;/p&gt;</content:encoded></item><item><title>DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images</title><link>https://arxiv.org/abs/2512.03004v1</link><guid>http://arxiv.org/abs/2512.03004v1</guid><pubDate>Tue, 02 Dec 2025 18:29:18 +0000</pubDate><dc:creator>Xiaoxue Chen</dc:creator><dc:creator>Ziyi Xiong</dc:creator><dc:creator>Yuantao Chen</dc:creator><dc:creator>Gen Li</dc:creator><dc:creator>Nan Wang</dc:creator><dc:creator>Hongcheng Luo</dc:creator><dc:creator>Long Chen</dc:creator><dc:creator>Haiyang Sun</dc:creator><dc:creator>Bing Wang</dc:creator><dc:creator>Guang Chen</dc:creator><dc:creator>Hangjun Ye</dc:creator><dc:creator>Hongyang Li</dc:creator><dc:creator>Ya-Qin Zhang</dc:creator><dc:creator>Hao Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.
Published: 2025-12-02T18:29:18+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxue Chen; Ziyi Xiong; Yuantao Chen; Gen Li; Nan Wang; Hongcheng Luo; Long Chen; Haiyang Sun; Bing Wang; Guang Chen; Hangjun Ye; Hongyang Li; Ya-Qin Zhang; Hao Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.&lt;/p&gt;</content:encoded></item></channel></rss>