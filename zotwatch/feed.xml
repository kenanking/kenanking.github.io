<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 07 Jan 2026 02:54:42 +0000</lastBuildDate><item><title>LRANet++: Low-Rank Approximation Network for Accurate and Efficient Text Spotting</title><link>https://doi.org/10.1109/tpami.2026.3650769</link><guid>10.1109/tpami.2026.3650769</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Yuchen Su</dc:creator><dc:creator>Zhineng Chen</dc:creator><dc:creator>Yongkun Du</dc:creator><dc:creator>Zuxuan Wu</dc:creator><dc:creator>Hongtao Xie</dc:creator><dc:creator>Yu-Gang Jiang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650769</prism:doi><description>End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.832 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuchen Su; Zhineng Chen; Yongkun Du; Zuxuan Wu; Hongtao Xie; Yu-Gang Jiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650769"&gt;10.1109/tpami.2026.3650769&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.832 (must_read)&lt;/p&gt;
&lt;p&gt;End-to-end text spotting aims to jointly optimize text detection and recognition within a unified framework. Despite significant progress, designing an accurate and efficient end-to-end text spotter for arbitrary-shaped text remains challenging. We identify the primary bottleneck as the lack of a reliable and efficient text detection method. To address this, we propose a novel parameterized text shape representation based on low-rank approximation for precise detection and a triple assignment detection head for fast inference. Specifically, unlike current data-irrelevant shape representation methods, we exploit shape correlations among labeled text boundaries to construct a robust low-rank subspace. By minimizing an \ell _{1} \ell _{1} -norm objective, we extract orthogonal vectors that capture the intrinsic text shape from noisy annotations, enabling precise reconstruction via the linear combination of only a few basis vectors. Next, the triple assignment scheme decouples training complexity from inference speed. It utilizes a deep sparse branch to guide an ultra-lightweight inference branch, while a dense branch provides rich parallel supervision. Building upon these advancements, we integrate the enhanced detection module with a lightweight recognition branch to form an end-to-end text spotting framework, termed LRANet++, capable of accurately and efficiently spotting arbitrary-shaped text. Extensive experiments on challenging benchmarks demonstrate the superiority of LRANet++ compared to state-of-the-art methods. Code is available at: https://github.com/ychensu/LRANet-PP.&lt;/p&gt;</content:encoded></item><item><title>Beyond LLaVA-HD: Diving into High-Resolution Multimodal Large Language Models</title><link>https://doi.org/10.1109/tpami.2026.3650761</link><guid>10.1109/tpami.2026.3650761</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>YiFan Zhang</dc:creator><dc:creator>Qingsong Wen</dc:creator><dc:creator>Chaoyou Fu</dc:creator><dc:creator>Kun Wang</dc:creator><dc:creator>Xue Wang</dc:creator><dc:creator>Zhang Zhang</dc:creator><dc:creator>Liang Wang</dc:creator><dc:creator>Rong Jin</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650761</prism:doi><description>Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.831 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; YiFan Zhang; Qingsong Wen; Chaoyou Fu; Kun Wang; Xue Wang; Zhang Zhang; Liang Wang; Rong Jin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650761"&gt;10.1109/tpami.2026.3650761&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.831 (must_read)&lt;/p&gt;
&lt;p&gt;Seeing clearly with high resolution is a foundation of Multimodal Large Language Models (MLLMs), which has been proven to be vital for visual perception and reasoning. Existing works usually employ a straightforward resolution upscaling method, where the image consists of global and local branches, with the latter being the sliced image patches but resized to the same resolution as the former. This means that higher resolution requires more local patches, resulting in exorbitant computational expenses, and meanwhile, the dominance of local image tokens may diminish the global context. In this paper, we dive into the problems and propose a new framework as well as an elaborate optimization strategy. Specifically, we extract contextual information from the global view using a mixture of adapters, based on the observation that different adapters excel at different tasks. With regard to local patches, learnable query embeddings are introduced to reduce image tokens, the important tokens most relevant to the user question will be further selected by a similarity-based selector. Our empirical results demonstrate a ‘less is more’ pattern, where utilizing fewer but more informative local image tokens leads to improved performance. Besides, a significant challenge lies in the training strategy, as simultaneous end-to-end training of the global mining block and local compression block does not yield optimal results. We thus advocate for an alternating training way, ensuring balanced learning between global and local aspects. Finally, we also introduce a challenging dataset with high requirements for image detail, enhancing the training of the local compression layer. The proposed method, termed MLLM with Sophisticated Tasks, Local image compression, and Mixture of global Experts (SliME), achieves leading performance across various benchmarks with only 2 million training data.&lt;/p&gt;</content:encoded></item><item><title>UniSparseBEV: A Multi-Task Learning Framework with Unified Sparse Query for Autonomous Driving</title><link>https://doi.org/10.1109/tcsvt.2026.3651369</link><guid>10.1109/tcsvt.2026.3651369</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Hao Zhou</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Honggang Qi</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651369</prism:doi><description>Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.827 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Zhou; Yi Zhang; Honggang Qi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651369"&gt;10.1109/tcsvt.2026.3651369&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.827 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in vision-centric multi-task learning have greatly impacted autonomous driving, with a focus on constructing efficient and rich Bird’s Eye View (BEV) representations. While these methods achieve impressive performance, they often suffer from structural complexity and high computational costs due to the need for dense BEV representations. To address these challenges, we propose UniSparseBEV, a simple and efficient vision-based multi-task learning framework based on sparse queries. We introduce a set of learnable shared queries to facilitate information exchange across tasks. Additionally, we propose the Z-axis Deformable Cross-Attention (Z-DCA) module, which enables BEV segmentation task queries to directly extract information from image features without requiring dense BEV representations. To further enhance training efficiency, we incorporate 2D supervision into the network. Extensive experiments on the NuScenes dataset demonstrate that UniSparseBEV outperforms existing single-task methods in 3D object detection and BEV segmentation. A detailed robustness analysis is also conducted on the UniSparseBEV framework. We hope UniSparseBEV can serve as a strong baseline for multi-tasking in autonomous driving.&lt;/p&gt;</content:encoded></item><item><title>Target-Level SAR-to-Optical Image Translation Driven by Semantic Segmentation</title><link>https://doi.org/10.1109/taes.2025.3650511</link><guid>10.1109/taes.2025.3650511</guid><pubDate>Mon, 05 Jan 2026 18:42:42 +0000</pubDate><dc:creator>Huihui Li</dc:creator><dc:creator>Siyuan Liu</dc:creator><dc:creator>Zhou Liu</dc:creator><dc:creator>Hang Liu</dc:creator><dc:creator>Dawei Guo</dc:creator><dc:creator>Kun Liu</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3650511</prism:doi><description>Synthetic Aperture Radar (SAR) is widely used in military and civilian applications due to its all-weather, day-and-night imaging capability. However, interpreting SAR images is challenging for both experts and non-experts. Inspired by deep learning-based style transfer, researchers have employed Generative Adversarial Networks (GANs) to convert SAR images into more intuitive optical ones. Yet, current loss functions and evaluation metrics focus mainly on pixel-level differences, overlooking the structural coherence required for target recognition and downstream tasks. To address this, we propose a semantic segmentation-driven framework for target-level SAR-to-optical image translation. Compatible with various supervised models, it incorporates segmentation loss and uses SAR segmentation maps as additional inputs to preserve target structure. Experiments on custom datasets, built from Sentinel 1-2 imagery with road binary segmentation labels, as well as public datasets, confirm the method's effectiveness across different base translation models. The source code and the datasets used will be published at the following URL https://github.com/NWPU-LHH/SOIT-Seg-Driven.
Published: 2026-01-05T18:42:42+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.823 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huihui Li; Siyuan Liu; Zhou Liu; Hang Liu; Dawei Guo; Kun Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3650511"&gt;10.1109/taes.2025.3650511&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.823 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is widely used in military and civilian applications due to its all-weather, day-and-night imaging capability. However, interpreting SAR images is challenging for both experts and non-experts. Inspired by deep learning-based style transfer, researchers have employed Generative Adversarial Networks (GANs) to convert SAR images into more intuitive optical ones. Yet, current loss functions and evaluation metrics focus mainly on pixel-level differences, overlooking the structural coherence required for target recognition and downstream tasks. To address this, we propose a semantic segmentation-driven framework for target-level SAR-to-optical image translation. Compatible with various supervised models, it incorporates segmentation loss and uses SAR segmentation maps as additional inputs to preserve target structure. Experiments on custom datasets, built from Sentinel 1-2 imagery with road binary segmentation labels, as well as public datasets, confirm the method&amp;#x27;s effectiveness across different base translation models. The source code and the datasets used will be published at the following URL https://github.com/NWPU-LHH/SOIT-Seg-Driven.&lt;/p&gt;</content:encoded></item><item><title>Breaking Self-Attention Failure: Rethinking Query Initialization for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2601.02837v1</link><guid>http://arxiv.org/abs/2601.02837v1</guid><pubDate>Tue, 06 Jan 2026 09:14:01 +0000</pubDate><dc:creator>Yuteng Liu</dc:creator><dc:creator>Duanni Meng</dc:creator><dc:creator>Maoxun Yuan</dc:creator><dc:creator>Xingxing Wei</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.
Published: 2026-01-06T09:14:01+00:00
Venue: arXiv
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuteng Liu; Duanni Meng; Maoxun Yuan; Xingxing Wei&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) faces significant challenges due to the low signal-to-noise ratio (SNR), small target size, and complex cluttered backgrounds. Although recent DETR-based detectors benefit from global context modeling, they exhibit notable performance degradation on IRSTD. We revisit this phenomenon and reveal that the target-relevant embeddings of IRST are inevitably overwhelmed by dominant background features due to the self-attention mechanism, leading to unreliable query initialization and inaccurate target localization. To address this issue, we propose SEF-DETR, a novel framework that refines query initialization for IRSTD. Specifically, SEF-DETR consists of three components: Frequency-guided Patch Screening (FPS), Dynamic Embedding Enhancement (DEE), and Reliability-Consistency-aware Fusion (RCF). The FPS module leverages the Fourier spectrum of local patches to construct a target-relevant density map, suppressing background-dominated features. DEE strengthens multi-scale representations in a target-aware manner, while RCF further refines object queries by enforcing spatial-frequency consistency and reliability. Extensive experiments on three public IRSTD datasets demonstrate that SEF-DETR achieves superior detection performance compared to state-of-the-art methods, delivering a robust and efficient solution for infrared small target detection task.&lt;/p&gt;</content:encoded></item><item><title>MARSNet: A Mamba-driven adaptive framework for robust multisource remote sensing image matching in noisy environments</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.021</link><guid>10.1016/j.isprsjprs.2025.12.021</guid><pubDate>Mon, 05 Jan 2026 11:18:47 +0000</pubDate><dc:creator>Weipeng Jing</dc:creator><dc:creator>Peilun Kang</dc:creator><dc:creator>Donglin Di</dc:creator><dc:creator>Jian Wang</dc:creator><dc:creator>Yang Song</dc:creator><dc:creator>Chao Li</dc:creator><dc:creator>Lei Fan</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.021</prism:doi><description>Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.
Published: 2026-01-05T11:18:47+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weipeng Jing; Peilun Kang; Donglin Di; Jian Wang; Yang Song; Chao Li; Lei Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.021"&gt;10.1016/j.isprsjprs.2025.12.021&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Semi-dense matching of multi-source remote sensing images under noise interference remains a challenging task. Existing detector-free methods often exhibit low efficiency and reduced performance when faced with large viewpoint variations and significant noise disturbances. Due to the inherent noise and modality differences in multi-source remote sensing images, the accuracy and robustness of feature matching are substantially compromised. To address this issue, we propose a hybrid network for multi-source remote sensing image matching based on an efficient and robust Mamba framework, named MARSNet. The network achieves efficient and robust matching through the following innovative designs: First, it leverages the efficient Mamba network to capture long-range dependencies within image sequences, enhancing the modeling capability for complex scenes. Second, a frozen pre-trained DINOv2 foundation model is introduced as a robust feature extractor, effectively improving the model’s noise resistance. Finally, an adaptive fusion strategy is employed to integrate features, and the Mamba-like linear attention mechanism is adopted to refine the Transformer-based linear attention, further enhancing the efficiency and expressive power for long-sequence processing. To validate the effectiveness of the proposed method, extensive experiments were conducted on multi-source remote sensing image datasets, covering various scenarios such as noise-free, additive random noise, and periodic stripe noise. The experimental results demonstrate that the proposed method achieves significant improvements in matching accuracy and robustness compared to state-of-the-art methods. Additionally, by performing pose error evaluation on a large-scale general dataset, the superior performance of the proposed method in 3D reconstruction is validated, complementing the test results from the multi-source remote sensing dataset, thereby providing a more comprehensive assessment of the method’s generalization ability and robustness.&lt;/p&gt;</content:encoded></item><item><title>Beyond synthetic scenarios: Weakly-supervised super-resolution for spatiotemporally misaligned remote sensing images</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.019</link><guid>10.1016/j.isprsjprs.2025.12.019</guid><pubDate>Tue, 06 Jan 2026 09:49:35 +0000</pubDate><dc:creator>Quanyi Guo</dc:creator><dc:creator>Rui Liu</dc:creator><dc:creator>Yangtian Fang</dc:creator><dc:creator>Yi Gao</dc:creator><dc:creator>Jun Chen</dc:creator><dc:creator>Xin Tian</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.019</prism:doi><description>Deep learning-based remote sensing image super-resolution is crucial for enhancing the spatial resolution of Earth observation data. Due to the absence of perfectly aligned pairs of high- and low-resolution remote sensing images, most existing supervised and self-supervised approaches rely on synthetic degradation models or internal structural consistency to generate training data. Consequently, these methods suffer from the domain gap between synthetic and real datasets, which limits their ability to model realistic degradation and degrades their performance in real scenes. To overcome this challenge, we propose STANet, a weakly-supervised super-resolution method for spatiotemporally misaligned remote sensing images. In particular, STANet directly utilizes images of the same region captured by multiple satellites at different resolutions as datasets, to boost the real remote sensing image super-resolution performance. However, this approach also introduces new challenges related to spatiotemporal misalignment. To address this, we design a spatiotemporal align module that includes a Scale Align Module (SAM) and a Temporal Align Module (TAM). SAM uses affine transformations to align spatial features at both the pixel and global levels, while TAM applies window-based attention to adjust the weight of image content, mitigating the misleading effects of temporal misalignment on results. Besides, we also design a style encoder based on contrastive learning and a structure encoder based on variational inference, which guide SAM and TAM for feature alignment and enhance adaptability. Finally, the feature-aligned output, after upsampling, are fused with the high-frequency-enhancing output of the texture transfer module through the weighted fusion module to generate the super-resolution image. Extensive experiments on synthetic datasets based on AID and RSSR25, real datasets captured by GaoFen satellites, and cross-satellite experiments on Landsat-8 datasets demonstrate STANet’s superiority over other state-of-the-art methods.
Published: 2026-01-06T09:49:35+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Quanyi Guo; Rui Liu; Yangtian Fang; Yi Gao; Jun Chen; Xin Tian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.019"&gt;10.1016/j.isprsjprs.2025.12.019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning-based remote sensing image super-resolution is crucial for enhancing the spatial resolution of Earth observation data. Due to the absence of perfectly aligned pairs of high- and low-resolution remote sensing images, most existing supervised and self-supervised approaches rely on synthetic degradation models or internal structural consistency to generate training data. Consequently, these methods suffer from the domain gap between synthetic and real datasets, which limits their ability to model realistic degradation and degrades their performance in real scenes. To overcome this challenge, we propose STANet, a weakly-supervised super-resolution method for spatiotemporally misaligned remote sensing images. In particular, STANet directly utilizes images of the same region captured by multiple satellites at different resolutions as datasets, to boost the real remote sensing image super-resolution performance. However, this approach also introduces new challenges related to spatiotemporal misalignment. To address this, we design a spatiotemporal align module that includes a Scale Align Module (SAM) and a Temporal Align Module (TAM). SAM uses affine transformations to align spatial features at both the pixel and global levels, while TAM applies window-based attention to adjust the weight of image content, mitigating the misleading effects of temporal misalignment on results. Besides, we also design a style encoder based on contrastive learning and a structure encoder based on variational inference, which guide SAM and TAM for feature alignment and enhance adaptability. Finally, the feature-aligned output, after upsampling, are fused with the high-frequency-enhancing output of the texture transfer module through the weighted fusion module to generate the super-resolution image. Extensive experiments on synthetic datasets based on AID and RSSR25, real datasets captured by GaoFen satellites, and cross-satellite experiments on Landsat-8 datasets demonstrate STANet’s superiority over other state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Revisiting Out-of-Distribution Detection in Real-time Object Detection: From Benchmark Pitfalls to a New Mitigation Paradigm</title><link>https://doi.org/10.1109/tpami.2025.3650695</link><guid>10.1109/tpami.2025.3650695</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Changshun Wu</dc:creator><dc:creator>Weicheng He</dc:creator><dc:creator>Chih-Hong Cheng</dc:creator><dc:creator>Xiaowei Huang</dc:creator><dc:creator>Saddek Bensalem</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650695</prism:doi><description>Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changshun Wu; Weicheng He; Chih-Hong Cheng; Xiaowei Huang; Saddek Bensalem&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650695"&gt;10.1109/tpami.2025.3650695&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Out-of-distribution (OoD) inputs pose a persistent challenge to deep learning models, often triggering overconfident predictions on non-target objects. While prior work has primarily focused on refining scoring functions and adjusting test-time thresholds, such algorithmic improvements offer only incremental gains. We argue that a rethinking of the entire development lifecycle is needed to mitigate these risks effectively. This work addresses two overlooked dimensions of OoD detection in object detection. First, we reveal fundamental flaws in widely used evaluation benchmarks: contrary to their design intent, up to 13% of objects in the OoD test sets actually belong to in-distribution classes, and vice versa. These quality issues severely distort the reported performance of existing methods and contribute to their high false positive rates. Second, we introduce a novel training-time mitigation paradigm that operates independently of external OoD detectors. Instead of relying solely on post-hoc scoring, we fine-tune the detector using a carefully synthesized OoD dataset that semantically resembles in-distribution objects. This process shapes a defensive decision boundary by suppressing objectness on OoD objects, leading to a 91% reduction in hallucination error of a YOLO model on BDD-100K. Our methodology generalizes across detection paradigms such as YOLO, Faster R-CNN, and RT-DETR, and supports few-shot adaptation. Together, these contributions offer a principled and effective way to reduce OoD-induced hallucination in object detectors. Code and data are available at: https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.&lt;/p&gt;</content:encoded></item><item><title>A Feature-Enhanced Network-Based Target Detection Method for SAR Images of Ships in Complex Scenes</title><link>https://doi.org/10.3390/rs18010178</link><guid>10.3390/rs18010178</guid><pubDate>Mon, 05 Jan 2026 15:28:57 +0000</pubDate><dc:creator>Yunsheng Ba</dc:creator><dc:creator>Nan Xia</dc:creator><dc:creator>Weijia Lu</dc:creator><dc:creator>Junqiao Liu</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010178</prism:doi><description>In the context of ship target detection with Synthetic Aperture Radar (SAR) images, misdetection and missed detection are often caused by complex background interference and the variability in target size. To address these challenges, this paper proposes an innovative method based on image enhancement and feature fusion to reduce background noise and effectively handle the detection confusion caused by differences in ship sizes. Firstly, a feature-aware enhancement network is introduced, which preserves and strengthens the edge information of the target objects. Secondly, during the feature extraction phase, a dynamic hierarchical extraction module is proposed, significantly improving the feature capture ability of convolutional neural networks and overcoming the limitations of traditional fixed kernel receptive fields. Finally, a feature fusion module based on attention gating is employed to fully leverage the complementary information between the original and enhanced images, achieving precise modeling and efficient fusion of inter-feature correlations. The proposed method is integrated with the YOLOv8 detection framework for target detection. Experimental results in the publicly available SSDD and HRSID datasets demonstrate detection accuracies of 97.9% and 93.2%, respectively, thus validating the superiority and robustness of the proposed method.
Published: 2026-01-05T15:28:57+00:00
Venue: Remote Sensing
Score: 0.812 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunsheng Ba; Nan Xia; Weijia Lu; Junqiao Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010178"&gt;10.3390/rs18010178&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.812 (must_read)&lt;/p&gt;
&lt;p&gt;In the context of ship target detection with Synthetic Aperture Radar (SAR) images, misdetection and missed detection are often caused by complex background interference and the variability in target size. To address these challenges, this paper proposes an innovative method based on image enhancement and feature fusion to reduce background noise and effectively handle the detection confusion caused by differences in ship sizes. Firstly, a feature-aware enhancement network is introduced, which preserves and strengthens the edge information of the target objects. Secondly, during the feature extraction phase, a dynamic hierarchical extraction module is proposed, significantly improving the feature capture ability of convolutional neural networks and overcoming the limitations of traditional fixed kernel receptive fields. Finally, a feature fusion module based on attention gating is employed to fully leverage the complementary information between the original and enhanced images, achieving precise modeling and efficient fusion of inter-feature correlations. The proposed method is integrated with the YOLOv8 detection framework for target detection. Experimental results in the publicly available SSDD and HRSID datasets demonstrate detection accuracies of 97.9% and 93.2%, respectively, thus validating the superiority and robustness of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>PVF-DectNet++: Adaptive Multi-Modal Fusion with Perspective Voxels for 3D Object Detection</title><link>https://doi.org/10.1109/tcsvt.2025.3650671</link><guid>10.1109/tcsvt.2025.3650671</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Ke Wang</dc:creator><dc:creator>Weilin Gao</dc:creator><dc:creator>Kai Chen</dc:creator><dc:creator>Tianyi Shao</dc:creator><dc:creator>Liyang Li</dc:creator><dc:creator>Tianqiang Zhou</dc:creator><dc:creator>Jianbo Lu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2025.3650671</prism:doi><description>To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.811 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ke Wang; Weilin Gao; Kai Chen; Tianyi Shao; Liyang Li; Tianqiang Zhou; Jianbo Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2025.3650671"&gt;10.1109/tcsvt.2025.3650671&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.811 (must_read)&lt;/p&gt;
&lt;p&gt;To enhance 3D object detection in autonomous driving, recent work combines LiDAR and camera data. However, prior methods often suffer from inadequate image depth information and fixed-weight fusion strategies, limiting semantic extraction and adaptability. PVF-DectNet++ builds on our prior work by employing a perspective voxel projection technique to align both feature types. It introduces an adaptive image semantic feature extraction approach that interpolates image and point cloud intensity into a dense RGB-I multi-channel representation, facilitating the extraction of global, multi-level image features. Furthermore, during the fusion process, a learnable fusion module is designed to address the challenge of individual channels being unable to adapt to varying appearances, colors, and environmental conditions. Experiments on KITTI, nuScenes, and Waymo comprehensively validate PVF-DectNet++. On KITTI, it achieves detection accuracies of 66.3% for pedestrians, 78.8% for cyclists, and 86.8% for vehicles, yielding a 3.56% mAP improvement over PVF-DectNet. Additional tests show further gains, with mAP and NDS increases of 3.8% and 2.6% on nuScenes, and notable boosts in pedestrian and cyclist AP on Waymo. Compared with existing networks, PVF-DectNet++ consistently delivers superior performance, particularly for pedestrian and cyclist detection across diverse benchmarks. The code and model will be released at https://github.com/CQU-AVL/PVF-DectNet-.&lt;/p&gt;</content:encoded></item><item><title>MADTP++: Bridge the Gap Between Token and Weight Pruning for Accelerating VLTs</title><link>https://doi.org/10.1109/tpami.2025.3650545</link><guid>10.1109/tpami.2025.3650545</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Jianjian Cao</dc:creator><dc:creator>Chong Yu</dc:creator><dc:creator>Peng Ye</dc:creator><dc:creator>Tao Chen</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650545</prism:doi><description>Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianjian Cao; Chong Yu; Peng Ye; Tao Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650545"&gt;10.1109/tpami.2025.3650545&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Transformers (VLTs) have achieved remarkable success, but their computational costs pose a challenge due to the large number of input tokens and extensive model parameters. Existing VLT compression methods primarily rely on single-modality-based token pruning or coarse-grained weight pruning techniques. However, these methods face significant obstacles, such as ignoring the critical alignment of different modalities and lacking the flexibility to dynamically compress each layer for token pruning, exhibiting inevitable performance degradation due to coarse-grained weight pruning, and struggling with the simultaneous compression of both input tokens and model parameters. To address those limitations, we propose MADTP++, a novel approach that integrates custom-made token and weight pruning processes into a unified framework, achieving superior compression in both parameter counts and computational costs. Specifically, for the token pruning process, we introduce the Multi-modality Alignment Guidance (MAG) module and the Dynamic Token Pruning (DTP) module to align semantic features across different modalities and guide the dynamic elimination of redundant tokens based on different input instances. For the weight pruning process, we propose a Hardware-aware Weight Pruning (HWP) module that leverages the Sparse Tensor Cores across diverse hardware setups to enable fine-grained parameter pruning within VLTs. To further unify token and weight pruning, we also propose a Cooperative Optimization Training Strategy that automatically assigns the required reduction in GFLOPs and Params to each branch before pruning and employs Knowledge Distillation Constraints to facilitate joint optimization of both pruning dimensions. Extensive experiments conducted on various VLT models and datasets demonstrate that MADTP++ can significantly reduce model parameters and computational costs while maintaining competitive performance. We have made the code available at https://git...&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Benchmark for Spatio-Temporal Tensor-Based Infrared Small Target Detection</title><link>https://doi.org/10.1109/tgrs.2026.3651631</link><guid>10.1109/tgrs.2026.3651631</guid><pubDate>Tue, 06 Jan 2026 18:35:38 +0000</pubDate><dc:creator>Fengyi Wu</dc:creator><dc:creator>Siyu Chen</dc:creator><dc:creator>Simin Liu</dc:creator><dc:creator>Bingjie Tao</dc:creator><dc:creator>Junhai Luo</dc:creator><dc:creator>Zhenming Peng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3651631</prism:doi><description>Optimization-based methods have gained prominence in infrared small target detection (ISTD) due to their effectiveness in handling sparse targets. However, existing approaches often struggle with spatio-temporal modeling, resulting in suboptimal background suppression and target misclassification. Moreover, stringent recovery tolerances limit computational efficiency in sequential infrared processing. To overcome these challenges, we introduce a novel approach that enhances both background estimation and target detection by leveraging spatio-temporal features. Specifically, we integrate a four-dimensional (4-D) spatio-temporal tensor with a fully connected tensor network (FCTN) completion strategy, utilizing the sum of nuclear norms to maximize intermode correlations while minimizing clutter. For target extraction, we employ a dual Gaussian-core spatial saliency filter (SSF) and a temporal difference filter (TDF), enabling precise discrimination between true targets and static interference. The resulting model—Spatial-Temporal Prior-Assisted Fully Connected Tensor Network (STPA-FCTN)—balances background suppression with target preservation. Furthermore, we systematically analyze recovery convergence in STPA-FCTN and identify an optimal tolerance of 10-3 using the proposed open-source TensorISTD benchmark. Extensive evaluations on public datasets confirm the superior numerical and visual performance of our method over state-of-the-art techniques. Code and benchmark resources are available at https://github.com/fengyiwu98/TensorISTD.
Published: 2026-01-06T18:35:38+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fengyi Wu; Siyu Chen; Simin Liu; Bingjie Tao; Junhai Luo; Zhenming Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3651631"&gt;10.1109/tgrs.2026.3651631&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Optimization-based methods have gained prominence in infrared small target detection (ISTD) due to their effectiveness in handling sparse targets. However, existing approaches often struggle with spatio-temporal modeling, resulting in suboptimal background suppression and target misclassification. Moreover, stringent recovery tolerances limit computational efficiency in sequential infrared processing. To overcome these challenges, we introduce a novel approach that enhances both background estimation and target detection by leveraging spatio-temporal features. Specifically, we integrate a four-dimensional (4-D) spatio-temporal tensor with a fully connected tensor network (FCTN) completion strategy, utilizing the sum of nuclear norms to maximize intermode correlations while minimizing clutter. For target extraction, we employ a dual Gaussian-core spatial saliency filter (SSF) and a temporal difference filter (TDF), enabling precise discrimination between true targets and static interference. The resulting model—Spatial-Temporal Prior-Assisted Fully Connected Tensor Network (STPA-FCTN)—balances background suppression with target preservation. Furthermore, we systematically analyze recovery convergence in STPA-FCTN and identify an optimal tolerance of 10-3 using the proposed open-source TensorISTD benchmark. Extensive evaluations on public datasets confirm the superior numerical and visual performance of our method over state-of-the-art techniques. Code and benchmark resources are available at https://github.com/fengyiwu98/TensorISTD.&lt;/p&gt;</content:encoded></item><item><title>AMSA-YOLO: Real-time Object Detection with Adaptive Multi-Scale Attention Mechanism</title><link>https://doi.org/10.1016/j.neunet.2026.108545</link><guid>10.1016/j.neunet.2026.108545</guid><pubDate>Tue, 06 Jan 2026 00:19:01 +0000</pubDate><dc:creator>Canjin Wang</dc:creator><dc:creator>Peng Sun</dc:creator><dc:creator>Chunhui Yang</dc:creator><dc:creator>Xianglong Teng</dc:creator><dc:creator>Rijun Wang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108545</prism:doi><description>Object detection, as a fundamental task in computer vision, has extensive applications in autonomous driving, video surveillance, medical imaging, and other domains. The YOLO (You Only Look Once) series of algorithms has become the representative method for single-stage object detection due to their excellent real-time performance. However, existing YOLO algorithms still face challenges in small object detection and dense scene detection. This paper proposes AMSA-YOLO (Adaptive Multi-Scale Attention YOLO), an improved YOLO algorithm based on adaptive multi-scale attention mechanism. By introducing scale-aware modules, adaptive spatial attention, and adaptive channel attention, the proposed method significantly improves detection accuracy, particularly for small object detection. Experimental results demonstrate that AMSA-YOLO achieves a 2.3 percentage point improvement in mAP@0.5:0.95 compared to YOLOv8s on the COCO dataset, with a 3.6 percentage point improvement in small object detection AP, while maintaining inference speed with only a 10.3% decrease. Significant improvements are also achieved on specialized datasets such as VisDrone and CrowdHuman, proving the effectiveness and practicality of the proposed method.
Published: 2026-01-06T00:19:01+00:00
Venue: Neural Networks
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Canjin Wang; Peng Sun; Chunhui Yang; Xianglong Teng; Rijun Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108545"&gt;10.1016/j.neunet.2026.108545&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection, as a fundamental task in computer vision, has extensive applications in autonomous driving, video surveillance, medical imaging, and other domains. The YOLO (You Only Look Once) series of algorithms has become the representative method for single-stage object detection due to their excellent real-time performance. However, existing YOLO algorithms still face challenges in small object detection and dense scene detection. This paper proposes AMSA-YOLO (Adaptive Multi-Scale Attention YOLO), an improved YOLO algorithm based on adaptive multi-scale attention mechanism. By introducing scale-aware modules, adaptive spatial attention, and adaptive channel attention, the proposed method significantly improves detection accuracy, particularly for small object detection. Experimental results demonstrate that AMSA-YOLO achieves a 2.3 percentage point improvement in mAP@0.5:0.95 compared to YOLOv8s on the COCO dataset, with a 3.6 percentage point improvement in small object detection AP, while maintaining inference speed with only a 10.3% decrease. Significant improvements are also achieved on specialized datasets such as VisDrone and CrowdHuman, proving the effectiveness and practicality of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical Context Alignment With Disentangled Geometric and Temporal Modeling for Semantic Occupancy Prediction</title><link>https://doi.org/10.1109/tpami.2025.3650478</link><guid>10.1109/tpami.2025.3650478</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Bohan Li</dc:creator><dc:creator>Jiajun Deng</dc:creator><dc:creator>Yasheng Sun</dc:creator><dc:creator>Xiaofeng Wang</dc:creator><dc:creator>Xin Jin</dc:creator><dc:creator>Wenjun Zeng</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650478</prism:doi><description>Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bohan Li; Jiajun Deng; Yasheng Sun; Xiaofeng Wang; Xin Jin; Wenjun Zeng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650478"&gt;10.1109/tpami.2025.3650478&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Camera-based 3D Semantic Occupancy Prediction (SOP) is crucial for understanding complex 3D scenes from limited 2D image observations. Existing SOP methods typically aggregate contextual features to assist the occupancy representation learning, alleviating issues like occlusion or ambiguity. However, these solutions often face misalignment issues wherein the corresponding features at the same position across different frames may have different semantic meanings during the aggregation process, which leads to unreliable contextual fusion results and an unstable representation learning process. To address this problem, we introduce a new Hierarchical context alignment paradigm for a more accurate SOP (Hi-SOP). Hi-SOP first disentangles the geometric and temporal context for separate alignment, which two branches are then composed to enhance the reliability of SOP. This parsing of the visual input into a local-global alignment hierarchy includes: (I) disentangled geometric and temporal separate alignment, within each leverages depth confidence and camera pose as prior for relevant feature matching respectively; (II) global alignment and composition of the transformed geometric and temporal volumes based on semantics consistency. Our method outperforms SOTAs for semantic scene completion on the SemanticKITTI &amp;amp; NuScenes-Occupancy datasets and LiDAR semantic segmentation on the NuScenes dataset.&lt;/p&gt;</content:encoded></item><item><title>Fully Exploiting Vision Foundation Model's Profound Prior Knowledge for Generalizable RGB-Depth Driving Scene Parsing</title><link>https://doi.org/10.1109/tiv.2025.3650682</link><guid>10.1109/tiv.2025.3650682</guid><pubDate>Mon, 05 Jan 2026 18:40:39 +0000</pubDate><dc:creator>Sicen Guo</dc:creator><dc:creator>Tianyou Wen</dc:creator><dc:creator>Chuang-Wei Liu</dc:creator><dc:creator>Qijun Chen</dc:creator><dc:creator>Rui Fan</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Vehicles</prism:publicationName><prism:doi>10.1109/tiv.2025.3650682</prism:doi><description>Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.
Published: 2026-01-05T18:40:39+00:00
Venue: IEEE Transactions on Intelligent Vehicles
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sicen Guo; Tianyou Wen; Chuang-Wei Liu; Qijun Chen; Rui Fan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Vehicles&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tiv.2025.3650682"&gt;10.1109/tiv.2025.3650682&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Recent vision foundation models (VFMs), typically based on Vision Transformer (ViT), have significantly advanced numerous computer vision tasks. Despite their success in tasks focused solely on RGB images, the potential of VFMs in RGB-depth driving scene parsing remains largely under-explored. In this article, we take one step toward this emerging research area by investigating a feasible technique to fully exploit VFMs for generalizable RGB-depth driving scene parsing. Specifically, we explore the inherent characteristics of RGB and depth data, thereby presenting a Heterogeneous Feature Integration Transformer (HFIT). This network enables the efficient extraction and integration of comprehensive heterogeneous features without re-training ViTs. Relative depth prediction results from VFMs, used as inputs to the HFIT side adapter, overcome the limitations of the dependence on depth maps. Our proposed HFIT demonstrates superior performance compared to all other traditional single-modal and data-fusion scene parsing networks, pre-trained VFMs, and ViT adapters on the Cityscapes and KITTI Semantics datasets. We believe this novel strategy paves the way for future innovations in VFM-based data-fusion techniques for driving scene parsing. Our source code is publicly available at https://mias.group/HFIT.&lt;/p&gt;</content:encoded></item><item><title>Physical Attributes Embedded Prototypical Network for Incremental SAR Automatic Target Recognition</title><link>https://doi.org/10.1109/jstars.2025.3650513</link><guid>10.1109/jstars.2025.3650513</guid><pubDate>Mon, 05 Jan 2026 18:39:04 +0000</pubDate><dc:creator>Yanjie Xu</dc:creator><dc:creator>Hao Sun</dc:creator><dc:creator>Chenfang Liu</dc:creator><dc:creator>Kefeng Ji</dc:creator><dc:creator>Gangyao Kuang</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2025.3650513</prism:doi><description>In Synthetic Aperture Radar (SAR) applications, the continuous emergence of new target classes poses a significant challenge to Automatic Target Recognition (ATR) systems. Adapting to the distribution of new data can induce drastic alterations in the feature space of deep models, resulting in a decline in their ability to recognize old data, termed catastrophic forgetting. To address this challenge, we propose a novel class-incremental SAR ATR method based on Physical Attributes Embedded Prototypical Network (PAEPN). PAEPN embeds physical attributes derived from electromagnetic scattering and geometric priors into the deep model to achieve stable representations. These physical attributes, determined by the target's shape, structure, and material composition, remain invariant throughout the incremental learning process, thereby enhancing the stability and interpretability of deep models. Specifically, PAEPN first extracts and integrates physical attribute priors to establish feature anchors, guiding the deep model in extracting physically consistent features and preventing drastic changes in the feature space. Second, a spatial attention enhancement strategy is introduced to enable the deep model to reliably focus on the key regions of SAR targets. Finally, feature relations that represent semantic similarity are distilled to further mitigate catastrophic forgetting. During testing, PAEPN employs the cosine distance between the sample feature and class prototypes for recognition. Comprehensive experiments on three datasets demonstrate that PAEPN outperforms existing state-of-the-art methods.
Published: 2026-01-05T18:39:04+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanjie Xu; Hao Sun; Chenfang Liu; Kefeng Ji; Gangyao Kuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2025.3650513"&gt;10.1109/jstars.2025.3650513&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;In Synthetic Aperture Radar (SAR) applications, the continuous emergence of new target classes poses a significant challenge to Automatic Target Recognition (ATR) systems. Adapting to the distribution of new data can induce drastic alterations in the feature space of deep models, resulting in a decline in their ability to recognize old data, termed catastrophic forgetting. To address this challenge, we propose a novel class-incremental SAR ATR method based on Physical Attributes Embedded Prototypical Network (PAEPN). PAEPN embeds physical attributes derived from electromagnetic scattering and geometric priors into the deep model to achieve stable representations. These physical attributes, determined by the target&amp;#x27;s shape, structure, and material composition, remain invariant throughout the incremental learning process, thereby enhancing the stability and interpretability of deep models. Specifically, PAEPN first extracts and integrates physical attribute priors to establish feature anchors, guiding the deep model in extracting physically consistent features and preventing drastic changes in the feature space. Second, a spatial attention enhancement strategy is introduced to enable the deep model to reliably focus on the key regions of SAR targets. Finally, feature relations that represent semantic similarity are distilled to further mitigate catastrophic forgetting. During testing, PAEPN employs the cosine distance between the sample feature and class prototypes for recognition. Comprehensive experiments on three datasets demonstrate that PAEPN outperforms existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Mamba-driven Diffusion Model for Salient Object Detection in Optical Remote Sensing Images</title><link>https://doi.org/10.1109/tcsvt.2026.3651594</link><guid>10.1109/tcsvt.2026.3651594</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Jinsheng Yang</dc:creator><dc:creator>Bineng Zhong</dc:creator><dc:creator>Qihua Liang</dc:creator><dc:creator>Yufei Tan</dc:creator><dc:creator>Haiying Xia</dc:creator><dc:creator>Shuxiang Song</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651594</prism:doi><description>Existing Optical Remote Sensing Image Salient Object Detection (ORSI-SOD) methods mainly rely on a semantic segmentation paradigm, which relies on pixel-wise probabilities, leading to overconfident mispredictions. In contrast, the random sampling process of the diffusion model allows multiple possible predictions to be drawn from the mask distribution, effectively alleviating this problem. However, existing diffusion models mainly use Transformers as conditional feature extraction networks. Although they are good at global modeling, they have limited ability to handle long-range dependencies due to computational complexity. To overcome these challenges, we introduce MambaDif, an innovative diffusion model architecture based on Mamba. Specifically, we regard ORSI-SOD as a conditional mask generation task leveraging the diffusion model and achieving target distribution matching by adding noise to the mask and iteratively denoising it to match the target distribution. Then, we adopt Mamba to extract global features, efficiently process long sequences, and capture global contextual information with linear complexity. In addition, we introduce the global-local feature collaborative completion module (GLM), which combines the ability of convolutional layers to extract local features with the advantage of Mamba in capturing long-range dependencies, thereby achieving excellent denoising performance. Extensive experiments show that MambaDif outperforms SOTA methods in eight evaluation metrics on two standard datasets (EORSSD and ORSSD). We also report the generalization performance of the model on the challenging ORSI-4199 to evaluate its robustness.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinsheng Yang; Bineng Zhong; Qihua Liang; Yufei Tan; Haiying Xia; Shuxiang Song&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651594"&gt;10.1109/tcsvt.2026.3651594&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Existing Optical Remote Sensing Image Salient Object Detection (ORSI-SOD) methods mainly rely on a semantic segmentation paradigm, which relies on pixel-wise probabilities, leading to overconfident mispredictions. In contrast, the random sampling process of the diffusion model allows multiple possible predictions to be drawn from the mask distribution, effectively alleviating this problem. However, existing diffusion models mainly use Transformers as conditional feature extraction networks. Although they are good at global modeling, they have limited ability to handle long-range dependencies due to computational complexity. To overcome these challenges, we introduce MambaDif, an innovative diffusion model architecture based on Mamba. Specifically, we regard ORSI-SOD as a conditional mask generation task leveraging the diffusion model and achieving target distribution matching by adding noise to the mask and iteratively denoising it to match the target distribution. Then, we adopt Mamba to extract global features, efficiently process long sequences, and capture global contextual information with linear complexity. In addition, we introduce the global-local feature collaborative completion module (GLM), which combines the ability of convolutional layers to extract local features with the advantage of Mamba in capturing long-range dependencies, thereby achieving excellent denoising performance. Extensive experiments show that MambaDif outperforms SOTA methods in eight evaluation metrics on two standard datasets (EORSSD and ORSSD). We also report the generalization performance of the model on the challenging ORSI-4199 to evaluate its robustness.&lt;/p&gt;</content:encoded></item><item><title>MDADet: A Multimodal Dynamic Adaptation Framework for Efficient Small Object Detection in Aerial Images</title><link>https://doi.org/10.1109/tgrs.2026.3650963</link><guid>10.1109/tgrs.2026.3650963</guid><pubDate>Mon, 05 Jan 2026 18:38:39 +0000</pubDate><dc:creator>Jian Zhang</dc:creator><dc:creator>Jiarong Lv</dc:creator><dc:creator>Heng Zhang</dc:creator><dc:creator>Ming Li</dc:creator><dc:creator>Meng Huang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3650963</prism:doi><description>In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.
Published: 2026-01-05T18:38:39+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jian Zhang; Jiarong Lv; Heng Zhang; Ming Li; Meng Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3650963"&gt;10.1109/tgrs.2026.3650963&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we propose a multimodal dynamic adaptive detection framework tailored for small object detection named MDADet. Concretely, we utilize a Dynamic IoU-Centric Slicing-based Data Augmentation (DICSA) strategy to prioritize high-IoU regions during training. The strategy effectively eliminates redundant background information and significantly accelerates model convergence. Additionally, the Robustly Optimized BERT Pretraining Approach (RoBERTa) encodes bounding box annotations into semantic embedding, which are fused with image features via a transformer to generate multimodal representations for small object recognition. The knowledge distillation is utilized to transfer capabilities from the multimodal teacher model to a lightweight multimodal student model, reducing parameter scale and improving inference speed. During fine-tuning of the single-modal student model, the transformer encoder is frozen, and a lightweight feature pyramid integrated with Pixel-Shuffle and hierarchical detection heads is incorporated, ensuring robust performance even without textual input. Experimental results compared with other methods demonstrate the effectiveness and advancement of MDADet, achieving 81.07% mAP on DOTA 1.0, 86.76% on VEDAI, 73.55% on DIOR and 97.61% classification accuracy on NWPU VHR-10, with a model size of only 37.8M parameters.&lt;/p&gt;</content:encoded></item><item><title>Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization</title><link>https://doi.org/10.1109/tmm.2026.3651051</link><guid>10.1109/tmm.2026.3651051</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Ziyi Wang</dc:creator><dc:creator>Zhi Gao</dc:creator><dc:creator>Jin Chen</dc:creator><dc:creator>Qingjie Zhao</dc:creator><dc:creator>Xinxiao Wu</dc:creator><dc:creator>Jiebo Luo</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651051</prism:doi><description>Domain generalization (DG) aims to learn a model from source domains and apply it to unseen target domains with out-of-distribution data. Owing to CLIP's strong ability to encode semantic concepts, it has attracted increasing interest in domain generalization. However, CLIP often struggles to focus on task-relevant regions across domains, i.e., domain-invariant regions, resulting in suboptimal performance on unseen target domains. To address this challenge, we propose an attention-refocusing scheme, called Simulate, Refocus and Ensemble (SRE), which learns to reduce the domain shift by aligning the attention maps in CLIP via attention refocusing. SRE first simulates domain shifts by performing augmentation on the source data to generate simulated target domains. SRE then learns to reduce the domain shifts by refocusing the attention in CLIP between the source and simulated target domains. Finally, SRE utilizes ensemble learning to enhance the ability to capture domain-invariant attention maps between the source data and the simulated target data. Extensive experimental results on several datasets demonstrate that SRE generally achieves better results than state-of-the-art methods.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyi Wang; Zhi Gao; Jin Chen; Qingjie Zhao; Xinxiao Wu; Jiebo Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651051"&gt;10.1109/tmm.2026.3651051&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Domain generalization (DG) aims to learn a model from source domains and apply it to unseen target domains with out-of-distribution data. Owing to CLIP&amp;#x27;s strong ability to encode semantic concepts, it has attracted increasing interest in domain generalization. However, CLIP often struggles to focus on task-relevant regions across domains, i.e., domain-invariant regions, resulting in suboptimal performance on unseen target domains. To address this challenge, we propose an attention-refocusing scheme, called Simulate, Refocus and Ensemble (SRE), which learns to reduce the domain shift by aligning the attention maps in CLIP via attention refocusing. SRE first simulates domain shifts by performing augmentation on the source data to generate simulated target domains. SRE then learns to reduce the domain shifts by refocusing the attention in CLIP between the source and simulated target domains. Finally, SRE utilizes ensemble learning to enhance the ability to capture domain-invariant attention maps between the source data and the simulated target data. Extensive experimental results on several datasets demonstrate that SRE generally achieves better results than state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Learning Diffusion Priors for Inverse Rendering Under Unknown Illumination</title><link>https://doi.org/10.1109/tpami.2026.3650770</link><guid>10.1109/tpami.2026.3650770</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Sida Peng</dc:creator><dc:creator>Jiarui Guo</dc:creator><dc:creator>Xi Chen</dc:creator><dc:creator>Yuan Liu</dc:creator><dc:creator>Dongchen Yang</dc:creator><dc:creator>Hujun Bao</dc:creator><dc:creator>Xiaowei Zhou</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3650770</prism:doi><description>This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code is available at https://zju3dv.github.io/IntrinsicAnything/.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sida Peng; Jiarui Guo; Xi Chen; Yuan Liu; Dongchen Yang; Hujun Bao; Xiaowei Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3650770"&gt;10.1109/tpami.2026.3650770&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;This paper aims to recover object materials from posed images captured under an unknown static lighting condition. Recent methods solve this task by optimizing material parameters through differentiable physically based rendering. However, due to the coupling between object geometry, materials, and environment lighting, there is inherent ambiguity during the inverse rendering process, preventing previous methods from obtaining accurate results. To overcome this ill-posed problem, our key idea is to learn the material prior with a generative model for regularizing the optimization process. We observe that the general rendering equation can be split into diffuse and specular shading terms, and thus formulate the material prior as diffusion models of albedo and specular. Thanks to this design, our model can be trained using the existing abundant 3D object data, and naturally acts as a versatile tool to resolve the ambiguity when recovering material representations from RGB images. In addition, we develop a coarse-to-fine training strategy that leverages estimated materials to guide diffusion models to satisfy multi-view consistent constraints, leading to more stable and accurate results. Extensive experiments on real-world and synthetic datasets demonstrate that our approach achieves state-of-the-art performance on material recovery. The code is available at https://zju3dv.github.io/IntrinsicAnything/.&lt;/p&gt;</content:encoded></item><item><title>Offset-corrected query generation strategies for cross-modality misalignment in 3D object detection: aligning LiDAR and camera</title><link>https://doi.org/10.1016/j.neucom.2025.132582</link><guid>10.1016/j.neucom.2025.132582</guid><pubDate>Tue, 06 Jan 2026 00:19:26 +0000</pubDate><dc:creator>Jiayao Li</dc:creator><dc:creator>Chak Fong Cheang</dc:creator><dc:creator>Xiaoyuan Yu</dc:creator><dc:creator>Suigu Tang</dc:creator><dc:creator>Zhaolong Du</dc:creator><dc:creator>Qianxiang Cheng</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132582</prism:doi><description>Although cross-modality data fusion can effectively mitigate the limitations of monomodal approaches in 3D object detection through multi-source information complementarity, the issue of data misalignment caused by inherent discrepancies between modalities remains a critical challenge that hinders detection performance improvement. To address the perceptual degradation caused by inter-modal conflicts during fusion, we propose a multimodal fusion network for 3D object detection in autonomous driving using offset correction and query generation strategies (DADNet). The architecture features two innovative components: (1) an Offset Correction Module (OCM) that establishes learnable offset fields for pre-fusion spatial feature alignment, and (2) a Query Generation Module (QGM) designed to recover dissolved high-value objects from fusion heatmaps through monomodal feature mining. Specifically, the OCM aligns LiDAR and camera Bird’s-Eye-View (BEV) features into a unified distribution space via adaptive coordinate transformation, while the QGM reconstructs critical detection queries using attention-based feature reactivation from individual sensor modalities. Through rigorous benchmarking against 14 state-of-the-art detectors on the KITTI dataset, DADNet demonstrates superior performance across all evaluation scenarios. The core code will be released at https://github.com/ljyw17/3DDet .
Published: 2026-01-06T00:19:26+00:00
Venue: Neurocomputing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayao Li; Chak Fong Cheang; Xiaoyuan Yu; Suigu Tang; Zhaolong Du; Qianxiang Cheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132582"&gt;10.1016/j.neucom.2025.132582&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Although cross-modality data fusion can effectively mitigate the limitations of monomodal approaches in 3D object detection through multi-source information complementarity, the issue of data misalignment caused by inherent discrepancies between modalities remains a critical challenge that hinders detection performance improvement. To address the perceptual degradation caused by inter-modal conflicts during fusion, we propose a multimodal fusion network for 3D object detection in autonomous driving using offset correction and query generation strategies (DADNet). The architecture features two innovative components: (1) an Offset Correction Module (OCM) that establishes learnable offset fields for pre-fusion spatial feature alignment, and (2) a Query Generation Module (QGM) designed to recover dissolved high-value objects from fusion heatmaps through monomodal feature mining. Specifically, the OCM aligns LiDAR and camera Bird’s-Eye-View (BEV) features into a unified distribution space via adaptive coordinate transformation, while the QGM reconstructs critical detection queries using attention-based feature reactivation from individual sensor modalities. Through rigorous benchmarking against 14 state-of-the-art detectors on the KITTI dataset, DADNet demonstrates superior performance across all evaluation scenarios. The core code will be released at https://github.com/ljyw17/3DDet .&lt;/p&gt;</content:encoded></item><item><title>A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.023</link><guid>10.1016/j.isprsjprs.2025.12.023</guid><pubDate>Mon, 05 Jan 2026 14:32:55 +0000</pubDate><dc:creator>Hyunho Lee</dc:creator><dc:creator>Wenwen Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.023</prism:doi><description>Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. This is because SAR sensors can observe through cloud cover and operate both day and night, whereas Multispectral Imaging (MSI) data, despite providing higher mapping accuracy, are only available under cloud-free and daytime conditions. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Specifically, SMAGNet achieved the highest IoU score of 86.47% using SAR and MSI data and maintained the highest performance with an IoU score of 79.53% even when MSI data were entirely missing. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios. The source code is available at https://github.com/ASUcicilab/SMAGNet .
Published: 2026-01-05T14:32:55+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyunho Lee; Wenwen Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.023"&gt;10.1016/j.isprsjprs.2025.12.023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. This is because SAR sensors can observe through cloud cover and operate both day and night, whereas Multispectral Imaging (MSI) data, despite providing higher mapping accuracy, are only available under cloud-free and daytime conditions. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Specifically, SMAGNet achieved the highest IoU score of 86.47% using SAR and MSI data and maintained the highest performance with an IoU score of 79.53% even when MSI data were entirely missing. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios. The source code is available at https://github.com/ASUcicilab/SMAGNet .&lt;/p&gt;</content:encoded></item><item><title>A radiometrically and spatially consistent super-resolution framework for Sentinel-2</title><link>https://doi.org/10.1016/j.rse.2025.115222</link><guid>10.1016/j.rse.2025.115222</guid><pubDate>Tue, 06 Jan 2026 09:45:48 +0000</pubDate><dc:creator>Cesar Aybar</dc:creator><dc:creator>Julio Contreras</dc:creator><dc:creator>Simon Donike</dc:creator><dc:creator>Enrique Portalés-Julià</dc:creator><dc:creator>Gonzalo Mateo-García</dc:creator><dc:creator>Luis Gómez-Chova</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115222</prism:doi><description>Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .
Published: 2026-01-06T09:45:48+00:00
Venue: Remote Sensing of Environment
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cesar Aybar; Julio Contreras; Simon Donike; Enrique Portalés-Julià; Gonzalo Mateo-García; Luis Gómez-Chova&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115222"&gt;10.1016/j.rse.2025.115222&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning-based super-resolution (SR) models offer a promising approach to enhancing the effective spatial resolution of optical satellite images. However, existing SR implementations have shown that, while these models can reconstruct fine-scale details, they often introduce undesirable artifacts, such as nonexistent local structures, reflectance distortions, and geometric misalignment. To mitigate these issues, fully synthetic data approaches have been explored for training, as they provide complete control over the degradation process and allow precise supervision and ground-truth availability. However, challenges in domain transfer have limited their effectiveness when applied to real satellite images. In this work, we propose SEN2SR , a new deep learning framework trained to super-resolve Sentinel-2 images while preserving spectral and spatial alignment consistency. Our approach harmonizes synthetic training data to match the spectral and spatial characteristics of Sentinel-2, ensuring realistic and artifact-free enhancements. SEN2SR generates 2.5-meter resolution images for Sentinel-2, upsampling the 10-meter RGB and NIR bands and the 20-meter Red Edge and SWIR bands. To ensure that SR models focus exclusively on enhancing spatial resolution, we introduce a low-frequency hard constraint layer at the final stage of SR networks that always enforces spectral consistency by preserving the original low-frequency content. We evaluate a range of deep learning architectures, including Convolutional Neural Networks, Mamba, and Swin Transformers, within a comprehensive assessment framework that integrates Explainable AI (xAI) techniques. Quantitatively, our framework achieves superior PSNR while maintaining near-zero reflectance deviation and spatial misalignment, outperforming state-of-the-art SR frameworks. Moreover, we demonstrate maintained radiometric fidelity in downstream tasks that demand high-fidelity spectral information and reveal a significant correlation between model performance and pixel-level model activation. Qualitative results show that SR networks effectively handle diverse land cover scenarios without introducing spurious high-frequency details in out-of-distribution cases. Overall, this research underscores the potential of SR techniques in Earth observation, paving the way for more precise monitoring of the Earth’s surface. Models, code, and examples are publicly available at https://github.com/ESAOpenSR/SEN2SR .&lt;/p&gt;</content:encoded></item><item><title>Lifelong Learning of Large Language Model based Agents: A Roadmap</title><link>https://doi.org/10.1109/tpami.2025.3650546</link><guid>10.1109/tpami.2025.3650546</guid><pubDate>Mon, 05 Jan 2026 18:38:31 +0000</pubDate><dc:creator>Junhao Zheng</dc:creator><dc:creator>Chengming Shi</dc:creator><dc:creator>Xidi Cai</dc:creator><dc:creator>Qiuke Li</dc:creator><dc:creator>Duzhen Zhang</dc:creator><dc:creator>Chenxing Li</dc:creator><dc:creator>Dong Yu</dc:creator><dc:creator>Qianli Ma</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3650546</prism:doi><description>Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at https://github.com/qianlimalab/ awesome-lifelong-llm-agent.
Published: 2026-01-05T18:38:31+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junhao Zheng; Chengming Shi; Xidi Cai; Qiuke Li; Duzhen Zhang; Chenxing Li; Dong Yu; Qianli Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3650546"&gt;10.1109/tpami.2025.3650546&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at https://github.com/qianlimalab/ awesome-lifelong-llm-agent.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Multi-view Omnidirectional Depth Estimation with Semantic-Aware Cost Aggregation and Spatial Propagation</title><link>https://doi.org/10.1109/tcsvt.2026.3651056</link><guid>10.1109/tcsvt.2026.3651056</guid><pubDate>Mon, 05 Jan 2026 18:42:15 +0000</pubDate><dc:creator>Ming Li</dc:creator><dc:creator>Xuejiao Hu</dc:creator><dc:creator>Zihang Gao</dc:creator><dc:creator>Sidan Du</dc:creator><dc:creator>Yang Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651056</prism:doi><description>Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.
Published: 2026-01-05T18:42:15+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Li; Xuejiao Hu; Zihang Gao; Sidan Du; Yang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651056"&gt;10.1109/tcsvt.2026.3651056&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Omnidirectional depth estimation predicts 360-degree depth information using multiple fisheye cameras arranged in a surround-view configuration. However, due to the lack of reference panorama and differences between the predicted depth viewpoint and input cameras, it is challenging to construct and utilize semantic information to improve depth accuracy, resulting in limited accurate in complex regions such as non-overlapping, weak textures, object boundaries and occlusions. This paper proposes a novel model architecture that effectively extracts and leverages semantic information to enhance the accuracy of omnidirectional depth estimation. Specifically, the proposed algorithm combines the variance and mean of multi-view image features to construct the fused matching cost and utilize both geometry and semantic constraints. The model extracts 360-degree semantic context during matching cost aggregation, and predict the corresponding panoramas jointly with omnidirectional depth maps. A semantic-aware spatial propagation module is then employed to further refine the depth estimation. We leverage a multi-scale multi-task learning strategy to supervise the prediction of omnidirectional depth maps and panoramas jointly. The proposed approach achieves state-of-the-art performance on public datasets, and also demonstrates high-precision results on real-world data. The experiments with varying camera configurations validate the generalization ability and flexibility of the algorithm.&lt;/p&gt;</content:encoded></item><item><title>D$^3$R-DETR: DETR with Dual-Domain Density Refinement for Tiny Object Detection in Aerial Images</title><link>https://arxiv.org/abs/2601.02747v1</link><guid>http://arxiv.org/abs/2601.02747v1</guid><pubDate>Tue, 06 Jan 2026 06:21:50 +0000</pubDate><dc:creator>Zixiao Wen</dc:creator><dc:creator>Zhen Yang</dc:creator><dc:creator>Xianjie Bao</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Xiantai Xiang</dc:creator><dc:creator>Wenshuai Li</dc:creator><dc:creator>Yuhan Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.
Published: 2026-01-06T06:21:50+00:00
Venue: arXiv
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zixiao Wen; Zhen Yang; Xianjie Bao; Lei Zhang; Xiantai Xiang; Wenshuai Li; Yuhan Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Detecting tiny objects plays a vital role in remote sensing intelligent interpretation, as these objects often carry critical information for downstream applications. However, due to the extremely limited pixel information and significant variations in object density, mainstream Transformer-based detectors often suffer from slow convergence and inaccurate query-object matching. To address these challenges, we propose D$^3$R-DETR, a novel DETR-based detector with Dual-Domain Density Refinement. By fusing spatial and frequency domain information, our method refines low-level feature maps and utilizes their rich details to predict more accurate object density map, thereby guiding the model to precisely localize tiny objects. Extensive experiments on the AI-TOD-v2 dataset demonstrate that D$^3$R-DETR outperforms existing state-of-the-art detectors for tiny object detection.&lt;/p&gt;</content:encoded></item><item><title>LSFMamba: Local-enhanced Spiral Fusion Mamba for Multi-modal Land Cover Classification</title><link>https://doi.org/10.1109/tcsvt.2026.3651397</link><guid>10.1109/tcsvt.2026.3651397</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Honghao Chang</dc:creator><dc:creator>Haixia Bi</dc:creator><dc:creator>Chen Xu</dc:creator><dc:creator>Fan Li</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651397</prism:doi><description>Multi-modal learning, which fuses complementary information from different modalities, has significantly improved the accuracy of land cover classification, especially under adverse conditions like cloudy or rainy weather. Recent advancements in multi-modal remote sensing land cover classification (MMRLC) have witnessed the efficacy of approaches based on CNN and Transformer. However, CNN exhibits limitations in capturing long-range dependencies, whereas Transformer suffers from high computational complexity. Recently, Mamba has garnered widespread attention due to its superior long-range modeling capabilities with linear complexity. Nevertheless, Mamba demonstrates notable limitations when directly applied to MMRLC, including limited local contextual modeling capacity, suboptimal multi-modal feature fusion and lack of a task-specific spatial continuity scanning strategy. Hence, to fully explore the potential of Mamba in multi-modal land cover classification, we propose LSFMamba, which comprises multiple hierarchically connected local-enhanced fusion Mamba (LFM) modules. Within each LFM module, a local-enhanced visual state space (LVSS) block is designed to extract features from different modalities, while a cross-modal interaction state space (CISS) block is created to fuse these multi-modal features. In the LVSS block, we integrate a multi-kernel CNN block into the gating branch in Mamba to enhance its local modeling capabilities. In the CISS block, features from different modalities are interleaved, facilitating cross-modal feature interaction through the state space model. Furthermore, we introduce a novel spiral scanning strategy to reassess the significance of central pixels, a design driven by the unique characteristics of pixel-wise classification task. Extensive experimental results on three multi-modal remote sensing datasets demonstrate that the proposed LSFMamba achieves state-of-the-art performance with lower complexity. The code will be released at htt...
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Honghao Chang; Haixia Bi; Chen Xu; Fan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651397"&gt;10.1109/tcsvt.2026.3651397&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-modal learning, which fuses complementary information from different modalities, has significantly improved the accuracy of land cover classification, especially under adverse conditions like cloudy or rainy weather. Recent advancements in multi-modal remote sensing land cover classification (MMRLC) have witnessed the efficacy of approaches based on CNN and Transformer. However, CNN exhibits limitations in capturing long-range dependencies, whereas Transformer suffers from high computational complexity. Recently, Mamba has garnered widespread attention due to its superior long-range modeling capabilities with linear complexity. Nevertheless, Mamba demonstrates notable limitations when directly applied to MMRLC, including limited local contextual modeling capacity, suboptimal multi-modal feature fusion and lack of a task-specific spatial continuity scanning strategy. Hence, to fully explore the potential of Mamba in multi-modal land cover classification, we propose LSFMamba, which comprises multiple hierarchically connected local-enhanced fusion Mamba (LFM) modules. Within each LFM module, a local-enhanced visual state space (LVSS) block is designed to extract features from different modalities, while a cross-modal interaction state space (CISS) block is created to fuse these multi-modal features. In the LVSS block, we integrate a multi-kernel CNN block into the gating branch in Mamba to enhance its local modeling capabilities. In the CISS block, features from different modalities are interleaved, facilitating cross-modal feature interaction through the state space model. Furthermore, we introduce a novel spiral scanning strategy to reassess the significance of central pixels, a design driven by the unique characteristics of pixel-wise classification task. Extensive experimental results on three multi-modal remote sensing datasets demonstrate that the proposed LSFMamba achieves state-of-the-art performance with lower complexity. The code will be released at htt...&lt;/p&gt;</content:encoded></item><item><title>SRD2-VPR: Semantics-Enforced Feature Aggregation with Query Rejection for Visual Place Recognition</title><link>https://doi.org/10.1109/tcsvt.2026.3651681</link><guid>10.1109/tcsvt.2026.3651681</guid><pubDate>Tue, 06 Jan 2026 18:37:39 +0000</pubDate><dc:creator>Zhi Hu</dc:creator><dc:creator>Liang Liao</dc:creator><dc:creator>Weisi Lin</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3651681</prism:doi><description>Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.
Published: 2026-01-06T18:37:39+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhi Hu; Liang Liao; Weisi Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3651681"&gt;10.1109/tcsvt.2026.3651681&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Visual place recognition is a fundamental task essential for applications like visual localization and loop closure detection. Existing methods perform well under controlled environments, but often fail in scenarios with significant domain shifts, such as drastic day-to-night transitions and severe occlusions. This limitation arises because existing approaches are globally optimized without explicit supervision for out-of-distribution (OOD) adaptation and overlook semantics as a complementary modality for improving OOD robustness via local context refinement. To address this, we propose a dual-branch network that jointly optimizes feature attention and feature description under semantic guidance, achieving improved OOD adaptation with overhead comparable to existing methods. The feature attention branch is guided by semantically-informed context richness, while the feature description branch is supervised through inter-class repelling and intra-class re-ranking. Additionally, we introduce a simple yet effective query rejection module that leverages the learned attention to assess an image’s informativeness, allowing it to exclude queries that lack place-representative context. Extensive experiments demonstrate that our method raises the average Recall@1 and Recall@5 by 3.5 and 3.9 percentage points over its state-of-the-art counterpart, and accelerates feature matching by 28% for downstream visual localization without performance degradation.&lt;/p&gt;</content:encoded></item><item><title>Few-shot object detection via dynamic feature enhancement and attention template matching</title><link>https://doi.org/10.1007/s10489-025-06997-y</link><guid>10.1007/s10489-025-06997-y</guid><pubDate>Mon, 05 Jan 2026 15:40:29 +0000</pubDate><dc:creator>Ruqi Su</dc:creator><dc:creator>Kai Zhang</dc:creator><dc:creator>Songhao Zhu</dc:creator><prism:publicationName>Applied Intelligence</prism:publicationName><prism:doi>10.1007/s10489-025-06997-y</prism:doi><description>With the rapid advancement of deep learning and computer vision, few-shot object detection (FSOD) has emerged as a critical research frontier. A key challenge in FSOD lies in extracting discriminative feature representations from limited samples, which severely degrades detection performance. To mitigate this issue, we propose a novel FSOD framework that integrates cross-domain adaptive feature enhancement and attention-guided proposal generation, effectively leveraging support set information to improve query set detection accuracy. Our method introduces three key innovations. (1) Dynamic Kernel Generation. A learnable kernel generator produces sample-specific convolutional kernels to adaptively enhance query features using support set cues. (2) Attention-Driven Region Proposals. An attention-based region proposal network (ARPN) suppresses irrelevant regions while prioritizing semantically relevant areas. (3) Template-Aware Scoring. A matching module evaluates candidate boxes against support templates to ensure geometric and semantic consistency. Extensive experiments on PASCAL VOC and MS COCO benchmarks demonstrate our method outperforming existing approaches by 3.2 AP50 on 10-shot tasks. The results validate the efficacy of cross-domain adaptation and attention mechanisms in addressing data scarcity challenges.
Published: 2026-01-05T15:40:29+00:00
Venue: Applied Intelligence
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruqi Su; Kai Zhang; Songhao Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Applied Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s10489-025-06997-y"&gt;10.1007/s10489-025-06997-y&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;With the rapid advancement of deep learning and computer vision, few-shot object detection (FSOD) has emerged as a critical research frontier. A key challenge in FSOD lies in extracting discriminative feature representations from limited samples, which severely degrades detection performance. To mitigate this issue, we propose a novel FSOD framework that integrates cross-domain adaptive feature enhancement and attention-guided proposal generation, effectively leveraging support set information to improve query set detection accuracy. Our method introduces three key innovations. (1) Dynamic Kernel Generation. A learnable kernel generator produces sample-specific convolutional kernels to adaptively enhance query features using support set cues. (2) Attention-Driven Region Proposals. An attention-based region proposal network (ARPN) suppresses irrelevant regions while prioritizing semantically relevant areas. (3) Template-Aware Scoring. A matching module evaluates candidate boxes against support templates to ensure geometric and semantic consistency. Extensive experiments on PASCAL VOC and MS COCO benchmarks demonstrate our method outperforming existing approaches by 3.2 AP50 on 10-shot tasks. The results validate the efficacy of cross-domain adaptation and attention mechanisms in addressing data scarcity challenges.&lt;/p&gt;</content:encoded></item><item><title>Multi-Scale Spatial Channel Joint Representation for General Multi-Modality Image Fusion With Self-Supervision</title><link>https://doi.org/10.1109/tmm.2026.3651073</link><guid>10.1109/tmm.2026.3651073</guid><pubDate>Tue, 06 Jan 2026 18:36:32 +0000</pubDate><dc:creator>Jiawei Li</dc:creator><dc:creator>Jiansheng Chen</dc:creator><dc:creator>Jinyuan Liu</dc:creator><dc:creator>Hongwei Yu</dc:creator><dc:creator>Xinlong Ding</dc:creator><dc:creator>Huimin Ma</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3651073</prism:doi><description>The rapid advancement of multi-modality image fusion technology enables researchers to simultaneously acquire information from different modalities within a single fused image. In existing methods, some general approaches can implement both infrared and visible image fusion (IVIF) and medical image fusion (MIF) in the same framework. Nevertheless, these methods often ignore the learning of specific features in different modalities, resulting in unsatisfactory performance in fused results. To overcome this issue, we propose a multi-scale joint framework with self-supervision for general multi-modality image fusion, abbreviated as SCSFusion. It enables more targeted and robust implementation of IVIF and MIF. Specifically, in the fusion network, a joint attention module is employed to parallelly capture self-attention features in spatial and channel domains, which can keep fused results accurate in visual representation. Meanwhile, we utilize source images of different modalities to generate visual-focused maps as pseudo labels for self-supervised training of the fusion results. It effectively preserves the salient details in each fused image from being disrupted by other extracted information. Moreover, a medical dataset with segmentation labels, termed M2DF, is reorganized for fusion and down-stream tasks in MIF. With the help of M2DF, a pre-trained segmentation model can be cascaded with the fusion network, aiming to obtain high-level semantic features from inputs and enhance the data generalization in our general framework. We have conducted extensive experiments and analyses on SCSFusion in M m ^{3} m ^{3} FD, FMB, and M2DF datasets, respectively. The results indicate that the fused images generated by SCSFusion can not only achieve visually appealing results and superior performance metrics in MIF and IVIF, but also exhibit satisfactory performance in down-stream tasks.
Published: 2026-01-06T18:36:32+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiawei Li; Jiansheng Chen; Jinyuan Liu; Hongwei Yu; Xinlong Ding; Huimin Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3651073"&gt;10.1109/tmm.2026.3651073&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid advancement of multi-modality image fusion technology enables researchers to simultaneously acquire information from different modalities within a single fused image. In existing methods, some general approaches can implement both infrared and visible image fusion (IVIF) and medical image fusion (MIF) in the same framework. Nevertheless, these methods often ignore the learning of specific features in different modalities, resulting in unsatisfactory performance in fused results. To overcome this issue, we propose a multi-scale joint framework with self-supervision for general multi-modality image fusion, abbreviated as SCSFusion. It enables more targeted and robust implementation of IVIF and MIF. Specifically, in the fusion network, a joint attention module is employed to parallelly capture self-attention features in spatial and channel domains, which can keep fused results accurate in visual representation. Meanwhile, we utilize source images of different modalities to generate visual-focused maps as pseudo labels for self-supervised training of the fusion results. It effectively preserves the salient details in each fused image from being disrupted by other extracted information. Moreover, a medical dataset with segmentation labels, termed M2DF, is reorganized for fusion and down-stream tasks in MIF. With the help of M2DF, a pre-trained segmentation model can be cascaded with the fusion network, aiming to obtain high-level semantic features from inputs and enhance the data generalization in our general framework. We have conducted extensive experiments and analyses on SCSFusion in M m ^{3} m ^{3} FD, FMB, and M2DF datasets, respectively. The results indicate that the fused images generated by SCSFusion can not only achieve visually appealing results and superior performance metrics in MIF and IVIF, but also exhibit satisfactory performance in down-stream tasks.&lt;/p&gt;</content:encoded></item></channel></rss>