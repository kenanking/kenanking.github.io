<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 11 Feb 2026 04:07:48 +0000</lastBuildDate><item><title>EAV-DETR: Efficient Arbitrary-View oriented object detection with probabilistic guarantees for UAV imagery</title><link>https://doi.org/10.1016/j.isprsjprs.2026.02.009</link><guid>10.1016/j.isprsjprs.2026.02.009</guid><pubDate>Tue, 10 Feb 2026 05:19:37 +0000</pubDate><dc:creator>Haoyu Zuo</dc:creator><dc:creator>Minghao Ning</dc:creator><dc:creator>Yiming Shu</dc:creator><dc:creator>Shucheng Huang</dc:creator><dc:creator>Chen Sun</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.02.009</prism:doi><description>Oriented object detection is critical for enhancing the visual perception of unmanned aerial vehicles (UAVs). However, existing detectors primarily designed for general aerial imagery often struggle to address the unique challenges of UAV imagery, including substantial scale variations, dense clustering, and arbitrary orientations. Furthermore, these models lack probabilistic guarantees required for safety-critical applications. To address these challenges, we propose EAV-DETR, an efficient oriented object detection transformer designed for UAV imagery. Specifically, we first propose a novel scale-adaptive center supervision (SACS) strategy that explicitly enhances the encoder’s feature representations by imposing pixel-level localization constraints with zero inference overhead. Second, we design an anisotropic decoupled rotational attention (ADRA) module, which achieves superior feature alignment for objects of arbitrary morphology by generating a non-rigid adaptive sampling field. Finally, we propose a pose-aware Mondrian conformal prediction (PA-MCP) method, which utilizes the UAV’s flight pose as a physical prior to generate prediction sets with conditional coverage guarantees, thereby providing reliable uncertainty quantification. Extensive experiments on multiple aerial imagery datasets validate the effectiveness of our model. Compared to previous state-of-the-art methods, EAV-DETR improves AP 75 " role="presentation"&gt; AP 75 AP 75 on CODrone by 1.76% while achieving a 52% faster inference speed (46.38 vs 30.55 FPS), and improves AP 50 : 95 " role="presentation"&gt; AP 50 : 95 AP 50 : 95 on UAV-ROD by 3.17%. Our code is available at https://github.com/zzzhak/EAV-DETR .
Published: 2026-02-10T05:19:37+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.843 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haoyu Zuo; Minghao Ning; Yiming Shu; Shucheng Huang; Chen Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.02.009"&gt;10.1016/j.isprsjprs.2026.02.009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.843 (must_read)&lt;/p&gt;
&lt;p&gt;Oriented object detection is critical for enhancing the visual perception of unmanned aerial vehicles (UAVs). However, existing detectors primarily designed for general aerial imagery often struggle to address the unique challenges of UAV imagery, including substantial scale variations, dense clustering, and arbitrary orientations. Furthermore, these models lack probabilistic guarantees required for safety-critical applications. To address these challenges, we propose EAV-DETR, an efficient oriented object detection transformer designed for UAV imagery. Specifically, we first propose a novel scale-adaptive center supervision (SACS) strategy that explicitly enhances the encoder’s feature representations by imposing pixel-level localization constraints with zero inference overhead. Second, we design an anisotropic decoupled rotational attention (ADRA) module, which achieves superior feature alignment for objects of arbitrary morphology by generating a non-rigid adaptive sampling field. Finally, we propose a pose-aware Mondrian conformal prediction (PA-MCP) method, which utilizes the UAV’s flight pose as a physical prior to generate prediction sets with conditional coverage guarantees, thereby providing reliable uncertainty quantification. Extensive experiments on multiple aerial imagery datasets validate the effectiveness of our model. Compared to previous state-of-the-art methods, EAV-DETR improves AP 75 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; AP 75 AP 75 on CODrone by 1.76% while achieving a 52% faster inference speed (46.38 vs 30.55 FPS), and improves AP 50 : 95 &amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt; AP 50 : 95 AP 50 : 95 on UAV-ROD by 3.17%. Our code is available at https://github.com/zzzhak/EAV-DETR .&lt;/p&gt;</content:encoded></item><item><title>SSD-YOLOv12: an improved YOLOv12n model for ship detection using SAR data</title><link>https://doi.org/10.1007/s00521-025-11735-z</link><guid>10.1007/s00521-025-11735-z</guid><pubDate>Mon, 09 Feb 2026 05:25:13 +0000</pubDate><dc:creator>Phat T. Nguyen</dc:creator><dc:creator>Linh V. Cao</dc:creator><prism:publicationName>Neural Computing and Applications</prism:publicationName><prism:doi>10.1007/s00521-025-11735-z</prism:doi><description>Ship detection in Synthetic Aperture Radar (SAR) imagery plays a critical role in maritime surveillance applications, ensuring security and defense, and the management of territorial waters. This task, however, remains challenging due to the complex characteristics of SAR data, including strong background noise, side-lobe effects, and ambiguous target signals. In this context, deep learning methods, particularly real-time object detection architectures like You Only Look Once (YOLO), have shown considerable potential. Nevertheless, their effectiveness on SAR imagery is still limited by suboptimal feature extraction and performance reduction in high-noise environments. This paper proposes an improved version of the YOLOv12n architecture, which integrates an M-MBConvBlock module (an enhanced variant of the MBConvBlock from EfficientNet) into the backbone to enhance representational capacity and adapt to SAR images. Additionally, the loss function is refined by replacing the Complete Intersection over Union (CIoU) with an I-ShapeIoU (improved Shape Intersection over Union) to optimize localization accuracy. Empirical validation demonstrates that the proposed architecture achieves a compelling accuracy of 90.1% mAP@0.5 while maintaining exceptional computational efficiency. Crucially, SSD-YOLOv12 accomplishes this with a mere 1.16 million parameters and a compact 2.8 MB memory footprint, a substantial reduction compared to contemporary YOLO variants such as YOLOv8n (3.01M parameters, 6.3 MB) and the YOLOv12n baseline (2.56M parameters, 5.5 MB). This synergy between high precision and model compactness validates its suitability for real-time ship detection in SAR imagery, particularly on resource-constrained platforms.
Published: 2026-02-09T05:25:13+00:00
Venue: Neural Computing and Applications
Score: 0.829 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Phat T. Nguyen; Linh V. Cao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Computing and Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s00521-025-11735-z"&gt;10.1007/s00521-025-11735-z&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.829 (must_read)&lt;/p&gt;
&lt;p&gt;Ship detection in Synthetic Aperture Radar (SAR) imagery plays a critical role in maritime surveillance applications, ensuring security and defense, and the management of territorial waters. This task, however, remains challenging due to the complex characteristics of SAR data, including strong background noise, side-lobe effects, and ambiguous target signals. In this context, deep learning methods, particularly real-time object detection architectures like You Only Look Once (YOLO), have shown considerable potential. Nevertheless, their effectiveness on SAR imagery is still limited by suboptimal feature extraction and performance reduction in high-noise environments. This paper proposes an improved version of the YOLOv12n architecture, which integrates an M-MBConvBlock module (an enhanced variant of the MBConvBlock from EfficientNet) into the backbone to enhance representational capacity and adapt to SAR images. Additionally, the loss function is refined by replacing the Complete Intersection over Union (CIoU) with an I-ShapeIoU (improved Shape Intersection over Union) to optimize localization accuracy. Empirical validation demonstrates that the proposed architecture achieves a compelling accuracy of 90.1% mAP@0.5 while maintaining exceptional computational efficiency. Crucially, SSD-YOLOv12 accomplishes this with a mere 1.16 million parameters and a compact 2.8 MB memory footprint, a substantial reduction compared to contemporary YOLO variants such as YOLOv8n (3.01M parameters, 6.3 MB) and the YOLOv12n baseline (2.56M parameters, 5.5 MB). This synergy between high precision and model compactness validates its suitability for real-time ship detection in SAR imagery, particularly on resource-constrained platforms.&lt;/p&gt;</content:encoded></item><item><title>CAIR-Net: Reliability-Aware Information Routing for Robust Multimodal Object Detection under Modality Degradation</title><link>https://doi.org/10.1109/tcsvt.2026.3662605</link><guid>10.1109/tcsvt.2026.3662605</guid><pubDate>Mon, 09 Feb 2026 21:09:48 +0000</pubDate><dc:creator>Yudi Su</dc:creator><dc:creator>Jialei Ni</dc:creator><dc:creator>Tiansheng Wen</dc:creator><dc:creator>Hongwei Liu</dc:creator><dc:creator>Hongtao Su</dc:creator><dc:creator>Bo Chen</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662605</prism:doi><description>Multimodal remote sensing combines optical and synthetic aperture radar (SAR) imagery to improve perception, yet real deployments face spatially varying degradations (e.g., clouds, low light, sensor interference) that can corrupt fusion. To make robustness measurable, we introduce a controlled mixed-severity setting in which only the optical stream is synthetically cloud-degraded while SAR remains intact, providing a standardized testbed for evaluating multimodal detection under modality imbalance. We further present CAIR-Net, a reliability–aware information routing network that follows a denoise-then-fuse principle: a Local Reliability Modulation (LRM) module learns soft, spatial reliability maps to suppress degraded regions before cross-modal interaction, and a Global Information Selection Mechanism (GISM) performs confidence-aware expert routing across optical, fused, and SAR experts. On the mixed-severity benchmark, CAIR-Net consistently outperforms strong unimodal and fusion baselines and exhibits a substantially smaller performance drop under severe clouds (only a 7.3% AP reduction versus drops exceeding 25% for representative alternatives). These results indicate that explicit reliability modeling and quality-guided routing provide a practical path toward robust multimodal detection when one modality is partially or nearly completely occluded.
Published: 2026-02-09T21:09:48+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yudi Su; Jialei Ni; Tiansheng Wen; Hongwei Liu; Hongtao Su; Bo Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662605"&gt;10.1109/tcsvt.2026.3662605&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal remote sensing combines optical and synthetic aperture radar (SAR) imagery to improve perception, yet real deployments face spatially varying degradations (e.g., clouds, low light, sensor interference) that can corrupt fusion. To make robustness measurable, we introduce a controlled mixed-severity setting in which only the optical stream is synthetically cloud-degraded while SAR remains intact, providing a standardized testbed for evaluating multimodal detection under modality imbalance. We further present CAIR-Net, a reliability–aware information routing network that follows a denoise-then-fuse principle: a Local Reliability Modulation (LRM) module learns soft, spatial reliability maps to suppress degraded regions before cross-modal interaction, and a Global Information Selection Mechanism (GISM) performs confidence-aware expert routing across optical, fused, and SAR experts. On the mixed-severity benchmark, CAIR-Net consistently outperforms strong unimodal and fusion baselines and exhibits a substantially smaller performance drop under severe clouds (only a 7.3% AP reduction versus drops exceeding 25% for representative alternatives). These results indicate that explicit reliability modeling and quality-guided routing provide a practical path toward robust multimodal detection when one modality is partially or nearly completely occluded.&lt;/p&gt;</content:encoded></item><item><title>FCFNet: A Frequency-Domain Guided Cross-Modal Feature Fusion Network for Cloud Removal</title><link>https://doi.org/10.1109/tgrs.2026.3662395</link><guid>10.1109/tgrs.2026.3662395</guid><pubDate>Mon, 09 Feb 2026 21:05:24 +0000</pubDate><dc:creator>Caifeng Wu</dc:creator><dc:creator>Feng Xu</dc:creator><dc:creator>Xin Li</dc:creator><dc:creator>Fulian Zhao</dc:creator><dc:creator>Zhennan Xu</dc:creator><dc:creator>Xin Lyu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3662395</prism:doi><description>Optical remote sensing images are frequently contaminated by clouds and shadows, leading to information loss that significantly degrades the performance of Earth observation tasks. Synthetic Aperture Radar (SAR), with its cloud-penetrating capability, serves as a powerful complement to optical imagery under dense cloud coverage. However, most existing SAR-optical fusion approaches mainly rely on spatial-domain operations, underutilizing frequency-domain structural cues and lacking effective strategies to resolve spatial misalignment between heterogeneous modalities. To address the limitations above-mentioned, we propose FCFNet, a Frequency-Domain Guided Cross-Modal Feature Fusion Network that jointly exploits spatial and frequency representations for cloud removal. Specifically, the FCFNet comprises three key components: a Deformable Gated Fusion (DGF) module to adaptively align SAR and optical features via learnable spatial offsets and channel-wise modulation; a Frequency-Aware Fusion (FAF) module that decomposes features into distinct frequency bands for selective integration of structural and textural information; and a Frequency-Domain Attention (FDA) mechanism that enhances high-frequency detail recovery in decoding. Additionally, we formulate a jointly optimized loss function that aligns with the network’s dual-domain design, promoting accurate reconstruction through spatial supervision and frequency-based constraints. Extensive experiments on the SEN12MS-CR dataset demonstrate that FCFNet outperforms state-of-the-art methods across various quantitative metrics, particularly under heavy cloud occlusion scenarios, validating the effectiveness of our frequency-space cooperative modeling strategy.
Published: 2026-02-09T21:05:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Caifeng Wu; Feng Xu; Xin Li; Fulian Zhao; Zhennan Xu; Xin Lyu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3662395"&gt;10.1109/tgrs.2026.3662395&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Optical remote sensing images are frequently contaminated by clouds and shadows, leading to information loss that significantly degrades the performance of Earth observation tasks. Synthetic Aperture Radar (SAR), with its cloud-penetrating capability, serves as a powerful complement to optical imagery under dense cloud coverage. However, most existing SAR-optical fusion approaches mainly rely on spatial-domain operations, underutilizing frequency-domain structural cues and lacking effective strategies to resolve spatial misalignment between heterogeneous modalities. To address the limitations above-mentioned, we propose FCFNet, a Frequency-Domain Guided Cross-Modal Feature Fusion Network that jointly exploits spatial and frequency representations for cloud removal. Specifically, the FCFNet comprises three key components: a Deformable Gated Fusion (DGF) module to adaptively align SAR and optical features via learnable spatial offsets and channel-wise modulation; a Frequency-Aware Fusion (FAF) module that decomposes features into distinct frequency bands for selective integration of structural and textural information; and a Frequency-Domain Attention (FDA) mechanism that enhances high-frequency detail recovery in decoding. Additionally, we formulate a jointly optimized loss function that aligns with the network’s dual-domain design, promoting accurate reconstruction through spatial supervision and frequency-based constraints. Extensive experiments on the SEN12MS-CR dataset demonstrate that FCFNet outperforms state-of-the-art methods across various quantitative metrics, particularly under heavy cloud occlusion scenarios, validating the effectiveness of our frequency-space cooperative modeling strategy.&lt;/p&gt;</content:encoded></item><item><title>Cross-Modal Mapping: Mitigating the Modality Gap for Few-Shot Classification</title><link>https://doi.org/10.1016/j.patcog.2026.113285</link><guid>10.1016/j.patcog.2026.113285</guid><pubDate>Tue, 10 Feb 2026 15:59:50 +0000</pubDate><dc:creator>Xi Yang</dc:creator><dc:creator>Wulin Xie</dc:creator><dc:creator>Pai Peng</dc:creator><dc:creator>Jie Wen</dc:creator><dc:creator>Xiaohuan Lu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113285</prism:doi><description>Few-shot classification remains a critical challenge in the field of computer vision, particularly in data-scarce environments. Existing methods typically rely on pre-trained visual-language models, such as CLIP. However, due to the modality gap, which is the inconsistent distribution of image and text features in the joint embedding space, directly using these features as class prototypes often leads to suboptimal performance. To address this issue, we propose a novel Cross-Modal Mapping (CMM) method. This method globally aligns image features with the text feature space through linear transformation and optimizes their local spatial relationships using triplet loss, thereby significantly enhancing cross-modal consistency. Experimental results show that compared to other methods, CMM simplifies the training process and demonstrates higher efficiency. Furthermore, CMM improves the average Top-1 accuracy by 1.06% on 11 benchmark datasets compared to methods that partially fine-tune the backbone, and it exhibits excellent performance on 4 distribution-shifted datasets. Notably, CMM effectively mitigates the modality gap in pre-trained models, enabling text features to serve as effective class prototypes for image features, thus providing an efficient and highly generalizable solution for few-shot learning.
Published: 2026-02-10T15:59:50+00:00
Venue: Pattern Recognition
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xi Yang; Wulin Xie; Pai Peng; Jie Wen; Xiaohuan Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113285"&gt;10.1016/j.patcog.2026.113285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot classification remains a critical challenge in the field of computer vision, particularly in data-scarce environments. Existing methods typically rely on pre-trained visual-language models, such as CLIP. However, due to the modality gap, which is the inconsistent distribution of image and text features in the joint embedding space, directly using these features as class prototypes often leads to suboptimal performance. To address this issue, we propose a novel Cross-Modal Mapping (CMM) method. This method globally aligns image features with the text feature space through linear transformation and optimizes their local spatial relationships using triplet loss, thereby significantly enhancing cross-modal consistency. Experimental results show that compared to other methods, CMM simplifies the training process and demonstrates higher efficiency. Furthermore, CMM improves the average Top-1 accuracy by 1.06% on 11 benchmark datasets compared to methods that partially fine-tune the backbone, and it exhibits excellent performance on 4 distribution-shifted datasets. Notably, CMM effectively mitigates the modality gap in pre-trained models, enabling text features to serve as effective class prototypes for image features, thus providing an efficient and highly generalizable solution for few-shot learning.&lt;/p&gt;</content:encoded></item><item><title>Dual Knowledge Distillation Framework with Class-Adaptive Temperature and TopK Feature Perturbation for Few-Shot Prompt Learning</title><link>https://doi.org/10.1109/tcsvt.2026.3662460</link><guid>10.1109/tcsvt.2026.3662460</guid><pubDate>Mon, 09 Feb 2026 21:09:48 +0000</pubDate><dc:creator>Wenjie Chen</dc:creator><dc:creator>Weisheng Li</dc:creator><dc:creator>Yucheng Shu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662460</prism:doi><description>Pre-trained vision-language models have shown great potential in few-shot learning. However, existing methods typically employ either KL divergence or feature similarity-based knowledge distillation, and rarely integrate both. Our analysis reveals that a naive simultaneous deployment of these two strategies yields suboptimal results. To address this, we propose a unified dual knowledge distillation framework. This framework is grounded in a theoretical derivation of class-adaptive temperature parameters, effectively resolving the incompatibility between KL divergence and feature similarity approaches. Furthermore, we introduce a top-K feature perturbation technique that targets specific features for more consistent enhancement than traditional noise regularization. Experimental results across 11 diverse benchmarks show that our approach yields consistent performance gains over various baselines. Notably, it improves the harmonic mean (H) by 0.41% to 0.72% and enhances generalization to unseen classes with an accuracy boost of up to 1.41%. Our source code is available at: https://github.com/sydney72380/DKL.
Published: 2026-02-09T21:09:48+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.813 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenjie Chen; Weisheng Li; Yucheng Shu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662460"&gt;10.1109/tcsvt.2026.3662460&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.813 (must_read)&lt;/p&gt;
&lt;p&gt;Pre-trained vision-language models have shown great potential in few-shot learning. However, existing methods typically employ either KL divergence or feature similarity-based knowledge distillation, and rarely integrate both. Our analysis reveals that a naive simultaneous deployment of these two strategies yields suboptimal results. To address this, we propose a unified dual knowledge distillation framework. This framework is grounded in a theoretical derivation of class-adaptive temperature parameters, effectively resolving the incompatibility between KL divergence and feature similarity approaches. Furthermore, we introduce a top-K feature perturbation technique that targets specific features for more consistent enhancement than traditional noise regularization. Experimental results across 11 diverse benchmarks show that our approach yields consistent performance gains over various baselines. Notably, it improves the harmonic mean (H) by 0.41% to 0.72% and enhances generalization to unseen classes with an accuracy boost of up to 1.41%. Our source code is available at: https://github.com/sydney72380/DKL.&lt;/p&gt;</content:encoded></item><item><title>Style-Guided Source Data Augmentation and Target Feature Optimization for Cross-Domain Few-Shot Image Classification</title><link>https://doi.org/10.1109/tcsvt.2026.3662777</link><guid>10.1109/tcsvt.2026.3662777</guid><pubDate>Mon, 09 Feb 2026 21:09:48 +0000</pubDate><dc:creator>Qing Liu</dc:creator><dc:creator>Xianlun Tang</dc:creator><dc:creator>Xingchen Li</dc:creator><dc:creator>Ying Wang</dc:creator><dc:creator>Wuquan Deng</dc:creator><dc:creator>Qiu Chen</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662777</prism:doi><description>In Cross-Domain Few-Shot Learning (CD-FSL), models are required to identify novel classes while addressing domain discrepancies caused by visual style variations. Simple style transformations often fail to extend beyond the source domain’s distribution, and unrepresentative support samples in the target task may lead to ambiguous or biased decision boundaries. To address these challenges, a Style-Guided Source Data Augmentation and Target Feature Optimization (SSDATFO) approach is proposed. Specifically, Style-Guided Source Data Augmentation is introduced, employing Style Transformation and Source Data Augmentation techniques to create more challenging source data, thereby expanding the source domain’s style distribution. Target Feature Optimization is subsequently introduced, comprising two distinct modules. The Domain Attention Shift Transformation enhances low-magnitude feature channels, thereby reactivating target domain feature channels previously overlooked by the source domain-trained feature extractor. Additionally, the Task Category Differentiation Enhancement Transformation calibrates the features of support samples and eliminates the commonality component along both the task-specific and inter-class commonality directions for all features within the novel task, thereby acquiring more discriminative features. Extensive experiments on eight distinct target datasets demonstrate the efficacy of the proposed method, while comprehensive ablation studies and detailed visualization experiments elucidate its nuanced and compelling aspects.
Published: 2026-02-09T21:09:48+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qing Liu; Xianlun Tang; Xingchen Li; Ying Wang; Wuquan Deng; Qiu Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662777"&gt;10.1109/tcsvt.2026.3662777&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;In Cross-Domain Few-Shot Learning (CD-FSL), models are required to identify novel classes while addressing domain discrepancies caused by visual style variations. Simple style transformations often fail to extend beyond the source domain’s distribution, and unrepresentative support samples in the target task may lead to ambiguous or biased decision boundaries. To address these challenges, a Style-Guided Source Data Augmentation and Target Feature Optimization (SSDATFO) approach is proposed. Specifically, Style-Guided Source Data Augmentation is introduced, employing Style Transformation and Source Data Augmentation techniques to create more challenging source data, thereby expanding the source domain’s style distribution. Target Feature Optimization is subsequently introduced, comprising two distinct modules. The Domain Attention Shift Transformation enhances low-magnitude feature channels, thereby reactivating target domain feature channels previously overlooked by the source domain-trained feature extractor. Additionally, the Task Category Differentiation Enhancement Transformation calibrates the features of support samples and eliminates the commonality component along both the task-specific and inter-class commonality directions for all features within the novel task, thereby acquiring more discriminative features. Extensive experiments on eight distinct target datasets demonstrate the efficacy of the proposed method, while comprehensive ablation studies and detailed visualization experiments elucidate its nuanced and compelling aspects.&lt;/p&gt;</content:encoded></item><item><title>Complex-Valued Source-Free Domain Adaptation for PolSAR Image Classification</title><link>https://doi.org/10.1109/tgrs.2026.3663159</link><guid>10.1109/tgrs.2026.3663159</guid><pubDate>Tue, 10 Feb 2026 21:05:00 +0000</pubDate><dc:creator>Ningwei Wang</dc:creator><dc:creator>Weiqiang Jin</dc:creator><dc:creator>Haixia Bi</dc:creator><dc:creator>Chen Xu</dc:creator><dc:creator>Fan Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3663159</prism:doi><description>The discrepancies of sensors, acquisition conditions and terrain class distributions in polarimetric synthetic aperture radar (PolSAR) data has aroused the cross domain PolSAR image classification problem. Domain adaptation, which aims to improve the classification performance of target domain with knowledge from the source domain, is a promising approach to address this issue. However, conventional domain adaptation methods typically assume access to both source and target domain data, which may be infeasible in real-world settings due to data privacy and confidentiality concerns. Therefore, it is crucial to develop source-free domain adaptation PolSAR image classification methods which rely only on target domain data. In this scenario, how to make full use of the label sparse target domain data is a consequent challenge to overcome. To this end, we propose CVSFDA, a complex-valued source-free domain adaptation framework tailored for PolSAR image classification. CVSFDA incorporates a complex-valued multiscale prototypical matching module (CVMP) and a complex-valued relation network (CVRN). Given the pretrained source domain model and the limited labeled samples in target domain, CVMP captures the similarities between samples leveraging the hierarchical spatial information across multiple encoder layers. A similarity convolutional network is further devised in CVRN, to comprehensively model class-specific information in the target domain. Extensive experiments on four benchmark PolSAR datasets demonstrate that CVSFDA achieves superior classification accuracy and generalization ability compared to existing domain adaptation methods.
Published: 2026-02-10T21:05:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.808 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ningwei Wang; Weiqiang Jin; Haixia Bi; Chen Xu; Fan Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3663159"&gt;10.1109/tgrs.2026.3663159&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.808 (must_read)&lt;/p&gt;
&lt;p&gt;The discrepancies of sensors, acquisition conditions and terrain class distributions in polarimetric synthetic aperture radar (PolSAR) data has aroused the cross domain PolSAR image classification problem. Domain adaptation, which aims to improve the classification performance of target domain with knowledge from the source domain, is a promising approach to address this issue. However, conventional domain adaptation methods typically assume access to both source and target domain data, which may be infeasible in real-world settings due to data privacy and confidentiality concerns. Therefore, it is crucial to develop source-free domain adaptation PolSAR image classification methods which rely only on target domain data. In this scenario, how to make full use of the label sparse target domain data is a consequent challenge to overcome. To this end, we propose CVSFDA, a complex-valued source-free domain adaptation framework tailored for PolSAR image classification. CVSFDA incorporates a complex-valued multiscale prototypical matching module (CVMP) and a complex-valued relation network (CVRN). Given the pretrained source domain model and the limited labeled samples in target domain, CVMP captures the similarities between samples leveraging the hierarchical spatial information across multiple encoder layers. A similarity convolutional network is further devised in CVRN, to comprehensively model class-specific information in the target domain. Extensive experiments on four benchmark PolSAR datasets demonstrate that CVSFDA achieves superior classification accuracy and generalization ability compared to existing domain adaptation methods.&lt;/p&gt;</content:encoded></item><item><title>TGCADNet: Text-Guided Context-Aware Detection via CLIP for Small Objects in UAV Scenes</title><link>https://doi.org/10.1109/tcsvt.2026.3662475</link><guid>10.1109/tcsvt.2026.3662475</guid><pubDate>Mon, 09 Feb 2026 21:09:48 +0000</pubDate><dc:creator>Fengqian Sun</dc:creator><dc:creator>Deqiang Cheng</dc:creator><dc:creator>Ping Zheng</dc:creator><dc:creator>Tianshu Song</dc:creator><dc:creator>Liangliang Chen</dc:creator><dc:creator>Qiqi Kou</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662475</prism:doi><description>Recent studies have highlighted the importance of contextual information for small object detection. However, existing methods rely solely on visual features and lack additional semantic guidance, which limits their ability to model key scene-level context in semantically rich, globally complex environments and to suppress irrelevant local context in densely cluttered scenes. These limitations hinder their effectiveness in Unmanned Aerial Vehicle (UAV) and similar complex scenes. To address these challenges, we propose TGCADNet (Text-Guided Context-Aware Detection Network). TGCADNet is a small object detection framework that leverages the CLIP (Contrastive Language– Image Pretraining) model’s global semantic understanding and image-text alignment capabilities for enhancing context-aware detection. TGCADNet mainly consists of Text-Guided Scene-level Context-Aware (TG-SCA) and Text-Guided Local-Context Filtering (TG-LCF). Specifically, TG-SCA uses CLIP-generated text features to guide the model in accurately extracting key scene-level context from globally complex environments. Meanwhile, TG-LCF performs interactive computation between text and image features to filter high-quality local context, thereby reducing the impact of dense and cluttered local regions in UAV scenes. We validate the effectiveness of TGCADNet on the VisDrone, UAVDT, and AI-TOD-v2 datasets. Compared to the baseline, TGCADNet achieves an improvement of 1.8 in mAP@50 and 1.3 in mAP@50:95 on the VisDrone dataset. On the UAVDT and AI-TOD-v2 datasets, TGCADNet observes improvements of 2.5 and 2.3 in mAP@50, respectively. Furthermore, TGCADNet surpasses recent SOTA methods in both accuracy and efficiency, demonstrating its effectiveness in detecting small objects in UAV and similar remote sensing scenes.
Published: 2026-02-09T21:09:48+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fengqian Sun; Deqiang Cheng; Ping Zheng; Tianshu Song; Liangliang Chen; Qiqi Kou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662475"&gt;10.1109/tcsvt.2026.3662475&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Recent studies have highlighted the importance of contextual information for small object detection. However, existing methods rely solely on visual features and lack additional semantic guidance, which limits their ability to model key scene-level context in semantically rich, globally complex environments and to suppress irrelevant local context in densely cluttered scenes. These limitations hinder their effectiveness in Unmanned Aerial Vehicle (UAV) and similar complex scenes. To address these challenges, we propose TGCADNet (Text-Guided Context-Aware Detection Network). TGCADNet is a small object detection framework that leverages the CLIP (Contrastive Language– Image Pretraining) model’s global semantic understanding and image-text alignment capabilities for enhancing context-aware detection. TGCADNet mainly consists of Text-Guided Scene-level Context-Aware (TG-SCA) and Text-Guided Local-Context Filtering (TG-LCF). Specifically, TG-SCA uses CLIP-generated text features to guide the model in accurately extracting key scene-level context from globally complex environments. Meanwhile, TG-LCF performs interactive computation between text and image features to filter high-quality local context, thereby reducing the impact of dense and cluttered local regions in UAV scenes. We validate the effectiveness of TGCADNet on the VisDrone, UAVDT, and AI-TOD-v2 datasets. Compared to the baseline, TGCADNet achieves an improvement of 1.8 in mAP@50 and 1.3 in mAP@50:95 on the VisDrone dataset. On the UAVDT and AI-TOD-v2 datasets, TGCADNet observes improvements of 2.5 and 2.3 in mAP@50, respectively. Furthermore, TGCADNet surpasses recent SOTA methods in both accuracy and efficiency, demonstrating its effectiveness in detecting small objects in UAV and similar remote sensing scenes.&lt;/p&gt;</content:encoded></item><item><title>Vehicle-centric Perception via Multimodal Structured Pre-training</title><link>https://doi.org/10.1109/tcsvt.2026.3663409</link><guid>10.1109/tcsvt.2026.3663409</guid><pubDate>Tue, 10 Feb 2026 21:06:49 +0000</pubDate><dc:creator>Wentao Wu</dc:creator><dc:creator>Xiao Wang</dc:creator><dc:creator>Chenglong Li</dc:creator><dc:creator>Jin Tang</dc:creator><dc:creator>Bin Luo</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3663409</prism:doi><description>Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches typically employ general pre-trained weights to initialize backbone networks, followed by task-specific fine-tuning. However, these models lack effective learning of vehiclerelated knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model’s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the prob23 ability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2. The sour...
Published: 2026-02-10T21:06:49+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wentao Wu; Xiao Wang; Chenglong Li; Jin Tang; Bin Luo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3663409"&gt;10.1109/tcsvt.2026.3663409&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches typically employ general pre-trained weights to initialize backbone networks, followed by task-specific fine-tuning. However, these models lack effective learning of vehiclerelated knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model’s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the prob23 ability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2. The sour...&lt;/p&gt;</content:encoded></item><item><title>Monotonic Rank Knowledge Distillation via Kendall Correlation</title><link>https://doi.org/10.1109/tcsvt.2026.3662408</link><guid>10.1109/tcsvt.2026.3662408</guid><pubDate>Mon, 09 Feb 2026 21:09:48 +0000</pubDate><dc:creator>Xuewan He</dc:creator><dc:creator>Jielei Wang</dc:creator><dc:creator>Yuchen Su</dc:creator><dc:creator>Dongnan Liu</dc:creator><dc:creator>Junbo Zhao</dc:creator><dc:creator>Guoming Lu</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662408</prism:doi><description>The computational and memory demands of deep neural networks for vision tasks remain a critical barrier to their deployment on resource-constrained edge devices. Although knowledge distillation (KD) effectively transfers over-parameterized models’ knowledge into compact students, its efficacy diminishes substantially when a significant capacity gap exists between them. Current approaches often impose linear mapping constraints between output distributions, an assumption that becomes prohibitively restrictive under such capacity gaps. This paper proposes a fundamental relaxation of alignment requirements. Specifically, rather than enforcing strict parametric relationships, we experimentally validate that preserving monotonic rank correlation between teacher and student outputs suffices for effective knowledge transfer. To operationalize this insight, we introduce Monotonic Rank Knowledge Distillation, a novel framework that leverages differentiable approximations of Kendall’s rank correlation coefficient to measure and optimize rank-order consistency. Our methodology further decomposes rank correlation into inter-class and intra-class components, ensuring the student network retains both global discriminative patterns and fine-grained categorical distinctions inherent to the teacher’s outputs. Extensive experiments across CIFAR-100 and ImageNet-1K benchmarks validate the effectiveness of our approach, demonstrating consistent performance gains over state-of-the-art distillation methods. The proposed framework achieves superior generalization across diverse architectures, including CNN-based, MLP-based, and ViT-based, with particular efficacy in various compression scenarios.
Published: 2026-02-09T21:09:48+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuewan He; Jielei Wang; Yuchen Su; Dongnan Liu; Junbo Zhao; Guoming Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662408"&gt;10.1109/tcsvt.2026.3662408&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;The computational and memory demands of deep neural networks for vision tasks remain a critical barrier to their deployment on resource-constrained edge devices. Although knowledge distillation (KD) effectively transfers over-parameterized models’ knowledge into compact students, its efficacy diminishes substantially when a significant capacity gap exists between them. Current approaches often impose linear mapping constraints between output distributions, an assumption that becomes prohibitively restrictive under such capacity gaps. This paper proposes a fundamental relaxation of alignment requirements. Specifically, rather than enforcing strict parametric relationships, we experimentally validate that preserving monotonic rank correlation between teacher and student outputs suffices for effective knowledge transfer. To operationalize this insight, we introduce Monotonic Rank Knowledge Distillation, a novel framework that leverages differentiable approximations of Kendall’s rank correlation coefficient to measure and optimize rank-order consistency. Our methodology further decomposes rank correlation into inter-class and intra-class components, ensuring the student network retains both global discriminative patterns and fine-grained categorical distinctions inherent to the teacher’s outputs. Extensive experiments across CIFAR-100 and ImageNet-1K benchmarks validate the effectiveness of our approach, demonstrating consistent performance gains over state-of-the-art distillation methods. The proposed framework achieves superior generalization across diverse architectures, including CNN-based, MLP-based, and ViT-based, with particular efficacy in various compression scenarios.&lt;/p&gt;</content:encoded></item><item><title>WHFNet: A Wavelet-Driven Heterogeneous Fusion Network for High-Frequency Enhanced Optical-SAR Remote Sensing Segmentation</title><link>https://doi.org/10.1109/tgrs.2026.3662689</link><guid>10.1109/tgrs.2026.3662689</guid><pubDate>Mon, 09 Feb 2026 21:05:24 +0000</pubDate><dc:creator>Bo Ren</dc:creator><dc:creator>Qianfang Wang</dc:creator><dc:creator>Bo Liu</dc:creator><dc:creator>Biao Hou</dc:creator><dc:creator>Chen Yang</dc:creator><dc:creator>Licheng Jiao</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3662689</prism:doi><description>Synergizing the visual richness of optical imagery with the structural robustness of SAR is pivotal for land cover classification. However, the inherent heterogeneity and distinct data distributions between these active and passive modalities often lead to ”negative transfer”, where valuable modality-specific information is mixed during fusion. To overcome these limitations, this paper proposes a wavelet-driven heterogeneous fusion network (WHFNet) for co-registered optical and SAR images. Unlike normally symmetric architectures, WHFNet adopts a decoupled heterogeneous encoding strategy, leveraging the distinct inductive biases of Transformers and CNNs to preserve independent semantic representations. To bridge the modality gap, we introduce a cross-modal interactor (CMI) grounded in the 2D discrete wavelet transform. This module explicitly injects high-frequency information into spatial features, enhancing the representation of details. Furthermore, a spatial-frequency fusion module (SFFM) is devised to dynamically calibrate the discrepancies between modalities via subtraction operation, while a structural consistency constraint promotes semantically aligned predictions across modalities. Extensive experiments on four benchmark datasets (Xi’an, Pohang, WHU-OPT-SAR, and PIE-RGB-SAR) demonstrate that WHFNet establishes new state-of-the-art performance. Notably, it achieves substantial accuracy gains, particularly improving mIoU by 1.25% on the cloudless WHU-OPT-SAR dataset and 0.92% on the high-precision Xi’an dataset. The code will be publicly available at https://github.com/XD-MG/WHFNet.
Published: 2026-02-09T21:05:24+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.804 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bo Ren; Qianfang Wang; Bo Liu; Biao Hou; Chen Yang; Licheng Jiao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3662689"&gt;10.1109/tgrs.2026.3662689&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.804 (must_read)&lt;/p&gt;
&lt;p&gt;Synergizing the visual richness of optical imagery with the structural robustness of SAR is pivotal for land cover classification. However, the inherent heterogeneity and distinct data distributions between these active and passive modalities often lead to ”negative transfer”, where valuable modality-specific information is mixed during fusion. To overcome these limitations, this paper proposes a wavelet-driven heterogeneous fusion network (WHFNet) for co-registered optical and SAR images. Unlike normally symmetric architectures, WHFNet adopts a decoupled heterogeneous encoding strategy, leveraging the distinct inductive biases of Transformers and CNNs to preserve independent semantic representations. To bridge the modality gap, we introduce a cross-modal interactor (CMI) grounded in the 2D discrete wavelet transform. This module explicitly injects high-frequency information into spatial features, enhancing the representation of details. Furthermore, a spatial-frequency fusion module (SFFM) is devised to dynamically calibrate the discrepancies between modalities via subtraction operation, while a structural consistency constraint promotes semantically aligned predictions across modalities. Extensive experiments on four benchmark datasets (Xi’an, Pohang, WHU-OPT-SAR, and PIE-RGB-SAR) demonstrate that WHFNet establishes new state-of-the-art performance. Notably, it achieves substantial accuracy gains, particularly improving mIoU by 1.25% on the cloudless WHU-OPT-SAR dataset and 0.92% on the high-precision Xi’an dataset. The code will be publicly available at https://github.com/XD-MG/WHFNet.&lt;/p&gt;</content:encoded></item><item><title>Test-time Domain-agnostic Meta-prompt Learning for Multi-source Few-shot Domain Adaptation</title><link>https://doi.org/10.1109/tcsvt.2026.3662700</link><guid>10.1109/tcsvt.2026.3662700</guid><pubDate>Mon, 09 Feb 2026 21:09:48 +0000</pubDate><dc:creator>Kuanghong Liu</dc:creator><dc:creator>Jin Wang</dc:creator><dc:creator>Kangjian He</dc:creator><dc:creator>Dan Xu</dc:creator><dc:creator>Xuejie Zhang</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662700</prism:doi><description>Multi-source few-shot domain adaptation (MFDA) is a more common and challenging scenario, as only limited annotated source domain data is provided and a large amount of data is unlabeled. Conventional solutions are difficult to achieve for large-scale vision-language models (VLM) since multiple-source domains need to be aligned and more parameters need to be tuned. To efficiently transfer VLM to the target domain in the MFDA, this study first summarizes the previous prompt tuning for domain adaptation methods as a transductive prompt learning (TPL) paradigm. Then, it introduces a new inductive and transductive prompt learning (I&amp;TPL) paradigm for MFDA. Based on the I&amp;TPL paradigm, a test-time domain-agnostic meta-prompt learning (TDMP) method is further proposed, which is suitable for few-shot annotated multi-source domain data and is compatible with existing prompt tuning methods. As a result, the proposed TDMP does not require multiple complex prompts, constructed source-target pairs, extra auxiliary loss, and pseudo-target labels. Specifically, the proposed TDMP includes domain-agnostic meta-prompt learning and test-time domain-agnostic prompt tuning for target domain adaptation. The first stage is mainly optimized based on the Reptile optimization algorithm. Domain mixup is used to expand the diversity and the number of meta-training tasks. In the second stage, the learned domain-agnostic meta-prompt initializes the test-time prompt to further adapt to the target domain. Extensive experiments are conducted on the OfficeHome, DomainNet, Office, and TerraIncognita datasets of MFDA, achieving better performance with fewer learnable parameters and demonstrating the effectiveness of TDMP.
Published: 2026-02-09T21:09:48+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kuanghong Liu; Jin Wang; Kangjian He; Dan Xu; Xuejie Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662700"&gt;10.1109/tcsvt.2026.3662700&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-source few-shot domain adaptation (MFDA) is a more common and challenging scenario, as only limited annotated source domain data is provided and a large amount of data is unlabeled. Conventional solutions are difficult to achieve for large-scale vision-language models (VLM) since multiple-source domains need to be aligned and more parameters need to be tuned. To efficiently transfer VLM to the target domain in the MFDA, this study first summarizes the previous prompt tuning for domain adaptation methods as a transductive prompt learning (TPL) paradigm. Then, it introduces a new inductive and transductive prompt learning (I&amp;amp;TPL) paradigm for MFDA. Based on the I&amp;amp;TPL paradigm, a test-time domain-agnostic meta-prompt learning (TDMP) method is further proposed, which is suitable for few-shot annotated multi-source domain data and is compatible with existing prompt tuning methods. As a result, the proposed TDMP does not require multiple complex prompts, constructed source-target pairs, extra auxiliary loss, and pseudo-target labels. Specifically, the proposed TDMP includes domain-agnostic meta-prompt learning and test-time domain-agnostic prompt tuning for target domain adaptation. The first stage is mainly optimized based on the Reptile optimization algorithm. Domain mixup is used to expand the diversity and the number of meta-training tasks. In the second stage, the learned domain-agnostic meta-prompt initializes the test-time prompt to further adapt to the target domain. Extensive experiments are conducted on the OfficeHome, DomainNet, Office, and TerraIncognita datasets of MFDA, achieving better performance with fewer learnable parameters and demonstrating the effectiveness of TDMP.&lt;/p&gt;</content:encoded></item><item><title>HSI-LiDAR Joint Classification via Progressive Spatial-Spectral-Frequency Fusion Learning</title><link>https://doi.org/10.1109/tgrs.2026.3663376</link><guid>10.1109/tgrs.2026.3663376</guid><pubDate>Tue, 10 Feb 2026 21:05:00 +0000</pubDate><dc:creator>Xinxin Liu</dc:creator><dc:creator>Xiaoqing Tang</dc:creator><dc:creator>Ting Lu</dc:creator><dc:creator>Kexin Ding</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3663376</prism:doi><description>Hyperspectral imagery (HSI) and light detection and ranging (LiDAR) data joint classification leverages multi-modal information to improve classification performance. However, existing methods often suffer from inadequate feature representation due to their reliance on spatial-spectral domains while ignoring complementary frequency-domain cues, limiting their ability to capture cross-modal variations. Moreover, even when frequency information is incorporated, most approaches fail to achieve deep cross-domain interaction, either treating spatial and frequency features in isolation or relying on single-stage fusion, which hinders effective modality gap bridging. To address these challenges, we propose a novel progressive spatial-spectral-frequency fusion learning (PS2F2L) network, a lightweight yet powerful framework for HSI and LiDAR classification. Our method employs a multi-stage progressive fusion strategy to hierarchically integrate multimodal features, mitigating modal conflicts while capturing discriminative features. In the first stage, dual branches—spatial-spectral feature learning (S2FL) and spatial-frequency feature learning (SF2L)—jointly extract features from complementary domains, overcoming single-domain limitations. The S2FL branch combines 1D/2D convolutions to model spectral-spatial relationships, while SF2L utilizes discrete wavelet transforms to capture spatial-frequency patterns. In the second stage, an interactive spatial-spectral-frequency fusion module enhances feature discriminability by promoting deep information exchange between spatial-spectral and spatial-requency representations. Finally, adaptive decision-level fusion refines classification by consolidating multi-domain predictions. Extensive experiments on three public datasets demonstrate the superiority of PS2F2L, validating its effectiveness in achieving robust and accurate multimodal classification.
Published: 2026-02-10T21:05:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinxin Liu; Xiaoqing Tang; Ting Lu; Kexin Ding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3663376"&gt;10.1109/tgrs.2026.3663376&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral imagery (HSI) and light detection and ranging (LiDAR) data joint classification leverages multi-modal information to improve classification performance. However, existing methods often suffer from inadequate feature representation due to their reliance on spatial-spectral domains while ignoring complementary frequency-domain cues, limiting their ability to capture cross-modal variations. Moreover, even when frequency information is incorporated, most approaches fail to achieve deep cross-domain interaction, either treating spatial and frequency features in isolation or relying on single-stage fusion, which hinders effective modality gap bridging. To address these challenges, we propose a novel progressive spatial-spectral-frequency fusion learning (PS2F2L) network, a lightweight yet powerful framework for HSI and LiDAR classification. Our method employs a multi-stage progressive fusion strategy to hierarchically integrate multimodal features, mitigating modal conflicts while capturing discriminative features. In the first stage, dual branches—spatial-spectral feature learning (S2FL) and spatial-frequency feature learning (SF2L)—jointly extract features from complementary domains, overcoming single-domain limitations. The S2FL branch combines 1D/2D convolutions to model spectral-spatial relationships, while SF2L utilizes discrete wavelet transforms to capture spatial-frequency patterns. In the second stage, an interactive spatial-spectral-frequency fusion module enhances feature discriminability by promoting deep information exchange between spatial-spectral and spatial-requency representations. Finally, adaptive decision-level fusion refines classification by consolidating multi-domain predictions. Extensive experiments on three public datasets demonstrate the superiority of PS2F2L, validating its effectiveness in achieving robust and accurate multimodal classification.&lt;/p&gt;</content:encoded></item><item><title>Monocular Multi-object 3D Visual Language Tracking</title><link>https://doi.org/10.1109/tip.2026.3661407</link><guid>10.1109/tip.2026.3661407</guid><pubDate>Tue, 10 Feb 2026 21:07:05 +0000</pubDate><dc:creator>Hongkai Wei</dc:creator><dc:creator>Rong Wang</dc:creator><dc:creator>Haixiang Hu</dc:creator><dc:creator>Shijie Sun</dc:creator><dc:creator>Xiangyu Song</dc:creator><dc:creator>Mingtao Feng</dc:creator><dc:creator>Keyu Guo</dc:creator><dc:creator>Yongle Huang</dc:creator><dc:creator>Hua Cui</dc:creator><dc:creator>Naveed Akhtar</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661407</prism:doi><description>Visual Language Tracking (VLT) enables machines to perform tracking in real world through human-like language descriptions. However, existing VLT methods are limited to 2D spatial tracking or single-object 3D tracking and do not support multi-object 3D tracking within monocular video. This limitation arises because advancements in 3D multi-object tracking have predominantly relied on sensor-based data (e.g., point clouds, depth sensors) that lacks corresponding language descriptions. Moreover, natural language descriptions in existing VLT literature often suffer from redundancy, impeding the efficient and precise localization of multiple objects. We present the first technique to extend VLT to multi-object 3D tracking using monocular video. We introduce a comprehensive framework that includes (i) a Monocular Multi-object 3D Visual Language Tracking (MoMo-3DVLT) task, (ii) a large-scale dataset, MoMo-3DRoVLT, tailored for this task, and (iii) a custom neural model. Our dataset, generated with the aid of Large Language Models (LLMs) and manual verification, contains 8,216 video sequences annotated with both 2D and 3D bounding boxes, with each sequence accompanied by three freely generated, human-level textual descriptions. We propose MoMo-3DVLTracker, the first neural model specifically designed for MoMo-3DVLT. This model integrates a multimodal feature extractor, a visual language encoder-decoder, and modules for detection and tracking, setting a strong baseline for MoMo-3DVLT. Beyond existing paradigms, it introduces a task-specific structural coupling that integrates a differentiable linked-memory mechanism with depth-guided and language-conditioned reasoning for robust monocular 3D multi-object tracking. Experimental results demonstrate that our approach outperforms existing methods on the MoMo-3DRoVLT dataset. Our dataset and code are available at Github.
Published: 2026-02-10T21:07:05+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongkai Wei; Rong Wang; Haixiang Hu; Shijie Sun; Xiangyu Song; Mingtao Feng; Keyu Guo; Yongle Huang; Hua Cui; Naveed Akhtar&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661407"&gt;10.1109/tip.2026.3661407&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Visual Language Tracking (VLT) enables machines to perform tracking in real world through human-like language descriptions. However, existing VLT methods are limited to 2D spatial tracking or single-object 3D tracking and do not support multi-object 3D tracking within monocular video. This limitation arises because advancements in 3D multi-object tracking have predominantly relied on sensor-based data (e.g., point clouds, depth sensors) that lacks corresponding language descriptions. Moreover, natural language descriptions in existing VLT literature often suffer from redundancy, impeding the efficient and precise localization of multiple objects. We present the first technique to extend VLT to multi-object 3D tracking using monocular video. We introduce a comprehensive framework that includes (i) a Monocular Multi-object 3D Visual Language Tracking (MoMo-3DVLT) task, (ii) a large-scale dataset, MoMo-3DRoVLT, tailored for this task, and (iii) a custom neural model. Our dataset, generated with the aid of Large Language Models (LLMs) and manual verification, contains 8,216 video sequences annotated with both 2D and 3D bounding boxes, with each sequence accompanied by three freely generated, human-level textual descriptions. We propose MoMo-3DVLTracker, the first neural model specifically designed for MoMo-3DVLT. This model integrates a multimodal feature extractor, a visual language encoder-decoder, and modules for detection and tracking, setting a strong baseline for MoMo-3DVLT. Beyond existing paradigms, it introduces a task-specific structural coupling that integrates a differentiable linked-memory mechanism with depth-guided and language-conditioned reasoning for robust monocular 3D multi-object tracking. Experimental results demonstrate that our approach outperforms existing methods on the MoMo-3DRoVLT dataset. Our dataset and code are available at Github.&lt;/p&gt;</content:encoded></item><item><title>A Novel Implicit Cross-Attention Framework for RGB-T Object Detection</title><link>https://doi.org/10.1016/j.eswa.2026.131570</link><guid>10.1016/j.eswa.2026.131570</guid><pubDate>Mon, 09 Feb 2026 17:20:05 +0000</pubDate><dc:creator>Zinan Liu</dc:creator><dc:creator>Chunyu Zhu</dc:creator><dc:creator>Yachao Li</dc:creator><dc:creator>Pei Ye</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131570</prism:doi><description>RGB and thermal infrared image (RGB-T) fusion is critical for object detection in complex scenes. Mainstream detection methods typically employ a dual-branch architecture to perform multimodal feature fusion on discrete 2D feature grids. However, existing approaches fail to fully exploit the intrinsic continuity of visual signals in the physical world, thus limiting the scope and depth of cross-modal feature interactions. To address this limitation, this study proposes a novel I mplicit F eature C ross A ttention F usion (IFCAF) method, which aims to extend the multimodal fusion process from discrete space to continuous function space. Specifically, we introduce an implicit feature interaction module that uses Implicit Neural Representation (INR) to learn continuous mapping functions between cross-modal feature spaces, thereby establishing cross-modal feature correspondences. Moreover, we design an innovative cross-scale fusion strategy based on the continuity advantage of INR, which enables lossless alignment of features across scales, facilitates the global fusion of information across different semantic levels, and fully leverages the complementary information between scales to enhance fusion performance. As a flexible and easily integrable module, IFCAF can seamlessly be incorporated into various existing detection backbone networks. Experimental results on three publicly available RGB-T object detection datasets demonstrate that the proposed method outperforms current state-of-the-art techniques in detection accuracy and robustness, highlighting its superior generalizability and practical value. The code will be available at https://github.com/chunyuzhu/IFCAF .
Published: 2026-02-09T17:20:05+00:00
Venue: Expert Systems with Applications
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zinan Liu; Chunyu Zhu; Yachao Li; Pei Ye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131570"&gt;10.1016/j.eswa.2026.131570&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;RGB and thermal infrared image (RGB-T) fusion is critical for object detection in complex scenes. Mainstream detection methods typically employ a dual-branch architecture to perform multimodal feature fusion on discrete 2D feature grids. However, existing approaches fail to fully exploit the intrinsic continuity of visual signals in the physical world, thus limiting the scope and depth of cross-modal feature interactions. To address this limitation, this study proposes a novel I mplicit F eature C ross A ttention F usion (IFCAF) method, which aims to extend the multimodal fusion process from discrete space to continuous function space. Specifically, we introduce an implicit feature interaction module that uses Implicit Neural Representation (INR) to learn continuous mapping functions between cross-modal feature spaces, thereby establishing cross-modal feature correspondences. Moreover, we design an innovative cross-scale fusion strategy based on the continuity advantage of INR, which enables lossless alignment of features across scales, facilitates the global fusion of information across different semantic levels, and fully leverages the complementary information between scales to enhance fusion performance. As a flexible and easily integrable module, IFCAF can seamlessly be incorporated into various existing detection backbone networks. Experimental results on three publicly available RGB-T object detection datasets demonstrate that the proposed method outperforms current state-of-the-art techniques in detection accuracy and robustness, highlighting its superior generalizability and practical value. The code will be available at https://github.com/chunyuzhu/IFCAF .&lt;/p&gt;</content:encoded></item><item><title>Mobile-RetinaNet: A Lightweight Integrated Framework for Efficient Rotated Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/jstars.2026.3662490</link><guid>10.1109/jstars.2026.3662490</guid><pubDate>Mon, 09 Feb 2026 21:06:22 +0000</pubDate><dc:creator>Xin Lin</dc:creator><dc:creator>Junli Chen</dc:creator><dc:creator>Jing Liu</dc:creator><dc:creator>Tao Yi</dc:creator><dc:creator>Haolin Zhan</dc:creator><dc:creator>Guozhong Chen</dc:creator><dc:creator>Xiangcheng Wan</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3662490</prism:doi><description>Rotated object detection plays a vital role in remote sensing interpretation, with broad applications in urban planning, port monitoring, and disaster response. However, the significant scale variations, complex orientations, and cluttered backgrounds in remote sensing images pose considerable challenges to accurate detection. To address these issues, this paper proposes an efficient rotated object detection framework that integrates state space models with vision transformers to achieve an optimal balance between accuracy and computational efficiency.The proposed framework employs a MobileMamba backbone enhanced with a Multi-Receptive Field Feature Interaction (MRFFI) module for effective local-global feature representation. An EfficientViT-FPN neck enables efficient multi-scale feature fusion, while a refined Rotated RetinaNet head incorporates five-parameter rotated box regression with angle-aware constraints to improve orientation estimation. Comprehensive experiments on the DOTA-v1.0 and SRSDD-v1.0 datasets demonstrate that our approach achieves superior detection accuracy with significantly reduced computational overhead, making it particularly suitable for practical remote sensing applications
Published: 2026-02-09T21:06:22+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.798 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Lin; Junli Chen; Jing Liu; Tao Yi; Haolin Zhan; Guozhong Chen; Xiangcheng Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3662490"&gt;10.1109/jstars.2026.3662490&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.798 (must_read)&lt;/p&gt;
&lt;p&gt;Rotated object detection plays a vital role in remote sensing interpretation, with broad applications in urban planning, port monitoring, and disaster response. However, the significant scale variations, complex orientations, and cluttered backgrounds in remote sensing images pose considerable challenges to accurate detection. To address these issues, this paper proposes an efficient rotated object detection framework that integrates state space models with vision transformers to achieve an optimal balance between accuracy and computational efficiency.The proposed framework employs a MobileMamba backbone enhanced with a Multi-Receptive Field Feature Interaction (MRFFI) module for effective local-global feature representation. An EfficientViT-FPN neck enables efficient multi-scale feature fusion, while a refined Rotated RetinaNet head incorporates five-parameter rotated box regression with angle-aware constraints to improve orientation estimation. Comprehensive experiments on the DOTA-v1.0 and SRSDD-v1.0 datasets demonstrate that our approach achieves superior detection accuracy with significantly reduced computational overhead, making it particularly suitable for practical remote sensing applications&lt;/p&gt;</content:encoded></item><item><title>CCMANet: A Cross-Layer Cascade Network with Multi-Attention Mechanisms for Remote Sensing Object Detection</title><link>https://doi.org/10.1109/jstars.2026.3663387</link><guid>10.1109/jstars.2026.3663387</guid><pubDate>Tue, 10 Feb 2026 21:05:21 +0000</pubDate><dc:creator>Jinlong Mei</dc:creator><dc:creator>Wentao Lyu</dc:creator><dc:creator>Qing Guo</dc:creator><dc:creator>Yuzhen Xu</dc:creator><dc:creator>Zhijiang Deng</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3663387</prism:doi><description>Remote sensing object detection plays a vital role in both civilian applications and national defense security. However, remote sensing images typically exhibit characteristics such as wide coverage, significant variations in object scales, dense object distribution, and severe background interference. These factors greatly limit the applicability of existing detection methods in remote sensing scenarios. To address these challenges, this paper proposes a cross-layer cascade network with multi-attention mechanisms for remote sensing object detection (CCMANet). The proposed network incorporates different types of attention mechanisms at different stages to tackle background interference and dense multi-target detection, and leverages cross-layer cascading to progressively optimize feature representations, thereby achieving higher detection accuracy. Specifically, a multi-attention collaborative module is first introduced for feature filtering and suppression of complex backgrounds, highlighting useful remote sensing object features. Then, a maximum feature fusion module is employed in the feature fusion stage to enhance the diversity and representational capacity of the fused features. Finally, an improved dual-spatial pyramid pooling module combines two distinct spatial feature representations to further enrich target features in remote sensing images, ensuring that the dense and diverse remote sensing object information is preserved throughout the detection pipeline. Experiments on the DIOR, NWPU VHR-10, and RSOD datasets validate the effectiveness of the proposed method, achieving the highest mAP of 0.773, 0.952, and 0.973, respectively. Our code is available at https://github.com/meijinlong/CCMANet.
Published: 2026-02-10T21:05:21+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinlong Mei; Wentao Lyu; Qing Guo; Yuzhen Xu; Zhijiang Deng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3663387"&gt;10.1109/jstars.2026.3663387&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing object detection plays a vital role in both civilian applications and national defense security. However, remote sensing images typically exhibit characteristics such as wide coverage, significant variations in object scales, dense object distribution, and severe background interference. These factors greatly limit the applicability of existing detection methods in remote sensing scenarios. To address these challenges, this paper proposes a cross-layer cascade network with multi-attention mechanisms for remote sensing object detection (CCMANet). The proposed network incorporates different types of attention mechanisms at different stages to tackle background interference and dense multi-target detection, and leverages cross-layer cascading to progressively optimize feature representations, thereby achieving higher detection accuracy. Specifically, a multi-attention collaborative module is first introduced for feature filtering and suppression of complex backgrounds, highlighting useful remote sensing object features. Then, a maximum feature fusion module is employed in the feature fusion stage to enhance the diversity and representational capacity of the fused features. Finally, an improved dual-spatial pyramid pooling module combines two distinct spatial feature representations to further enrich target features in remote sensing images, ensuring that the dense and diverse remote sensing object information is preserved throughout the detection pipeline. Experiments on the DIOR, NWPU VHR-10, and RSOD datasets validate the effectiveness of the proposed method, achieving the highest mAP of 0.773, 0.952, and 0.973, respectively. Our code is available at https://github.com/meijinlong/CCMANet.&lt;/p&gt;</content:encoded></item><item><title>You Only Train Once: A Unified Framework for Both Full-Reference and No-Reference Image Quality Assessment</title><link>https://doi.org/10.1109/tip.2026.3661408</link><guid>10.1109/tip.2026.3661408</guid><pubDate>Tue, 10 Feb 2026 21:07:05 +0000</pubDate><dc:creator>Yi Ke Yun</dc:creator><dc:creator>Weisi Lin</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3661408</prism:doi><description>Existing Image Quality Assessment (IQA) models are limited to either full reference or no reference evaluation tasks, while humans can seamlessly switch between these assessment types. This motivates us to explore resolving these two tasks using a versatile model. In this work, we propose a novel framework that unifies full reference and no reference IQA. Our approach utilizes an encoder to extract multi-level features from images and introduces a Hierarchical Attention module to adaptively handle spatial distortions for both full reference and no reference inputs. Additionally, we develop a Semantic Distortion Aware module to analyze feature correlations between shallow and deep layers of the encoder, thereby accounting for the varying effects of different distortions on these layers. Our proposed framework achieves state-of-the-art performance for both full-reference and no-reference IQA tasks when trained separately. Furthermore, when the model is trained jointly on both types of tasks, it not only enhances performance in no-reference IQA but also maintains competitive results in full-reference IQA. This integrated approach facilitates a single training process that efficiently addresses both IQA tasks, representing a significant advancement in model versatility and performance.
Published: 2026-02-10T21:07:05+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yi Ke Yun; Weisi Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3661408"&gt;10.1109/tip.2026.3661408&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Existing Image Quality Assessment (IQA) models are limited to either full reference or no reference evaluation tasks, while humans can seamlessly switch between these assessment types. This motivates us to explore resolving these two tasks using a versatile model. In this work, we propose a novel framework that unifies full reference and no reference IQA. Our approach utilizes an encoder to extract multi-level features from images and introduces a Hierarchical Attention module to adaptively handle spatial distortions for both full reference and no reference inputs. Additionally, we develop a Semantic Distortion Aware module to analyze feature correlations between shallow and deep layers of the encoder, thereby accounting for the varying effects of different distortions on these layers. Our proposed framework achieves state-of-the-art performance for both full-reference and no-reference IQA tasks when trained separately. Furthermore, when the model is trained jointly on both types of tasks, it not only enhances performance in no-reference IQA but also maintains competitive results in full-reference IQA. This integrated approach facilitates a single training process that efficiently addresses both IQA tasks, representing a significant advancement in model versatility and performance.&lt;/p&gt;</content:encoded></item><item><title>LACT-Fusion: Linear Attention-Guided Cross-Modal Learning for Infrared and Visible Image Fusion</title><link>https://doi.org/10.1016/j.knosys.2026.115531</link><guid>10.1016/j.knosys.2026.115531</guid><pubDate>Tue, 10 Feb 2026 00:57:40 +0000</pubDate><dc:creator>Zhao Cai</dc:creator><dc:creator>Yong Ma</dc:creator><dc:creator>Qi Peng</dc:creator><dc:creator>Weizhong Li</dc:creator><dc:creator>Ge Wang</dc:creator><dc:creator>Jun Huang</dc:creator><dc:creator>Fan Fan</dc:creator><dc:creator>Qian Wang</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115531</prism:doi><description>Infrared and visible image fusion aims to extract intrinsic features from both modalities and generate high-quality images that preserve complementary information. Despite the success of Transformer-based image fusion methods in modeling global dependencies, they inherently lack local inductive biases, often resulting in the loss of fine-grained details in the fused images. Moreover, adaptive interaction across modalities remains suboptimal, limiting the preservation of modality-specific information during fusion. To address these challenges, we propose LACT-Fusion, a novel fusion framework based on Transformer. Specifically, a linear attention module with an auxiliary matrix is developed to replace the conventional self-attention mechanism, effectively reducing computational complexity while improving the adaptive modeling of complementary features from different modalities. In addition, a Local Attention-based Multi-scale Feature Enhancement Block (LFEB) is designed to strengthen texture and structural representation, enhancing the clarity and fidelity of the fused images. Extensive experiments on multiple public datasets demonstrate that LACT-Fusion consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, confirming its superior fusion performance and strong potential for practical applications. The sources code will be published in https://github.com/zc617/LACTFusion .
Published: 2026-02-10T00:57:40+00:00
Venue: Knowledge-Based Systems
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhao Cai; Yong Ma; Qi Peng; Weizhong Li; Ge Wang; Jun Huang; Fan Fan; Qian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115531"&gt;10.1016/j.knosys.2026.115531&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared and visible image fusion aims to extract intrinsic features from both modalities and generate high-quality images that preserve complementary information. Despite the success of Transformer-based image fusion methods in modeling global dependencies, they inherently lack local inductive biases, often resulting in the loss of fine-grained details in the fused images. Moreover, adaptive interaction across modalities remains suboptimal, limiting the preservation of modality-specific information during fusion. To address these challenges, we propose LACT-Fusion, a novel fusion framework based on Transformer. Specifically, a linear attention module with an auxiliary matrix is developed to replace the conventional self-attention mechanism, effectively reducing computational complexity while improving the adaptive modeling of complementary features from different modalities. In addition, a Local Attention-based Multi-scale Feature Enhancement Block (LFEB) is designed to strengthen texture and structural representation, enhancing the clarity and fidelity of the fused images. Extensive experiments on multiple public datasets demonstrate that LACT-Fusion consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, confirming its superior fusion performance and strong potential for practical applications. The sources code will be published in https://github.com/zc617/LACTFusion .&lt;/p&gt;</content:encoded></item><item><title>MambaFlow: A Novel and Flow-Guided State Space Model for Scene Flow Estimation</title><link>https://doi.org/10.1109/tiv.2026.3663171</link><guid>10.1109/tiv.2026.3663171</guid><pubDate>Tue, 10 Feb 2026 21:06:37 +0000</pubDate><dc:creator>Jiehao Luo</dc:creator><dc:creator>Jintao Cheng</dc:creator><dc:creator>Qingwen Zhang</dc:creator><dc:creator>Bohuan Xue</dc:creator><dc:creator>Rui Fan</dc:creator><dc:creator>Xiaoyu Tang</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Vehicles</prism:publicationName><prism:doi>10.1109/tiv.2026.3663171</prism:doi><description>Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to devoxelize shared voxel representations into point-wise features. To further enhance the model's generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion patterns. Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios. The code is available at https://github.com/SCNU-RISLAB/MambaFlow.
Published: 2026-02-10T21:06:37+00:00
Venue: IEEE Transactions on Intelligent Vehicles
Score: 0.795 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiehao Luo; Jintao Cheng; Qingwen Zhang; Bohuan Xue; Rui Fan; Xiaoyu Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Vehicles&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tiv.2026.3663171"&gt;10.1109/tiv.2026.3663171&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.795 (must_read)&lt;/p&gt;
&lt;p&gt;Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to devoxelize shared voxel representations into point-wise features. To further enhance the model&amp;#x27;s generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion patterns. Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios. The code is available at https://github.com/SCNU-RISLAB/MambaFlow.&lt;/p&gt;</content:encoded></item><item><title>Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion</title><link>https://doi.org/10.1109/tip.2026.3660576</link><guid>10.1109/tip.2026.3660576</guid><pubDate>Mon, 09 Feb 2026 21:10:16 +0000</pubDate><dc:creator>Zhiwen Yang</dc:creator><dc:creator>Yuxin Peng</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3660576</prism:doi><description>Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a Multi-Resolution Alignment (MRA) approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. Extensive experiments on the SemanticKITTI and SSCBench-KITTI-360 datasets demonstrate that our MRA approach significantly outperforms existing state-of-the-art methods, showcasing its effectiveness in mitigating the impact of sparse voxel labels. The code is available at https://github.com/PKU-ICST-MIPL/MRA_TIP.
Published: 2026-02-09T21:10:16+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiwen Yang; Yuxin Peng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3660576"&gt;10.1109/tip.2026.3660576&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a Multi-Resolution Alignment (MRA) approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. Extensive experiments on the SemanticKITTI and SSCBench-KITTI-360 datasets demonstrate that our MRA approach significantly outperforms existing state-of-the-art methods, showcasing its effectiveness in mitigating the impact of sparse voxel labels. The code is available at https://github.com/PKU-ICST-MIPL/MRA_TIP.&lt;/p&gt;</content:encoded></item><item><title>AquaticCLIP: A Vision-Language Foundation Model and Dataset for Underwater Scene Analysis</title><link>https://doi.org/10.1109/tnnls.2026.3657138</link><guid>10.1109/tnnls.2026.3657138</guid><pubDate>Mon, 09 Feb 2026 21:07:11 +0000</pubDate><dc:creator>Basit Alawode</dc:creator><dc:creator>Iyyakutti Iyappan Ganapathi</dc:creator><dc:creator>Sajid Javed</dc:creator><dc:creator>Mohammed Bennamoun</dc:creator><dc:creator>Arif Mahmood</dc:creator><prism:publicationName>IEEE Transactions on Neural Networks and Learning Systems</prism:publicationName><prism:doi>10.1109/tnnls.2026.3657138</prism:doi><description>The preservation of aquatic biodiversity is critical in mitigating the effects of climate change. Aquatic scene understanding plays a pivotal role in aiding marine scientists in their decision-making processes. In this article, we introduce AquaticCLIP, a novel contrastive language-image pretraining (CLIP) model tailored for aquatic scene understanding. AquaticCLIP presents an underwater domain-specific learning framework that aligns images and texts in aquatic environments, enabling tasks such as segmentation, classification, detection, and object counting. By leveraging our large-scale underwater image-text paired dataset without the need for ground-truth (GT) annotations, our model enriches existing vision-language models (VLMs) in the aquatic domain. For this purpose, we construct a 2-million underwater image-text paired dataset using heterogeneous resources, including YouTube, Netflix, National Geographic (NatGeo), etc. To fine-tune AquaticCLIP, we propose a prompt-guided vision encoder (PGVE) that progressively aggregates patch features via learnable prompts, while a vision-guided mechanism enhances the language encoder by incorporating visual context. The model is optimized through a contrastive pretraining loss to align visual and textual modalities. AquaticCLIP achieves notable performance improvements in zero-shot settings across multiple underwater computer vision tasks, outperforming existing methods in both accuracy and robustness. Our model sets a new benchmark for vision-language applications in underwater environments. The code and dataset for AquaticCLIP are publicly available on GitHub at: https://github.com/BasitAlawode/AquaticCLIP
Published: 2026-02-09T21:07:11+00:00
Venue: IEEE Transactions on Neural Networks and Learning Systems
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Basit Alawode; Iyyakutti Iyappan Ganapathi; Sajid Javed; Mohammed Bennamoun; Arif Mahmood&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Neural Networks and Learning Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tnnls.2026.3657138"&gt;10.1109/tnnls.2026.3657138&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;The preservation of aquatic biodiversity is critical in mitigating the effects of climate change. Aquatic scene understanding plays a pivotal role in aiding marine scientists in their decision-making processes. In this article, we introduce AquaticCLIP, a novel contrastive language-image pretraining (CLIP) model tailored for aquatic scene understanding. AquaticCLIP presents an underwater domain-specific learning framework that aligns images and texts in aquatic environments, enabling tasks such as segmentation, classification, detection, and object counting. By leveraging our large-scale underwater image-text paired dataset without the need for ground-truth (GT) annotations, our model enriches existing vision-language models (VLMs) in the aquatic domain. For this purpose, we construct a 2-million underwater image-text paired dataset using heterogeneous resources, including YouTube, Netflix, National Geographic (NatGeo), etc. To fine-tune AquaticCLIP, we propose a prompt-guided vision encoder (PGVE) that progressively aggregates patch features via learnable prompts, while a vision-guided mechanism enhances the language encoder by incorporating visual context. The model is optimized through a contrastive pretraining loss to align visual and textual modalities. AquaticCLIP achieves notable performance improvements in zero-shot settings across multiple underwater computer vision tasks, outperforming existing methods in both accuracy and robustness. Our model sets a new benchmark for vision-language applications in underwater environments. The code and dataset for AquaticCLIP are publicly available on GitHub at: https://github.com/BasitAlawode/AquaticCLIP&lt;/p&gt;</content:encoded></item><item><title>Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency</title><link>https://doi.org/10.1109/tpami.2026.3662389</link><guid>10.1109/tpami.2026.3662389</guid><pubDate>Mon, 09 Feb 2026 21:05:10 +0000</pubDate><dc:creator>Yanbiao Ma</dc:creator><dc:creator>Wei Dai</dc:creator><dc:creator>Zhiwu Lu</dc:creator><dc:creator>Bowei Liu</dc:creator><dc:creator>Jiayi Chen</dc:creator><dc:creator>Wenke Huang</dc:creator><dc:creator>Junchi Yan</dc:creator><dc:creator>Guancheng Wan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3662389</prism:doi><description>Despite the fast progress of deep learning, one standing challenge is the gap of the observed training samples and the underlying true distribution. There are multiple reasons for the causing of this gap e.g., sampling bias, noise etc. In the era of foundation models, we show that when leveraging the off-the-shelf (vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the geometric shapes of the resulting feature distributions exhibit remarkable transferability across domains and datasets. To verify its practical usefulness, we embody our geometric knowledge-guided distribution calibration framework in two popular and challenging settings: federated learning and long-tailed recognition. In the federated setting, we devise a technique of acquiring the global geometric shape under privacy constraints, then leverage this knowledge to generate new samples for clients, in the aim of bridging the gap between local and global observations. In long-tailed learning, it utilizes the geometric knowledge transferred from sample-rich categories to recover the true distribution for sample-scarce tail classes. Comprehensive experiments show that our proposed geometric knowledge-guided distribution calibration effectively overcomes information deficits caused by data heterogeneity and sample imbalance, with boosted performance across benchmarks. Code published at: https://github.com/WeiDai-David/2025CVPR GGEUR.
Published: 2026-02-09T21:05:10+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yanbiao Ma; Wei Dai; Zhiwu Lu; Bowei Liu; Jiayi Chen; Wenke Huang; Junchi Yan; Guancheng Wan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3662389"&gt;10.1109/tpami.2026.3662389&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Despite the fast progress of deep learning, one standing challenge is the gap of the observed training samples and the underlying true distribution. There are multiple reasons for the causing of this gap e.g., sampling bias, noise etc. In the era of foundation models, we show that when leveraging the off-the-shelf (vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the geometric shapes of the resulting feature distributions exhibit remarkable transferability across domains and datasets. To verify its practical usefulness, we embody our geometric knowledge-guided distribution calibration framework in two popular and challenging settings: federated learning and long-tailed recognition. In the federated setting, we devise a technique of acquiring the global geometric shape under privacy constraints, then leverage this knowledge to generate new samples for clients, in the aim of bridging the gap between local and global observations. In long-tailed learning, it utilizes the geometric knowledge transferred from sample-rich categories to recover the true distribution for sample-scarce tail classes. Comprehensive experiments show that our proposed geometric knowledge-guided distribution calibration effectively overcomes information deficits caused by data heterogeneity and sample imbalance, with boosted performance across benchmarks. Code published at: https://github.com/WeiDai-David/2025CVPR GGEUR.&lt;/p&gt;</content:encoded></item><item><title>GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration</title><link>https://doi.org/10.1109/tgrs.2026.3663235</link><guid>10.1109/tgrs.2026.3663235</guid><pubDate>Tue, 10 Feb 2026 21:05:00 +0000</pubDate><dc:creator>Juanqin Liu</dc:creator><dc:creator>Leonardo Plotegher</dc:creator><dc:creator>Eloy Roura</dc:creator><dc:creator>Shaoming He</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3663235</prism:doi><description>The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.
Published: 2026-02-10T21:05:00+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Juanqin Liu; Leonardo Plotegher; Eloy Roura; Shaoming He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3663235"&gt;10.1109/tgrs.2026.3663235&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.&lt;/p&gt;</content:encoded></item><item><title>Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval</title><link>https://arxiv.org/abs/2602.08224v2</link><guid>http://arxiv.org/abs/2602.08224v2</guid><pubDate>Mon, 09 Feb 2026 02:58:33 +0000</pubDate><dc:creator>Jing Zhang</dc:creator><dc:creator>Zhikai Li</dc:creator><dc:creator>Xuewen Liu</dc:creator><dc:creator>Qingyi Gu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.
Published: 2026-02-09T02:58:33+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jing Zhang; Zhikai Li; Xuewen Liu; Qingyi Gu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Vision Transformer with Shift Expansion Linear Attention for Image Classification and Object Tracking</title><link>https://doi.org/10.1109/tcsvt.2026.3662708</link><guid>10.1109/tcsvt.2026.3662708</guid><pubDate>Mon, 09 Feb 2026 21:09:48 +0000</pubDate><dc:creator>Sai Zhou</dc:creator><dc:creator>Meiqin Liu</dc:creator><dc:creator>Jing Zhou</dc:creator><dc:creator>Ronghao Zheng</dc:creator><prism:publicationName>IEEE Transactions on Circuits and Systems for Video Technology</prism:publicationName><prism:doi>10.1109/tcsvt.2026.3662708</prism:doi><description>As an effective feature extractor, Vision Transformer (ViT) has been widely applied to both image classification and object tracking tasks. In this paper, we revisit and enhance the classic Data-efficient image Transformer (DeiT) for these two tasks. The DeiT is optimized step-by-step across different modules, including its patch stem, position embedding, and the development of efficient linear attention mechanisms. To address the performance degradation of linear attention, we propose Shift Expansion Linear Attention (SELA) which generates new heads with rich feature diversity through a simple but efficient cyclic shift operation. Additionally, SELA similarity minimization is added to cross-entropy loss to further enhance feature diversity. Based on these improvements, we develop SELA-ViT for image classification and further build SELA-Track for object tracking. With comparable model size and speed, SELA-ViT-T achieves a +4.8% improvement in Top-1 accuracy over DeiT-T on ImageNet-1K and establishes a new state-of-the-art performance among linear attention methods. Furthermore, we validate SELA-ViT on five small datasets. On four benchmark object tracking datasets, SELA-Track exhibits improved tracking performance. The code and models are available at: https://github.com/saizhou777/SELA-ViT.
Published: 2026-02-09T21:09:48+00:00
Venue: IEEE Transactions on Circuits and Systems for Video Technology
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Sai Zhou; Meiqin Liu; Jing Zhou; Ronghao Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Circuits and Systems for Video Technology&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcsvt.2026.3662708"&gt;10.1109/tcsvt.2026.3662708&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;As an effective feature extractor, Vision Transformer (ViT) has been widely applied to both image classification and object tracking tasks. In this paper, we revisit and enhance the classic Data-efficient image Transformer (DeiT) for these two tasks. The DeiT is optimized step-by-step across different modules, including its patch stem, position embedding, and the development of efficient linear attention mechanisms. To address the performance degradation of linear attention, we propose Shift Expansion Linear Attention (SELA) which generates new heads with rich feature diversity through a simple but efficient cyclic shift operation. Additionally, SELA similarity minimization is added to cross-entropy loss to further enhance feature diversity. Based on these improvements, we develop SELA-ViT for image classification and further build SELA-Track for object tracking. With comparable model size and speed, SELA-ViT-T achieves a +4.8% improvement in Top-1 accuracy over DeiT-T on ImageNet-1K and establishes a new state-of-the-art performance among linear attention methods. Furthermore, we validate SELA-ViT on five small datasets. On four benchmark object tracking datasets, SELA-Track exhibits improved tracking performance. The code and models are available at: https://github.com/saizhou777/SELA-ViT.&lt;/p&gt;</content:encoded></item><item><title>VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization</title><link>https://arxiv.org/abs/2602.09934v1</link><guid>http://arxiv.org/abs/2602.09934v1</guid><pubDate>Tue, 10 Feb 2026 16:08:19 +0000</pubDate><dc:creator>Yikun Liu</dc:creator><dc:creator>Yuan Liu</dc:creator><dc:creator>Shangzhe Di</dc:creator><dc:creator>Haicheng Wang</dc:creator><dc:creator>Zhongyin Zhao</dc:creator><dc:creator>Le Tian</dc:creator><dc:creator>Xiao Zhou</dc:creator><dc:creator>Jie Zhou</dc:creator><dc:creator>Jiangchao Yao</dc:creator><dc:creator>Yanfeng Wang</dc:creator><dc:creator>Weidi Xie</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.
Published: 2026-02-10T16:08:19+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yikun Liu; Yuan Liu; Shangzhe Di; Haicheng Wang; Zhongyin Zhao; Le Tian; Xiao Zhou; Jie Zhou; Jiangchao Yao; Yanfeng Wang; Weidi Xie&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.&lt;/p&gt;</content:encoded></item><item><title>Multi-Masking Strategies for Self-Supervised Low- and High-Level Text Representation Learning</title><link>https://doi.org/10.1016/j.patcog.2026.113273</link><guid>10.1016/j.patcog.2026.113273</guid><pubDate>Mon, 09 Feb 2026 08:03:22 +0000</pubDate><dc:creator>Zhengmi Tang</dc:creator><dc:creator>Yuto Mitsui</dc:creator><dc:creator>Tomo Miyazaki</dc:creator><dc:creator>Shinichiro Omachi</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113273</prism:doi><description>Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level text representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.
Published: 2026-02-09T08:03:22+00:00
Venue: Pattern Recognition
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhengmi Tang; Yuto Mitsui; Tomo Miyazaki; Shinichiro Omachi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113273"&gt;10.1016/j.patcog.2026.113273&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level text representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.&lt;/p&gt;</content:encoded></item><item><title>rePIRL: Learn PRM with Inverse RL for LLM Reasoning</title><link>https://arxiv.org/abs/2602.07832v1</link><guid>http://arxiv.org/abs/2602.07832v1</guid><pubDate>Sun, 08 Feb 2026 05:47:27 +0000</pubDate><dc:creator>Xian Wu</dc:creator><dc:creator>Kaijie Zhu</dc:creator><dc:creator>Ying Zhang</dc:creator><dc:creator>Lun Wang</dc:creator><dc:creator>Wenbo Guo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.
Published: 2026-02-08T05:47:27+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xian Wu; Kaijie Zhu; Ying Zhang; Lun Wang; Wenbo Guo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.&lt;/p&gt;</content:encoded></item></channel></rss>