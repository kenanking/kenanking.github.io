<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 04 Feb 2026 03:34:18 +0000</lastBuildDate><item><title>Self-supervised global − local collaborative network for real SAR despeckling</title><link>https://doi.org/10.1016/j.jag.2026.105135</link><guid>10.1016/j.jag.2026.105135</guid><pubDate>Tue, 03 Feb 2026 01:02:06 +0000</pubDate><dc:creator>Yang Yang</dc:creator><dc:creator>Jiangong Xu</dc:creator><dc:creator>Yuchuan Bai</dc:creator><dc:creator>Liangyu Chen</dc:creator><dc:creator>Junli Li</dc:creator><dc:creator>Jun Pan</dc:creator><dc:creator>Mi Wang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105135</prism:doi><description>Synthetic Aperture Radar (SAR) is crucial for Earth observation because it can acquire high-resolution images in all weather conditions. However, the presence of speckles—an inherent multiplicative noise caused by the coherent imaging process—severely degrades image quality and impairs the performance of subsequent interpretation tasks. To effectively capture both global contextual cues and fine-grained structural details in SAR image despeckling, we design a dual-branch Global-Local Collaborative Network (GLCNet) based on blind-spot convolution. GLCNet is trained in a self-supervised manner, requiring only original images for learning, making it well-suited for SAR data without ground truth. In the global branch, the SAR image is first decomposed into multiple frequency sub-bands through a Wavelet-Shuffle Downsampling (WSD), which decorrelates speckle components across scales and frequencies. A multi-scale blind-spot convolution is then applied to each sub-band in parallel, enabling the extraction of global textures without introducing speckle bias. In contrast, the local branch focuses on structure-aware restoration by jointly modeling frequency and spatial priors. By leveraging neighboring-pixel dependencies, this branch enhances local detail recovery and edge sharpness. Finally, an adaptive Detail-Guided Module (DGM) dynamically integrates complementary features from both branches, ensuring a harmonious balance between texture smoothness and structural fidelity. The proposed method is validated using various SAR sensors, including Sentinel-1, GF-3, TerraSAR-X, and Capella-X, demonstrating its superiority over traditional and deep learning approaches. Additionally, the application analysis confirms that the method enhances both the visual quality and analytical reliability of SAR images, making it a valuable preprocessing step for real-world scenarios. For reproducibility, our code and data are available at https://github.com/yangyang12318/LGCN.
Published: 2026-02-03T01:02:06+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.817 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Yang; Jiangong Xu; Yuchuan Bai; Liangyu Chen; Junli Li; Jun Pan; Mi Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105135"&gt;10.1016/j.jag.2026.105135&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.817 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) is crucial for Earth observation because it can acquire high-resolution images in all weather conditions. However, the presence of speckles—an inherent multiplicative noise caused by the coherent imaging process—severely degrades image quality and impairs the performance of subsequent interpretation tasks. To effectively capture both global contextual cues and fine-grained structural details in SAR image despeckling, we design a dual-branch Global-Local Collaborative Network (GLCNet) based on blind-spot convolution. GLCNet is trained in a self-supervised manner, requiring only original images for learning, making it well-suited for SAR data without ground truth. In the global branch, the SAR image is first decomposed into multiple frequency sub-bands through a Wavelet-Shuffle Downsampling (WSD), which decorrelates speckle components across scales and frequencies. A multi-scale blind-spot convolution is then applied to each sub-band in parallel, enabling the extraction of global textures without introducing speckle bias. In contrast, the local branch focuses on structure-aware restoration by jointly modeling frequency and spatial priors. By leveraging neighboring-pixel dependencies, this branch enhances local detail recovery and edge sharpness. Finally, an adaptive Detail-Guided Module (DGM) dynamically integrates complementary features from both branches, ensuring a harmonious balance between texture smoothness and structural fidelity. The proposed method is validated using various SAR sensors, including Sentinel-1, GF-3, TerraSAR-X, and Capella-X, demonstrating its superiority over traditional and deep learning approaches. Additionally, the application analysis confirms that the method enhances both the visual quality and analytical reliability of SAR images, making it a valuable preprocessing step for real-world scenarios. For reproducibility, our code and data are available at https://github.com/yangyang12318/LGCN.&lt;/p&gt;</content:encoded></item><item><title>Dynamic High-frequency Convolution for Infrared Small Target Detection</title><link>https://arxiv.org/abs/2602.02969v1</link><guid>http://arxiv.org/abs/2602.02969v1</guid><pubDate>Tue, 03 Feb 2026 01:07:55 +0000</pubDate><dc:creator>Ruojing Li</dc:creator><dc:creator>Chao Xiao</dc:creator><dc:creator>Qian Yin</dc:creator><dc:creator>Wei An</dc:creator><dc:creator>Nuo Chen</dc:creator><dc:creator>Xinyi Ying</dc:creator><dc:creator>Miao Li</dc:creator><dc:creator>Yingqian Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TCSVT.2026.3661285</prism:doi><description>Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.
Published: 2026-02-03T01:07:55+00:00
Venue: arXiv
Score: 0.802 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruojing Li; Chao Xiao; Qian Yin; Wei An; Nuo Chen; Xinyi Ying; Miao Li; Yingqian Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TCSVT.2026.3661285"&gt;10.1109/TCSVT.2026.3661285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.802 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.&lt;/p&gt;</content:encoded></item><item><title>Multi-Scale Pattern-Aware Task-Gating Network for Aerial Small Object Detection</title><link>https://doi.org/10.1016/j.neunet.2026.108680</link><guid>10.1016/j.neunet.2026.108680</guid><pubDate>Tue, 03 Feb 2026 07:45:19 +0000</pubDate><dc:creator>Ben Liang</dc:creator><dc:creator>Yuan Liu</dc:creator><dc:creator>Chao Sui</dc:creator><dc:creator>Yihong Wang</dc:creator><dc:creator>Lin Xiao</dc:creator><dc:creator>Xiubao Sui</dc:creator><dc:creator>Qian Chen</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2026.108680</prism:doi><description>With the advancement of high-precision remote sensing equipment and precision measurement technology, object detection based on remote sensing images (RSIs) has been widely used in military and civilian fields. Different from traditional general-purpose environments, remote sensing presents unique challenges that significantly complicate the detection process. Specifically: (1) RSIs cover extensive monitoring areas, resulting in complex and textured backgrounds; and (2) objects often exhibit cluttered distributions, small sizes, and considerable scale variations across categories. To effectively address these challenges, we propose a Multi-Scale Pattern-Aware Task-Gating Network (MPTNet) for remote sensing object detection. First, we design a Multi-Scale Pattern-Aware Network (MPNet) backbone that employs a small and large kernel convolutional complementary strategy to capture both large-scale and small-scale spatial patterns, yielding more comprehensive semantic features. Next, we introduce a Multi-Head Cross-Space Encoder (MCE) that improves semantic fusion and spatial representation across hierarchical levels. By combining a multi-head mechanism with directional one-dimensional strip convolutions, MCE enhances spatial sensitivity at the pixel level, thus improving object localization in densely textured scenes. To harmonize cross-task synergy, we propose a Dynamic Task-Gating (DTG) head that adaptively recalibrates spatial feature representations between classification and localization branches. Extensive experimental validations on three publicly available datasets, including VisDrone, DIOR, and COCO-mini, demonstrate that our method achieves excellent performance, obtaining AP 50 scores of 43.3%, 80.6%, and 49.5%, respectively.
Published: 2026-02-03T07:45:19+00:00
Venue: Neural Networks
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ben Liang; Yuan Liu; Chao Sui; Yihong Wang; Lin Xiao; Xiubao Sui; Qian Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2026.108680"&gt;10.1016/j.neunet.2026.108680&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;With the advancement of high-precision remote sensing equipment and precision measurement technology, object detection based on remote sensing images (RSIs) has been widely used in military and civilian fields. Different from traditional general-purpose environments, remote sensing presents unique challenges that significantly complicate the detection process. Specifically: (1) RSIs cover extensive monitoring areas, resulting in complex and textured backgrounds; and (2) objects often exhibit cluttered distributions, small sizes, and considerable scale variations across categories. To effectively address these challenges, we propose a Multi-Scale Pattern-Aware Task-Gating Network (MPTNet) for remote sensing object detection. First, we design a Multi-Scale Pattern-Aware Network (MPNet) backbone that employs a small and large kernel convolutional complementary strategy to capture both large-scale and small-scale spatial patterns, yielding more comprehensive semantic features. Next, we introduce a Multi-Head Cross-Space Encoder (MCE) that improves semantic fusion and spatial representation across hierarchical levels. By combining a multi-head mechanism with directional one-dimensional strip convolutions, MCE enhances spatial sensitivity at the pixel level, thus improving object localization in densely textured scenes. To harmonize cross-task synergy, we propose a Dynamic Task-Gating (DTG) head that adaptively recalibrates spatial feature representations between classification and localization branches. Extensive experimental validations on three publicly available datasets, including VisDrone, DIOR, and COCO-mini, demonstrate that our method achieves excellent performance, obtaining AP 50 scores of 43.3%, 80.6%, and 49.5%, respectively.&lt;/p&gt;</content:encoded></item><item><title>PARTNER: Level Up the Polar Representation for 3D Object Detection</title><link>https://doi.org/10.1007/s11263-026-02735-0</link><guid>10.1007/s11263-026-02735-0</guid><pubDate>Tue, 03 Feb 2026 07:41:43 +0000</pubDate><dc:creator>Ming Nie</dc:creator><dc:creator>Chunwei Wang</dc:creator><dc:creator>Hang Xu</dc:creator><dc:creator>Li Zhang</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-026-02735-0</prism:doi><description>Polar-based representation has shown promising properties in perceptual tasks. Unlike Cartesian-based approaches, which divide space into a uniform grid regardless of the uneven distribution of the foreground along the radial direction, representing space as polar coordinates aligns more closely with the physical properties of sensors, whether LiDAR or surround cameras. Moreover, polar-based methods are recognized as a superior alternative due to (1) their advantages in robust performance across different resolutions and (2) their efficacy in streaming-based approaches. However, state-of-the-art polar-based detection methods inevitably suffer from the feature distortion problem because of the non-uniform division of polar representation, resulting in a non-negligible performance gap compared to Cartesian-based approaches. To tackle this issue, we present PARTNER, a novel 3D object detector in the polar coordinate. PARTNER alleviates the dilemma of feature distortion with global representation re-alignment and facilitates the regression by introducing instance-level geometric information into the detection head. Building upon this, we further propose a novel polar-coordinate-based PV-to-BEV view transformation module, enabling a unified framework for multi-modal detection in polar space. Extensive experiments demonstrate the superiority of our method in streaming-based detection and across varying resolutions, outperforming prior polar-based approaches on Waymo, nuScenes and ONCE. Beyond empirical results, we also explore whether more effective partitioning strategies and regression targets can be designed specifically for the polar coordinate system. We provide in-depth insights into the nature of feature distortion in polar space and present visualizations that demonstrate the corrective effects of our proposed modules, further validating the design rationale and effectiveness of our approach.
Published: 2026-02-03T07:41:43+00:00
Venue: International Journal of Computer Vision
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Nie; Chunwei Wang; Hang Xu; Li Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-026-02735-0"&gt;10.1007/s11263-026-02735-0&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Polar-based representation has shown promising properties in perceptual tasks. Unlike Cartesian-based approaches, which divide space into a uniform grid regardless of the uneven distribution of the foreground along the radial direction, representing space as polar coordinates aligns more closely with the physical properties of sensors, whether LiDAR or surround cameras. Moreover, polar-based methods are recognized as a superior alternative due to (1) their advantages in robust performance across different resolutions and (2) their efficacy in streaming-based approaches. However, state-of-the-art polar-based detection methods inevitably suffer from the feature distortion problem because of the non-uniform division of polar representation, resulting in a non-negligible performance gap compared to Cartesian-based approaches. To tackle this issue, we present PARTNER, a novel 3D object detector in the polar coordinate. PARTNER alleviates the dilemma of feature distortion with global representation re-alignment and facilitates the regression by introducing instance-level geometric information into the detection head. Building upon this, we further propose a novel polar-coordinate-based PV-to-BEV view transformation module, enabling a unified framework for multi-modal detection in polar space. Extensive experiments demonstrate the superiority of our method in streaming-based detection and across varying resolutions, outperforming prior polar-based approaches on Waymo, nuScenes and ONCE. Beyond empirical results, we also explore whether more effective partitioning strategies and regression targets can be designed specifically for the polar coordinate system. We provide in-depth insights into the nature of feature distortion in polar space and present visualizations that demonstrate the corrective effects of our proposed modules, further validating the design rationale and effectiveness of our approach.&lt;/p&gt;</content:encoded></item><item><title>BIMIM: Band-Independent Masked Image Modeling with Transformer for Multi-Spectral Satellite Imagery</title><link>https://doi.org/10.1109/jstars.2026.3660330</link><guid>10.1109/jstars.2026.3660330</guid><pubDate>Mon, 02 Feb 2026 20:44:32 +0000</pubDate><dc:creator>Jia Song</dc:creator><dc:creator>Luosheng Xia</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3660330</prism:doi><description>Self-supervised learning (SSL) offers a promising solution to reduce reliance on labeled data. Among SSL approaches, Masked Image Modeling (MIM) has demonstrated significant potential in remote sensing applications such as scene classification and semantic segmentation, owing to its ability to capture pixel-level details. However, existing MIM frameworks, originally designed for natural images, struggle to adapt to the spectral-spatial characteristics of multi-spectral satellite imagery. While recent studies have introduced spectral-enhanced MIM SSL methods, most rely on band-group embedding, which imposes constraints on band utilization flexibility in downstream fine-tuning tasks and limits the granularity of spectral feature learning. To address these challenges, this study proposes Band-Independent Masked Image Modeling (BIMIM) with Transformer, a novel self-supervised learning framework specifically designed for multi-spectral satellite imagery. BIMIM not only enables finer band-specific spectral feature extraction, allowing for more effective capture of subtle spectral variations, but also introduces spatially random masking at the single-band level, facilitating more efficient inter-band feature learning. Extensive experiments on publicly available remote sensing datasets demonstrate that BIMIM achieves state-of-the-art performance in downstream tasks such as scene classification and semantic segmentation. This study provides a new perspective on SSL for multi-spectral remote sensing, paving the way for more effective spectral-spatial feature extraction and adaptation in self-supervised learning frameworks.
Published: 2026-02-02T20:44:32+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jia Song; Luosheng Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3660330"&gt;10.1109/jstars.2026.3660330&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised learning (SSL) offers a promising solution to reduce reliance on labeled data. Among SSL approaches, Masked Image Modeling (MIM) has demonstrated significant potential in remote sensing applications such as scene classification and semantic segmentation, owing to its ability to capture pixel-level details. However, existing MIM frameworks, originally designed for natural images, struggle to adapt to the spectral-spatial characteristics of multi-spectral satellite imagery. While recent studies have introduced spectral-enhanced MIM SSL methods, most rely on band-group embedding, which imposes constraints on band utilization flexibility in downstream fine-tuning tasks and limits the granularity of spectral feature learning. To address these challenges, this study proposes Band-Independent Masked Image Modeling (BIMIM) with Transformer, a novel self-supervised learning framework specifically designed for multi-spectral satellite imagery. BIMIM not only enables finer band-specific spectral feature extraction, allowing for more effective capture of subtle spectral variations, but also introduces spatially random masking at the single-band level, facilitating more efficient inter-band feature learning. Extensive experiments on publicly available remote sensing datasets demonstrate that BIMIM achieves state-of-the-art performance in downstream tasks such as scene classification and semantic segmentation. This study provides a new perspective on SSL for multi-spectral remote sensing, paving the way for more effective spectral-spatial feature extraction and adaptation in self-supervised learning frameworks.&lt;/p&gt;</content:encoded></item><item><title>Learning Modality Knowledge with Proxy for RGB-Infrared Object Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113227</link><guid>10.1016/j.patcog.2026.113227</guid><pubDate>Tue, 03 Feb 2026 07:42:22 +0000</pubDate><dc:creator>You Ma</dc:creator><dc:creator>Lin Chai</dc:creator><dc:creator>Shihan Mao</dc:creator><dc:creator>Yucheng Zhang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113227</prism:doi><description>RGB-infrared object detection aims to improve detection performance in complex environments by integrating complementary information from RGB and infrared images. While transformer-based methods have demonstrated significant advancements in this field by directly modeling dense relationships between modality tokens to enable cross-modality long-range interactions, they neglect the inherent discrepancies in feature distributions across modalities. Such discrepancies attenuate the reliability of the established relationships, thereby restricting the effective exploitation of complementary information between modalities. To alleviate this problem, we propose a framework for learning modality knowledge with proxy. The core innovation lies in the design of a proxy-guided cross-modality feature fusion module, which realizes dual-modality interactions by using lightweight proxy tokens as intermediate representations. Specifically, self-attention is firstly utilized to facilitate the proxy tokens to learn the global information of each modality; then, the relationship between dual-modality proxy tokens is constructed to capture modality complementary information while also mitigating the interference of modality discrepancies; and finally, the knowledge in the updated proxy tokens is fed back to each modality through cross-attention for enhancing the features of each modality. Additionally, a mixture of knowledge decoupled experts module is designed to effectively fuse enhanced features of the two modalities. This module leverages multiple gating networks to assign modality-specific and modality-shared knowledge to separate expert groups for learning, thus highlighting the advantageous features of the different modalities. Extensive experiments on four RGB-infrared datasets demonstrate that our method outperforms existing state-of-the-art methods.
Published: 2026-02-03T07:42:22+00:00
Venue: Pattern Recognition
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; You Ma; Lin Chai; Shihan Mao; Yucheng Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113227"&gt;10.1016/j.patcog.2026.113227&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;RGB-infrared object detection aims to improve detection performance in complex environments by integrating complementary information from RGB and infrared images. While transformer-based methods have demonstrated significant advancements in this field by directly modeling dense relationships between modality tokens to enable cross-modality long-range interactions, they neglect the inherent discrepancies in feature distributions across modalities. Such discrepancies attenuate the reliability of the established relationships, thereby restricting the effective exploitation of complementary information between modalities. To alleviate this problem, we propose a framework for learning modality knowledge with proxy. The core innovation lies in the design of a proxy-guided cross-modality feature fusion module, which realizes dual-modality interactions by using lightweight proxy tokens as intermediate representations. Specifically, self-attention is firstly utilized to facilitate the proxy tokens to learn the global information of each modality; then, the relationship between dual-modality proxy tokens is constructed to capture modality complementary information while also mitigating the interference of modality discrepancies; and finally, the knowledge in the updated proxy tokens is fed back to each modality through cross-attention for enhancing the features of each modality. Additionally, a mixture of knowledge decoupled experts module is designed to effectively fuse enhanced features of the two modalities. This module leverages multiple gating networks to assign modality-specific and modality-shared knowledge to separate expert groups for learning, thus highlighting the advantageous features of the different modalities. Extensive experiments on four RGB-infrared datasets demonstrate that our method outperforms existing state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Rethinking Point Cloud Representation Learning for Freeing Transformer to Perceive Local</title><link>https://doi.org/10.1109/tmm.2026.3660148</link><guid>10.1109/tmm.2026.3660148</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Yuan Tang</dc:creator><dc:creator>Yunlong Yu</dc:creator><dc:creator>Xianzhi Li</dc:creator><dc:creator>Rui Wang</dc:creator><dc:creator>Jinfeng Xu</dc:creator><dc:creator>Qiao Yu</dc:creator><dc:creator>Yixue Hao</dc:creator><dc:creator>Long Hu</dc:creator><dc:creator>Min Chen</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660148</prism:doi><description>Transformers are widely utilized in the point cloud domain. However, existing methods tend to overburden Transformer with the dual task of local geometric perception and global feature extraction, limiting its ability to capture highlevel semantic knowledge. To address this issue, we present Representation Decoder (R-Decoder), a novel representation extraction module compatible with various point cloud Transformer methods, enabling the Transformer to focus on its excellent local perception. The R-Decoder iteratively extracts multiple global features from tokens generated by Transformer, refining them to construct an overall representation of point cloud. To ensure full adaptation of the R-Decoder to the knowledge of pre-trained Transformers, we design a cross-modal representation alignment task that leverages multimodal knowledge to specifically pre-train the R-Decoder. As a post-processing module, the R-Decoder seamlessly integrates with Transformers, while decoupling local perception and global representation. This design allows the Transformer to focus on the semantic encoding role for point tokens. Extensive experiments show that our RDecoder significantly boosts the capabilities of 3D representation learning in various point cloud Transformer methods. Notably, it achieves impressive classification accuracies of 95.1% on the ScanObjectNN dataset and 95.3% on the ModelNet40 dataset. Moreover, our method obtains new SOTA on all benchmarks of few-shot and zero-shot classification, while enhancing the multimodal task capabilities of pre-trained Transformers. Code and weights are available at https://github.com/TangYuan96/RDecoder.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuan Tang; Yunlong Yu; Xianzhi Li; Rui Wang; Jinfeng Xu; Qiao Yu; Yixue Hao; Long Hu; Min Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660148"&gt;10.1109/tmm.2026.3660148&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers are widely utilized in the point cloud domain. However, existing methods tend to overburden Transformer with the dual task of local geometric perception and global feature extraction, limiting its ability to capture highlevel semantic knowledge. To address this issue, we present Representation Decoder (R-Decoder), a novel representation extraction module compatible with various point cloud Transformer methods, enabling the Transformer to focus on its excellent local perception. The R-Decoder iteratively extracts multiple global features from tokens generated by Transformer, refining them to construct an overall representation of point cloud. To ensure full adaptation of the R-Decoder to the knowledge of pre-trained Transformers, we design a cross-modal representation alignment task that leverages multimodal knowledge to specifically pre-train the R-Decoder. As a post-processing module, the R-Decoder seamlessly integrates with Transformers, while decoupling local perception and global representation. This design allows the Transformer to focus on the semantic encoding role for point tokens. Extensive experiments show that our RDecoder significantly boosts the capabilities of 3D representation learning in various point cloud Transformer methods. Notably, it achieves impressive classification accuracies of 95.1% on the ScanObjectNN dataset and 95.3% on the ModelNet40 dataset. Moreover, our method obtains new SOTA on all benchmarks of few-shot and zero-shot classification, while enhancing the multimodal task capabilities of pre-trained Transformers. Code and weights are available at https://github.com/TangYuan96/RDecoder.&lt;/p&gt;</content:encoded></item><item><title>MDA-MAA: A Collaborative Augmentation Approach for Generalizing Cross-Domain Retrieval</title><link>https://doi.org/10.1109/tip.2026.3658223</link><guid>10.1109/tip.2026.3658223</guid><pubDate>Mon, 02 Feb 2026 20:46:59 +0000</pubDate><dc:creator>Ming Jin</dc:creator><dc:creator>Richang Hong</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3658223</prism:doi><description>In video-text cross-domain retrieval tasks, the generalization ability of the retrieval models is key to improving their performance and is crucial for enhancing their practical applicability. However, existing retrieval models exhibit significant deficiencies in cross-domain generalization. On one hand, models tend to overfit specific training domain data, resulting in poor cross-domain matching and significantly reduced retrieval accuracy when dealing with data from different, new, or mixed domains. On the other hand, although data augmentation is a vital strategy for enhancing model generalization, most existing methods focus on unimodal augmentation and fail to fully exploit the multimodal correlations between video and text. As a result, the augmented data lack semantic diversity, which further limits the model’s ability to understand and perform in complex cross-domain scenarios. To address these challenges, this paper proposes an innovative collaborative augmentation approach named MDA-MAA, which includes two core modules: the Masked Attention Augmentation (MAA) module and the Multimodal Diffusion Augmentation (MDA) module. The MAA module applies masking to the original video frame features and uses an attention mechanism to predict the masked features, effectively reducing overfitting to training data and enhancing model generalization. The MDA module generates subtitles from video frames and uses the LLaMA model to infer comprehensive video captions. These captions, combined with the original video frames, are integrated into a diffusion model for joint learning, ultimately generating semantically enriched augmented video frames. This process leverages the multimodal relationship between video and text to increase the diversity of the training data distribution. Experimental results demonstrate that this collaborative augmentation method significantly improves the performance of video-text cross-domain retrieval models, validating its effectiveness in enhan...
Published: 2026-02-02T20:46:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Jin; Richang Hong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3658223"&gt;10.1109/tip.2026.3658223&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;In video-text cross-domain retrieval tasks, the generalization ability of the retrieval models is key to improving their performance and is crucial for enhancing their practical applicability. However, existing retrieval models exhibit significant deficiencies in cross-domain generalization. On one hand, models tend to overfit specific training domain data, resulting in poor cross-domain matching and significantly reduced retrieval accuracy when dealing with data from different, new, or mixed domains. On the other hand, although data augmentation is a vital strategy for enhancing model generalization, most existing methods focus on unimodal augmentation and fail to fully exploit the multimodal correlations between video and text. As a result, the augmented data lack semantic diversity, which further limits the model’s ability to understand and perform in complex cross-domain scenarios. To address these challenges, this paper proposes an innovative collaborative augmentation approach named MDA-MAA, which includes two core modules: the Masked Attention Augmentation (MAA) module and the Multimodal Diffusion Augmentation (MDA) module. The MAA module applies masking to the original video frame features and uses an attention mechanism to predict the masked features, effectively reducing overfitting to training data and enhancing model generalization. The MDA module generates subtitles from video frames and uses the LLaMA model to infer comprehensive video captions. These captions, combined with the original video frames, are integrated into a diffusion model for joint learning, ultimately generating semantically enriched augmented video frames. This process leverages the multimodal relationship between video and text to increase the diversity of the training data distribution. Experimental results demonstrate that this collaborative augmentation method significantly improves the performance of video-text cross-domain retrieval models, validating its effectiveness in enhan...&lt;/p&gt;</content:encoded></item><item><title>SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?</title><link>https://arxiv.org/abs/2602.02765v1</link><guid>http://arxiv.org/abs/2602.02765v1</guid><pubDate>Mon, 02 Feb 2026 20:17:34 +0000</pubDate><dc:creator>Haruhiko Murata</dc:creator><dc:creator>Kazuhiro Hotta</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.
Published: 2026-02-02T20:17:34+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haruhiko Murata; Kazuhiro Hotta&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.&lt;/p&gt;</content:encoded></item><item><title>Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment</title><link>https://arxiv.org/abs/2602.00531v1</link><guid>http://arxiv.org/abs/2602.00531v1</guid><pubDate>Sat, 31 Jan 2026 05:51:45 +0000</pubDate><dc:creator>Tianyi Zhang</dc:creator><dc:creator>Antoine Simoulin</dc:creator><dc:creator>Kai Li</dc:creator><dc:creator>Sana Lakdawala</dc:creator><dc:creator>Shiqing Yu</dc:creator><dc:creator>Arpit Mittal</dc:creator><dc:creator>Hongyu Fu</dc:creator><dc:creator>Yu Lin</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.
Published: 2026-01-31T05:51:45+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianyi Zhang; Antoine Simoulin; Kai Li; Sana Lakdawala; Shiqing Yu; Arpit Mittal; Hongyu Fu; Yu Lin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.&lt;/p&gt;</content:encoded></item><item><title>Explicitly Learning Semantic Relevance for Salient Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1016/j.eswa.2026.131387</link><guid>10.1016/j.eswa.2026.131387</guid><pubDate>Mon, 02 Feb 2026 15:51:31 +0000</pubDate><dc:creator>Tao Gao</dc:creator><dc:creator>Weiguang Zhao</dc:creator><dc:creator>Mengkun Liu</dc:creator><dc:creator>Ting Chen</dc:creator><dc:creator>Ziqi Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131387</prism:doi><description>Salient object detection in remote sensing images (RSI-SOD) is crucial for computer vision in both high-altitude and low-altitude scenarios. Most existing methods primarily focus on multiscale feature integration, yet they encounter difficulties in achieving precise segmentation, particularly when confronted with complex object topologies and cluttered backgrounds. To address this, we propose a novel framework, ELSRNet, tailored to capturing the intrinsic semantic differences among features with diverse attributes, thereby facilitating pixel-wise separation of salient regions. This approach incorporates the deployment of a Foreground-Background Semantic Perception module (FBSP), which explicitly scrutinizes the semantic interactions through a more comprehensive Attention Guided Loss, ultimately strengthening the capacity to learn objects with complex structural characteristics. Going further, considering that the coupling between noise norms and convolutional kernels in cluttered backgrounds may amplify irrelevant responses and lead to false saliency predictions, the Non-Matching Feature Enhancement block (NMFE) is introduced to suppress such interference based on matching scores, and further refine the features through a gating mechanism. Concluding the process, the Global Perceptual Feature Aggregation module (GPFA) is designed to decouple features into semantic and structural information. It achieves saliency region localization while preserving fine-grained boundaries, producing high-quality saliency detection results. Experimental results and theoretical analysis reveal that the proposed network outperforms existing methods in enhancing detection capabilities across three benchmark datasets.
Published: 2026-02-02T15:51:31+00:00
Venue: Expert Systems with Applications
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Gao; Weiguang Zhao; Mengkun Liu; Ting Chen; Ziqi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131387"&gt;10.1016/j.eswa.2026.131387&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Salient object detection in remote sensing images (RSI-SOD) is crucial for computer vision in both high-altitude and low-altitude scenarios. Most existing methods primarily focus on multiscale feature integration, yet they encounter difficulties in achieving precise segmentation, particularly when confronted with complex object topologies and cluttered backgrounds. To address this, we propose a novel framework, ELSRNet, tailored to capturing the intrinsic semantic differences among features with diverse attributes, thereby facilitating pixel-wise separation of salient regions. This approach incorporates the deployment of a Foreground-Background Semantic Perception module (FBSP), which explicitly scrutinizes the semantic interactions through a more comprehensive Attention Guided Loss, ultimately strengthening the capacity to learn objects with complex structural characteristics. Going further, considering that the coupling between noise norms and convolutional kernels in cluttered backgrounds may amplify irrelevant responses and lead to false saliency predictions, the Non-Matching Feature Enhancement block (NMFE) is introduced to suppress such interference based on matching scores, and further refine the features through a gating mechanism. Concluding the process, the Global Perceptual Feature Aggregation module (GPFA) is designed to decouple features into semantic and structural information. It achieves saliency region localization while preserving fine-grained boundaries, producing high-quality saliency detection results. Experimental results and theoretical analysis reveal that the proposed network outperforms existing methods in enhancing detection capabilities across three benchmark datasets.&lt;/p&gt;</content:encoded></item><item><title>Multimodal large language models meet self-supervised diffusion for real-world aerial image super-resolution</title><link>https://doi.org/10.1016/j.jag.2026.105136</link><guid>10.1016/j.jag.2026.105136</guid><pubDate>Mon, 02 Feb 2026 13:09:38 +0000</pubDate><dc:creator>Lijing Lu</dc:creator><dc:creator>Zhou Huang</dc:creator><dc:creator>Yi Bao</dc:creator><dc:creator>Lin Wan</dc:creator><dc:creator>Zhihang Li</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105136</prism:doi><description>Real-world aerial image super-resolution (SR) remains particularly challenging because degradations in remote-sensing imagery involve random combinations of anisotropic blur, signal-dependent noise, and unknown downsampling kernels. Most existing SR methods either rely on simplified degradation assumptions or lack semantic perception of degradation, resulting in limited generalization to real-world conditions. To address these gaps, we propose a novel diffusion-based SR framework that integrates Multi-modal Large Language Models (MLLMs) and self-supervised contrastive learning for extracting degradation-insensitive representation. Specifically, we introduce a contrastive learning strategy into a ControlNet module, where the HR and LR counterparts of the same image are regarded as positive pairs, while representations from different images serve as negative pairs, enabling the network to learn degradation-insensitive structural features. To further enhance semantic awareness of degradation, an MLLM-generated change caption is incorporated into the diffusion process as textual guidance, allowing the model to explicitly perceive and reconstruct different degradation types. Moreover, a classifier-free guidance (CFG) distillation strategy compresses the original dual-branch diffusion model into a single lightweight network, substantially improving inference efficiency while maintaining high reconstruction fidelity. Extensive experiments conducted on various datasets have showcased the superior performance of our proposed model compared to existing state-of-the-art methods. Furthermore, our distillation algorithm achieves a twofold reduction in inference time compared to its non-distilled counterpart, making it more feasible for real-time and resource-limited applications.
Published: 2026-02-02T13:09:38+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lijing Lu; Zhou Huang; Yi Bao; Lin Wan; Zhihang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105136"&gt;10.1016/j.jag.2026.105136&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Real-world aerial image super-resolution (SR) remains particularly challenging because degradations in remote-sensing imagery involve random combinations of anisotropic blur, signal-dependent noise, and unknown downsampling kernels. Most existing SR methods either rely on simplified degradation assumptions or lack semantic perception of degradation, resulting in limited generalization to real-world conditions. To address these gaps, we propose a novel diffusion-based SR framework that integrates Multi-modal Large Language Models (MLLMs) and self-supervised contrastive learning for extracting degradation-insensitive representation. Specifically, we introduce a contrastive learning strategy into a ControlNet module, where the HR and LR counterparts of the same image are regarded as positive pairs, while representations from different images serve as negative pairs, enabling the network to learn degradation-insensitive structural features. To further enhance semantic awareness of degradation, an MLLM-generated change caption is incorporated into the diffusion process as textual guidance, allowing the model to explicitly perceive and reconstruct different degradation types. Moreover, a classifier-free guidance (CFG) distillation strategy compresses the original dual-branch diffusion model into a single lightweight network, substantially improving inference efficiency while maintaining high reconstruction fidelity. Extensive experiments conducted on various datasets have showcased the superior performance of our proposed model compared to existing state-of-the-art methods. Furthermore, our distillation algorithm achieves a twofold reduction in inference time compared to its non-distilled counterpart, making it more feasible for real-time and resource-limited applications.&lt;/p&gt;</content:encoded></item><item><title>SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection</title><link>https://arxiv.org/abs/2602.01843v1</link><guid>http://arxiv.org/abs/2602.01843v1</guid><pubDate>Mon, 02 Feb 2026 09:15:29 +0000</pubDate><dc:creator>Qian Xu</dc:creator><dc:creator>Xi Li</dc:creator><dc:creator>Fei Gao</dc:creator><dc:creator>Jie Guo</dc:creator><dc:creator>Haojuan Yuan</dc:creator><dc:creator>Shuaipeng Fan</dc:creator><dc:creator>Mingjin Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.
Published: 2026-02-02T09:15:29+00:00
Venue: arXiv
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Xu; Xi Li; Fei Gao; Jie Guo; Haojuan Yuan; Shuaipeng Fan; Mingjin Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.&lt;/p&gt;</content:encoded></item><item><title>Frequency-Decomposed Interaction Network for Stereo Image Restoration</title><link>https://doi.org/10.1109/tip.2026.3658219</link><guid>10.1109/tip.2026.3658219</guid><pubDate>Mon, 02 Feb 2026 20:46:59 +0000</pubDate><dc:creator>Xianmin Tian</dc:creator><dc:creator>Jin Xie</dc:creator><dc:creator>Ronghua Xu</dc:creator><dc:creator>Jing Nie</dc:creator><dc:creator>Jiale Cao</dc:creator><dc:creator>Yanwei Pang</dc:creator><dc:creator>Xuelong Li</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3658219</prism:doi><description>Stereo image restoration in adverse environments, such as low-light conditions, rain, and low resolution, requires effective exploitation of cross-view complementary information to recover degraded visual content. In monocular image restoration, frequency decomposition has proven effective, where high-frequency components aid in recovering fine textures and reducing blur, while low-frequency components facilitate noise suppression and illumination correction. However, existing stereo restoration methods have yet to explore cross-view interactions by frequency decomposition, which is a promising direction for enhancing restoration quality. To address this, we propose a frequency-aware framework comprising a Frequency Decomposition Module (FDM), Detail Interaction Module (DIM), Structural Interaction Module (SIM), and Adaptive Fusion Module (AFM). FDM employs learnable filters to decompose the image into high- and low-frequency components. DIM enhances the high-frequency branch by capturing local detail cues through deformable convolution. SIM processes the low-frequency branch by modeling global structural correlations via a cross-view row-wise attention mechanism. Finally, AFM adaptively fuses the complementary frequency-specific information to generate high-quality restored images. Extensive experiments demonstrate the efficacy and generalizability of our framework across three diverse stereo restoration tasks, where it achieves state-of-the-art performance in low-light enhancement, rain removal, alongside highly competitive results in super-resolution.
Published: 2026-02-02T20:46:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xianmin Tian; Jin Xie; Ronghua Xu; Jing Nie; Jiale Cao; Yanwei Pang; Xuelong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3658219"&gt;10.1109/tip.2026.3658219&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Stereo image restoration in adverse environments, such as low-light conditions, rain, and low resolution, requires effective exploitation of cross-view complementary information to recover degraded visual content. In monocular image restoration, frequency decomposition has proven effective, where high-frequency components aid in recovering fine textures and reducing blur, while low-frequency components facilitate noise suppression and illumination correction. However, existing stereo restoration methods have yet to explore cross-view interactions by frequency decomposition, which is a promising direction for enhancing restoration quality. To address this, we propose a frequency-aware framework comprising a Frequency Decomposition Module (FDM), Detail Interaction Module (DIM), Structural Interaction Module (SIM), and Adaptive Fusion Module (AFM). FDM employs learnable filters to decompose the image into high- and low-frequency components. DIM enhances the high-frequency branch by capturing local detail cues through deformable convolution. SIM processes the low-frequency branch by modeling global structural correlations via a cross-view row-wise attention mechanism. Finally, AFM adaptively fuses the complementary frequency-specific information to generate high-quality restored images. Extensive experiments demonstrate the efficacy and generalizability of our framework across three diverse stereo restoration tasks, where it achieves state-of-the-art performance in low-light enhancement, rain removal, alongside highly competitive results in super-resolution.&lt;/p&gt;</content:encoded></item><item><title>FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion</title><link>https://arxiv.org/abs/2602.03137v1</link><guid>http://arxiv.org/abs/2602.03137v1</guid><pubDate>Tue, 03 Feb 2026 05:45:22 +0000</pubDate><dc:creator>Chen-Bin Feng</dc:creator><dc:creator>Youyang Sha</dc:creator><dc:creator>Longfei Liu</dc:creator><dc:creator>Yongjun Yu</dc:creator><dc:creator>Chi Man Vong</dc:creator><dc:creator>Xuanlong Yu</dc:creator><dc:creator>Xi Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.
Published: 2026-02-03T05:45:22+00:00
Venue: arXiv
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chen-Bin Feng; Youyang Sha; Longfei Liu; Yongjun Yu; Chi Man Vong; Xuanlong Yu; Xi Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.&lt;/p&gt;</content:encoded></item><item><title>Ranking Vision-Language Models in Fully Unlabeled Tasks</title><link>https://doi.org/10.1109/tmm.2026.3660133</link><guid>10.1109/tmm.2026.3660133</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Yuhe Ding</dc:creator><dc:creator>Bo Jiang</dc:creator><dc:creator>Aihua Zheng</dc:creator><dc:creator>Qin Xu</dc:creator><dc:creator>Jian Liang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660133</prism:doi><description>Vision language models (VLMs) like CLIP show stellar zero-shot capability on classification benchmarks. However, selecting the VLM with the highest performance on the unlabeled downstream task is non-trivial. Existing VLM selection methods focus on the class-name-only setting, relying on supervised auxiliary datasets and large language models, which may not be accessible or feasible during deployment. This paper introduces the problem of unsupervised vision-language model selection, where only unsupervised downstream datasets are available, with no additional information provided. To solve this problem, we propose a method termed Visual-tExtual Graph Alignment (VEGA), to select VLMs without any annotations by measuring the alignment of the VLM between the two modalities on the downstream task. VEGA is motivated by the pretraining paradigm of VLMs, which aligns features with the same semantics from the visual and textual modalities, thereby mapping both modalities into a shared representation space. Specifically, we first construct two graphs on the vision and textual features, respectively. VEGA is then defined as the overall similarity between the visual and textual graphs at both node and edge levels. Extensive experiments across three different benchmarks, covering a variety of application scenarios and downstream datasets, demonstrate that VEGA consistently provides reliable and accurate estimates of VLMs' performance on unlabeled downstream tasks.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuhe Ding; Bo Jiang; Aihua Zheng; Qin Xu; Jian Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660133"&gt;10.1109/tmm.2026.3660133&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Vision language models (VLMs) like CLIP show stellar zero-shot capability on classification benchmarks. However, selecting the VLM with the highest performance on the unlabeled downstream task is non-trivial. Existing VLM selection methods focus on the class-name-only setting, relying on supervised auxiliary datasets and large language models, which may not be accessible or feasible during deployment. This paper introduces the problem of unsupervised vision-language model selection, where only unsupervised downstream datasets are available, with no additional information provided. To solve this problem, we propose a method termed Visual-tExtual Graph Alignment (VEGA), to select VLMs without any annotations by measuring the alignment of the VLM between the two modalities on the downstream task. VEGA is motivated by the pretraining paradigm of VLMs, which aligns features with the same semantics from the visual and textual modalities, thereby mapping both modalities into a shared representation space. Specifically, we first construct two graphs on the vision and textual features, respectively. VEGA is then defined as the overall similarity between the visual and textual graphs at both node and edge levels. Extensive experiments across three different benchmarks, covering a variety of application scenarios and downstream datasets, demonstrate that VEGA consistently provides reliable and accurate estimates of VLMs&amp;#x27; performance on unlabeled downstream tasks.&lt;/p&gt;</content:encoded></item><item><title>Quantization-Aware Regularizers for Deep Neural Networks Compression</title><link>https://arxiv.org/abs/2602.03614v1</link><guid>http://arxiv.org/abs/2602.03614v1</guid><pubDate>Tue, 03 Feb 2026 15:07:43 +0000</pubDate><dc:creator>Dario Malchiodi</dc:creator><dc:creator>Mattia Ferraretto</dc:creator><dc:creator>Marco Frasca</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.
Published: 2026-02-03T15:07:43+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dario Malchiodi; Mattia Ferraretto; Marco Frasca&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.&lt;/p&gt;</content:encoded></item><item><title>CompoVis: Is Cross-modal Semantic Alignment of CLIP Optimal? A Visual Analysis Attempt</title><link>https://doi.org/10.1109/tmm.2026.3660158</link><guid>10.1109/tmm.2026.3660158</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Tong Li</dc:creator><dc:creator>Guodao Sun</dc:creator><dc:creator>Xueqian Zheng</dc:creator><dc:creator>Qi Jiang</dc:creator><dc:creator>Wang Xia</dc:creator><dc:creator>Xu Tan</dc:creator><dc:creator>Haidong Gao</dc:creator><dc:creator>Haixia Wang</dc:creator><dc:creator>Ronghua Liang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660158</prism:doi><description>Vision-language pre-trained models (VLMs) have shown impressive cross-modal understanding, yet their “compositional understanding” ability remains under investigation. We introduce CompoVis, a framework for visually probing cross-modal gaps in VLMs. CompoVis optimizes the grid layout to highlight alignment clusters and boundaries, visually interprets multi-head attention and semantic drift, and enables interactive fine-tuning unconstrained by closed datasets or offline models. Quantitative experiments and case studies explore key insights: VLMs rely on entity shortcuts rather than comprehension-driven; stubborn global modality isolation and suboptimal fine-grained alignment remain; fine-tuning with negative samples does not fundamentally alleviate the gaps. Approximately 89% of participants ( n=27 n=27 ) found that, compared to methods relying solely on data metrics, CompoVis offers a more innovative and effective approach for investigating modality gaps in VLMs.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tong Li; Guodao Sun; Xueqian Zheng; Qi Jiang; Wang Xia; Xu Tan; Haidong Gao; Haixia Wang; Ronghua Liang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660158"&gt;10.1109/tmm.2026.3660158&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-language pre-trained models (VLMs) have shown impressive cross-modal understanding, yet their “compositional understanding” ability remains under investigation. We introduce CompoVis, a framework for visually probing cross-modal gaps in VLMs. CompoVis optimizes the grid layout to highlight alignment clusters and boundaries, visually interprets multi-head attention and semantic drift, and enables interactive fine-tuning unconstrained by closed datasets or offline models. Quantitative experiments and case studies explore key insights: VLMs rely on entity shortcuts rather than comprehension-driven; stubborn global modality isolation and suboptimal fine-grained alignment remain; fine-tuning with negative samples does not fundamentally alleviate the gaps. Approximately 89% of participants ( n=27 n=27 ) found that, compared to methods relying solely on data metrics, CompoVis offers a more innovative and effective approach for investigating modality gaps in VLMs.&lt;/p&gt;</content:encoded></item><item><title>MA-Net: Multi-Granularity Attention Network for Fine-Grained Classification of Ship Targets in Remote Sensing Images</title><link>https://doi.org/10.3390/rs18030462</link><guid>10.3390/rs18030462</guid><pubDate>Mon, 02 Feb 2026 14:12:46 +0000</pubDate><dc:creator>Jiamin Qi</dc:creator><dc:creator>Peifeng Li</dc:creator><dc:creator>Guangyao Zhou</dc:creator><dc:creator>Ben Niu</dc:creator><dc:creator>Feng Wang</dc:creator><dc:creator>Qiantong Wang</dc:creator><dc:creator>Yuxin Hu</dc:creator><dc:creator>Xiantai Xiang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030462</prism:doi><description>The classification of ship targets in remote sensing images holds significant application value in fields such as marine monitoring and national defence. Although existing research has yielded considerable achievements in ship classification, current methods struggle to distinguish highly similar ship categories for fine-grained classification tasks due to a lack of targeted design. Specifically, they exhibit the following shortcomings: limited ability to extract locally discriminative features; inadequate fusion of features at high and low levels of representation granularity; and sensitivity of model performance to background noise. To address this issue, this paper proposes a fine-grained classification framework for ship targets in remote sensing images based on Multi-Granularity Attention Network (MA-Net), specifically designed to tackle the aforementioned three major challenges encountered in fine-grained classification tasks for ship targets in remote sensing. This framework first performs multi-level feature extraction through a backbone network, subsequently introducing an Adaptive Local Feature Attention (ALFA) module. This module employs dynamic overlapping region segmentation techniques to assist the network in learning spatial structural combinations, thereby optimising the representation of local features. Secondly, a Dynamic Multi-Granularity Feature Fusion (DMGFF) module is designed to dynamically fuse feature maps of varying representational granularities and select key attribute features. Finally, a Feature-Based Data Augmentation (FBDA) method is developed to effectively highlight target detail features, thereby enhancing feature expression capabilities. On the public FGSC-23 and FGSCR-42 datasets, MA-Net attains top-performing accuracies of 93.12% and 98.40%, surpassing the previous best methods and establishing a new state of the art for fine-grained classification of ship targets in remote sensing images.
Published: 2026-02-02T14:12:46+00:00
Venue: Remote Sensing
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiamin Qi; Peifeng Li; Guangyao Zhou; Ben Niu; Feng Wang; Qiantong Wang; Yuxin Hu; Xiantai Xiang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030462"&gt;10.3390/rs18030462&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;The classification of ship targets in remote sensing images holds significant application value in fields such as marine monitoring and national defence. Although existing research has yielded considerable achievements in ship classification, current methods struggle to distinguish highly similar ship categories for fine-grained classification tasks due to a lack of targeted design. Specifically, they exhibit the following shortcomings: limited ability to extract locally discriminative features; inadequate fusion of features at high and low levels of representation granularity; and sensitivity of model performance to background noise. To address this issue, this paper proposes a fine-grained classification framework for ship targets in remote sensing images based on Multi-Granularity Attention Network (MA-Net), specifically designed to tackle the aforementioned three major challenges encountered in fine-grained classification tasks for ship targets in remote sensing. This framework first performs multi-level feature extraction through a backbone network, subsequently introducing an Adaptive Local Feature Attention (ALFA) module. This module employs dynamic overlapping region segmentation techniques to assist the network in learning spatial structural combinations, thereby optimising the representation of local features. Secondly, a Dynamic Multi-Granularity Feature Fusion (DMGFF) module is designed to dynamically fuse feature maps of varying representational granularities and select key attribute features. Finally, a Feature-Based Data Augmentation (FBDA) method is developed to effectively highlight target detail features, thereby enhancing feature expression capabilities. On the public FGSC-23 and FGSCR-42 datasets, MA-Net attains top-performing accuracies of 93.12% and 98.40%, surpassing the previous best methods and establishing a new state of the art for fine-grained classification of ship targets in remote sensing images.&lt;/p&gt;</content:encoded></item><item><title>FGDepth: Fine-Grained Boundary Perception Enhancement in Self-Supervised Indoor Depth Estimation</title><link>https://doi.org/10.1109/tmm.2026.3660182</link><guid>10.1109/tmm.2026.3660182</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Chenggong Han</dc:creator><dc:creator>Chen Lv</dc:creator><dc:creator>He Jiang</dc:creator><dc:creator>Qiqi Kou</dc:creator><dc:creator>Deqiang Cheng</dc:creator><dc:creator>Stefano Mattoccia</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660182</prism:doi><description>Self-supervised depth estimation has been widely ap plied in indoor environments. However, the presence of numerous objects and complex structural boundaries often leads existing methods to generate blurred or imprecise depth edges. To address this challenge, we propose FGDepth, a framework designed to enhance depth estimation through fine-grained boundary perception. Firstly, we introduce an SR (Super Resolution) auxiliary training branch that shares feature layers with the encoder of the depth estimation network. By utilizing the powerful detail recovery capabilities of the SR task, we improve the depth network's sensitivity to indoor object boundaries. Notably, the SR branch is used only during training, ensuring no added computational cost during inference. As far as we know, we are the first to employ the SR task as an auxiliary method for indoor self-supervised depth estimation. Second, we observe that outdoor scenes display significant depth variations due to their broader depth range, whereas indoor scenes typically lack clear boundary distinctions in background areas. This is because distant objects in indoor settings often appear as continuous surfaces with similar depths. This characteristic necessitates careful mitigation of background depth uniformity interference when enhancing depth boundaries in indoor scenes. Therefore, we design a depth-adaptive object mask to provide target object boundary information and use a triplet loss to align these differences with the depth map. Experimental results on three benchmark datasets show that our method outperforms existing approaches. We also conduct ablation studies to validate the contributions of each component.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenggong Han; Chen Lv; He Jiang; Qiqi Kou; Deqiang Cheng; Stefano Mattoccia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660182"&gt;10.1109/tmm.2026.3660182&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Self-supervised depth estimation has been widely ap plied in indoor environments. However, the presence of numerous objects and complex structural boundaries often leads existing methods to generate blurred or imprecise depth edges. To address this challenge, we propose FGDepth, a framework designed to enhance depth estimation through fine-grained boundary perception. Firstly, we introduce an SR (Super Resolution) auxiliary training branch that shares feature layers with the encoder of the depth estimation network. By utilizing the powerful detail recovery capabilities of the SR task, we improve the depth network&amp;#x27;s sensitivity to indoor object boundaries. Notably, the SR branch is used only during training, ensuring no added computational cost during inference. As far as we know, we are the first to employ the SR task as an auxiliary method for indoor self-supervised depth estimation. Second, we observe that outdoor scenes display significant depth variations due to their broader depth range, whereas indoor scenes typically lack clear boundary distinctions in background areas. This is because distant objects in indoor settings often appear as continuous surfaces with similar depths. This characteristic necessitates careful mitigation of background depth uniformity interference when enhancing depth boundaries in indoor scenes. Therefore, we design a depth-adaptive object mask to provide target object boundary information and use a triplet loss to align these differences with the depth map. Experimental results on three benchmark datasets show that our method outperforms existing approaches. We also conduct ablation studies to validate the contributions of each component.&lt;/p&gt;</content:encoded></item><item><title>Toward Smooth Depth Driven by Selective Attention and Selective Aggregation</title><link>https://doi.org/10.1109/tmm.2026.3660136</link><guid>10.1109/tmm.2026.3660136</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Cheol-Hoon Park</dc:creator><dc:creator>Woo-Jin Ahn</dc:creator><dc:creator>Hyun-Duck Choi</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660136</prism:doi><description>The challenges in single-image depth prediction (SIDP) are mainly due to the lack of smooth depth ground truth and the presence of irregular and complex objects. While window-based attention mechanisms, which balance long-range dependency capture with computational efficiency by processing elements within a fixed grid, have advanced SIDP research, they are limited by a constrained search range. This limitation can impede smooth depth estimation in irregularity and complexity. To address these challenges, we propose a novel attention mechanism that selectively identifies and aggregates only the most relevant information. Our approach enables flexible and efficient exploration by using data-dependent movable offsets to select substantial tokens and designating them as key-value pairs. Furthermore, we overcome the issue of small softmax values in traditional attention mechanisms through score-based grouping with top-k selection. Our feed-forward network, which incorporates a gating mechanism and grouped convolutions with varying cardinalities, refines features before passing them to subsequent layers, allowing for targeted focus on input features. Finally, we utilize feature maps from hierarchical decoders to estimate bin centers and per-pixel probability distributions. We introduce a 4-way selective scanning technique to aggregate these perpixel probability distributions smoothly, resulting in a dense and continuous depth map. The proposed network, named selective attention and selective aggregate depth (SA2Depth), demonstrates state-of-the-art performance across multiple datasets compared to previous methods. The code is available at https://github.com/towardsDLCV/SA2DEPTH.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Cheol-Hoon Park; Woo-Jin Ahn; Hyun-Duck Choi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660136"&gt;10.1109/tmm.2026.3660136&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;The challenges in single-image depth prediction (SIDP) are mainly due to the lack of smooth depth ground truth and the presence of irregular and complex objects. While window-based attention mechanisms, which balance long-range dependency capture with computational efficiency by processing elements within a fixed grid, have advanced SIDP research, they are limited by a constrained search range. This limitation can impede smooth depth estimation in irregularity and complexity. To address these challenges, we propose a novel attention mechanism that selectively identifies and aggregates only the most relevant information. Our approach enables flexible and efficient exploration by using data-dependent movable offsets to select substantial tokens and designating them as key-value pairs. Furthermore, we overcome the issue of small softmax values in traditional attention mechanisms through score-based grouping with top-k selection. Our feed-forward network, which incorporates a gating mechanism and grouped convolutions with varying cardinalities, refines features before passing them to subsequent layers, allowing for targeted focus on input features. Finally, we utilize feature maps from hierarchical decoders to estimate bin centers and per-pixel probability distributions. We introduce a 4-way selective scanning technique to aggregate these perpixel probability distributions smoothly, resulting in a dense and continuous depth map. The proposed network, named selective attention and selective aggregate depth (SA2Depth), demonstrates state-of-the-art performance across multiple datasets compared to previous methods. The code is available at https://github.com/towardsDLCV/SA2DEPTH.&lt;/p&gt;</content:encoded></item><item><title>Bidirectional Cross-Modal Collaborative Alignment via Semantic-Guided Visual Embeddings for Partially Relevant Video Retrieval</title><link>https://doi.org/10.1109/tip.2026.3658218</link><guid>10.1109/tip.2026.3658218</guid><pubDate>Mon, 02 Feb 2026 20:46:59 +0000</pubDate><dc:creator>Huafeng Li</dc:creator><dc:creator>Jialong Zhao</dc:creator><dc:creator>Yafei Zhang</dc:creator><dc:creator>Jie Wen</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3658218</prism:doi><description>Partially Relevant Video Retrieval (PRVR) aims to retrieve videos that match a given textual query only partially. This task is inherently challenging due to the modality gap between text and video, which is further exacerbated by the partial semantic correspondence between linguistic descriptions and visual content. To address these challenges, we propose a bidirectional cross-modal alignment mechanism that collaboratively optimizes both visual and textual modalities. In the visual modality, a major difficulty lies in the absence of visual cues that directly correspond to textual semantics, limiting the models ability to align visual representations with textual meanings under unsupervised conditions. To overcome this issue, we construct a semantic-visual association library, which stores paired visual and textual features with semantic annotations. During training, the model dynamically retrieves the most semantically similar visual samples from this library based on the current visual feature vector. These retrieved samples, preliminarily associated with semantics via cross-modal matching, are used to form dynamic anchors that guide visual representation learning. By leveraging these enriched visual features, the model progressively refines the visual representations to achieve better alignment with the corresponding textual inputs, thereby enhancing cross-modal consistency. In the textual modality, we enhance textual representations by integrating semantically aligned visual features selected from the same association library, further narrowing the modality gap. Extensive experiments on benchmark datasets under partial semantic correspondence scenarios demonstrate that our method achieves state-of-the-art performance. The source code of the paper is available at https://github.com/cyanlll/BOA.
Published: 2026-02-02T20:46:59+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huafeng Li; Jialong Zhao; Yafei Zhang; Jie Wen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3658218"&gt;10.1109/tip.2026.3658218&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Partially Relevant Video Retrieval (PRVR) aims to retrieve videos that match a given textual query only partially. This task is inherently challenging due to the modality gap between text and video, which is further exacerbated by the partial semantic correspondence between linguistic descriptions and visual content. To address these challenges, we propose a bidirectional cross-modal alignment mechanism that collaboratively optimizes both visual and textual modalities. In the visual modality, a major difficulty lies in the absence of visual cues that directly correspond to textual semantics, limiting the models ability to align visual representations with textual meanings under unsupervised conditions. To overcome this issue, we construct a semantic-visual association library, which stores paired visual and textual features with semantic annotations. During training, the model dynamically retrieves the most semantically similar visual samples from this library based on the current visual feature vector. These retrieved samples, preliminarily associated with semantics via cross-modal matching, are used to form dynamic anchors that guide visual representation learning. By leveraging these enriched visual features, the model progressively refines the visual representations to achieve better alignment with the corresponding textual inputs, thereby enhancing cross-modal consistency. In the textual modality, we enhance textual representations by integrating semantically aligned visual features selected from the same association library, further narrowing the modality gap. Extensive experiments on benchmark datasets under partial semantic correspondence scenarios demonstrate that our method achieves state-of-the-art performance. The source code of the paper is available at https://github.com/cyanlll/BOA.&lt;/p&gt;</content:encoded></item><item><title>DiffusionLight-Turbo: Accelerated Light Probes for Free Via Single-Pass Chrome Ball Inpainting</title><link>https://doi.org/10.1109/tpami.2026.3660066</link><guid>10.1109/tpami.2026.3660066</guid><pubDate>Mon, 02 Feb 2026 20:44:02 +0000</pubDate><dc:creator>Worameth Chinchuthakun</dc:creator><dc:creator>Pakkapon Phongthawee</dc:creator><dc:creator>Amit Raj</dc:creator><dc:creator>Varun Jampani</dc:creator><dc:creator>Pramook Khungurn</dc:creator><dc:creator>Supasorn Suwajanakorn</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3660066</prism:doi><description>We introduce a simple yet effective technique for estimating lighting from a single low-dynamic-range (LDR) image by reframing the task as a chrome ball inpainting problem. This approach leverages a pre-trained diffusion model, Stable Diffusion XL, to overcome the generalization failures of existing methods that rely on limited HDR panorama datasets. While conceptually simple, the task remains challenging because diffusion models often insert incorrect or inconsistent content and cannot readily generate chrome balls in HDR format. Our analysis reveals that the inpainting process is highly sensitive to the initial noise in the diffusion process, occasionally resulting in unrealistic outputs. To address this, we first introduce DiffusionLight [1], which uses iterative inpainting to compute a median chrome ball from multiple outputs to serve as a stable, low-frequency lighting prior that guides the generation of a high-quality final result. To generate high-dynamic-range (HDR) light probes, an Exposure LoRA is fine-tuned to create LDR images at multiple exposure values, which are then merged. While effective, DiffusionLight is time-intensive, requiring approximately 30 minutes per estimation. To reduce this overhead, we introduce DiffusionLight- Turbo, which reduces the runtime to about 30 seconds with minimal quality loss. This 60x speedup is achieved by training a Turbo LoRA to directly predict the averaged chrome balls from the iterative process. Inference is further streamlined into a single denoising pass using a LoRA swapping technique. Experimental results that show our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios. Our code is available at https://diffusionlight.github.io/turbo.
Published: 2026-02-02T20:44:02+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Worameth Chinchuthakun; Pakkapon Phongthawee; Amit Raj; Varun Jampani; Pramook Khungurn; Supasorn Suwajanakorn&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3660066"&gt;10.1109/tpami.2026.3660066&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce a simple yet effective technique for estimating lighting from a single low-dynamic-range (LDR) image by reframing the task as a chrome ball inpainting problem. This approach leverages a pre-trained diffusion model, Stable Diffusion XL, to overcome the generalization failures of existing methods that rely on limited HDR panorama datasets. While conceptually simple, the task remains challenging because diffusion models often insert incorrect or inconsistent content and cannot readily generate chrome balls in HDR format. Our analysis reveals that the inpainting process is highly sensitive to the initial noise in the diffusion process, occasionally resulting in unrealistic outputs. To address this, we first introduce DiffusionLight [1], which uses iterative inpainting to compute a median chrome ball from multiple outputs to serve as a stable, low-frequency lighting prior that guides the generation of a high-quality final result. To generate high-dynamic-range (HDR) light probes, an Exposure LoRA is fine-tuned to create LDR images at multiple exposure values, which are then merged. While effective, DiffusionLight is time-intensive, requiring approximately 30 minutes per estimation. To reduce this overhead, we introduce DiffusionLight- Turbo, which reduces the runtime to about 30 seconds with minimal quality loss. This 60x speedup is achieved by training a Turbo LoRA to directly predict the averaged chrome balls from the iterative process. Inference is further streamlined into a single denoising pass using a LoRA swapping technique. Experimental results that show our method produces convincing light estimates across diverse settings and demonstrates superior generalization to in-the-wild scenarios. Our code is available at https://diffusionlight.github.io/turbo.&lt;/p&gt;</content:encoded></item><item><title>DyC-CLIP: Dynamic Context-Aware Multi-Modal Prompt Learning for Zero-Shot Anomaly Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113215</link><guid>10.1016/j.patcog.2026.113215</guid><pubDate>Mon, 02 Feb 2026 07:11:22 +0000</pubDate><dc:creator>Peng Chen</dc:creator><dc:creator>Fangjun Huang</dc:creator><dc:creator>Chao Huang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113215</prism:doi><description>Vision-language models (VLMs) have demonstrated remarkable potential in zero-shot anomaly detection (ZSAD) tasks due to their strong generalization capabilities, enabling the identification of anomalies in unseen categories without additional supervision. However, their robustness and adaptability under challenging visual conditions remain limited, as existing approaches typically rely on meticulously designed textual prompts, which require extensive domain expertise and manual effort. Moreover, simple prompt formulations struggle to capture the complex structural characteristics inherent in images. To address these limitations, we propose DyC-CLIP, a novel dynamic context-aware prompt learning method for ZSAD. DyC-CLIP enhances anomaly localization by enabling text embeddings to dynamically adapt to fine-grained patch features. Specifically, we propose a Frequency-domain Dynamic Adapter (FDA) that integrates global visual information into textual prompts, reducing the reliance on product-specific prompts. To further facilitate cross-modal alignment, we develop a Cross-Modal Guided Sparse Attention (CGSA) module, which dynamically refines text embeddings based on fine-grained image features. Additionally, we design an Anomaly-Aware Semantic Aggregation (ASA) module to integrate local contextual information and enhance the model’s ability to discriminate anomalous patterns. Extensive experiments on 14 datasets spanning industrial and medical domains demonstrate that DyC-CLIP achieves state-of-the-art performance. Code will be publicly available upon publication.
Published: 2026-02-02T07:11:22+00:00
Venue: Pattern Recognition
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peng Chen; Fangjun Huang; Chao Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113215"&gt;10.1016/j.patcog.2026.113215&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-language models (VLMs) have demonstrated remarkable potential in zero-shot anomaly detection (ZSAD) tasks due to their strong generalization capabilities, enabling the identification of anomalies in unseen categories without additional supervision. However, their robustness and adaptability under challenging visual conditions remain limited, as existing approaches typically rely on meticulously designed textual prompts, which require extensive domain expertise and manual effort. Moreover, simple prompt formulations struggle to capture the complex structural characteristics inherent in images. To address these limitations, we propose DyC-CLIP, a novel dynamic context-aware prompt learning method for ZSAD. DyC-CLIP enhances anomaly localization by enabling text embeddings to dynamically adapt to fine-grained patch features. Specifically, we propose a Frequency-domain Dynamic Adapter (FDA) that integrates global visual information into textual prompts, reducing the reliance on product-specific prompts. To further facilitate cross-modal alignment, we develop a Cross-Modal Guided Sparse Attention (CGSA) module, which dynamically refines text embeddings based on fine-grained image features. Additionally, we design an Anomaly-Aware Semantic Aggregation (ASA) module to integrate local contextual information and enhance the model’s ability to discriminate anomalous patterns. Extensive experiments on 14 datasets spanning industrial and medical domains demonstrate that DyC-CLIP achieves state-of-the-art performance. Code will be publicly available upon publication.&lt;/p&gt;</content:encoded></item><item><title>Leveraging Modal Interaction and Window Dilation in Attention Network for Hyperspectral and Multispectral Remote Sensing Image Fusion</title><link>https://doi.org/10.1109/tgrs.2026.3659928</link><guid>10.1109/tgrs.2026.3659928</guid><pubDate>Mon, 02 Feb 2026 20:44:11 +0000</pubDate><dc:creator>Xuanfu Huo</dc:creator><dc:creator>Hongping Gan</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3659928</prism:doi><description>Hyperspectral and multispectral (HS-MS) image fusion aims to reconstruct high-resolution hyperspectral images (HR-HSIs) from low-resolution hyperspectral images (LR-HSI) and high-resolution multispectral images (HR-MSI). Convolutional neural networks (CNN) have been extensively applied to this fusion task due to their powerful feature extraction capabilities. However, the limited receptive fields of CNN-based methods make it challenging to capture long-range dependencies in images, thereby restricting their performance in fusion tasks. Attention mechanisms, which can effectively capture global correlations in data, have emerged as a popular research topic. Despite this, existing attention-based hyperspectral fusion methods have not fully integrated and coordinated the two modalities, i.e., LR-HSI and HR-MSI, resulting in limited reconstruction quality. In this paper, we propose a Modalities Interaction and Window Dilation Attention Network for HS-MS image fusion, dubbed as MIAN. Specifically, our proposed MIAN introduces a modalities interaction attention block designed to explore, coordinate, and fuse the two modalities, facilitating information exchange and supplementation across different modalities. Additionally, we design a window dilation attention block that overcomes the local limitations of conventional window attention by establishing long-range connections across windows, thereby facilitating the handling of cross-scale self-similarity in remote sensing images. Furthermore, we incorporate channel attention blocks into each attention block to learn the channel dependencies which are the most important features for hyperspectral tasks. Extensive experiments demonstrate that our MIAN outperforms recent approaches on several hyperspectral datasets, achieving state-of-the-art performance.
Published: 2026-02-02T20:44:11+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuanfu Huo; Hongping Gan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3659928"&gt;10.1109/tgrs.2026.3659928&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral and multispectral (HS-MS) image fusion aims to reconstruct high-resolution hyperspectral images (HR-HSIs) from low-resolution hyperspectral images (LR-HSI) and high-resolution multispectral images (HR-MSI). Convolutional neural networks (CNN) have been extensively applied to this fusion task due to their powerful feature extraction capabilities. However, the limited receptive fields of CNN-based methods make it challenging to capture long-range dependencies in images, thereby restricting their performance in fusion tasks. Attention mechanisms, which can effectively capture global correlations in data, have emerged as a popular research topic. Despite this, existing attention-based hyperspectral fusion methods have not fully integrated and coordinated the two modalities, i.e., LR-HSI and HR-MSI, resulting in limited reconstruction quality. In this paper, we propose a Modalities Interaction and Window Dilation Attention Network for HS-MS image fusion, dubbed as MIAN. Specifically, our proposed MIAN introduces a modalities interaction attention block designed to explore, coordinate, and fuse the two modalities, facilitating information exchange and supplementation across different modalities. Additionally, we design a window dilation attention block that overcomes the local limitations of conventional window attention by establishing long-range connections across windows, thereby facilitating the handling of cross-scale self-similarity in remote sensing images. Furthermore, we incorporate channel attention blocks into each attention block to learn the channel dependencies which are the most important features for hyperspectral tasks. Extensive experiments demonstrate that our MIAN outperforms recent approaches on several hyperspectral datasets, achieving state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Hierarchical Attention-Driven Detection of Small Objects in Remote Sensing Imagery</title><link>https://doi.org/10.3390/rs18030455</link><guid>10.3390/rs18030455</guid><pubDate>Mon, 02 Feb 2026 14:12:46 +0000</pubDate><dc:creator>Xinyu Liu</dc:creator><dc:creator>Xiongwei Sun</dc:creator><dc:creator>Jile Wang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030455</prism:doi><description>Accurate detection of small objects in remote sensing imagery remains challenging due to their limited texture, sparse features, and weak contrast. To address this, an enhanced small object detection model integrating top–down and bottom–up attention mechanisms is proposed. First, we design two statistical model-constrained feature pre-extraction networks to enhance the spatial patterns of small objects before feeding them into the backbone network. Next, a top–down attention mechanism followed by an overview-then-refinement process is employed to guide region-level feature extraction. Finally, a bottom–up feature fusion strategy is utilized to integrate micro features and macro structural features in a bottom–up manner, enhancing the representational capacity of limited features for small objects. Evaluations on the AI-TOD and SODA-A datasets show that our method outperforms existing benchmark models. On the AI-TOD dataset, it improves AP and AP0.5 by 0.3% and 2.7%, respectively. More notably, on the more challenging SODA-A dataset, it achieves significant gains of 0.5% in AP and 1.4% in AP0.5. These consistent enhancements across different datasets verify the effectiveness of our method in boosting detection performance, particularly for small targets.
Published: 2026-02-02T14:12:46+00:00
Venue: Remote Sensing
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Liu; Xiongwei Sun; Jile Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030455"&gt;10.3390/rs18030455&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate detection of small objects in remote sensing imagery remains challenging due to their limited texture, sparse features, and weak contrast. To address this, an enhanced small object detection model integrating top–down and bottom–up attention mechanisms is proposed. First, we design two statistical model-constrained feature pre-extraction networks to enhance the spatial patterns of small objects before feeding them into the backbone network. Next, a top–down attention mechanism followed by an overview-then-refinement process is employed to guide region-level feature extraction. Finally, a bottom–up feature fusion strategy is utilized to integrate micro features and macro structural features in a bottom–up manner, enhancing the representational capacity of limited features for small objects. Evaluations on the AI-TOD and SODA-A datasets show that our method outperforms existing benchmark models. On the AI-TOD dataset, it improves AP and AP0.5 by 0.3% and 2.7%, respectively. More notably, on the more challenging SODA-A dataset, it achieves significant gains of 0.5% in AP and 1.4% in AP0.5. These consistent enhancements across different datasets verify the effectiveness of our method in boosting detection performance, particularly for small targets.&lt;/p&gt;</content:encoded></item><item><title>How Well Do Vision Models Understand Tasks With Multiple Labels?</title><link>https://doi.org/10.1016/j.eswa.2026.131479</link><guid>10.1016/j.eswa.2026.131479</guid><pubDate>Tue, 03 Feb 2026 00:32:48 +0000</pubDate><dc:creator>Yunus Can Bilge</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131479</prism:doi><description>The increasing availability of pre-trained vision backbones has significantly advanced multi-label image classification, yet the comparative transferability and generalization behavior of these models across diverse target domains remain underexplored. In this study, we present a comprehensive empirical analysis of 80 pre-trained backbones, evaluated in a consistent setting across five benchmark datasets: MS-COCO, NUS-WIDE, CelebA, PA-100K, and MS-COCO-2012. While the architectures and benchmarks used in our study are established, our work provides the first large-scale, standardized analysis of backbone transferability in multi-label settings, offering practical insights and reproducible tools that are currently lacking in the literature and remain highly relevant for real-world deployment and benchmarking. Using a standardized multi-label image classification framework and seven evaluation metrics, we systematically assess the performance, robustness, and efficiency of each model. We investigate the influence of object scale, dataset diversity and size, classifier depth, and relationship between evaluation metrics, and evaluate the alignment of them. We further observe that accuracy and recall metrics are strongly aligned, while instance-level precision behaves more independently, suggesting the need for a measure for backbone selection. To support it, we introduce TAME and TAME eff , composite scoring strategies that account for predictive performance and model efficiency. Our findings provide actionable insights and a composite metric and efficiency analysis to guide backbone selection in multi-label settings in real-world and resource-constrained multi-label applications. All model outputs, evaluation scripts, and diagnostics will be publicly available to support reproducibility and further research.
Published: 2026-02-03T00:32:48+00:00
Venue: Expert Systems with Applications
Score: 0.779 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunus Can Bilge&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131479"&gt;10.1016/j.eswa.2026.131479&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (must_read)&lt;/p&gt;
&lt;p&gt;The increasing availability of pre-trained vision backbones has significantly advanced multi-label image classification, yet the comparative transferability and generalization behavior of these models across diverse target domains remain underexplored. In this study, we present a comprehensive empirical analysis of 80 pre-trained backbones, evaluated in a consistent setting across five benchmark datasets: MS-COCO, NUS-WIDE, CelebA, PA-100K, and MS-COCO-2012. While the architectures and benchmarks used in our study are established, our work provides the first large-scale, standardized analysis of backbone transferability in multi-label settings, offering practical insights and reproducible tools that are currently lacking in the literature and remain highly relevant for real-world deployment and benchmarking. Using a standardized multi-label image classification framework and seven evaluation metrics, we systematically assess the performance, robustness, and efficiency of each model. We investigate the influence of object scale, dataset diversity and size, classifier depth, and relationship between evaluation metrics, and evaluate the alignment of them. We further observe that accuracy and recall metrics are strongly aligned, while instance-level precision behaves more independently, suggesting the need for a measure for backbone selection. To support it, we introduce TAME and TAME eff , composite scoring strategies that account for predictive performance and model efficiency. Our findings provide actionable insights and a composite metric and efficiency analysis to guide backbone selection in multi-label settings in real-world and resource-constrained multi-label applications. All model outputs, evaluation scripts, and diagnostics will be publicly available to support reproducibility and further research.&lt;/p&gt;</content:encoded></item><item><title>Parameter-Driven Simulation of Synthetic InSAR Data Using Deep Learning</title><link>https://doi.org/10.1109/jstars.2026.3660374</link><guid>10.1109/jstars.2026.3660374</guid><pubDate>Mon, 02 Feb 2026 20:44:32 +0000</pubDate><dc:creator>Philipp Sibler</dc:creator><dc:creator>Francescopaolo Sica</dc:creator><dc:creator>Michael Schmitt</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3660374</prism:doi><description>Synthetic simulation of Interferometric Synthetic Aperture Radar (InSAR) data plays a critical role in algorithm development, mission design, and machine learning pretraining. However, most existing approaches rely on handcrafted simulation pipelines or rigid data generation setups that lack adaptability to varying sensor configurations. In this work, we propose a novel deep learning-based framework for generating realistic, complex-valued InSAR data that is explicitly conditioned on acquisition parameters, such as wavelength, baseline, incidence angle, and revisit time. The proposed multitask model jointly synthesizes scene reflectivity, coherence magnitude, and interferometric phase while enabling the simulation of temporal decorrelation and noise characteristics via both analytical and data-driven modules. We further introduce a reparameterizable noise model for speckle and phase noise, whose statistics are tuned using Particle Swarm Optimization to match real sensor characteristics. Extensive experiments on the high-resolution GeoNRW TDX dataset demonstrate the realism and flexibility of our simulation framework, which can generalize to unseen acquisition scenarios and support SAR mission design.
Published: 2026-02-02T20:44:32+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Philipp Sibler; Francescopaolo Sica; Michael Schmitt&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3660374"&gt;10.1109/jstars.2026.3660374&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic simulation of Interferometric Synthetic Aperture Radar (InSAR) data plays a critical role in algorithm development, mission design, and machine learning pretraining. However, most existing approaches rely on handcrafted simulation pipelines or rigid data generation setups that lack adaptability to varying sensor configurations. In this work, we propose a novel deep learning-based framework for generating realistic, complex-valued InSAR data that is explicitly conditioned on acquisition parameters, such as wavelength, baseline, incidence angle, and revisit time. The proposed multitask model jointly synthesizes scene reflectivity, coherence magnitude, and interferometric phase while enabling the simulation of temporal decorrelation and noise characteristics via both analytical and data-driven modules. We further introduce a reparameterizable noise model for speckle and phase noise, whose statistics are tuned using Particle Swarm Optimization to match real sensor characteristics. Extensive experiments on the high-resolution GeoNRW TDX dataset demonstrate the realism and flexibility of our simulation framework, which can generalize to unseen acquisition scenarios and support SAR mission design.&lt;/p&gt;</content:encoded></item><item><title>LSCNet: An Adaptive Cloud Detection Network Via Local-Global Spatial Context</title><link>https://doi.org/10.1109/jstars.2026.3659848</link><guid>10.1109/jstars.2026.3659848</guid><pubDate>Mon, 02 Feb 2026 20:44:32 +0000</pubDate><dc:creator>Xinyu Cui</dc:creator><dc:creator>Bing Tu</dc:creator><dc:creator>Bo Liu</dc:creator><dc:creator>Yan He</dc:creator><dc:creator>Antonio Plaza</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3659848</prism:doi><description>The ubiquitous presence of clouds in optical remote sensing images (RSIs) degrades image quality. Thin and fragmented clouds often exhibit low contrast and diverse morphology in complex scenes, which poses a significant challenge for accurate detection. To address the current challenges in thin cloud detection, this study proposes an Adaptive Cloud Detection Network in Remote Sensing Images via Local-Global Spatial Context (LSCNet). It aims to process thin cloud features within global and local spatial contexts, thereby improving the accuracy and robustness of thin cloud detection in complex environments. Specifically, the network simulates a dual perspective by constructing a Mamba-based Multiscale Fusion (MDF) block. This block utilizes learnable fusion weights to adaptively integrate differential and complementary information, thereby capturing thin cloud variations across both spatial and spectral dimensions in RSIs. Additionally, we propose a Local Gated Mamba (LGM) block for detailed feature enhancement. This module utilizes a spatial gating mechanism inspired by Long Short-Term Memory to capture key thin-cloud features and suppress residual background noise. By fully leveraging the spatial structure and morphology of thin clouds and building connections between thin cloud features across different spatial scales, the module achieves precise segmentation of cloud and ground features, thereby boosting the classification performance for thin and thick clouds in identical observational contexts. Extensive experiments conducted on the L8-Biome dataset and the WHUS2-CD+ dataset demonstrate that our method outperforms other existing cloud detection methods.
Published: 2026-02-02T20:44:32+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xinyu Cui; Bing Tu; Bo Liu; Yan He; Antonio Plaza&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3659848"&gt;10.1109/jstars.2026.3659848&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;The ubiquitous presence of clouds in optical remote sensing images (RSIs) degrades image quality. Thin and fragmented clouds often exhibit low contrast and diverse morphology in complex scenes, which poses a significant challenge for accurate detection. To address the current challenges in thin cloud detection, this study proposes an Adaptive Cloud Detection Network in Remote Sensing Images via Local-Global Spatial Context (LSCNet). It aims to process thin cloud features within global and local spatial contexts, thereby improving the accuracy and robustness of thin cloud detection in complex environments. Specifically, the network simulates a dual perspective by constructing a Mamba-based Multiscale Fusion (MDF) block. This block utilizes learnable fusion weights to adaptively integrate differential and complementary information, thereby capturing thin cloud variations across both spatial and spectral dimensions in RSIs. Additionally, we propose a Local Gated Mamba (LGM) block for detailed feature enhancement. This module utilizes a spatial gating mechanism inspired by Long Short-Term Memory to capture key thin-cloud features and suppress residual background noise. By fully leveraging the spatial structure and morphology of thin clouds and building connections between thin cloud features across different spatial scales, the module achieves precise segmentation of cloud and ground features, thereby boosting the classification performance for thin and thick clouds in identical observational contexts. Extensive experiments conducted on the L8-Biome dataset and the WHUS2-CD+ dataset demonstrate that our method outperforms other existing cloud detection methods.&lt;/p&gt;</content:encoded></item><item><title>Modality-Collaborative Low-Rank Decomposers for Few-Shot Video Domain Adaptation</title><link>https://doi.org/10.1109/tmm.2026.3660140</link><guid>10.1109/tmm.2026.3660140</guid><pubDate>Mon, 02 Feb 2026 20:45:03 +0000</pubDate><dc:creator>Yuyang Wanyan</dc:creator><dc:creator>Xiaoshan Yang</dc:creator><dc:creator>Weiming Dong</dc:creator><dc:creator>Changsheng Xu</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3660140</prism:doi><description>In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative Low Rank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and sub routers, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.
Published: 2026-02-02T20:45:03+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuyang Wanyan; Xiaoshan Yang; Weiming Dong; Changsheng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3660140"&gt;10.1109/tmm.2026.3660140&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;In this paper, we study the challenging task of Few-Shot Video Domain Adaptation (FSVDA). The multimodal nature of videos introduces unique challenges, necessitating the simultaneous consideration of both domain alignment and modality collaboration in a few-shot scenario, which is ignored in previous literature. We observe that, under the influence of domain shift, the generalization performance on the target domain of each individual modality, as well as that of fused multimodal features, is constrained. Because each modality is comprised of coupled features with multiple components that exhibit different domain shifts. This variability increases the complexity of domain adaptation, thereby reducing the effectiveness of multimodal feature integration. To address these challenges, we introduce a novel framework of Modality-Collaborative Low Rank Decomposers (MC-LRD) to decompose modality-unique and modality-shared features with different domain shift levels from each modality that are more friendly for domain alignment. The MC-LRD comprises multiple decomposers for each modality and Multimodal Decomposition Routers (MDR). Each decomposer has progressively shared parameters across different modalities. The MDR is leveraged to selectively activate the decomposers to produce modality-unique and modality-shared features. To ensure efficient decomposition, we apply orthogonal decorrelation constraints separately to decomposers and sub routers, enhancing their diversity. Furthermore, we propose a cross-domain activation consistency loss to guarantee that target and source samples of the same category exhibit consistent activation preferences of the decomposers, thereby facilitating domain alignment. Extensive experimental results on three public benchmarks demonstrate that our model achieves significant improvements over existing methods.&lt;/p&gt;</content:encoded></item></channel></rss>