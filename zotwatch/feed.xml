<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sat, 27 Dec 2025 02:35:45 +0000</lastBuildDate><item><title>SAM-I2V++: Efficiently Upgrading SAM for Promptable Video Segmentation</title><link>https://doi.org/10.1109/tpami.2025.3648863</link><guid>10.1109/tpami.2025.3648863</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Haiyang Mei</dc:creator><dc:creator>Pengyu Zhang</dc:creator><dc:creator>Mike Zheng Shou</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648863</prism:doi><description>Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V++, a training-efficient image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM's static image encoder to enable spatiotemporal video perception, (ii) a memory selective associator that retrieves the most relevant past frames via similarity-driven selection and uses multiscale-enhanced cross-attention to associate selected memory features with the current frame, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves 93% of SAM 2's performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Project page: https://github.com/showlab/SAM-I2V.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.851 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haiyang Mei; Pengyu Zhang; Mike Zheng Shou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648863"&gt;10.1109/tpami.2025.3648863&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.851 (must_read)&lt;/p&gt;
&lt;p&gt;Foundation models like the Segment Anything Model (SAM) have significantly advanced promptable image segmentation in computer vision. However, extending these capabilities to videos presents substantial challenges, particularly in ensuring precise and temporally consistent mask propagation in dynamic scenes. SAM 2 attempts to address this by training a model on massive image and video data from scratch to learn complex spatiotemporal associations, resulting in huge training costs that hinder research and practical deployment. In this paper, we introduce SAM-I2V++, a training-efficient image-to-video upgradation method for cultivating a promptable video segmentation (PVS) model. Our approach strategically upgrades the pre-trained SAM to support PVS, significantly reducing training complexity and resource requirements. To achieve this, we introduce three key innovations: (i) an image-to-video feature extraction upgrader built upon SAM&amp;#x27;s static image encoder to enable spatiotemporal video perception, (ii) a memory selective associator that retrieves the most relevant past frames via similarity-driven selection and uses multiscale-enhanced cross-attention to associate selected memory features with the current frame, and (iii) a memory-as-prompt mechanism leveraging object memory to ensure temporally consistent mask propagation in dynamic scenes. Comprehensive experiments demonstrate that our method achieves 93% of SAM 2&amp;#x27;s performance while using only 0.2% of its training cost. Our work presents a resource-efficient pathway to PVS, lowering barriers for further research in PVS model design and enabling broader applications and advancements in the field. Project page: https://github.com/showlab/SAM-I2V.&lt;/p&gt;</content:encoded></item><item><title>Data-Driven Bidirectional Spatial-Adaptive Network for Weakly Supervised Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tpami.2025.3646464</link><guid>10.1109/tpami.2025.3646464</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Zebin Wu</dc:creator><dc:creator>Shangdong Zheng</dc:creator><dc:creator>Yang Xu</dc:creator><dc:creator>Le Wang</dc:creator><dc:creator>Zhihui Wei</dc:creator><dc:creator>Gang Hua</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3646464</prism:doi><description>Weakly-supervised object detection (WSOD) learns detectors with only image-level classification annotations. Without precise instance-level labels, most previous WSOD methods in remote sensing images (RSIs) select the highest-scoring proposals as the final detection results, which are confronted by two major challenges: (1) instances with small scale or rare poses are easily neglected; (2) optimizing network by the top-scoring region inevitably overlooks many valuable candidate proposals. To mitigate the above-mentioned challenges, we propose a data-driven bidirectional spatial-adaptive network (BSANet). It contains a forward-reverse spatial dropout (FRSD) module to reduce instance ambiguity induced from extreme scales and poses, as well as crowded scene, and to better excavate the entire instances. From attention learning perspective, the proposed FRSD is conceptually similar to a data-driven hard attention mechanism, which adaptively samples and reconstructs the spatially related regions for mining more latent feature responses. Meanwhile, our FRSD effectively alleviates the inherent problem that non-parametric hard attention learning fashion cannot adapt to different datasets. In addition, we build a soft attention branch to simultaneously model soft pixel-level and hard region-level attention information for exploring the complementary benefit between soft and hard attention learning. We evaluate our BSANet on the challenging NWPU VHR-10.v2 and DIOR datasets. Experimental results demonstrate that our method sets a new state-of-the-art.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.848 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zebin Wu; Shangdong Zheng; Yang Xu; Le Wang; Zhihui Wei; Gang Hua&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3646464"&gt;10.1109/tpami.2025.3646464&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.848 (must_read)&lt;/p&gt;
&lt;p&gt;Weakly-supervised object detection (WSOD) learns detectors with only image-level classification annotations. Without precise instance-level labels, most previous WSOD methods in remote sensing images (RSIs) select the highest-scoring proposals as the final detection results, which are confronted by two major challenges: (1) instances with small scale or rare poses are easily neglected; (2) optimizing network by the top-scoring region inevitably overlooks many valuable candidate proposals. To mitigate the above-mentioned challenges, we propose a data-driven bidirectional spatial-adaptive network (BSANet). It contains a forward-reverse spatial dropout (FRSD) module to reduce instance ambiguity induced from extreme scales and poses, as well as crowded scene, and to better excavate the entire instances. From attention learning perspective, the proposed FRSD is conceptually similar to a data-driven hard attention mechanism, which adaptively samples and reconstructs the spatially related regions for mining more latent feature responses. Meanwhile, our FRSD effectively alleviates the inherent problem that non-parametric hard attention learning fashion cannot adapt to different datasets. In addition, we build a soft attention branch to simultaneously model soft pixel-level and hard region-level attention information for exploring the complementary benefit between soft and hard attention learning. We evaluate our BSANet on the challenging NWPU VHR-10.v2 and DIOR datasets. Experimental results demonstrate that our method sets a new state-of-the-art.&lt;/p&gt;</content:encoded></item><item><title>Synergistic Fusion of Multi-Temporal and Multi-Resolution SAR Data: A Hierarchical Prior Transfer Approach for Aircraft Detection</title><link>https://doi.org/10.1109/tgrs.2025.3648806</link><guid>10.1109/tgrs.2025.3648806</guid><pubDate>Fri, 26 Dec 2025 18:23:46 +0000</pubDate><dc:creator>Yipeng Zhang</dc:creator><dc:creator>Fengming Hu</dc:creator><dc:creator>Haipeng Wang</dc:creator><dc:creator>Likang Zhu</dc:creator><dc:creator>Feng Xu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648806</prism:doi><description>Current deep learning methods for synthetic aperture radar (SAR) aircraft detection typically rely on single-temporal, high-resolution imagery, requiring extensive labeled datasets and limiting generalization across sensors and resolutions. Meanwhile, abundant multi-temporal, coarse- and moderate-resolution SAR images from satellites such as Sentinel-1, offering valuable temporal cues about stable aircraft positions at designated parking stands, remain largely underutilized. Additionally, existing techniques inadequately exploit the complementary advantages of multi-resolution SAR data, overlooking the synergy between global context from coarse imagery and detailed localization from fine-resolution imagery. To bridge this gap, we propose the hierarchical prior transfer approach (HiPTA), a novel framework for synergistic fusion of multi-temporal and multi-resolution SAR data in a coarse-to-fine detection paradigm. First, the coarse-resolution multi-temporal prior extraction (CMPE) module mines low-resolution time series to identify robust parking-stand priors. Next, the intermediate-resolution prior fusion and refinement (IPFR) and hierarchical multi-resolution prior registration (HMPR) modules align and refine these priors across intermediate and high-resolution domains, drastically narrowing the search space. Finally, the Fine-Resolution Aircraft Detection (FRAD) module employs a domain-adaptive frequency-adaptive filtering network to classify candidate regions in high-resolution imagery. Extensive experiments on multi-source C- and Ku-band SAR datasets show that HiPTA consistently improves precision–recall trade-offs and maintains robustness across sensors and resolutions, delivering consistent gains over single-resolution detectors and airport-detection-aided methods.
Published: 2025-12-26T18:23:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.833 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yipeng Zhang; Fengming Hu; Haipeng Wang; Likang Zhu; Feng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648806"&gt;10.1109/tgrs.2025.3648806&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.833 (must_read)&lt;/p&gt;
&lt;p&gt;Current deep learning methods for synthetic aperture radar (SAR) aircraft detection typically rely on single-temporal, high-resolution imagery, requiring extensive labeled datasets and limiting generalization across sensors and resolutions. Meanwhile, abundant multi-temporal, coarse- and moderate-resolution SAR images from satellites such as Sentinel-1, offering valuable temporal cues about stable aircraft positions at designated parking stands, remain largely underutilized. Additionally, existing techniques inadequately exploit the complementary advantages of multi-resolution SAR data, overlooking the synergy between global context from coarse imagery and detailed localization from fine-resolution imagery. To bridge this gap, we propose the hierarchical prior transfer approach (HiPTA), a novel framework for synergistic fusion of multi-temporal and multi-resolution SAR data in a coarse-to-fine detection paradigm. First, the coarse-resolution multi-temporal prior extraction (CMPE) module mines low-resolution time series to identify robust parking-stand priors. Next, the intermediate-resolution prior fusion and refinement (IPFR) and hierarchical multi-resolution prior registration (HMPR) modules align and refine these priors across intermediate and high-resolution domains, drastically narrowing the search space. Finally, the Fine-Resolution Aircraft Detection (FRAD) module employs a domain-adaptive frequency-adaptive filtering network to classify candidate regions in high-resolution imagery. Extensive experiments on multi-source C- and Ku-band SAR datasets show that HiPTA consistently improves precision–recall trade-offs and maintains robustness across sensors and resolutions, delivering consistent gains over single-resolution detectors and airport-detection-aided methods.&lt;/p&gt;</content:encoded></item><item><title>Semantic-Anchored Cross-Modal Distillation Framework With Foundation Models for SAR Ship Recognition</title><link>https://doi.org/10.1109/taes.2025.3648374</link><guid>10.1109/taes.2025.3648374</guid><pubDate>Thu, 25 Dec 2025 18:28:22 +0000</pubDate><dc:creator>Xuemeng Hui</dc:creator><dc:creator>Zhunga Liu</dc:creator><dc:creator>Shun Yao</dc:creator><dc:creator>Meiqin Liu</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3648374</prism:doi><description>Synthetic Aperture Radar (SAR) offers day-and-night capability for ship recognition, but its scattering mechanism results in limited textural and spectral detail compared to optical imagery, hindering fine-grained semantic interpretation and recognition. Existing cross-modal transfer methods mainly align features or pixels between SAR and optical imagery, yet they fail to guarantee semantic consistency across modalities. To address this, we propose a Semantic-anchored Cross-modal Distillation Framework (SCDF) with foundation models. SCDF introduces textual semantic descriptors for each ship category as semantic anchors to ensure cross-modal semantic consistency, while incorporating scattering topology maps into SAR images, thus enabling effective transfer without sacrificing modality-specific discriminability. Within this framework, a language foundation model encodes semantic anchors into text embeddings as class references, formulating ship recognition as aligning visual features with semantic anchors. To enhance the alignment between SAR features and anchors, a scattering-aware student model integrates scattering topology maps with SAR imagery, emphasizing key ship structures. This alignment is further guided by a vision foundation model acting as the optical teacher, which provides reliable optical-semantic similarity for distillation. Instead of simply transferring labels or features, the semantic-anchored distillation transfers semantic discriminability from the optical domain to SAR while preserving SAR-specific scattering topology features. Extensive experiments on the FUSAR-Ship dataset and fine-grained optical datasets (FGSC-23 and FGSCR-42) demonstrate that SCDF effectively bridges SAR and optical imagery and enhances SAR ship recognition.
Published: 2025-12-25T18:28:22+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuemeng Hui; Zhunga Liu; Shun Yao; Meiqin Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3648374"&gt;10.1109/taes.2025.3648374&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) offers day-and-night capability for ship recognition, but its scattering mechanism results in limited textural and spectral detail compared to optical imagery, hindering fine-grained semantic interpretation and recognition. Existing cross-modal transfer methods mainly align features or pixels between SAR and optical imagery, yet they fail to guarantee semantic consistency across modalities. To address this, we propose a Semantic-anchored Cross-modal Distillation Framework (SCDF) with foundation models. SCDF introduces textual semantic descriptors for each ship category as semantic anchors to ensure cross-modal semantic consistency, while incorporating scattering topology maps into SAR images, thus enabling effective transfer without sacrificing modality-specific discriminability. Within this framework, a language foundation model encodes semantic anchors into text embeddings as class references, formulating ship recognition as aligning visual features with semantic anchors. To enhance the alignment between SAR features and anchors, a scattering-aware student model integrates scattering topology maps with SAR imagery, emphasizing key ship structures. This alignment is further guided by a vision foundation model acting as the optical teacher, which provides reliable optical-semantic similarity for distillation. Instead of simply transferring labels or features, the semantic-anchored distillation transfers semantic discriminability from the optical domain to SAR while preserving SAR-specific scattering topology features. Extensive experiments on the FUSAR-Ship dataset and fine-grained optical datasets (FGSC-23 and FGSCR-42) demonstrate that SCDF effectively bridges SAR and optical imagery and enhances SAR ship recognition.&lt;/p&gt;</content:encoded></item><item><title>Self-supervised representation learning for cloud detection using Sentinel-2 images</title><link>https://doi.org/10.1016/j.rse.2025.115205</link><guid>10.1016/j.rse.2025.115205</guid><pubDate>Thu, 25 Dec 2025 04:44:10 +0000</pubDate><dc:creator>Yawogan Jean Eudes Gbodjo</dc:creator><dc:creator>Lloyd Haydn Hughes</dc:creator><dc:creator>Matthieu Molinier</dc:creator><dc:creator>Devis Tuia</dc:creator><dc:creator>Jun Li</dc:creator><prism:publicationName>Remote Sensing of Environment</prism:publicationName><prism:doi>10.1016/j.rse.2025.115205</prism:doi><description>The unavoidable presence of clouds and their shadows in optical satellite imagery hinders the true spectral response of the Earth’s underlying surface. Accurate cloud and cloud shadow detection is therefore a crucial preprocessing step for optical satellite images and any downstream analysis. Various methods have been developed to address this critical task and can be broadly categorized into physical rule-based methods and learning based methods. In recent years, machine learning based methods, particularly deep learning frameworks, have proven to outperform physical rule-based models. However, these approaches are mostly fully supervised and require a large amount of pixel-level annotations whose acquisition is costly and time consuming. In this work, we propose to address cloud and cloud shadow detection in optical satellite images using self-supervised representation learning, a machine learning paradigm that focuses on extracting relevant representations from unlabeled data, which can then be used as an effective starting point to fine-tune models with few labeled data in a supervised fashion. These approaches have been shown to perform competitively with fully supervised methods without the requirement of large annotation datasets. Specifically, we assessed two self-supervised representation learning methods that use different philosophies about self-supervision: Momentum Contrast (MoCo), based on contrastive learning and DeepCluster, based on clustering. Using two publicly available Sentinel-2 cloud datasets, namely WHUS2–CD+ and CloudSEN12, we show that MoCo and DeepCluster, trained with only 25 % of the annotated data, can perform better than physical rule-based methods such as FMask and Sen2Cor, weakly supervised methods and even several fully supervised methods. These results highlight the strong applicability of self-supervised representation learning methods to the task of cloud and cloud shadow detection with self-supervised pretraining leading to fine-tuned models that outperform industry standards and achieve near state-of-the-art performance with a fraction of the data.
Published: 2025-12-25T04:44:10+00:00
Venue: Remote Sensing of Environment
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yawogan Jean Eudes Gbodjo; Lloyd Haydn Hughes; Matthieu Molinier; Devis Tuia; Jun Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing of Environment&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.rse.2025.115205"&gt;10.1016/j.rse.2025.115205&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;The unavoidable presence of clouds and their shadows in optical satellite imagery hinders the true spectral response of the Earth’s underlying surface. Accurate cloud and cloud shadow detection is therefore a crucial preprocessing step for optical satellite images and any downstream analysis. Various methods have been developed to address this critical task and can be broadly categorized into physical rule-based methods and learning based methods. In recent years, machine learning based methods, particularly deep learning frameworks, have proven to outperform physical rule-based models. However, these approaches are mostly fully supervised and require a large amount of pixel-level annotations whose acquisition is costly and time consuming. In this work, we propose to address cloud and cloud shadow detection in optical satellite images using self-supervised representation learning, a machine learning paradigm that focuses on extracting relevant representations from unlabeled data, which can then be used as an effective starting point to fine-tune models with few labeled data in a supervised fashion. These approaches have been shown to perform competitively with fully supervised methods without the requirement of large annotation datasets. Specifically, we assessed two self-supervised representation learning methods that use different philosophies about self-supervision: Momentum Contrast (MoCo), based on contrastive learning and DeepCluster, based on clustering. Using two publicly available Sentinel-2 cloud datasets, namely WHUS2–CD+ and CloudSEN12, we show that MoCo and DeepCluster, trained with only 25 % of the annotated data, can perform better than physical rule-based methods such as FMask and Sen2Cor, weakly supervised methods and even several fully supervised methods. These results highlight the strong applicability of self-supervised representation learning methods to the task of cloud and cloud shadow detection with self-supervised pretraining leading to fine-tuned models that outperform industry standards and achieve near state-of-the-art performance with a fraction of the data.&lt;/p&gt;</content:encoded></item><item><title>A Spatio-Spectral-Temporal Progressive Algorithm for Infrared Tiny Target Detection in Cluttered Scenes</title><link>https://doi.org/10.1109/tgrs.2025.3648555</link><guid>10.1109/tgrs.2025.3648555</guid><pubDate>Thu, 25 Dec 2025 18:26:10 +0000</pubDate><dc:creator>Jiacheng Wang</dc:creator><dc:creator>Feng Pan</dc:creator><dc:creator>Xinheng Han</dc:creator><dc:creator>Xiuli Xin</dc:creator><dc:creator>Jielei Xu</dc:creator><dc:creator>Haoyuan Zhang</dc:creator><dc:creator>Weixing Li</dc:creator><dc:creator>Ji Liu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648555</prism:doi><description>Infrared tiny target detection is of great value in fields such as military reconnaissance and security early warning but faces challenges including low signal-to-noise ratio, performance-efficiency trade-offs, and detection-false alarm compromises in complex dynamic scenarios. To solve these questions, we propose a novel Spatio-Spectral-Temporal Progressive (SSTP) Algorithm, integrating spatial, spectral and temporal features for infrared tiny target detection in cluttered scenes. First, it adopts anisotropic gradient difference detection algorithm to construct a spatial candidate target set based on the anisotropic radiation characteristics of target neighborhoods. Then, we use the isolation penalty adaptive clustering algorithm to obtain boundaries via outlier-enhanced clustering, and design a multilateral context filling algorithm to generate suspected regions and fill internal boundary information. Additionally, we develop an adaptive nonlinear geometric filter for point screening using nonlinear structural features, apply a multi-scale wavelet energy filter to capture high-frequency features, and utilize a target-background local difference measurement algorithm to extract regional independence for screening. Based on the proposed single frame detection method, a multi-dimensional feature fusion-based dynamic target tracking algorithm is employed to extract moving targets. Experiments show that on multi-frame datasets DSAT and single-frame datasets SIRST, the proposed method significantly outperforms mainstream algorithms, achieving detection rates of 98.75% and 98.23% as well as false alarm rates of 2.56 × 10−6 and 10.86 × 10−6, respectively. The algorithm not only performs well in multi frame detection, but also has good performance in single frame detection. It thus provides a solution with high robustness and real-time performance for infrared early warning systems in complex environments.
Published: 2025-12-25T18:26:10+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiacheng Wang; Feng Pan; Xinheng Han; Xiuli Xin; Jielei Xu; Haoyuan Zhang; Weixing Li; Ji Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648555"&gt;10.1109/tgrs.2025.3648555&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared tiny target detection is of great value in fields such as military reconnaissance and security early warning but faces challenges including low signal-to-noise ratio, performance-efficiency trade-offs, and detection-false alarm compromises in complex dynamic scenarios. To solve these questions, we propose a novel Spatio-Spectral-Temporal Progressive (SSTP) Algorithm, integrating spatial, spectral and temporal features for infrared tiny target detection in cluttered scenes. First, it adopts anisotropic gradient difference detection algorithm to construct a spatial candidate target set based on the anisotropic radiation characteristics of target neighborhoods. Then, we use the isolation penalty adaptive clustering algorithm to obtain boundaries via outlier-enhanced clustering, and design a multilateral context filling algorithm to generate suspected regions and fill internal boundary information. Additionally, we develop an adaptive nonlinear geometric filter for point screening using nonlinear structural features, apply a multi-scale wavelet energy filter to capture high-frequency features, and utilize a target-background local difference measurement algorithm to extract regional independence for screening. Based on the proposed single frame detection method, a multi-dimensional feature fusion-based dynamic target tracking algorithm is employed to extract moving targets. Experiments show that on multi-frame datasets DSAT and single-frame datasets SIRST, the proposed method significantly outperforms mainstream algorithms, achieving detection rates of 98.75% and 98.23% as well as false alarm rates of 2.56 × 10−6 and 10.86 × 10−6, respectively. The algorithm not only performs well in multi frame detection, but also has good performance in single frame detection. It thus provides a solution with high robustness and real-time performance for infrared early warning systems in complex environments.&lt;/p&gt;</content:encoded></item><item><title>LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</title><link>https://arxiv.org/abs/2512.20217v1</link><guid>http://arxiv.org/abs/2512.20217v1</guid><pubDate>Tue, 23 Dec 2025 10:16:33 +0000</pubDate><dc:creator>Xiangxuan Ren</dc:creator><dc:creator>Zhongdao Wang</dc:creator><dc:creator>Pin Tang</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Jilai Zheng</dc:creator><dc:creator>Chao Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.
Published: 2025-12-23T10:16:33+00:00
Venue: arXiv
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangxuan Ren; Zhongdao Wang; Pin Tang; Guoqing Wang; Jilai Zheng; Chao Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.&lt;/p&gt;</content:encoded></item><item><title>Attention-driven Contrastive Learning for Cross-Modal Hashing with Prototypical Separation</title><link>https://doi.org/10.1016/j.inffus.2025.104078</link><guid>10.1016/j.inffus.2025.104078</guid><pubDate>Thu, 25 Dec 2025 16:05:29 +0000</pubDate><dc:creator>Zhipeng He</dc:creator><dc:creator>Wenzhe Liu</dc:creator><dc:creator>Lian Wu</dc:creator><dc:creator>Jinrong Cui</dc:creator><dc:creator>Jie Wen</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104078</prism:doi><description>Effective retrieval and structuring of heterogeneous data have grown more difficult due to the exponential development of multimedia data. The surge in data volume emphasizes the importance of efficient cross-modal hashing techniques, known for their rapid retrieval speed and minimal storage requirements, which have garnered attention recently. However, existing unsupervised cross-modal hashing methods often fail to capture latent semantic structures and meaningful modality interactions, which limits their retrieval performance. To address these challenges, we propose Attention-driven Contrastive Learning for Cross-Modal Hashing via Prototypical Separation (ACoPSe). The method introduces a modality-aware fusion mechanism to enhance cross-modal feature interaction and a prototype alignment strategy that reduces heterogeneity at the cluster level by leveraging pseudo-labels derived from clustering. Extensive experiments demonstrate that our method achieves comparable performance to state-of-the-art approaches.
Published: 2025-12-25T16:05:29+00:00
Venue: Information Fusion
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhipeng He; Wenzhe Liu; Lian Wu; Jinrong Cui; Jie Wen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104078"&gt;10.1016/j.inffus.2025.104078&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Effective retrieval and structuring of heterogeneous data have grown more difficult due to the exponential development of multimedia data. The surge in data volume emphasizes the importance of efficient cross-modal hashing techniques, known for their rapid retrieval speed and minimal storage requirements, which have garnered attention recently. However, existing unsupervised cross-modal hashing methods often fail to capture latent semantic structures and meaningful modality interactions, which limits their retrieval performance. To address these challenges, we propose Attention-driven Contrastive Learning for Cross-Modal Hashing via Prototypical Separation (ACoPSe). The method introduces a modality-aware fusion mechanism to enhance cross-modal feature interaction and a prototype alignment strategy that reduces heterogeneity at the cluster level by leveraging pseudo-labels derived from clustering. Extensive experiments demonstrate that our method achieves comparable performance to state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>DAWDet: A Dynamic Content-Aware Multi-Branch Framework with Adaptive Wavelet Boosting for Small Object Detection</title><link>https://doi.org/10.1016/j.patcog.2025.112979</link><guid>10.1016/j.patcog.2025.112979</guid><pubDate>Thu, 25 Dec 2025 15:57:06 +0000</pubDate><dc:creator>Yuting Wu</dc:creator><dc:creator>Shaolei Liu</dc:creator><dc:creator>Dongchen Zhu</dc:creator><dc:creator>Lei Wang</dc:creator><dc:creator>Jiamao Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112979</prism:doi><description>Small Object Detection (SOD) aims to classify and localize objects with limited regions, playing a pivotal role in surveillance systems, intelligent transportation, and aerial inspection applications. Compared to general object detection, SOD confronts three fundamental limitations: inadequate discriminative feature representation, scarcity of high-quality training samples, and severe information loss. To address these problems, we propose DAWDet, a novel framework tailored for SOD tasks. First, we design a Dynamic content-aware Multi-branch Feature Pyramid Network (DMFPN) based on adaptive content-aware grid sampling and refined network topology, to obtain richer location information and semantic representation of small objects. Second, we develop an Adaptive Label Assignment Strategy (ALAS) to increase the quantity of high-quality positive samples, which optimizes the regression branch for high-quality small object samples via a designed overlap transformation function. Third, to mitigate information loss, we incorporate lightweight Haar Wavelet transform Downsampling (HWD) modules into the feature fusion process, effectively preserving crucial high-frequency details during resolution reduction. Comprehensive evaluations on standard SOD benchmarks demonstrate our framework achieves state-of-the-art performance while maintaining computational efficiency.
Published: 2025-12-25T15:57:06+00:00
Venue: Pattern Recognition
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuting Wu; Shaolei Liu; Dongchen Zhu; Lei Wang; Jiamao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112979"&gt;10.1016/j.patcog.2025.112979&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Small Object Detection (SOD) aims to classify and localize objects with limited regions, playing a pivotal role in surveillance systems, intelligent transportation, and aerial inspection applications. Compared to general object detection, SOD confronts three fundamental limitations: inadequate discriminative feature representation, scarcity of high-quality training samples, and severe information loss. To address these problems, we propose DAWDet, a novel framework tailored for SOD tasks. First, we design a Dynamic content-aware Multi-branch Feature Pyramid Network (DMFPN) based on adaptive content-aware grid sampling and refined network topology, to obtain richer location information and semantic representation of small objects. Second, we develop an Adaptive Label Assignment Strategy (ALAS) to increase the quantity of high-quality positive samples, which optimizes the regression branch for high-quality small object samples via a designed overlap transformation function. Third, to mitigate information loss, we incorporate lightweight Haar Wavelet transform Downsampling (HWD) modules into the feature fusion process, effectively preserving crucial high-frequency details during resolution reduction. Comprehensive evaluations on standard SOD benchmarks demonstrate our framework achieves state-of-the-art performance while maintaining computational efficiency.&lt;/p&gt;</content:encoded></item><item><title>SAREval: A Multi-Dimensional and Multi-Task Benchmark for Evaluating Visual Language Models on SAR Image Understanding</title><link>https://doi.org/10.3390/rs18010082</link><guid>10.3390/rs18010082</guid><pubDate>Fri, 26 Dec 2025 03:06:02 +0000</pubDate><dc:creator>Ziyan Wang</dc:creator><dc:creator>Lei Liu</dc:creator><dc:creator>Gang Wan</dc:creator><dc:creator>Yuchen Lu</dc:creator><dc:creator>Fengjie Zheng</dc:creator><dc:creator>Guangde Sun</dc:creator><dc:creator>Yixiang Huang</dc:creator><dc:creator>Shihao Guo</dc:creator><dc:creator>Xinyi Li</dc:creator><dc:creator>Liang Yuan</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18010082</prism:doi><description>Vision-Language Models (VLMs) demonstrate significant potential for remote sensing interpretation through multimodal fusion and semantic representation of imagery. However, their adaptation to Synthetic Aperture Radar (SAR) remains challenging due to fundamental differences in imaging mechanisms and physical properties compared to optical remote sensing. SAREval, the first comprehensive benchmark specifically designed for SAR image understanding, incorporates SAR-specific characteristics, including scattering mechanisms and polarization features, through a hierarchical framework spanning perception, reasoning, and robustness capabilities. It encompasses 20 tasks from image classification to physical-attribute inference with over 10,000 high-quality image–text pairs. Extensive experiments conducted on 11 mainstream VLMs reveal substantial limitations in SAR image interpretation. Models achieve merely 25.35% accuracy in fine-grained ship classification tasks and demonstrate significant difficulties in establishing mappings between visual features and physical parameters. Furthermore, certain models exhibit unexpected performance improvements under certain noise conditions that challenge conventional robustness understanding. SAREval establishes an essential foundation for developing and evaluating VLMs in SAR image interpretation, providing standardized assessment protocols and quality-controlled annotations for cross-modal remote sensing research.
Published: 2025-12-26T03:06:02+00:00
Venue: Remote Sensing
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ziyan Wang; Lei Liu; Gang Wan; Yuchen Lu; Fengjie Zheng; Guangde Sun; Yixiang Huang; Shihao Guo; Xinyi Li; Liang Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18010082"&gt;10.3390/rs18010082&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Vision-Language Models (VLMs) demonstrate significant potential for remote sensing interpretation through multimodal fusion and semantic representation of imagery. However, their adaptation to Synthetic Aperture Radar (SAR) remains challenging due to fundamental differences in imaging mechanisms and physical properties compared to optical remote sensing. SAREval, the first comprehensive benchmark specifically designed for SAR image understanding, incorporates SAR-specific characteristics, including scattering mechanisms and polarization features, through a hierarchical framework spanning perception, reasoning, and robustness capabilities. It encompasses 20 tasks from image classification to physical-attribute inference with over 10,000 high-quality image–text pairs. Extensive experiments conducted on 11 mainstream VLMs reveal substantial limitations in SAR image interpretation. Models achieve merely 25.35% accuracy in fine-grained ship classification tasks and demonstrate significant difficulties in establishing mappings between visual features and physical parameters. Furthermore, certain models exhibit unexpected performance improvements under certain noise conditions that challenge conventional robustness understanding. SAREval establishes an essential foundation for developing and evaluating VLMs in SAR image interpretation, providing standardized assessment protocols and quality-controlled annotations for cross-modal remote sensing research.&lt;/p&gt;</content:encoded></item><item><title>MFF-MTT: A Multi-feature Fusion-based Deep Learning Algorithm for Maneuvering Target Tracking</title><link>https://doi.org/10.1016/j.inffus.2025.104093</link><guid>10.1016/j.inffus.2025.104093</guid><pubDate>Thu, 25 Dec 2025 00:16:04 +0000</pubDate><dc:creator>Xiaoqing Hu</dc:creator><dc:creator>Hongyan Zhu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104093</prism:doi><description>In target tracking applications, traditional model-driven algorithms suffer from the model mismatch due to the lack of prior knowledge. Recently, some data-driven algorithms have been showing increasing potential in dealing with uncertain target maneuvering behaviors. To further enhance robustness to high maneuverability, we propose a multi-feature fusion-based deep learning algorithm for maneuvering target tracking (MFF-MTT) by combining the convolution and transformer network. Thereinto, the convolution network extracts the local information to capture the transition law of rapidly changing states. The Multi-Head Self-Attention (MHSA) in transformer network enables MFF-MTT to exploit the global information by weighting different parts of input sequence and integrating diverse subspace representations of queries, keys, and values. The local and global features are then fused in two forms of merge and cross to capture the short-term maneuvers and long-term trends of the trajectory jointly. Moreover, we also develop a novel encoder-decoder framework that decodes the fused features by Bi-directional Long Short-Term Memory (Bi-LSTM). In this way, a comprehensive understanding about the inherent structure of the data can be obtained to facilitate the high-accuracy state estimation. Extensive simulation results demonstrate that the proposed MFF-MTT outperforms other comparative methods on estimation precision and robustness in maneuvering target tracking scenarios.
Published: 2025-12-25T00:16:04+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoqing Hu; Hongyan Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104093"&gt;10.1016/j.inffus.2025.104093&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;In target tracking applications, traditional model-driven algorithms suffer from the model mismatch due to the lack of prior knowledge. Recently, some data-driven algorithms have been showing increasing potential in dealing with uncertain target maneuvering behaviors. To further enhance robustness to high maneuverability, we propose a multi-feature fusion-based deep learning algorithm for maneuvering target tracking (MFF-MTT) by combining the convolution and transformer network. Thereinto, the convolution network extracts the local information to capture the transition law of rapidly changing states. The Multi-Head Self-Attention (MHSA) in transformer network enables MFF-MTT to exploit the global information by weighting different parts of input sequence and integrating diverse subspace representations of queries, keys, and values. The local and global features are then fused in two forms of merge and cross to capture the short-term maneuvers and long-term trends of the trajectory jointly. Moreover, we also develop a novel encoder-decoder framework that decodes the fused features by Bi-directional Long Short-Term Memory (Bi-LSTM). In this way, a comprehensive understanding about the inherent structure of the data can be obtained to facilitate the high-accuracy state estimation. Extensive simulation results demonstrate that the proposed MFF-MTT outperforms other comparative methods on estimation precision and robustness in maneuvering target tracking scenarios.&lt;/p&gt;</content:encoded></item><item><title>PFI-Net: A Parallel Feature Interaction Network for Infrared and Visible Target Detection</title><link>https://doi.org/10.1016/j.patcog.2025.113003</link><guid>10.1016/j.patcog.2025.113003</guid><pubDate>Thu, 25 Dec 2025 00:03:38 +0000</pubDate><dc:creator>Xiaoxia Wang</dc:creator><dc:creator>Jiangtao Xi</dc:creator><dc:creator>Fengbao Yang</dc:creator><dc:creator>Yunjia Yang</dc:creator><dc:creator>Minglu Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113003</prism:doi><description>Detection networks based on deep learning mainly adopt a single feature interaction mechanism to capture the deep features of targets. As the quality of infrared or visible images deteriorates moderately or severely, this often results in the insignificance of deep feature. To surmount this deficiency, we present a parallel feature interaction network, termed PFI-Net. This architecture involves dual-branch feature extraction and enhancement, parallel feature interaction and decision fusion detector. With dual-branch feature extraction and enhancement as the premise, we construct a parallel feature interaction module with different interaction mode to avoid mutual interference between features of infrared and visible image. This parallel feature interaction module can ensure the features of infrared and visible are guided into two separate independent channels. Additionally, we devise a weighted detection boxes fusion module to achieve the integration of the parallel detection results. This module integrates the advantages of detection results from different channels to promote detection accuracy and stability. Finally, comprehensive experiments on multiple benchmark models demonstrate that the proposed PFI-Net delivers promising detection performance, outperforming other advanced alternatives.
Published: 2025-12-25T00:03:38+00:00
Venue: Pattern Recognition
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoxia Wang; Jiangtao Xi; Fengbao Yang; Yunjia Yang; Minglu Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113003"&gt;10.1016/j.patcog.2025.113003&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Detection networks based on deep learning mainly adopt a single feature interaction mechanism to capture the deep features of targets. As the quality of infrared or visible images deteriorates moderately or severely, this often results in the insignificance of deep feature. To surmount this deficiency, we present a parallel feature interaction network, termed PFI-Net. This architecture involves dual-branch feature extraction and enhancement, parallel feature interaction and decision fusion detector. With dual-branch feature extraction and enhancement as the premise, we construct a parallel feature interaction module with different interaction mode to avoid mutual interference between features of infrared and visible image. This parallel feature interaction module can ensure the features of infrared and visible are guided into two separate independent channels. Additionally, we devise a weighted detection boxes fusion module to achieve the integration of the parallel detection results. This module integrates the advantages of detection results from different channels to promote detection accuracy and stability. Finally, comprehensive experiments on multiple benchmark models demonstrate that the proposed PFI-Net delivers promising detection performance, outperforming other advanced alternatives.&lt;/p&gt;</content:encoded></item><item><title>Language Embedded 3D Gaussians for Open-Vocabulary Scene Querying</title><link>https://doi.org/10.1109/tpami.2025.3648837</link><guid>10.1109/tpami.2025.3648837</guid><pubDate>Fri, 26 Dec 2025 18:23:41 +0000</pubDate><dc:creator>Miao Wang</dc:creator><dc:creator>Jin-Chuan Shi</dc:creator><dc:creator>Shao-Hua Guan</dc:creator><dc:creator>Hao-Bin Duan</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3648837</prism:doi><description>Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.
Published: 2025-12-26T18:23:41+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Miao Wang; Jin-Chuan Shi; Shao-Hua Guan; Hao-Bin Duan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3648837"&gt;10.1109/tpami.2025.3648837&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Open-vocabulary querying in 3D space is challenging but essential for scene understanding tasks such as object localization and segmentation. Language embedded scene representations have made progress by incorporating language features into 3D spaces. However, their efficacy heavily depends on neural networks that are resource-intensive in training and rendering. Although recent 3D Gaussians offer efficient and high-quality novel view synthesis, directly embedding language features in them leads to prohibitive memory usage and decreased performance. In this work, we introduce Language Embedded 3D Gaussians, a novel scene representation for open-vocabulary query tasks. Instead of embedding high-dimensional raw semantic features on 3D Gaussians, we propose a dedicated quantization scheme that drastically alleviates the memory requirement, and a novel embedding procedure that achieves smoother yet high accuracy query, countering the multi-view feature inconsistencies and the high-frequency inductive bias in point-based representations. Our comprehensive experiments show that our representation achieves the best visual quality and language querying accuracy across current language embedded representations, while maintaining real-time rendering frame rates on a single desktop GPU.&lt;/p&gt;</content:encoded></item><item><title>IceShipvsNet:A Joint Network for Ship Detection in Ice-Infested Waters Using Visible and SWIR Images</title><link>https://doi.org/10.1109/lgrs.2025.3648659</link><guid>10.1109/lgrs.2025.3648659</guid><pubDate>Fri, 26 Dec 2025 18:25:46 +0000</pubDate><dc:creator>Bingxin Liu</dc:creator><dc:creator>Yulong Du</dc:creator><dc:creator>Yikai Huang</dc:creator><dc:creator>Peilin Wang</dc:creator><dc:creator>Peixin Cai</dc:creator><dc:creator>Ying Li</dc:creator><dc:creator>Peng Chen</dc:creator><dc:creator>Peng Liu</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3648659</prism:doi><description>In ice-infested waters, the complexity of background makes it challenging to accurately detect ship targets using only visible (VIS) remote sensing images. To address this challenge, we introduce short-wave infrared (SWIR) images to enhance target visibility and propose a joint network for ship detection in ice-infested waters using both VIS and SWIR images, termed IceShipvsNet. The joint detection backbone of this network is implemented using C2Former, which is known for its strong performance in multi-modal feature learning. To improve the multimodal feature extraction, we introduce a spectral frequency augmentation (SFA) module, which adaptively enhances or suppresses high-frequency features from VIS and SWIR images to improve target response and suppress background interference. Specifically, the SFA module incorporates two components: Frequency-Aware Modulation for VIS (FAM-VI), which regulates high-frequency noise in VIS images and enhances ship characteristics; SWIR High-Frequency Guided Attention (FGA-S), which boosts the high-frequency information in SWIR images and suppresses irrelevant background noise. Given the lack of publicly available datasets for ship detection in ice-infested waters, we construct a VIS-SWIR dataset (VS-IceShip) and use it for experimental evaluation. The results demonstrate that IceShipvsNet achieves superior detection accuracy compared to single-modal baselines on various detectors. Ablation studies further validate the effectiveness of SFA, with both FAM-VI and FGA-S contributing significantly to performance improvement.
Published: 2025-12-26T18:25:46+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bingxin Liu; Yulong Du; Yikai Huang; Peilin Wang; Peixin Cai; Ying Li; Peng Chen; Peng Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3648659"&gt;10.1109/lgrs.2025.3648659&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;In ice-infested waters, the complexity of background makes it challenging to accurately detect ship targets using only visible (VIS) remote sensing images. To address this challenge, we introduce short-wave infrared (SWIR) images to enhance target visibility and propose a joint network for ship detection in ice-infested waters using both VIS and SWIR images, termed IceShipvsNet. The joint detection backbone of this network is implemented using C2Former, which is known for its strong performance in multi-modal feature learning. To improve the multimodal feature extraction, we introduce a spectral frequency augmentation (SFA) module, which adaptively enhances or suppresses high-frequency features from VIS and SWIR images to improve target response and suppress background interference. Specifically, the SFA module incorporates two components: Frequency-Aware Modulation for VIS (FAM-VI), which regulates high-frequency noise in VIS images and enhances ship characteristics; SWIR High-Frequency Guided Attention (FGA-S), which boosts the high-frequency information in SWIR images and suppresses irrelevant background noise. Given the lack of publicly available datasets for ship detection in ice-infested waters, we construct a VIS-SWIR dataset (VS-IceShip) and use it for experimental evaluation. The results demonstrate that IceShipvsNet achieves superior detection accuracy compared to single-modal baselines on various detectors. Ablation studies further validate the effectiveness of SFA, with both FAM-VI and FGA-S contributing significantly to performance improvement.&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Benchmark of Spatial Encoding Methods for Tabular Data with Deep Neural Networks</title><link>https://doi.org/10.1016/j.inffus.2025.104088</link><guid>10.1016/j.inffus.2025.104088</guid><pubDate>Thu, 25 Dec 2025 16:05:27 +0000</pubDate><dc:creator>Jiayun Liu</dc:creator><dc:creator>Manuel Castillo-Cara</dc:creator><dc:creator>Raúl García-Castro</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104088</prism:doi><description>Despite the success of deep neural networks on perceptual data, their performance on tabular data remains limited, where traditional models still outperform them. A promising alternative is to transform tabular data into synthetic images, enabling the use of vision architectures such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, the literature lacks a large-scale, standardized benchmark evaluating these transformation techniques. This work presents the first comprehensive evaluation of nine spatial encoding methods across 24 diverse regression and classification datasets. We assess performance, scalability, and computational trade-offs under a unified framework with rigorous hyperparameter optimization. Our results reveal a performance landscape structured by data regimes, defined by sample size ( N ) and dimensionality ( d ), and show that the transformation method exerts a significantly stronger influence on predictive performance than the chosen vision architecture. In particular, REFINED emerges as the most robust transformation across tasks and datasets. Hybrid models (CNN+MLP, ViT+MLP) consistently reduce predictive variance, offering advantages especially in smaller datasets, yet play a secondary role. These findings suggest that transforming tabular data into synthetic images is a powerful, yet data-dependent, strategy. This benchmark provides clear guidance for researchers and practitioners, offering key insights into scalability, transformation behavior, and architectural interplay, establishing a comprehensive reference for future research on spatial encodings for tabular data.
Published: 2025-12-25T16:05:27+00:00
Venue: Information Fusion
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiayun Liu; Manuel Castillo-Cara; Raúl García-Castro&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104088"&gt;10.1016/j.inffus.2025.104088&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Despite the success of deep neural networks on perceptual data, their performance on tabular data remains limited, where traditional models still outperform them. A promising alternative is to transform tabular data into synthetic images, enabling the use of vision architectures such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). However, the literature lacks a large-scale, standardized benchmark evaluating these transformation techniques. This work presents the first comprehensive evaluation of nine spatial encoding methods across 24 diverse regression and classification datasets. We assess performance, scalability, and computational trade-offs under a unified framework with rigorous hyperparameter optimization. Our results reveal a performance landscape structured by data regimes, defined by sample size ( N ) and dimensionality ( d ), and show that the transformation method exerts a significantly stronger influence on predictive performance than the chosen vision architecture. In particular, REFINED emerges as the most robust transformation across tasks and datasets. Hybrid models (CNN+MLP, ViT+MLP) consistently reduce predictive variance, offering advantages especially in smaller datasets, yet play a secondary role. These findings suggest that transforming tabular data into synthetic images is a powerful, yet data-dependent, strategy. This benchmark provides clear guidance for researchers and practitioners, offering key insights into scalability, transformation behavior, and architectural interplay, establishing a comprehensive reference for future research on spatial encodings for tabular data.&lt;/p&gt;</content:encoded></item><item><title>Two-stage offline knowledge distillation for onboard registration of multispectral satellite images</title><link>https://doi.org/10.1016/j.isprsjprs.2025.12.009</link><guid>10.1016/j.isprsjprs.2025.12.009</guid><pubDate>Fri, 26 Dec 2025 14:26:48 +0000</pubDate><dc:creator>Darshana Priyasad</dc:creator><dc:creator>Tharindu Fernando</dc:creator><dc:creator>Maryam Haghighat</dc:creator><dc:creator>Harshala Gammulle</dc:creator><dc:creator>Roberto Del Prete</dc:creator><dc:creator>Clinton Fookes</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2025.12.009</prism:doi><description>Multi-band optical sensors onboard modern Earth observation satellites capture complementary spectral responses across varying spatial and spectral resolutions. To effectively fuse this information for downstream applications, accurate band co-registration is critical. However, for real-time processing, such registration must be performed onboard, where sensor distortions, platform-induced motion, and spectral disparities introduce significant challenges. Traditional feature matching algorithms struggle to cope with these variations or are often too computationally intensive for the constrained hardware typically found on small satellites. As a result, real-time onboard multimodal fusion has remained largely impractical in operational settings. With the emergence of next-generation satellites equipped with AI-enabled onboard processing, such as Australia’s Kanyini mission, there is now an opportunity to overcome these limitations. In this work, we introduce a deep learning-based, lightweight band registration framework specifically designed for real-time onboard deployment. Our approach features a band-independent teacher network that jointly leverages adversarial learning and supervised regression to estimate affine registration parameters across spectral bands. To meet hardware constraints, we employ a two-stage knowledge distillation strategy that produces a compact yet accurate student model. Experimental results demonstrate that our method delivers robust and efficient registration performance, enabling real-time spectral alignment and significantly enhancing the potential for onboard multimodal data fusion in Earth observation missions.
Published: 2025-12-26T14:26:48+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Darshana Priyasad; Tharindu Fernando; Maryam Haghighat; Harshala Gammulle; Roberto Del Prete; Clinton Fookes&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2025.12.009"&gt;10.1016/j.isprsjprs.2025.12.009&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-band optical sensors onboard modern Earth observation satellites capture complementary spectral responses across varying spatial and spectral resolutions. To effectively fuse this information for downstream applications, accurate band co-registration is critical. However, for real-time processing, such registration must be performed onboard, where sensor distortions, platform-induced motion, and spectral disparities introduce significant challenges. Traditional feature matching algorithms struggle to cope with these variations or are often too computationally intensive for the constrained hardware typically found on small satellites. As a result, real-time onboard multimodal fusion has remained largely impractical in operational settings. With the emergence of next-generation satellites equipped with AI-enabled onboard processing, such as Australia’s Kanyini mission, there is now an opportunity to overcome these limitations. In this work, we introduce a deep learning-based, lightweight band registration framework specifically designed for real-time onboard deployment. Our approach features a band-independent teacher network that jointly leverages adversarial learning and supervised regression to estimate affine registration parameters across spectral bands. To meet hardware constraints, we employ a two-stage knowledge distillation strategy that produces a compact yet accurate student model. Experimental results demonstrate that our method delivers robust and efficient registration performance, enabling real-time spectral alignment and significantly enhancing the potential for onboard multimodal data fusion in Earth observation missions.&lt;/p&gt;</content:encoded></item><item><title>A Unified Spatial-Spectral-Temporal Network for Hyperspectral Object Tracking</title><link>https://doi.org/10.1016/j.patcog.2025.113005</link><guid>10.1016/j.patcog.2025.113005</guid><pubDate>Thu, 25 Dec 2025 22:52:51 +0000</pubDate><dc:creator>Zhuanfeng Li</dc:creator><dc:creator>Jing Wang</dc:creator><dc:creator>Jue Zhang</dc:creator><dc:creator>Dong Zhao</dc:creator><dc:creator>Guanyiman Fu</dc:creator><dc:creator>Jiangtao Wang</dc:creator><dc:creator>Jianfeng Lu</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.113005</prism:doi><description>Hyperspectral object tracking offers superior performance over conventional color-based tracking by leveraging rich spectral information to enhance material discrimination ability. Due to the limited availability of hyperspectral video datasets, many hyperspectral trackers rely on spectral correlation modeling to bridge hyperspectral images (HSIs) and color images. They are often combined with pre-trained deep feature extractors for robust representation. However, these methods face two key limitations: 1) they ignore the intrinsic relationship between the object template in the initial frame and the search image in the current frame during spectral correlation modeling. This limits the ability to distinguish spectral differences between objects and backgrounds; and 2) they insufficiently utilize temporal information, which prevents the construction of a robust spatial-spectral-temporal representation and thereby limits the improvement of tracking performance. To overcome these two issues, we propose CSSTrack, a novel unified network for end-to-end hyperspectral object tracking. First and foremost, we propose a spectral-aware representation enhancement (SaRE) module that employs physical models of spectral self-expression to perform cross-frame spectral correlations between the template and search images. Different from previous works, our method enhances the discrimination of foreground-background spectral differences, thereby facilitating the extraction of discriminative spatial-spectral features. Moreover, we design a spatial-spectral-temporal modeling (S2TM) module, which utilizes a sequence of autoregressive temporal embeddings to capture motion dynamics across spectral bands and integrates static and dynamic features through a fusion network. Extensive experiments on the HOT2020 and IMEC25 datasets demonstrate the effectiveness of our proposed CSSTrack, which achieves state-of-the-art tracking performance. The source code is available at https://github.com/hscv/CSSTrack .
Published: 2025-12-25T22:52:51+00:00
Venue: Pattern Recognition
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhuanfeng Li; Jing Wang; Jue Zhang; Dong Zhao; Guanyiman Fu; Jiangtao Wang; Jianfeng Lu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.113005"&gt;10.1016/j.patcog.2025.113005&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral object tracking offers superior performance over conventional color-based tracking by leveraging rich spectral information to enhance material discrimination ability. Due to the limited availability of hyperspectral video datasets, many hyperspectral trackers rely on spectral correlation modeling to bridge hyperspectral images (HSIs) and color images. They are often combined with pre-trained deep feature extractors for robust representation. However, these methods face two key limitations: 1) they ignore the intrinsic relationship between the object template in the initial frame and the search image in the current frame during spectral correlation modeling. This limits the ability to distinguish spectral differences between objects and backgrounds; and 2) they insufficiently utilize temporal information, which prevents the construction of a robust spatial-spectral-temporal representation and thereby limits the improvement of tracking performance. To overcome these two issues, we propose CSSTrack, a novel unified network for end-to-end hyperspectral object tracking. First and foremost, we propose a spectral-aware representation enhancement (SaRE) module that employs physical models of spectral self-expression to perform cross-frame spectral correlations between the template and search images. Different from previous works, our method enhances the discrimination of foreground-background spectral differences, thereby facilitating the extraction of discriminative spatial-spectral features. Moreover, we design a spatial-spectral-temporal modeling (S2TM) module, which utilizes a sequence of autoregressive temporal embeddings to capture motion dynamics across spectral bands and integrates static and dynamic features through a fusion network. Extensive experiments on the HOT2020 and IMEC25 datasets demonstrate the effectiveness of our proposed CSSTrack, which achieves state-of-the-art tracking performance. The source code is available at https://github.com/hscv/CSSTrack .&lt;/p&gt;</content:encoded></item><item><title>Towards Arbitrary-Scale Spacecraft Image Super-Resolution via Salient Region-Guidance</title><link>https://doi.org/10.1016/j.patcog.2025.112973</link><guid>10.1016/j.patcog.2025.112973</guid><pubDate>Thu, 25 Dec 2025 15:56:50 +0000</pubDate><dc:creator>Jingfan Yang</dc:creator><dc:creator>Hu Gao</dc:creator><dc:creator>Ying Zhang</dc:creator><dc:creator>Depeng Dang</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2025.112973</prism:doi><description>Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose an efficient salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results on spacecraft radar image dataset and optical image dataset demonstrate that the proposed SGSASR outperforms state-of-the-art approaches. The codes are available at: https://github.com/shenduke/SGSASR .
Published: 2025-12-25T15:56:50+00:00
Venue: Pattern Recognition
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingfan Yang; Hu Gao; Ying Zhang; Depeng Dang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2025.112973"&gt;10.1016/j.patcog.2025.112973&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose an efficient salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results on spacecraft radar image dataset and optical image dataset demonstrate that the proposed SGSASR outperforms state-of-the-art approaches. The codes are available at: https://github.com/shenduke/SGSASR .&lt;/p&gt;</content:encoded></item><item><title>IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection</title><link>https://doi.org/10.1016/j.inffus.2025.104097</link><guid>10.1016/j.inffus.2025.104097</guid><pubDate>Thu, 25 Dec 2025 16:05:23 +0000</pubDate><dc:creator>Xuanming Cao</dc:creator><dc:creator>Chengyu Tao</dc:creator><dc:creator>Yifeng Cheng</dc:creator><dc:creator>Juan Du</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104097</prism:doi><description>Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.
Published: 2025-12-25T16:05:23+00:00
Venue: Information Fusion
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xuanming Cao; Chengyu Tao; Yifeng Cheng; Juan Du&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104097"&gt;10.1016/j.inffus.2025.104097&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Surface anomaly detection is pivotal for ensuring product quality in industrial manufacturing. While 2D image-based methods have achieved remarkable success, 3D point cloud-based detection remains underexplored despite its richer geometric cues. We argue that the key bottleneck is the absence of powerful pretrained foundation backbones in 3D comparable to those in 2D. To bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an ensemble framework that synergizes 2D pretrained expert with 3D expert models. However, naively fusing predictions from disparate sources is non-trivial: existing strategies can be affected by a poorly performing modality and thus degrade overall accuracy. To address this challenge, We introduce a novel Importance-Aware Fusion (IAF) module that dynamically assesses the contribution of each source and reweights their anomaly scores. Furthermore, we devise critical loss functions that explicitly guide the optimization of IAF, enabling it to combine the collective knowledge of the source experts but also preserve their unique strengths, thereby enhancing the overall performance of anomaly detection. Extensive experiments show that IAENet achieves a new state-of-the-art for point-level localization and ranks second at object-level on MVTec 3D-AD dataset. On the Eyecandies dataset, it achieves the best performance in both levels. Additionally, it substantially reduces false positive rates, underscoring its practical value for industrial deployment.&lt;/p&gt;</content:encoded></item><item><title>Beyond Weight Adaptation: Feature-Space Domain Injection for Cross-Modal Ship Re-Identification</title><link>https://arxiv.org/abs/2512.20892v1</link><guid>http://arxiv.org/abs/2512.20892v1</guid><pubDate>Wed, 24 Dec 2025 02:30:23 +0000</pubDate><dc:creator>Tingfeng Xian</dc:creator><dc:creator>Wenlve Zhou</dc:creator><dc:creator>Zhiheng Zhou</dc:creator><dc:creator>Zhelin Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM's pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.
Published: 2025-12-24T02:30:23+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tingfeng Xian; Wenlve Zhou; Zhiheng Zhou; Zhelin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-Modality Ship Re-Identification (CMS Re-ID) is critical for achieving all-day and all-weather maritime target tracking, yet it is fundamentally challenged by significant modality discrepancies. Mainstream solutions typically rely on explicit modality alignment strategies; however, this paradigm heavily depends on constructing large-scale paired datasets for pre-training. To address this, grounded in the Platonic Representation Hypothesis, we explore the potential of Vision Foundation Models (VFMs) in bridging modality gaps. Recognizing the suboptimal performance of existing generic Parameter-Efficient Fine-Tuning (PEFT) methods that operate within the weight space, particularly on limited-capacity models, we shift the optimization perspective to the feature space and propose a novel PEFT strategy termed Domain Representation Injection (DRI). Specifically, while keeping the VFM fully frozen to maximize the preservation of general knowledge, we design a lightweight, learnable Offset Encoder to extract domain-specific representations rich in modality and identity attributes from raw inputs. Guided by the contextual information of intermediate features at different layers, a Modulator adaptively transforms these representations. Subsequently, they are injected into the intermediate layers via additive fusion, dynamically reshaping the feature distribution to adapt to the downstream task without altering the VFM&amp;#x27;s pre-trained weights. Extensive experimental results demonstrate the superiority of our method, achieving State-of-the-Art (SOTA) performance with minimal trainable parameters. For instance, on the HOSS-ReID dataset, we attain 57.9\% and 60.5\% mAP using only 1.54M and 7.05M parameters, respectively. The code is available at https://github.com/TingfengXian/DRI.&lt;/p&gt;</content:encoded></item><item><title>From Pixels to Meters: Ground Sampling Distance Priors for Remote Sensing Detection</title><link>https://doi.org/10.1109/lgrs.2025.3648944</link><guid>10.1109/lgrs.2025.3648944</guid><pubDate>Fri, 26 Dec 2025 18:25:46 +0000</pubDate><dc:creator>Shihao Yu</dc:creator><dc:creator>Yifan Dong</dc:creator><dc:creator>Yun Su</dc:creator><dc:creator>Yang Zhao</dc:creator><dc:creator>Xiaoqiang Jia</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3648944</prism:doi><description>Object detection in remote sensing imagery is challenging because targets often appear at markedly different scales and may become visually similar when the image resolution varies. A key advantage of remote sensing imagery is that each image provides a ground sample distance (GSD) value linking pixel units to real-world size, offering physical cues that can mitigate such scale-induced ambiguity. While recent methods have attempted to exploit GSD, many rely on additional subnetworks or heuristic weighting and still fail to capture category-specific size characteristics. To address this, we propose a lightweight module, Pixels-to-Meters Prior Fusion (P2M-PF), which integrates physical-size priors into the classification branch without altering the detection architecture. Experiments on the GSD-labelled subset of DOTA v1.0 demonstrate consistent improvements over strong baselines, with notable gains for categories susceptible to cross-scale confusion.
Published: 2025-12-26T18:25:46+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shihao Yu; Yifan Dong; Yun Su; Yang Zhao; Xiaoqiang Jia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3648944"&gt;10.1109/lgrs.2025.3648944&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection in remote sensing imagery is challenging because targets often appear at markedly different scales and may become visually similar when the image resolution varies. A key advantage of remote sensing imagery is that each image provides a ground sample distance (GSD) value linking pixel units to real-world size, offering physical cues that can mitigate such scale-induced ambiguity. While recent methods have attempted to exploit GSD, many rely on additional subnetworks or heuristic weighting and still fail to capture category-specific size characteristics. To address this, we propose a lightweight module, Pixels-to-Meters Prior Fusion (P2M-PF), which integrates physical-size priors into the classification branch without altering the detection architecture. Experiments on the GSD-labelled subset of DOTA v1.0 demonstrate consistent improvements over strong baselines, with notable gains for categories susceptible to cross-scale confusion.&lt;/p&gt;</content:encoded></item><item><title>Embracing the Power of Known Class Bias in Open Set Recognition from A Reconstruction Perspective</title><link>https://doi.org/10.1109/tip.2025.3644791</link><guid>10.1109/tip.2025.3644791</guid><pubDate>Fri, 26 Dec 2025 18:25:24 +0000</pubDate><dc:creator>Heyang Sun</dc:creator><dc:creator>Chuanxing Geng</dc:creator><dc:creator>Songcan Chen</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644791</prism:doi><description>The open set known class bias is conventionally viewed as a fatal problem i.e., the models trained solely on known classes tend to fit unknown classes to known classes with high confidence in inference. Thus existing methods, without exception make a choice in two manners: most methods opt for eliminating the known class bias as much as possible with tireless efforts, while others circumvent the known class bias by employing a reconstruction method. However, in this paper, we challenge the two widely accepted approaches and present a novel proposition: the so-called harmful known class bias for most methods is, exactly conversely, beneficial for the reconstruction-based method and thus such known class bias can serve as a positive-incentive to the Open set recognition (OSR) models from a reconstruction perspective. Along this line, we propose the Bias Enhanced Reconstruction Learning (BERL) framework to enhance the known class bias respectively from the class level, model level and sample level. Specifically, at the class level, a specific representation is constructed in a supervised contrastive manner to avoid overgeneralization, while a diffusion model is employed by injecting the class prior to guide the biased reconstruction at the model level. Additionally, we leverage the advantages of the diffusion model to design a self-adaptive strategy, enabling effective sample-level biased sampling based on the information bottleneck theory. Experiments on various benchmarks demonstrate the effectiveness and performance superiority of the proposed method.
Published: 2025-12-26T18:25:24+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Heyang Sun; Chuanxing Geng; Songcan Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644791"&gt;10.1109/tip.2025.3644791&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;The open set known class bias is conventionally viewed as a fatal problem i.e., the models trained solely on known classes tend to fit unknown classes to known classes with high confidence in inference. Thus existing methods, without exception make a choice in two manners: most methods opt for eliminating the known class bias as much as possible with tireless efforts, while others circumvent the known class bias by employing a reconstruction method. However, in this paper, we challenge the two widely accepted approaches and present a novel proposition: the so-called harmful known class bias for most methods is, exactly conversely, beneficial for the reconstruction-based method and thus such known class bias can serve as a positive-incentive to the Open set recognition (OSR) models from a reconstruction perspective. Along this line, we propose the Bias Enhanced Reconstruction Learning (BERL) framework to enhance the known class bias respectively from the class level, model level and sample level. Specifically, at the class level, a specific representation is constructed in a supervised contrastive manner to avoid overgeneralization, while a diffusion model is employed by injecting the class prior to guide the biased reconstruction at the model level. Additionally, we leverage the advantages of the diffusion model to design a self-adaptive strategy, enabling effective sample-level biased sampling based on the information bottleneck theory. Experiments on various benchmarks demonstrate the effectiveness and performance superiority of the proposed method.&lt;/p&gt;</content:encoded></item><item><title>TPIN: Text-based Parallel Interaction Network with Modality-Common and Modality-Specific for Multimodal Sentiment Analysis</title><link>https://doi.org/10.1016/j.inffus.2025.104087</link><guid>10.1016/j.inffus.2025.104087</guid><pubDate>Thu, 25 Dec 2025 00:16:18 +0000</pubDate><dc:creator>Changbin Wang</dc:creator><dc:creator>Fengrui Ji</dc:creator><dc:creator>Baolin Liu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104087</prism:doi><description>Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.
Published: 2025-12-25T00:16:18+00:00
Venue: Information Fusion
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changbin Wang; Fengrui Ji; Baolin Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104087"&gt;10.1016/j.inffus.2025.104087&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Learning an effective joint representation is fundamental for Multimodal Sentiment Analysis (MSA). Existing studies typically adopt complex networks to construct joint multimodal representations directly, yet often overlook the heterogeneity among different modalities as well as the preservation of modality-specific information. Moreover, current methods tend to treat all modalities equally, failing to exploit the rich emotional cues in the text modality. To address these issues, we propose a Text-based Parallel Interaction Network (TPIN) that aims to trade off the commonality and specificity of different modalities. The TPIN consists of two components: Modality-Common Information Processing (MCIP) and Modality-Specific Information Processing (MSIP). In MCIP, we innovatively propose a contrastive learning algorithm with Hard Negative Mining (HNM), which is integrated into our designed Two-Stage Contrastive Learning (TSCL) to mitigate inter-modal heterogeneity. Additionally, we design a Text-Guided Dynamic Semantic Aggregation (TG-DSA) module to enable deep multimodal fusion under the guidance of text modality. In MSIP, we devise a dynamic routing mechanism, which iteratively optimizes routing weights to better capture modality-specific information in visual and acoustic modalities. Experimental results demonstrate that our method achieves state-of-the-art performance on both the CMU-MOSI and CMU-MOSEI datasets, showing consistent gains of 0.5%–1.2% across major evaluation metrics compared with recent advanced models.&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Benchmark for Evaluating Night-time Visual Object Tracking</title><link>https://doi.org/10.1007/s11263-025-02661-7</link><guid>10.1007/s11263-025-02661-7</guid><pubDate>Fri, 26 Dec 2025 17:58:36 +0000</pubDate><dc:creator>Yu Liu</dc:creator><dc:creator>Arif Mahmood</dc:creator><dc:creator>Muhammad Haris Khan</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02661-7</prism:doi><description>Several existing visual object tracking benchmarks, including OTB100, NfS, UAV123, LaSOT, and GOT-10K, mostly feature day-time scenarios. However, the challenges posed by the night-time remain relatively underexplored. We attribute this primarily to the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. In this paper, we first introduce NT-VOT211, a novel benchmark specifically designed to thoroughly evaluate visual object tracking algorithms under a wide range of challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. After conducting a comprehensive analysis of the results from 42 distinct tracking algorithms on the NT-VOT211 dataset, we develop a simple yet effective zero-shot domain adaptation method to significantly enhance the tracking performance of state-of-the-art trackers under low-light conditions. In this method, a newly designed module aims to distinguish the background token from the target token before feeding into the MLP Head. Our module enables for more accurate position estimation. Remarkably, it accomplishes this enhancement with merely 11 epochs of fine-tuning on a standard daylight dataset. Our dataset, code and other assets can be found at: https://github.com/LiuYuML/NV-VOT211
Published: 2025-12-26T17:58:36+00:00
Venue: International Journal of Computer Vision
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yu Liu; Arif Mahmood; Muhammad Haris Khan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02661-7"&gt;10.1007/s11263-025-02661-7&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Several existing visual object tracking benchmarks, including OTB100, NfS, UAV123, LaSOT, and GOT-10K, mostly feature day-time scenarios. However, the challenges posed by the night-time remain relatively underexplored. We attribute this primarily to the lack of a large-scale, well-annotated night-time benchmark for rigorously evaluating tracking algorithms. In this paper, we first introduce NT-VOT211, a novel benchmark specifically designed to thoroughly evaluate visual object tracking algorithms under a wide range of challenging night-time conditions. NT-VOT211 consists of 211 diverse videos, offering 211,000 well-annotated frames with 8 attributes including camera motion, deformation, fast motion, motion blur, tiny target, distractors, occlusion and out-of-view. After conducting a comprehensive analysis of the results from 42 distinct tracking algorithms on the NT-VOT211 dataset, we develop a simple yet effective zero-shot domain adaptation method to significantly enhance the tracking performance of state-of-the-art trackers under low-light conditions. In this method, a newly designed module aims to distinguish the background token from the target token before feeding into the MLP Head. Our module enables for more accurate position estimation. Remarkably, it accomplishes this enhancement with merely 11 epochs of fine-tuning on a standard daylight dataset. Our dataset, code and other assets can be found at: https://github.com/LiuYuML/NV-VOT211&lt;/p&gt;</content:encoded></item><item><title>Satellite Image Denoising Techniques using CSC Data Fidelity with Adaptive Total Variation Regularization</title><link>https://doi.org/10.1109/tgrs.2025.3648809</link><guid>10.1109/tgrs.2025.3648809</guid><pubDate>Fri, 26 Dec 2025 18:23:46 +0000</pubDate><dc:creator>Kalukuri Princy Niveditha</dc:creator><dc:creator>Amit Vishwakarma</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648809</prism:doi><description>Satellite image denoising is a fundamental preprocessing step for restoring the original visual and spectral quality of remotely sensed data, as noise can significantly distort spatial and spectral information. Conventional denoising methods often struggle to effectively handle high-resolution satellite imagery, leading to loss of fine textures and structural integrity. To address these challenges, this paper introduces a novel Convolutional Sparse Representation (CSR) based denoising framework that integrates adaptive Total Variation (TV) regularization with noise specific data fidelity terms. For Gaussian noise, an L2TV model is employed to ensure smooth restoration, while for Impulse noise, an L1TV model is utilized to robustly suppress sparse outliers. Experimental evaluations conducted on benchmark satellite datasets demonstrate that the proposed framework achieves superior quantitative performance in terms of Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), Visual Information Fidelity (VIF), Figure of Merit (FOM) as well as improved visual quality when compared with traditional and contemporary state-of-the-art techniques. The proposed approach thus provides an efficient and generalized solution for enhancing the quality and interpretability of satellite imagery in diverse remote sensing applications.
Published: 2025-12-26T18:23:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kalukuri Princy Niveditha; Amit Vishwakarma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648809"&gt;10.1109/tgrs.2025.3648809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Satellite image denoising is a fundamental preprocessing step for restoring the original visual and spectral quality of remotely sensed data, as noise can significantly distort spatial and spectral information. Conventional denoising methods often struggle to effectively handle high-resolution satellite imagery, leading to loss of fine textures and structural integrity. To address these challenges, this paper introduces a novel Convolutional Sparse Representation (CSR) based denoising framework that integrates adaptive Total Variation (TV) regularization with noise specific data fidelity terms. For Gaussian noise, an L2TV model is employed to ensure smooth restoration, while for Impulse noise, an L1TV model is utilized to robustly suppress sparse outliers. Experimental evaluations conducted on benchmark satellite datasets demonstrate that the proposed framework achieves superior quantitative performance in terms of Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM), Visual Information Fidelity (VIF), Figure of Merit (FOM) as well as improved visual quality when compared with traditional and contemporary state-of-the-art techniques. The proposed approach thus provides an efficient and generalized solution for enhancing the quality and interpretability of satellite imagery in diverse remote sensing applications.&lt;/p&gt;</content:encoded></item><item><title>Fast SAM2 with Text-Driven Token Pruning</title><link>https://arxiv.org/abs/2512.21333v1</link><guid>http://arxiv.org/abs/2512.21333v1</guid><pubDate>Wed, 24 Dec 2025 18:59:05 +0000</pubDate><dc:creator>Avilasha Mandal</dc:creator><dc:creator>Chaoning Zhang</dc:creator><dc:creator>Fachrina Dewi Puspitasari</dc:creator><dc:creator>Xudong Wang</dc:creator><dc:creator>Jiaquan Zhang</dc:creator><dc:creator>Caiyan Qin</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Heng Tao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.
Published: 2025-12-24T18:59:05+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Avilasha Mandal; Chaoning Zhang; Fachrina Dewi Puspitasari; Xudong Wang; Jiaquan Zhang; Caiyan Qin; Guoqing Wang; Yang Yang; Heng Tao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.&lt;/p&gt;</content:encoded></item><item><title>Frequency-Guided Denoising Network for Semantic Segmentation of Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3648408</link><guid>10.1109/tgrs.2025.3648408</guid><pubDate>Fri, 26 Dec 2025 18:23:46 +0000</pubDate><dc:creator>Xin Li</dc:creator><dc:creator>Feng Xu</dc:creator><dc:creator>Jue Zhang</dc:creator><dc:creator>Hongsheng Zhang</dc:creator><dc:creator>Xin Lyu</dc:creator><dc:creator>Fan Liu</dc:creator><dc:creator>Hongmin Gao</dc:creator><dc:creator>André Kaup</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3648408</prism:doi><description>Semantic segmentation of high-resolution remote sensing images remains challenging due to the degradation of high-frequency semantic cues during convolutional encoding and the lack of frequency consistency in multi-stage feature fusion. To address these issues, we propose FreDNet, a frequency-guided denoising network that explicitly enhances frequency-sensitive representations throughout the segmentation process. Specifically, we introduce the Dual-path Residual Block (DRB), which incorporates a Frequency-aware Denoising Module (FDM) and a Frequency-aware Fusion Module (FFM) to suppress frequency-domain noise while preserving edge structures. Furthermore, we design a Frequency-aware Cross-level Fusion Module (FCFM) that leverages frequency intensity response maps to adaptively fuse encoder and decoder features. These components work collaboratively to enhance the frequency robustness and spatial consistency of the segmentation predictions. Extensive experiments on three challenging benchmarks, ISPRS Vaihingen, ISPRS Potsdam, and LoveDA, demonstrate that FreDNet achieves superior performance, surpassing the latest state-of-the-art approaches by up to 0.8% in mean IoU and 0.9% in overall accuracy, while maintaining a lightweight inference cost. In addition, ablation study confirms the contribution of each component of FreDNet.
Published: 2025-12-26T18:23:46+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.781 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Li; Feng Xu; Jue Zhang; Hongsheng Zhang; Xin Lyu; Fan Liu; Hongmin Gao; André Kaup&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3648408"&gt;10.1109/tgrs.2025.3648408&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (consider)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of high-resolution remote sensing images remains challenging due to the degradation of high-frequency semantic cues during convolutional encoding and the lack of frequency consistency in multi-stage feature fusion. To address these issues, we propose FreDNet, a frequency-guided denoising network that explicitly enhances frequency-sensitive representations throughout the segmentation process. Specifically, we introduce the Dual-path Residual Block (DRB), which incorporates a Frequency-aware Denoising Module (FDM) and a Frequency-aware Fusion Module (FFM) to suppress frequency-domain noise while preserving edge structures. Furthermore, we design a Frequency-aware Cross-level Fusion Module (FCFM) that leverages frequency intensity response maps to adaptively fuse encoder and decoder features. These components work collaboratively to enhance the frequency robustness and spatial consistency of the segmentation predictions. Extensive experiments on three challenging benchmarks, ISPRS Vaihingen, ISPRS Potsdam, and LoveDA, demonstrate that FreDNet achieves superior performance, surpassing the latest state-of-the-art approaches by up to 0.8% in mean IoU and 0.9% in overall accuracy, while maintaining a lightweight inference cost. In addition, ablation study confirms the contribution of each component of FreDNet.&lt;/p&gt;</content:encoded></item><item><title>BACFormer: a robust boundary-aware transformer for medical image segmentation</title><link>https://doi.org/10.1016/j.knosys.2025.115209</link><guid>10.1016/j.knosys.2025.115209</guid><pubDate>Fri, 26 Dec 2025 16:36:15 +0000</pubDate><dc:creator>Zhiyong Huang</dc:creator><dc:creator>Mingyu Wang</dc:creator><dc:creator>Mingyang Hou</dc:creator><dc:creator>Zhi Yu</dc:creator><dc:creator>Shiwei Wang</dc:creator><dc:creator>Xiaoyu Li</dc:creator><dc:creator>Jiahong Wang</dc:creator><dc:creator>Yan Yan</dc:creator><dc:creator>Yushi Liu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2025.115209</prism:doi><description>Recent advances in Transformer-based models have greatly improved the global context in medical image segmentation. However, these models often struggle to capture fine-grained local details, especially at organ boundaries. These details are critical for accurate segmentation in medical imaging tasks. To address this challenge, we propose the Boundary-Aware Convolutional Transformer (BACFormer), a novel U-shaped hierarchical Transformer architecture aimed at enhancing boundary and local detail segmentation, while maintaining long-range dependencies. BACFormer has two key innovations: (1) Hierarchical Attention Module (HAM). It combines the Boundary-Aware Convolutional Attention Module (BACAM) with Dilated Grid Attention (DGA). This improves boundary perception and multi-scale feature extraction. This module is highly portable, making it suitable for a wide range of vision tasks requiring robust multi-scale feature extraction. (2) Symmetric Convolutional Feed-Forward Network (SC-FFN), which facilitates local feature fusion and redistribution to improve segmentation accuracy, especially for small organs and blurred edges. BACFormer demonstrates the strong capacity to maintain long-range dependencies while simultaneously enhancing local boundary precision and detail capture. Extensive experiments on CT and MRI datasets show that BACFormer outperforms state-of-the-art methods, including those pre-trained on ImageNet. The code is publicly available at https://github.com/AmariJane/BACFormer .
Published: 2025-12-26T16:36:15+00:00
Venue: Knowledge-Based Systems
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiyong Huang; Mingyu Wang; Mingyang Hou; Zhi Yu; Shiwei Wang; Xiaoyu Li; Jiahong Wang; Yan Yan; Yushi Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2025.115209"&gt;10.1016/j.knosys.2025.115209&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in Transformer-based models have greatly improved the global context in medical image segmentation. However, these models often struggle to capture fine-grained local details, especially at organ boundaries. These details are critical for accurate segmentation in medical imaging tasks. To address this challenge, we propose the Boundary-Aware Convolutional Transformer (BACFormer), a novel U-shaped hierarchical Transformer architecture aimed at enhancing boundary and local detail segmentation, while maintaining long-range dependencies. BACFormer has two key innovations: (1) Hierarchical Attention Module (HAM). It combines the Boundary-Aware Convolutional Attention Module (BACAM) with Dilated Grid Attention (DGA). This improves boundary perception and multi-scale feature extraction. This module is highly portable, making it suitable for a wide range of vision tasks requiring robust multi-scale feature extraction. (2) Symmetric Convolutional Feed-Forward Network (SC-FFN), which facilitates local feature fusion and redistribution to improve segmentation accuracy, especially for small organs and blurred edges. BACFormer demonstrates the strong capacity to maintain long-range dependencies while simultaneously enhancing local boundary precision and detail capture. Extensive experiments on CT and MRI datasets show that BACFormer outperforms state-of-the-art methods, including those pre-trained on ImageNet. The code is publicly available at https://github.com/AmariJane/BACFormer .&lt;/p&gt;</content:encoded></item><item><title>Multi-stage Group Interaction and Cross-domain Fusion Network for Real-time Smoke Segmentation</title><link>https://doi.org/10.1109/tip.2025.3646455</link><guid>10.1109/tip.2025.3646455</guid><pubDate>Thu, 25 Dec 2025 18:28:37 +0000</pubDate><dc:creator>Kang Li</dc:creator><dc:creator>Feiniu Yuan</dc:creator><dc:creator>Chunmei Wang</dc:creator><dc:creator>Chunli Meng</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3646455</prism:doi><description>Lightweight smoke image segmentation is essential for fire warning systems, particularly on mobile devices. In recent years, although numerous high-precision, large-scale smoke segmentation models have been developed, there are few lightweight solutions specifically designed for mobile applications. Therefore, we propose a Multi-stage Group Interaction and Cross-domain Fusion Network (MGICFN) with low computational complexity for real-time smoke segmentation. To improve the model’s ability to effectively analyze smoke features, we incorporate a Cross-domain Interaction Attention Module (CIAM) to merge spatial and frequency domain features for creating a lightweight smoke encoder. To alleviate the loss of critical information from small smoke objects during downsampling, we design a Multi-stage Group Interaction Module (MGIM). The MGIM calibrates the information discrepancies between high and low-dimensional features. To enhance the boundary information of smoke targets, we introduce an Edge Enhancement Module (EEM), which utilizes predicted target boundaries as advanced guidance to refine lower-level smoke features. Furthermore, we implement a Group Convolutional Block Attention Module (GCBAM) and a Group Fusion Module (GFM) to connect the encoder and decoder efficiently. Experimental results demonstrate that MGICFN achieves an 88.70% Dice coefficient (Dice), an 81.16% mean Intersection over Union (mIoU), and a 91.93% accuracy (Acc) on the SFS3K dataset. It also achieves an 87.30% Dice, a 78.68% mIoU, and a 92.95% Acc on the SYN70K test dataset. Our MGICFN model has 0.73M parameters and requires 0.3G FLOPs.
Published: 2025-12-25T18:28:37+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.777 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kang Li; Feiniu Yuan; Chunmei Wang; Chunli Meng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3646455"&gt;10.1109/tip.2025.3646455&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.777 (consider)&lt;/p&gt;
&lt;p&gt;Lightweight smoke image segmentation is essential for fire warning systems, particularly on mobile devices. In recent years, although numerous high-precision, large-scale smoke segmentation models have been developed, there are few lightweight solutions specifically designed for mobile applications. Therefore, we propose a Multi-stage Group Interaction and Cross-domain Fusion Network (MGICFN) with low computational complexity for real-time smoke segmentation. To improve the model’s ability to effectively analyze smoke features, we incorporate a Cross-domain Interaction Attention Module (CIAM) to merge spatial and frequency domain features for creating a lightweight smoke encoder. To alleviate the loss of critical information from small smoke objects during downsampling, we design a Multi-stage Group Interaction Module (MGIM). The MGIM calibrates the information discrepancies between high and low-dimensional features. To enhance the boundary information of smoke targets, we introduce an Edge Enhancement Module (EEM), which utilizes predicted target boundaries as advanced guidance to refine lower-level smoke features. Furthermore, we implement a Group Convolutional Block Attention Module (GCBAM) and a Group Fusion Module (GFM) to connect the encoder and decoder efficiently. Experimental results demonstrate that MGICFN achieves an 88.70% Dice coefficient (Dice), an 81.16% mean Intersection over Union (mIoU), and a 91.93% accuracy (Acc) on the SFS3K dataset. It also achieves an 87.30% Dice, a 78.68% mIoU, and a 92.95% Acc on the SYN70K test dataset. Our MGICFN model has 0.73M parameters and requires 0.3G FLOPs.&lt;/p&gt;</content:encoded></item><item><title>CAHN-Net: Content-Adaptive Hierarchical Network for Cross-Modal Remote Sensing Object Detection</title><link>https://doi.org/10.1109/lgrs.2025.3648658</link><guid>10.1109/lgrs.2025.3648658</guid><pubDate>Thu, 25 Dec 2025 18:29:06 +0000</pubDate><dc:creator>Han Wang</dc:creator><dc:creator>Yiqing Li</dc:creator><dc:creator>Wen Zhou</dc:creator><prism:publicationName>IEEE Geoscience and Remote Sensing Letters</prism:publicationName><prism:doi>10.1109/lgrs.2025.3648658</prism:doi><description>Cross-modal remote sensing object detection faces fundamental challenges in fusing visible and infrared imagery due to their distinct information encoding mechanisms. Existing frameworks apply fixed convolution operations across modalities, creating conflicts between modality-specific feature representations and unified processing mechanisms. This paper presents Content-Adaptive Hierarchical Network (CAHN-Net) to address these challenges through three collaborative components. The Dynamic Receptive Field Module (DRFM) enables geometry-adaptive feature extraction via deformable kernel fusion. The Adaptive Sparse Attention Module (ASAM) focuses computation on modality-specific salient regions through sparse attention while handling non-Gaussian feature distributions. The Hierarchical Feature Fusion Module (HFFM) resolves semantic disparities through prompt-guided dual-granularity alignment. Experiments on DroneVehicle and VEDAI datasets demonstrate the effectiveness of our approach, achieving 72.7% and 79.7% mAP0.5 respectively. These results surpass several recent cross-modal detection methods while demonstrating robust performance across varying illumination and scene complexity. Source code is available at https://github.com/Han-Wang-RSLab/CAHN-Net.
Published: 2025-12-25T18:29:06+00:00
Venue: IEEE Geoscience and Remote Sensing Letters
Score: 0.776 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Han Wang; Yiqing Li; Wen Zhou&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Geoscience and Remote Sensing Letters&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/lgrs.2025.3648658"&gt;10.1109/lgrs.2025.3648658&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (consider)&lt;/p&gt;
&lt;p&gt;Cross-modal remote sensing object detection faces fundamental challenges in fusing visible and infrared imagery due to their distinct information encoding mechanisms. Existing frameworks apply fixed convolution operations across modalities, creating conflicts between modality-specific feature representations and unified processing mechanisms. This paper presents Content-Adaptive Hierarchical Network (CAHN-Net) to address these challenges through three collaborative components. The Dynamic Receptive Field Module (DRFM) enables geometry-adaptive feature extraction via deformable kernel fusion. The Adaptive Sparse Attention Module (ASAM) focuses computation on modality-specific salient regions through sparse attention while handling non-Gaussian feature distributions. The Hierarchical Feature Fusion Module (HFFM) resolves semantic disparities through prompt-guided dual-granularity alignment. Experiments on DroneVehicle and VEDAI datasets demonstrate the effectiveness of our approach, achieving 72.7% and 79.7% mAP0.5 respectively. These results surpass several recent cross-modal detection methods while demonstrating robust performance across varying illumination and scene complexity. Source code is available at https://github.com/Han-Wang-RSLab/CAHN-Net.&lt;/p&gt;</content:encoded></item></channel></rss>