<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 13 Jan 2026 02:39:20 +0000</lastBuildDate><item><title>DNN-aided Low-rank and Sparse Decomposition Model for Infrared Small Target Detection</title><link>https://doi.org/10.1016/j.patcog.2026.113070</link><guid>10.1016/j.patcog.2026.113070</guid><pubDate>Sun, 11 Jan 2026 07:02:27 +0000</pubDate><dc:creator>Jia-Jie Yin</dc:creator><dc:creator>Heng-Chao Li</dc:creator><dc:creator>Yu-Bang Zheng</dc:creator><dc:creator>Xiong-Fei Geng</dc:creator><dc:creator>Jie Pan</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113070</prism:doi><description>Recently, deep neural network-aided low-rank and sparse decomposition (DNN-aided LRSD) models have received increasing attention for infrared small target detection. The main idea of these methods is to utilize DNNs to learn a dataset-free deep prior as an implicit regularization for the background. In this work, we propose a novel DNN-aided LRSD model, which leverages DNNs to enhance the model’s ability to reconstruct low-rank background and detect sparse small targets. First, to efficiently and accurately reconstruct low-rank background, we propose a hierarchical tensor-ring-based background module (HTR) that captures the underlying low-rank structure of the background with compact nonlinear representation. In this module, nonlinear transforms using multilayer perceptrons (MLPs) and parameterized factor tensors are learned from data in an unsupervised manner. Second, to address the limitation of the l 1 norm in accurately describing sparse small targets in complex scenes, we specifically design an attention-guided sparse target module (SpAttention). It can progressively focus on the target region during the iterative process, thus improving target saliency and suppressing background structures. Comprehensive experiments on multiple real-world sequences validate the superior performance of our method in target detection and background suppression, surpassing state-of-the-art approaches. Code is available at: https://github.com/Yiniaie/DNN-aided-LRSD .
Published: 2026-01-11T07:02:27+00:00
Venue: Pattern Recognition
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jia-Jie Yin; Heng-Chao Li; Yu-Bang Zheng; Xiong-Fei Geng; Jie Pan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113070"&gt;10.1016/j.patcog.2026.113070&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, deep neural network-aided low-rank and sparse decomposition (DNN-aided LRSD) models have received increasing attention for infrared small target detection. The main idea of these methods is to utilize DNNs to learn a dataset-free deep prior as an implicit regularization for the background. In this work, we propose a novel DNN-aided LRSD model, which leverages DNNs to enhance the model’s ability to reconstruct low-rank background and detect sparse small targets. First, to efficiently and accurately reconstruct low-rank background, we propose a hierarchical tensor-ring-based background module (HTR) that captures the underlying low-rank structure of the background with compact nonlinear representation. In this module, nonlinear transforms using multilayer perceptrons (MLPs) and parameterized factor tensors are learned from data in an unsupervised manner. Second, to address the limitation of the l 1 norm in accurately describing sparse small targets in complex scenes, we specifically design an attention-guided sparse target module (SpAttention). It can progressively focus on the target region during the iterative process, thus improving target saliency and suppressing background structures. Comprehensive experiments on multiple real-world sequences validate the superior performance of our method in target detection and background suppression, surpassing state-of-the-art approaches. Code is available at: https://github.com/Yiniaie/DNN-aided-LRSD .&lt;/p&gt;</content:encoded></item><item><title>Cross-domain Few-shot Classification via Invariant-content Feature Reconstruction</title><link>https://doi.org/10.1007/s11263-025-02601-5</link><guid>10.1007/s11263-025-02601-5</guid><pubDate>Sun, 11 Jan 2026 14:52:44 +0000</pubDate><dc:creator>Hongduan Tian</dc:creator><dc:creator>Feng Liu</dc:creator><dc:creator>Ka Chun Cheung</dc:creator><dc:creator>Zhen Fang</dc:creator><dc:creator>Simon See</dc:creator><dc:creator>Tongliang Liu</dc:creator><dc:creator>Bo Han</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02601-5</prism:doi><description>Abstract In cross-domain few-shot classification (CFC), mainstream studies aim to train a simple module (e.g. a linear transformation head) to select or transform features (a.k.a., the high-level semantic features) for previously unseen domains with a few labeled training data available on top of a powerful pre-trained model. These studies usually assume that high-level semantic features are shared across these domains, and just simple feature selection or transformations are enough to adapt features to previously unseen domains. However, in this paper, we find that the simply transformed features are too general to fully cover the key content features regarding each class. Thus, we propose an effective method, invariant-content feature reconstruction (IFR), to train a simple module that simultaneously considers both high-level and fine-grained invariant-content features for the previously unseen domains. Specifically, the fine-grained invariant-content features are considered as a set of informative and discriminative features learned from a few labeled training data of tasks sampled from unseen domains and are extracted by retrieving features that are invariant to style modifications from a set of content-preserving augmented data in pixel level with an attention module. Extensive experiments on the Meta-Dataset benchmark show that IFR achieves good generalization performance on unseen domains, which demonstrates the effectiveness of the fusion of the high-level features and the fine-grained invariant-content features. Specifically, IFR improves the average accuracy on unseen domains by 1.6% and 6.5% respectively under two different cross-domain few-shot classification settings.
Published: 2026-01-11T14:52:44+00:00
Venue: International Journal of Computer Vision
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongduan Tian; Feng Liu; Ka Chun Cheung; Zhen Fang; Simon See; Tongliang Liu; Bo Han&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02601-5"&gt;10.1007/s11263-025-02601-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract In cross-domain few-shot classification (CFC), mainstream studies aim to train a simple module (e.g. a linear transformation head) to select or transform features (a.k.a., the high-level semantic features) for previously unseen domains with a few labeled training data available on top of a powerful pre-trained model. These studies usually assume that high-level semantic features are shared across these domains, and just simple feature selection or transformations are enough to adapt features to previously unseen domains. However, in this paper, we find that the simply transformed features are too general to fully cover the key content features regarding each class. Thus, we propose an effective method, invariant-content feature reconstruction (IFR), to train a simple module that simultaneously considers both high-level and fine-grained invariant-content features for the previously unseen domains. Specifically, the fine-grained invariant-content features are considered as a set of informative and discriminative features learned from a few labeled training data of tasks sampled from unseen domains and are extracted by retrieving features that are invariant to style modifications from a set of content-preserving augmented data in pixel level with an attention module. Extensive experiments on the Meta-Dataset benchmark show that IFR achieves good generalization performance on unseen domains, which demonstrates the effectiveness of the fusion of the high-level features and the fine-grained invariant-content features. Specifically, IFR improves the average accuracy on unseen domains by 1.6% and 6.5% respectively under two different cross-domain few-shot classification settings.&lt;/p&gt;</content:encoded></item><item><title>OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation</title><link>https://arxiv.org/abs/2601.06835v1</link><guid>http://arxiv.org/abs/2601.06835v1</guid><pubDate>Sun, 11 Jan 2026 09:57:04 +0000</pubDate><dc:creator>Hyunseo Lee</dc:creator><dc:creator>Sang Min Kim</dc:creator><dc:creator>Ho Kyung Shin</dc:creator><dc:creator>Taeheon Kim</dc:creator><dc:creator>Woo-Jeoung Nam</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.
Published: 2026-01-11T09:57:04+00:00
Venue: arXiv
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hyunseo Lee; Sang Min Kim; Ho Kyung Shin; Taeheon Kim; Woo-Jeoung Nam&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>Multi-Module Collaborative Optimization for SAR Image Aircraft Recognition: The SAR-YOLOv8l-ADE Network</title><link>https://doi.org/10.3390/rs18020236</link><guid>10.3390/rs18020236</guid><pubDate>Mon, 12 Jan 2026 07:30:34 +0000</pubDate><dc:creator>Xing Wang</dc:creator><dc:creator>Wen Hong</dc:creator><dc:creator>Qi Li</dc:creator><dc:creator>Yunqing Liu</dc:creator><dc:creator>Qiong Zhang</dc:creator><dc:creator>Ping Xin</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18020236</prism:doi><description>As a core node of the air transportation network, airports rely on aircraft model identification as a key link to support the development of smart aviation. Synthetic Aperture Radar (SAR), with its strong-penetration imaging capabilities, provides high-quality data support for this task. However, the field of SAR image interpretation faces numerous challenges. To address the core challenges in SAR image-based aircraft recognition, including insufficient dataset samples, single-dimensional target features, significant variations in target sizes, and high missed-detection rates for small targets, this study proposed an improved network architecture, SAR-YOLOv8l-ADE. Four modules achieve collaborative optimization: SAR-ACGAN integrates a self-attention mechanism to expand the dataset; SAR-DFE, a parameter-learnable dual-residual module, extracts multidimensional, detailed features; SAR-C2f, a residual module with multi-receptive field fusion, adapts to multi-scale targets; and 4SDC, a four-branch module with adaptive weights, enhances small-target recognition. Experimental results on the fused dataset SAR-Aircraft-EXT show that the mAP50 of the SAR-YOLOv8l-ADE network is 6.1% higher than that of the baseline network YOLOv8l, reaching 96.5%. Notably, its recognition accuracy for small aircraft targets shows a greater improvement, reaching 95.2%. The proposed network outperforms existing methods in terms of recognition accuracy and generalization under complex scenarios, providing technical support for airport management and control, as well as for emergency rescue in smart aviation.
Published: 2026-01-12T07:30:34+00:00
Venue: Remote Sensing
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xing Wang; Wen Hong; Qi Li; Yunqing Liu; Qiong Zhang; Ping Xin&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18020236"&gt;10.3390/rs18020236&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;As a core node of the air transportation network, airports rely on aircraft model identification as a key link to support the development of smart aviation. Synthetic Aperture Radar (SAR), with its strong-penetration imaging capabilities, provides high-quality data support for this task. However, the field of SAR image interpretation faces numerous challenges. To address the core challenges in SAR image-based aircraft recognition, including insufficient dataset samples, single-dimensional target features, significant variations in target sizes, and high missed-detection rates for small targets, this study proposed an improved network architecture, SAR-YOLOv8l-ADE. Four modules achieve collaborative optimization: SAR-ACGAN integrates a self-attention mechanism to expand the dataset; SAR-DFE, a parameter-learnable dual-residual module, extracts multidimensional, detailed features; SAR-C2f, a residual module with multi-receptive field fusion, adapts to multi-scale targets; and 4SDC, a four-branch module with adaptive weights, enhances small-target recognition. Experimental results on the fused dataset SAR-Aircraft-EXT show that the mAP50 of the SAR-YOLOv8l-ADE network is 6.1% higher than that of the baseline network YOLOv8l, reaching 96.5%. Notably, its recognition accuracy for small aircraft targets shows a greater improvement, reaching 95.2%. The proposed network outperforms existing methods in terms of recognition accuracy and generalization under complex scenarios, providing technical support for airport management and control, as well as for emergency rescue in smart aviation.&lt;/p&gt;</content:encoded></item><item><title>MCIVA: A Multi-View Pedestrian Detection Framework with Central Inverse Nearest Neighbor Map and View Adaptive Module</title><link>https://doi.org/10.1016/j.inffus.2026.104142</link><guid>10.1016/j.inffus.2026.104142</guid><pubDate>Sun, 11 Jan 2026 15:13:21 +0000</pubDate><dc:creator>He Li</dc:creator><dc:creator>Taiyu Liao</dc:creator><dc:creator>Weihang Kong</dc:creator><dc:creator>Xingchen Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104142</prism:doi><description>Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.
Published: 2026-01-11T15:13:21+00:00
Venue: Information Fusion
Score: 0.797 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; He Li; Taiyu Liao; Weihang Kong; Xingchen Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104142"&gt;10.1016/j.inffus.2026.104142&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.797 (must_read)&lt;/p&gt;
&lt;p&gt;Multi-view pedestrian detection is an important task and has many applications in areas such as surveillance and smart cities. Despite the significant performance improvements achieved in recent multi-view pedestrian detection methods, there are still three main challenges for this task. 1) In crowded areas, neighboring connected components may merge in dense regions, resulting in unclear localization of pixel peaks for each pedestrian. 2) The loss functions used in previous multi-view pedestrian detection methods have a high response to the background. 3) The camera parameters have not been fully utilized; they are only used to generate a fixed-value projection matrix. To address these challenges, we propose a novel multi-view pedestrian detection framework with Central Inverse Nearest Neighbor map and View Adaptive Module ( MCIVA ). A Central Inverse Nearest Neighbor (CINN) map is introduced to generate the ground-truth Probability Occupancy Map (POM) based on annotations, providing more precise location information for each pedestrian. To enhance the model’s attention to local structural information, we propose a local structural similarity loss to reduce the influence of false local maximum in background regions. Moreover, a novel plug-and-pull View Adaptive Module (VAM) is introduced to utilize the camera parameters to generate learnable weights for multi-view features fusion. We evaluate the proposed method on three benchmark datasets, and the results show that the proposed MCIVA significantly improves the quality of prediction map and achieves state-of-the-art performance.&lt;/p&gt;</content:encoded></item><item><title>Synthetic FMCW Radar Range Azimuth Maps Augmentation with Generative Diffusion Model</title><link>https://arxiv.org/abs/2601.06228v1</link><guid>http://arxiv.org/abs/2601.06228v1</guid><pubDate>Fri, 09 Jan 2026 10:59:46 +0000</pubDate><dc:creator>Zhaoze Wang</dc:creator><dc:creator>Changxu Zhang</dc:creator><dc:creator>Tai Fei</dc:creator><dc:creator>Christopher Grimm</dc:creator><dc:creator>Yi Jin</dc:creator><dc:creator>Claas Tebruegge</dc:creator><dc:creator>Ernst Warsitz</dc:creator><dc:creator>Markus Gardill</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The scarcity and low diversity of well-annotated automotive radar datasets often limit the performance of deep-learning-based environmental perception. To overcome these challenges, we propose a conditional generative framework for synthesizing realistic Frequency-Modulated Continuous-Wave radar Range-Azimuth Maps. Our approach leverages a generative diffusion model to generate radar data for multiple object categories, including pedestrians, cars, and cyclists. Specifically, conditioning is achieved via Confidence Maps, where each channel represents a semantic class and encodes Gaussian-distributed annotations at target locations. To address radar-specific characteristics, we incorporate Geometry Aware Conditioning and Temporal Consistency Regularization into the generative process. Experiments on the ROD2021 dataset demonstrate that signal reconstruction quality improves by \SI{3.6}{dB} in Peak Signal-to-Noise Ratio over baseline methods, while training with a combination of real and synthetic datasets improves overall mean Average Precision by 4.15% compared with conventional image-processing-based augmentation. These results indicate that our generative framework not only produces physically plausible and diverse radar spectrum but also substantially improves model generalization in downstream tasks.
Published: 2026-01-09T10:59:46+00:00
Venue: arXiv
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhaoze Wang; Changxu Zhang; Tai Fei; Christopher Grimm; Yi Jin; Claas Tebruegge; Ernst Warsitz; Markus Gardill&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;The scarcity and low diversity of well-annotated automotive radar datasets often limit the performance of deep-learning-based environmental perception. To overcome these challenges, we propose a conditional generative framework for synthesizing realistic Frequency-Modulated Continuous-Wave radar Range-Azimuth Maps. Our approach leverages a generative diffusion model to generate radar data for multiple object categories, including pedestrians, cars, and cyclists. Specifically, conditioning is achieved via Confidence Maps, where each channel represents a semantic class and encodes Gaussian-distributed annotations at target locations. To address radar-specific characteristics, we incorporate Geometry Aware Conditioning and Temporal Consistency Regularization into the generative process. Experiments on the ROD2021 dataset demonstrate that signal reconstruction quality improves by \SI{3.6}{dB} in Peak Signal-to-Noise Ratio over baseline methods, while training with a combination of real and synthetic datasets improves overall mean Average Precision by 4.15% compared with conventional image-processing-based augmentation. These results indicate that our generative framework not only produces physically plausible and diverse radar spectrum but also substantially improves model generalization in downstream tasks.&lt;/p&gt;</content:encoded></item><item><title>Dual-Phase LLM Reasoning: Self-Evolved Mathematical Frameworks</title><link>https://arxiv.org/abs/2601.05616v1</link><guid>http://arxiv.org/abs/2601.05616v1</guid><pubDate>Fri, 09 Jan 2026 08:19:11 +0000</pubDate><dc:creator>ShaoZhen Liu</dc:creator><dc:creator>Xinting Huang</dc:creator><dc:creator>Houwen Peng</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Xinyang Song</dc:creator><dc:creator>Qi Li</dc:creator><dc:creator>Zhenan Sun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models' self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model's ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models' intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.
Published: 2026-01-09T08:19:11+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; ShaoZhen Liu; Xinting Huang; Houwen Peng; Xin Chen; Xinyang Song; Qi Li; Zhenan Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, large language models (LLMs) have demonstrated significant potential in complex reasoning tasks like mathematical problem-solving. However, existing research predominantly relies on reinforcement learning (RL) frameworks while overlooking supervised fine-tuning (SFT) methods. This paper proposes a new two-stage training framework that enhances models&amp;#x27; self-correction capabilities through self-generated long chain-of-thought (CoT) data. During the first stage, a multi-turn dialogue strategy guides the model to generate CoT data incorporating verification, backtracking, subgoal decomposition, and backward reasoning, with predefined rules filtering high-quality samples for supervised fine-tuning. The second stage employs a difficulty-aware rejection sampling mechanism to dynamically optimize data distribution, strengthening the model&amp;#x27;s ability to handle complex problems. The approach generates reasoning chains extended over 4 times longer while maintaining strong scalability, proving that SFT effectively activates models&amp;#x27; intrinsic reasoning capabilities and provides a resource-efficient pathway for complex task optimization. Experimental results demonstrate performance improvements on mathematical benchmarks including GSM8K and MATH500, with the fine-tuned model achieving a substantial improvement on competition-level problems like AIME24. Code will be open-sourced.&lt;/p&gt;</content:encoded></item><item><title>Compressing image encoders via latent distillation</title><link>https://arxiv.org/abs/2601.05639v1</link><guid>http://arxiv.org/abs/2601.05639v1</guid><pubDate>Fri, 09 Jan 2026 08:50:38 +0000</pubDate><dc:creator>Caroline Mazini Rodrigues</dc:creator><dc:creator>Nicolas Keriven</dc:creator><dc:creator>Thomas Maugey</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.
Published: 2026-01-09T08:50:38+00:00
Venue: arXiv
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Caroline Mazini Rodrigues; Nicolas Keriven; Thomas Maugey&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Deep learning models for image compression often face practical limitations in hardware-constrained applications. Although these models achieve high-quality reconstructions, they are typically complex, heavyweight, and require substantial training data and computational resources. We propose a methodology to partially compress these networks by reducing the size of their encoders. Our approach uses a simplified knowledge distillation strategy to approximate the latent space of the original models with less data and shorter training, yielding lightweight encoders from heavyweight ones. We evaluate the resulting lightweight encoders across two different architectures on the image compression task. Experiments show that our method preserves reconstruction quality and statistical fidelity better than training lightweight encoders with the original loss, making it practical for resource-limited environments.&lt;/p&gt;</content:encoded></item><item><title>CLIMP: Contrastive Language-Image Mamba Pretraining</title><link>https://arxiv.org/abs/2601.06891v1</link><guid>http://arxiv.org/abs/2601.06891v1</guid><pubDate>Sun, 11 Jan 2026 12:31:55 +0000</pubDate><dc:creator>Nimrod Shabtay</dc:creator><dc:creator>Itamar Zimerman</dc:creator><dc:creator>Eli Schwartz</dc:creator><dc:creator>Raja Giryes</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI's CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP's fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.
Published: 2026-01-11T12:31:55+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Nimrod Shabtay; Itamar Zimerman; Eli Schwartz; Raja Giryes&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI&amp;#x27;s CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP&amp;#x27;s fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.&lt;/p&gt;</content:encoded></item><item><title>Arbitrary‑Scale Spatial–Spectral Fusion using Kernel Integral and Progressive Resampling</title><link>https://doi.org/10.1016/j.inffus.2026.104143</link><guid>10.1016/j.inffus.2026.104143</guid><pubDate>Mon, 12 Jan 2026 17:05:23 +0000</pubDate><dc:creator>Wei Li</dc:creator><dc:creator>Honghui Xu</dc:creator><dc:creator>Yueqian Quan</dc:creator><dc:creator>Zhe Chen</dc:creator><dc:creator>Jianwei Zheng</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104143</prism:doi><description>Benefiting from the booming deep learning techniques, spatial-spectral fusion (SSF) is considered as an ideal alternative to break the traditions of acquiring hyperspectral images (HSI) with costly devices. Yet with the remarkable progress, current solutions necessitate training and storing multiple models for different scaling factors. To overcome this dilemma, we propose a spatial–spectral fusion neural operator (SFNO) to perform arbitrary-scale SSF within the operator learning framework. Specifically, SFNO approaches the problem from the perspective of approximation theory by embedding the features of two degraded functions into a high-dimensional latent space through pointwise convolution layers, thereby capturing richer spectral feature information. Consequently, the mapping between function spaces is approximated via the Galerkin integral (GI) mechanism, which culminates in a final dimensionality reduction step to produce a high-resolution HSI. Moreover, we propose a progressive resampling integration (PR) that resamples the integrand’s domain in the triple kernel integration to provide non-local multi-scale information. The synergistic action of both integration mechanisms enables SFNO to effortlessly handle magnification factors it never encountered during training. Extensive experiments on the CAVE, Chikusei, Pavia Centre, Harvard, and real-world datasets demonstrate that our SFNO delivers substantial improvements over existing state-of-the-art methods. In particular, under the 8× upsampling setting on the CAVE, Chikusei, and Pavia Centre datasets, SFNO surpasses the second-best model by 0.56 dB, 1.05 dB, and 0.72 dB in PSNR, respectively. Our code is publicly available at https://github.com/weili419/SFNO .
Published: 2026-01-12T17:05:23+00:00
Venue: Information Fusion
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Li; Honghui Xu; Yueqian Quan; Zhe Chen; Jianwei Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104143"&gt;10.1016/j.inffus.2026.104143&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Benefiting from the booming deep learning techniques, spatial-spectral fusion (SSF) is considered as an ideal alternative to break the traditions of acquiring hyperspectral images (HSI) with costly devices. Yet with the remarkable progress, current solutions necessitate training and storing multiple models for different scaling factors. To overcome this dilemma, we propose a spatial–spectral fusion neural operator (SFNO) to perform arbitrary-scale SSF within the operator learning framework. Specifically, SFNO approaches the problem from the perspective of approximation theory by embedding the features of two degraded functions into a high-dimensional latent space through pointwise convolution layers, thereby capturing richer spectral feature information. Consequently, the mapping between function spaces is approximated via the Galerkin integral (GI) mechanism, which culminates in a final dimensionality reduction step to produce a high-resolution HSI. Moreover, we propose a progressive resampling integration (PR) that resamples the integrand’s domain in the triple kernel integration to provide non-local multi-scale information. The synergistic action of both integration mechanisms enables SFNO to effortlessly handle magnification factors it never encountered during training. Extensive experiments on the CAVE, Chikusei, Pavia Centre, Harvard, and real-world datasets demonstrate that our SFNO delivers substantial improvements over existing state-of-the-art methods. In particular, under the 8× upsampling setting on the CAVE, Chikusei, and Pavia Centre datasets, SFNO surpasses the second-best model by 0.56 dB, 1.05 dB, and 0.72 dB in PSNR, respectively. Our code is publicly available at https://github.com/weili419/SFNO .&lt;/p&gt;</content:encoded></item><item><title>Class Debiased Teacher for Source-free Object Detection</title><link>https://doi.org/10.1016/j.knosys.2026.115327</link><guid>10.1016/j.knosys.2026.115327</guid><pubDate>Mon, 12 Jan 2026 07:28:10 +0000</pubDate><dc:creator>You Ma</dc:creator><dc:creator>Shihan Mao</dc:creator><dc:creator>Lin Chai</dc:creator><dc:creator>Hongwei Tong</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115327</prism:doi><description>Source-free object detection (SFOD) aims to adapt a pre-trained model on a labeled source domain to an unlabeled target domain without requiring access to the source data. Existing SFOD methods primarily adopt a self-training paradigm, rendering the performance of the adaptive detector heavily dependent on the accuracy of pseudo-labels. To elevate the quality of pseudo-labels, some methods utilize contrastive learning to enhance the feature representation and have been proven to achieve performance gains. However, these methods overlook the problem of class imbalance, which may lead to rare class features converging closer to semantically similar large classes, thereby impeding the model from learning highly discriminative feature representations. To address this limitation, we propose a class debiased teacher framework. Specifically, we innovatively introduce a class imbalance calibration module in contrastive learning. This module jointly optimizes the inter-sample and sample-prototype dual contrastive losses, which improves the similarity of features within the same class while forcing different classes to be uniformly distributed in the feature space. In addition, we design a region-aware feature aggregation module that selectively aggregates region features from teacher and student models to enhance feature representation. This facilitates the generation of high-quality positive/negative sample pairs for contrastive loss. Extensive experiments conducted in multiple domain adaptation scenarios demonstrate that our method outperforms existing SFOD methods.
Published: 2026-01-12T07:28:10+00:00
Venue: Knowledge-Based Systems
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; You Ma; Shihan Mao; Lin Chai; Hongwei Tong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115327"&gt;10.1016/j.knosys.2026.115327&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Source-free object detection (SFOD) aims to adapt a pre-trained model on a labeled source domain to an unlabeled target domain without requiring access to the source data. Existing SFOD methods primarily adopt a self-training paradigm, rendering the performance of the adaptive detector heavily dependent on the accuracy of pseudo-labels. To elevate the quality of pseudo-labels, some methods utilize contrastive learning to enhance the feature representation and have been proven to achieve performance gains. However, these methods overlook the problem of class imbalance, which may lead to rare class features converging closer to semantically similar large classes, thereby impeding the model from learning highly discriminative feature representations. To address this limitation, we propose a class debiased teacher framework. Specifically, we innovatively introduce a class imbalance calibration module in contrastive learning. This module jointly optimizes the inter-sample and sample-prototype dual contrastive losses, which improves the similarity of features within the same class while forcing different classes to be uniformly distributed in the feature space. In addition, we design a region-aware feature aggregation module that selectively aggregates region features from teacher and student models to enhance feature representation. This facilitates the generation of high-quality positive/negative sample pairs for contrastive loss. Extensive experiments conducted in multiple domain adaptation scenarios demonstrate that our method outperforms existing SFOD methods.&lt;/p&gt;</content:encoded></item><item><title>SCVI: A semi-coupled visible-infrared small object detection method based on multimodal proposal-level probability fusion strategy</title><link>https://doi.org/10.1016/j.neucom.2026.132688</link><guid>10.1016/j.neucom.2026.132688</guid><pubDate>Mon, 12 Jan 2026 17:00:27 +0000</pubDate><dc:creator>Haozhi Xu</dc:creator><dc:creator>Xiaofang Yuan</dc:creator><dc:creator>Jinlei Wang</dc:creator><dc:creator>Yaonan Wang</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132688</prism:doi><description>Visible-Infrared (VI) fusion is widely adopted to improve robustness for all-weather object detection. However, VI small object detection remains challenging: small objects exhibit weaker feature than larger ones, while cross-modal misalignment and modality-specific degradation make feature-level fusion prone to suppressing or corrupting these fragile cues. Once such object’s feature is lost during fusion, later decoding stages can hardly recover it, leading to systematic small object omissions. To mitigate this issue, a semi-coupled VI detection framework, tailored for small objects is proposed, called SCVI. It first generates modality-specific candidate proposals independently from two branches. Then, a multimodal proposal-level probabilistic fusion strategy selectively matches, filters, and fuses candidates to form a consolidated set of high-quality queries, with improved tolerance to uncertainty and a preference for small objects. Finally, these queries interact with modality-specific features via modality-selective deformable attention, enabling controlled cross-modal collaboration without coupled feature fusion. Experiments on established VI small object detection benchmarks demonstrate that SCVI achieves competitive accuracy and robustness. The implementation code will be made publicly available at https://github.com/XUhaozhi88/SC-VI-SOD.git .
Published: 2026-01-12T17:00:27+00:00
Venue: Neurocomputing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haozhi Xu; Xiaofang Yuan; Jinlei Wang; Yaonan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132688"&gt;10.1016/j.neucom.2026.132688&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Visible-Infrared (VI) fusion is widely adopted to improve robustness for all-weather object detection. However, VI small object detection remains challenging: small objects exhibit weaker feature than larger ones, while cross-modal misalignment and modality-specific degradation make feature-level fusion prone to suppressing or corrupting these fragile cues. Once such object’s feature is lost during fusion, later decoding stages can hardly recover it, leading to systematic small object omissions. To mitigate this issue, a semi-coupled VI detection framework, tailored for small objects is proposed, called SCVI. It first generates modality-specific candidate proposals independently from two branches. Then, a multimodal proposal-level probabilistic fusion strategy selectively matches, filters, and fuses candidates to form a consolidated set of high-quality queries, with improved tolerance to uncertainty and a preference for small objects. Finally, these queries interact with modality-specific features via modality-selective deformable attention, enabling controlled cross-modal collaboration without coupled feature fusion. Experiments on established VI small object detection benchmarks demonstrate that SCVI achieves competitive accuracy and robustness. The implementation code will be made publicly available at https://github.com/XUhaozhi88/SC-VI-SOD.git .&lt;/p&gt;</content:encoded></item><item><title>PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning</title><link>https://arxiv.org/abs/2601.05593v1</link><guid>http://arxiv.org/abs/2601.05593v1</guid><pubDate>Fri, 09 Jan 2026 07:24:43 +0000</pubDate><dc:creator>Jingcheng Hu</dc:creator><dc:creator>Yinmin Zhang</dc:creator><dc:creator>Shijie Shang</dc:creator><dc:creator>Xiaobo Yang</dc:creator><dc:creator>Yue Peng</dc:creator><dc:creator>Zhewei Huang</dc:creator><dc:creator>Hebin Zhou</dc:creator><dc:creator>Xin Wu</dc:creator><dc:creator>Jie Cheng</dc:creator><dc:creator>Fanqi Wan</dc:creator><dc:creator>Xiangwen Kong</dc:creator><dc:creator>Chengyuan Yao</dc:creator><dc:creator>Kaiwen Yan</dc:creator><dc:creator>Ailin Huang</dc:creator><dc:creator>Hongyu Zhou</dc:creator><dc:creator>Qi Han</dc:creator><dc:creator>Zheng Ge</dc:creator><dc:creator>Daxin Jiang</dc:creator><dc:creator>Xiangyu Zhang</dc:creator><dc:creator>Heung-Yeung Shum</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.
Published: 2026-01-09T07:24:43+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingcheng Hu; Yinmin Zhang; Shijie Shang; Xiaobo Yang; Yue Peng; Zhewei Huang; Hebin Zhou; Xin Wu; Jie Cheng; Fanqi Wan; Xiangwen Kong; Chengyuan Yao; Kaiwen Yan; Ailin Huang; Hongyu Zhou; Qi Han; Zheng Ge; Daxin Jiang; Xiangyu Zhang; Heung-Yeung Shum&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5&amp;#x27;s 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.&lt;/p&gt;</content:encoded></item><item><title>Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation</title><link>https://arxiv.org/abs/2601.06882v1</link><guid>http://arxiv.org/abs/2601.06882v1</guid><pubDate>Sun, 11 Jan 2026 12:10:56 +0000</pubDate><dc:creator>Dillan Imans</dc:creator><dc:creator>Phuoc-Nguyen Bui</dc:creator><dc:creator>Duc-Tai Le</dc:creator><dc:creator>Hyunseung Choo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation
Published: 2026-01-11T12:10:56+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dillan Imans; Phuoc-Nguyen Bui; Duc-Tai Le; Hyunseung Choo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation&lt;/p&gt;</content:encoded></item><item><title>Few-Shot change detection in optical and SAR remote sensing images for disaster response</title><link>https://doi.org/10.1016/j.jag.2026.105100</link><guid>10.1016/j.jag.2026.105100</guid><pubDate>Mon, 12 Jan 2026 19:23:17 +0000</pubDate><dc:creator>Di Wang</dc:creator><dc:creator>Guorui Ma</dc:creator><dc:creator>Xiao Wang</dc:creator><dc:creator>Ronghao Yang</dc:creator><dc:creator>Yongxian Zhang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2026.105100</prism:doi><description>Few-shot change detection in optical and Synthetic Aperture Radar images is a critical task for disaster monitoring. offering significant application value in complex scenarios with extremely limited labeled samples. However, the randomness of disasters causes a notable data distribution shift between public datasets and real disaster scenarios. With only a few annotated image pairs, existing methods struggle to effectively fuse features from heterogeneous images, leading to severe performance degradation. To address this challenge, we propose a Dual-Stage Training framework for Change Detection (DSTCD), specifically designed for few-shot scenarios involving fewer than 20 labeled image pairs. DSTCD first undergoes source task pre-training on a heterogeneous image registration dataset. Subsequently, in the target task stage, it leverages task guided feature transfer module to transfer the structural and semantic features of image registration to the change detection task. This mechanism significantly enriches the feature representations under few-shot conditions, enabling accurate identification of affected areas. To validate its performance, we conducted comparative and ablation studies against eleven state-of-the-art methods on four public datasets covering both urban expansion and water expansion scenarios. Experimental results demonstrate that DSTCD achieves a significant performance lead. Its average F1-score surpasses the second-best method by 6.98% in urban expansion scenarios and by 13.09% in water expansion scenarios, proving its superior performance and strong multi-scenario adaptability. Furthermore, robustness analysis of varying training sample sizes and real-world disaster application validation further confirm the method’s practicality and robustness for data-scarce disaster monitoring tasks. The code of the proposed method will be made available at https://github.com/Lucky-DW/DSTCD .
Published: 2026-01-12T19:23:17+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Di Wang; Guorui Ma; Xiao Wang; Ronghao Yang; Yongxian Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2026.105100"&gt;10.1016/j.jag.2026.105100&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot change detection in optical and Synthetic Aperture Radar images is a critical task for disaster monitoring. offering significant application value in complex scenarios with extremely limited labeled samples. However, the randomness of disasters causes a notable data distribution shift between public datasets and real disaster scenarios. With only a few annotated image pairs, existing methods struggle to effectively fuse features from heterogeneous images, leading to severe performance degradation. To address this challenge, we propose a Dual-Stage Training framework for Change Detection (DSTCD), specifically designed for few-shot scenarios involving fewer than 20 labeled image pairs. DSTCD first undergoes source task pre-training on a heterogeneous image registration dataset. Subsequently, in the target task stage, it leverages task guided feature transfer module to transfer the structural and semantic features of image registration to the change detection task. This mechanism significantly enriches the feature representations under few-shot conditions, enabling accurate identification of affected areas. To validate its performance, we conducted comparative and ablation studies against eleven state-of-the-art methods on four public datasets covering both urban expansion and water expansion scenarios. Experimental results demonstrate that DSTCD achieves a significant performance lead. Its average F1-score surpasses the second-best method by 6.98% in urban expansion scenarios and by 13.09% in water expansion scenarios, proving its superior performance and strong multi-scenario adaptability. Furthermore, robustness analysis of varying training sample sizes and real-world disaster application validation further confirm the method’s practicality and robustness for data-scarce disaster monitoring tasks. The code of the proposed method will be made available at https://github.com/Lucky-DW/DSTCD .&lt;/p&gt;</content:encoded></item><item><title>A dual-path network for semantic scene completion of single-frame LiDAR point clouds</title><link>https://doi.org/10.1016/j.jag.2025.105020</link><guid>10.1016/j.jag.2025.105020</guid><pubDate>Mon, 12 Jan 2026 10:56:45 +0000</pubDate><dc:creator>Wei Liu</dc:creator><dc:creator>Ziwen Kang</dc:creator><dc:creator>Yongtao Yu</dc:creator><dc:creator>Zheng Gong</dc:creator><dc:creator>Yuchao Zheng</dc:creator><dc:creator>Xiaohui Huang</dc:creator><dc:creator>Haiyan Guan</dc:creator><dc:creator>Lingfei Ma</dc:creator><dc:creator>Dedong Zhang</dc:creator><prism:publicationName>International Journal of Applied Earth Observation and Geoinformation</prism:publicationName><prism:doi>10.1016/j.jag.2025.105020</prism:doi><description>Semantic Scene Completion (SSC) is a fundamental yet challenging task in 3D environment perception, as the sparsity and noise of LiDAR point clouds make it difficult to accurately recover both geometry and semantics. To address these challenges, we propose DPS2CNet, a novel Dual-Path SSC Network that integrates voxel-based and bird’s-eye view (BEV) representations to exploit their complementary strengths. Specifically, DPS2CNet employs a Cylinder3D-enhanced voxel branch to capture fine-grained 3D geometry and a UNet-based BEV branch to model large-scale contextual information. To further boost performance, we incorporate CARAFE for efficient feature upsampling and design a tailored loss function optimized for SSC. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 demonstrate that DPS2CNet achieves state-of-the-art results. In particular, it ranks first on the SemanticKITTI test set with an IoU of 62.6% among all open-source submissions 1 , highlighting its effectiveness in complex real-world driving scenarios.
Published: 2026-01-12T10:56:45+00:00
Venue: International Journal of Applied Earth Observation and Geoinformation
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wei Liu; Ziwen Kang; Yongtao Yu; Zheng Gong; Yuchao Zheng; Xiaohui Huang; Haiyan Guan; Lingfei Ma; Dedong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Applied Earth Observation and Geoinformation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.jag.2025.105020"&gt;10.1016/j.jag.2025.105020&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Semantic Scene Completion (SSC) is a fundamental yet challenging task in 3D environment perception, as the sparsity and noise of LiDAR point clouds make it difficult to accurately recover both geometry and semantics. To address these challenges, we propose DPS2CNet, a novel Dual-Path SSC Network that integrates voxel-based and bird’s-eye view (BEV) representations to exploit their complementary strengths. Specifically, DPS2CNet employs a Cylinder3D-enhanced voxel branch to capture fine-grained 3D geometry and a UNet-based BEV branch to model large-scale contextual information. To further boost performance, we incorporate CARAFE for efficient feature upsampling and design a tailored loss function optimized for SSC. Extensive experiments on SemanticKITTI and SSCBench-KITTI-360 demonstrate that DPS2CNet achieves state-of-the-art results. In particular, it ranks first on the SemanticKITTI test set with an IoU of 62.6% among all open-source submissions 1 , highlighting its effectiveness in complex real-world driving scenarios.&lt;/p&gt;</content:encoded></item><item><title>GeoSurDepth: Spatial Geometry-Consistent Self-Supervised Depth Estimation for Surround-View Cameras</title><link>https://arxiv.org/abs/2601.05839v1</link><guid>http://arxiv.org/abs/2601.05839v1</guid><pubDate>Fri, 09 Jan 2026 15:13:28 +0000</pubDate><dc:creator>Weimin Liu</dc:creator><dc:creator>Wenjun Wang</dc:creator><dc:creator>Joshua H. Meng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.
Published: 2026-01-09T15:13:28+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weimin Liu; Wenjun Wang; Joshua H. Meng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While prior studies have proposed various approaches that primarily focus on enforcing cross-view constraints at the photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize foundation models as a pseudo geometry prior and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal, spatial, and spatial-temporal contexts, and compensating for the limitations of single-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on DDAD and nuScenes demonstrate that GeoSurDepth achieves state-of-the-art performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised multi-view depth estimation.&lt;/p&gt;</content:encoded></item><item><title>Adapting Vision Transformers to Ultra-High Resolution Semantic Segmentation with Relay Tokens</title><link>https://arxiv.org/abs/2601.05927v1</link><guid>http://arxiv.org/abs/2601.05927v1</guid><pubDate>Fri, 09 Jan 2026 16:41:08 +0000</pubDate><dc:creator>Yohann Perron</dc:creator><dc:creator>Vladyslav Sydorov</dc:creator><dc:creator>Christophe Pottier</dc:creator><dc:creator>Loic Landrieu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .
Published: 2026-01-09T16:41:08+00:00
Venue: arXiv
Score: 0.780 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yohann Perron; Vladyslav Sydorov; Christophe Pottier; Loic Landrieu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.780 (must_read)&lt;/p&gt;
&lt;p&gt;Current approaches for segmenting ultra high resolution images either slide a window, thereby discarding global context, or downsample and lose fine detail. We propose a simple yet effective method that brings explicit multi scale reasoning to vision transformers, simultaneously preserving local details and global awareness. Concretely, we process each image in parallel at a local scale (high resolution, small crops) and a global scale (low resolution, large crops), and aggregate and propagate features between the two branches with a small set of learnable relay tokens. The design plugs directly into standard transformer backbones (eg ViT and Swin) and adds fewer than 2 % parameters. Extensive experiments on three ultra high resolution segmentation benchmarks, Archaeoscape, URUR, and Gleason, and on the conventional Cityscapes dataset show consistent gains, with up to 15 % relative mIoU improvement. Code and pretrained models are available at https://archaeoscape.ai/work/relay-tokens/ .&lt;/p&gt;</content:encoded></item><item><title>OMD: Optimal Transport-guided Multimodal Disentangled Learning for Leptomeningeal Metastasis Diagnosis</title><link>https://doi.org/10.1016/j.inffus.2025.104121</link><guid>10.1016/j.inffus.2025.104121</guid><pubDate>Sun, 11 Jan 2026 15:13:09 +0000</pubDate><dc:creator>Shengjia Chen</dc:creator><dc:creator>Huihua Hu</dc:creator><dc:creator>Hongfu Zeng</dc:creator><dc:creator>Chenxin Li</dc:creator><dc:creator>Qing Xu</dc:creator><dc:creator>Longfeng Zhang</dc:creator><dc:creator>Haipeng Xu</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104121</prism:doi><description>Leptomeningeal metastasis (LM) diagnosis represents a significant clinical challenge. Existing diagnostic approaches are often limited by their reliance on single-modality data and the inherent difficulties in effectively integrating heterogeneous information from imaging and genomics. To address these challenges, we propose OMD, an O ptimal Transport-guided M ultimodal D isentangled Learning framework that integrates MRI data with genomic information for enhanced diagnostic accuracy. Our method combines optimal transport-based cross-modal attention to robustly align heterogeneous features, information bottleneck compression to mitigate noise and redundancy, and feature disentanglement to explicitly model shared and modality-specific representations, integrated with hierarchical attention for MRI processing and graph-based cross-modal reasoning. Experimental results show that OMD achieves superior diagnostic accuracy, sensitivity, and specificity on our clinical dataset, substantially outperforming current state-of-the-art methods across all evaluation metrics. The model also provides interpretable insights into the cross-modal biomarkers associated with LM. The proposed OMD framework establishes a new paradigm for multimodal medical diagnosis that effectively addresses the complementary strengths of imaging and genomic data. Beyond its immediate application to LM diagnosis, our approach offers a generalizable methodology for integrating heterogeneous medical data sources while providing clinically relevant interpretability. This work represents an important step toward personalized medicine approaches that combine multiple data modalities for improved diagnostic accuracy and treatment planning.
Published: 2026-01-11T15:13:09+00:00
Venue: Information Fusion
Score: 0.779 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shengjia Chen; Huihua Hu; Hongfu Zeng; Chenxin Li; Qing Xu; Longfeng Zhang; Haipeng Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104121"&gt;10.1016/j.inffus.2025.104121&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.779 (consider)&lt;/p&gt;
&lt;p&gt;Leptomeningeal metastasis (LM) diagnosis represents a significant clinical challenge. Existing diagnostic approaches are often limited by their reliance on single-modality data and the inherent difficulties in effectively integrating heterogeneous information from imaging and genomics. To address these challenges, we propose OMD, an O ptimal Transport-guided M ultimodal D isentangled Learning framework that integrates MRI data with genomic information for enhanced diagnostic accuracy. Our method combines optimal transport-based cross-modal attention to robustly align heterogeneous features, information bottleneck compression to mitigate noise and redundancy, and feature disentanglement to explicitly model shared and modality-specific representations, integrated with hierarchical attention for MRI processing and graph-based cross-modal reasoning. Experimental results show that OMD achieves superior diagnostic accuracy, sensitivity, and specificity on our clinical dataset, substantially outperforming current state-of-the-art methods across all evaluation metrics. The model also provides interpretable insights into the cross-modal biomarkers associated with LM. The proposed OMD framework establishes a new paradigm for multimodal medical diagnosis that effectively addresses the complementary strengths of imaging and genomic data. Beyond its immediate application to LM diagnosis, our approach offers a generalizable methodology for integrating heterogeneous medical data sources while providing clinically relevant interpretability. This work represents an important step toward personalized medicine approaches that combine multiple data modalities for improved diagnostic accuracy and treatment planning.&lt;/p&gt;</content:encoded></item><item><title>MoE-DisCo:Low Economy Cost Training Mixture-of-Experts Models</title><link>https://arxiv.org/abs/2601.06857v1</link><guid>http://arxiv.org/abs/2601.06857v1</guid><pubDate>Sun, 11 Jan 2026 10:59:15 +0000</pubDate><dc:creator>Xin Ye</dc:creator><dc:creator>Daning Cheng</dc:creator><dc:creator>Boyang Zhang</dc:creator><dc:creator>Yunquan Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Training large-scale Mixture-of-Experts (MoE) models typically requires high-memory, high-bandwidth GPUs (e.g., A100), and their high cost has become a major barrier to large-model training. In contrast, affordable hardware is low-cost but constrained by memory capacity and bandwidth, making it unsuitable for direct LLM training. To address this, we propose MoE-DisCo (Mixture-of-Experts with Disentangled Clustering and Coordination), a staged training framework. MoE-DisCo decomposes the MoE model into multiple dense submodels, each consisting of a shared backbone and a single expert, and partitions the training data into subsets using unsupervised clustering. Each submodel is trained independently and in parallel on its assigned data subset using low-cost devices, without any inter-device communication. Subsequently, all experts are integrated into a complete MoE model and fine-tuned globally for a short period on high-memory, high-bandwidth GPUs. Experiments show that our method matches or even surpasses full-parameter training in performance across multiple downstream tasks, loss function, and perplexity (PPL), while reducing training cost by 47.6 percent to 69.5 percent on Qwen1.5-MoE-2.7B and Llama-MoE-3.5B across different datasets.
Published: 2026-01-11T10:59:15+00:00
Venue: arXiv
Score: 0.776 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xin Ye; Daning Cheng; Boyang Zhang; Yunquan Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (consider)&lt;/p&gt;
&lt;p&gt;Training large-scale Mixture-of-Experts (MoE) models typically requires high-memory, high-bandwidth GPUs (e.g., A100), and their high cost has become a major barrier to large-model training. In contrast, affordable hardware is low-cost but constrained by memory capacity and bandwidth, making it unsuitable for direct LLM training. To address this, we propose MoE-DisCo (Mixture-of-Experts with Disentangled Clustering and Coordination), a staged training framework. MoE-DisCo decomposes the MoE model into multiple dense submodels, each consisting of a shared backbone and a single expert, and partitions the training data into subsets using unsupervised clustering. Each submodel is trained independently and in parallel on its assigned data subset using low-cost devices, without any inter-device communication. Subsequently, all experts are integrated into a complete MoE model and fine-tuned globally for a short period on high-memory, high-bandwidth GPUs. Experiments show that our method matches or even surpasses full-parameter training in performance across multiple downstream tasks, loss function, and perplexity (PPL), while reducing training cost by 47.6 percent to 69.5 percent on Qwen1.5-MoE-2.7B and Llama-MoE-3.5B across different datasets.&lt;/p&gt;</content:encoded></item><item><title>High-Level Adaptive Feature Enhancement and Attention Mask-Guided Aggregation for Visual Place Recognition</title><link>https://doi.org/10.1016/j.knosys.2026.115285</link><guid>10.1016/j.knosys.2026.115285</guid><pubDate>Sun, 11 Jan 2026 15:11:40 +0000</pubDate><dc:creator>Longhao Wang</dc:creator><dc:creator>Chaozhen Lan</dc:creator><dc:creator>Beibei Wu</dc:creator><dc:creator>Fushan Yao</dc:creator><dc:creator>Zijun Wei</dc:creator><dc:creator>Tian Gao</dc:creator><dc:creator>Hanyang Yu</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115285</prism:doi><description>Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .
Published: 2026-01-11T15:11:40+00:00
Venue: Knowledge-Based Systems
Score: 0.775 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Longhao Wang; Chaozhen Lan; Beibei Wu; Fushan Yao; Zijun Wei; Tian Gao; Hanyang Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115285"&gt;10.1016/j.knosys.2026.115285&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (consider)&lt;/p&gt;
&lt;p&gt;Visual Place Recognition (VPR) is a fundamental capability that supports autonomous perception and localization for intelligent agents, as well as geolocation retrieval of web images. By comparing visual features to infer the geographic position of a query image, VPR plays a crucial role in applications such as map construction and augmented reality. However, factors such as dynamic occlusion, viewpoint variations, and environmental interference readily lead to unstable global feature matching, thereby constraining VPR robustness. To address this, we propose an enhanced VPR framework integrating High-Level Adaptive feature enhancement and Attention Mask-Guided Aggregation (HAM-VPR). This approach incorporates a lightweight AdapterFormer module within the high-level Transformer Block of the pre-trained DINOv2 model. This enhances semantic adaptability, preserves fine-grained features, and reduces parameter redundancy, ultimately generating structured image segmentation feature maps. This effectively bridges the representational gap between pre-trained visual models and VPR tasks. Concurrently, a lightweight attention module generates an implicit mask to guide global feature aggregation, suppressing irrelevant regions while amplifying discriminative area representations. A two-stage training strategy achieves seamless fusion of mask and segmentation features, enabling adaptive optimisation without re-extracting base features. This significantly enhances the discriminative power and robustness of global features. Furthermore, we constructed the VPR-City-Mask dataset with effective region annotations based on the GSV-City dataset, providing a real-world reference for the masking mechanism. Experimental results demonstrate superior performance across multiple VPR benchmark datasets, with accurate testing results on large-scale data, robustly validating our approach’s superiority. The code is publicly available at https://github.com/wlh-coder/HAM-VPR .&lt;/p&gt;</content:encoded></item><item><title>WildSci: Advancing Scientific Reasoning from In-the-Wild Literature</title><link>https://arxiv.org/abs/2601.05567v1</link><guid>http://arxiv.org/abs/2601.05567v1</guid><pubDate>Fri, 09 Jan 2026 06:35:23 +0000</pubDate><dc:creator>Tengxiao Liu</dc:creator><dc:creator>Deepak Nathani</dc:creator><dc:creator>Zekun Li</dc:creator><dc:creator>Kevin Yang</dc:creator><dc:creator>William Yang Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.
Published: 2026-01-09T06:35:23+00:00
Venue: arXiv
Score: 0.770 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tengxiao Liu; Deepak Nathani; Zekun Li; Kevin Yang; William Yang Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.770 (consider)&lt;/p&gt;
&lt;p&gt;Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.&lt;/p&gt;</content:encoded></item><item><title>DDR-YOLO: An efficient and accurate object detection algorithm for distracted driving behaviors</title><link>https://doi.org/10.1016/j.eswa.2026.131170</link><guid>10.1016/j.eswa.2026.131170</guid><pubDate>Sun, 11 Jan 2026 15:12:00 +0000</pubDate><dc:creator>Qian Shen</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Yan Zhang</dc:creator><dc:creator>Yuxiang Zhang</dc:creator><dc:creator>Shihao Liu</dc:creator><dc:creator>Yi Li</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131170</prism:doi><description>In recent years, researchers have employed image classification and object detection methods to recognize distracted driving behaviors (DDB). Nevertheless, a comprehensive comparative analysis of these two methods within the realm of distracted driving behavior recognition (DDR) remains underexplored, resulting in most existing algorithms struggling to balance efficiency and accuracy. Therefore, based on a comparative analysis of these two methods, this paper proposes a novel DDR algorithm named DDR-YOLO inspired by YOLO11. Initially, this paper explores the method that performs better in DDR using 250,000 manually labeled images from the 100-Drivers dataset. Furthermore, the lightweight DDR-YOLO algorithm that achieves high accuracy while improving efficiency is introduced. To accurately capture both the local details and overall postural features of DDB, an innovative Neck structure called MHMS is designed along with a new feature extraction module referred to as SGHCB. To further optimize model efficiency, this paper presents an efficient spatial-reorganization upsampling (ESU) module and a novel Shared Convolution Detection head (SCDetection). ESU restructures feature information across channel and spatial dimensions through channel shuffle and spatial shift, with a significant reduction in computational complexity and loss of feature information. By introducing a dedicated detection head branch for huge targets and sharing convolutional parameters across all four branches, SCDetection achieves enhanced detection capability for oversized objects and greater computational efficiency. Additionally, an adaptive dynamic label assignment strategy is developed to enhance the discriminative ability of both high-confidence class predictions and precisely regressed bounding box coordinates, thereby improving recognition accuracy. Moreover, a novel channel pruning method termed DG-LAMP is proposed to significantly reduce the computational cost of the model. Then knowledge distillation is implemented to compensate for the accuracy loss. Experimental results reveal that on the 100-Drivers dataset, most existing lightweight classification algorithms underperform, achieving classification accuracies of only 70% to 80%, and fail to classify multiple DDB occurring at the same time. The DDR-YOLO achieves accuracies of 91.6% and 88.8% on RGB and near-infrared modalities with a computational cost of 1.2 GFLOPs, a parameter count of 0.45M and approximately 2000 FPS. In addition, generalization experiments conducted on the StateFarm dataset and our self-collected dataset achieve accuracies of 44.3% and 87.6%, respectively. Furthermore, the proposed algorithm is deployed on an NVIDIA Jetson Orin Nano 8GB platform for practical validation. In high-power mode, DDR-YOLO runs stably for extended periods with the FPS remaining at around 29, and the operating temperature stays within a normal range. These results confirm that the proposed algorithm shows outstanding performance in terms of model size and real-time capability while maintaining high accuracy.
Published: 2026-01-11T15:12:00+00:00
Venue: Expert Systems with Applications
Score: 0.769 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qian Shen; Lei Zhang; Yan Zhang; Yuxiang Zhang; Shihao Liu; Yi Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131170"&gt;10.1016/j.eswa.2026.131170&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (consider)&lt;/p&gt;
&lt;p&gt;In recent years, researchers have employed image classification and object detection methods to recognize distracted driving behaviors (DDB). Nevertheless, a comprehensive comparative analysis of these two methods within the realm of distracted driving behavior recognition (DDR) remains underexplored, resulting in most existing algorithms struggling to balance efficiency and accuracy. Therefore, based on a comparative analysis of these two methods, this paper proposes a novel DDR algorithm named DDR-YOLO inspired by YOLO11. Initially, this paper explores the method that performs better in DDR using 250,000 manually labeled images from the 100-Drivers dataset. Furthermore, the lightweight DDR-YOLO algorithm that achieves high accuracy while improving efficiency is introduced. To accurately capture both the local details and overall postural features of DDB, an innovative Neck structure called MHMS is designed along with a new feature extraction module referred to as SGHCB. To further optimize model efficiency, this paper presents an efficient spatial-reorganization upsampling (ESU) module and a novel Shared Convolution Detection head (SCDetection). ESU restructures feature information across channel and spatial dimensions through channel shuffle and spatial shift, with a significant reduction in computational complexity and loss of feature information. By introducing a dedicated detection head branch for huge targets and sharing convolutional parameters across all four branches, SCDetection achieves enhanced detection capability for oversized objects and greater computational efficiency. Additionally, an adaptive dynamic label assignment strategy is developed to enhance the discriminative ability of both high-confidence class predictions and precisely regressed bounding box coordinates, thereby improving recognition accuracy. Moreover, a novel channel pruning method termed DG-LAMP is proposed to significantly reduce the computational cost of the model. Then knowledge distillation is implemented to compensate for the accuracy loss. Experimental results reveal that on the 100-Drivers dataset, most existing lightweight classification algorithms underperform, achieving classification accuracies of only 70% to 80%, and fail to classify multiple DDB occurring at the same time. The DDR-YOLO achieves accuracies of 91.6% and 88.8% on RGB and near-infrared modalities with a computational cost of 1.2 GFLOPs, a parameter count of 0.45M and approximately 2000 FPS. In addition, generalization experiments conducted on the StateFarm dataset and our self-collected dataset achieve accuracies of 44.3% and 87.6%, respectively. Furthermore, the proposed algorithm is deployed on an NVIDIA Jetson Orin Nano 8GB platform for practical validation. In high-power mode, DDR-YOLO runs stably for extended periods with the FPS remaining at around 29, and the operating temperature stays within a normal range. These results confirm that the proposed algorithm shows outstanding performance in terms of model size and real-time capability while maintaining high accuracy.&lt;/p&gt;</content:encoded></item><item><title>Prompt-Free SAM-Based Multi-Task Framework for Breast Ultrasound Lesion Segmentation and Classification</title><link>https://arxiv.org/abs/2601.05498v1</link><guid>http://arxiv.org/abs/2601.05498v1</guid><pubDate>Fri, 09 Jan 2026 03:02:41 +0000</pubDate><dc:creator>Samuel E. Johnny</dc:creator><dc:creator>Bernes L. Atabonfack</dc:creator><dc:creator>Israel Alagbe</dc:creator><dc:creator>Assane Gueye</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.
Published: 2026-01-09T03:02:41+00:00
Venue: arXiv
Score: 0.769 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Samuel E. Johnny; Bernes L. Atabonfack; Israel Alagbe; Assane Gueye&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (consider)&lt;/p&gt;
&lt;p&gt;Accurate tumor segmentation and classification in breast ultrasound (BUS) imaging remain challenging due to low contrast, speckle noise, and diverse lesion morphology. This study presents a multi-task deep learning framework that jointly performs lesion segmentation and diagnostic classification using embeddings from the Segment Anything Model (SAM) vision encoder. Unlike prompt-based SAM variants, our approach employs a prompt-free, fully supervised adaptation where high-dimensional SAM features are decoded through either a lightweight convolutional head or a UNet-inspired decoder for pixel-wise segmentation. The classification branch is enhanced via mask-guided attention, allowing the model to focus on lesion-relevant features while suppressing background artifacts. Experiments on the PRECISE 2025 breast ultrasound dataset, split per class into 80 percent training and 20 percent testing, show that the proposed method achieves a Dice Similarity Coefficient (DSC) of 0.887 and an accuracy of 92.3 percent, ranking among the top entries on the PRECISE challenge leaderboard. These results demonstrate that SAM-based representations, when coupled with segmentation-guided learning, significantly improve both lesion delineation and diagnostic prediction in breast ultrasound imaging.&lt;/p&gt;</content:encoded></item><item><title>FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching</title><link>https://arxiv.org/abs/2601.05684v1</link><guid>http://arxiv.org/abs/2601.05684v1</guid><pubDate>Fri, 09 Jan 2026 10:06:45 +0000</pubDate><dc:creator>Hongyaoxing Gul</dc:creator><dc:creator>Lijuan Hu</dc:creator><dc:creator>Shuzi Niu</dc:creator><dc:creator>Fangfang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.
Published: 2026-01-09T10:06:45+00:00
Venue: arXiv
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hongyaoxing Gul; Lijuan Hu; Shuzi Niu; Fangfang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.&lt;/p&gt;</content:encoded></item><item><title>Dr. Zero: Self-Evolving Search Agents without Training Data</title><link>https://arxiv.org/abs/2601.07055v1</link><guid>http://arxiv.org/abs/2601.07055v1</guid><pubDate>Sun, 11 Jan 2026 20:27:55 +0000</pubDate><dc:creator>Zhenrui Yue</dc:creator><dc:creator>Kartikeya Upasani</dc:creator><dc:creator>Xianjun Yang</dc:creator><dc:creator>Suyu Ge</dc:creator><dc:creator>Shaoliang Nie</dc:creator><dc:creator>Yuning Mao</dc:creator><dc:creator>Zhe Liu</dc:creator><dc:creator>Dong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query's individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.
Published: 2026-01-11T20:27:55+00:00
Venue: arXiv
Score: 0.768 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhenrui Yue; Kartikeya Upasani; Xianjun Yang; Suyu Ge; Shaoliang Nie; Yuning Mao; Zhe Liu; Dong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.768 (consider)&lt;/p&gt;
&lt;p&gt;As high-quality data becomes increasingly difficult to obtain, data-free self-evolution has emerged as a promising paradigm. This approach allows large language models (LLMs) to autonomously generate and solve complex problems, thereby improving their reasoning capabilities. However, multi-turn search agents struggle in data-free self-evolution due to the limited question diversity and the substantial compute required for multi-step reasoning and tool using. In this work, we introduce Dr. Zero, a framework enabling search agents to effectively self-evolve without any training data. In particular, we design a self-evolution feedback loop where a proposer generates diverse questions to train a solver initialized from the same base model. As the solver evolves, it incentivizes the proposer to produce increasingly difficult yet solvable tasks, thus establishing an automated curriculum to refine both agents. To enhance training efficiency, we also introduce hop-grouped relative policy optimization (HRPO). This method clusters structurally similar questions to construct group-level baselines, effectively minimizing the sampling overhead in evaluating each query&amp;#x27;s individual difficulty and solvability. Consequently, HRPO significantly reduces the compute requirements for solver training without compromising performance or stability. Extensive experiment results demonstrate that the data-free Dr. Zero matches or surpasses fully supervised search agents, proving that complex reasoning and search capabilities can emerge solely through self-evolution.&lt;/p&gt;</content:encoded></item><item><title>Multi-Head Attention Residual Unfolded Network for Model-Based Pansharpening</title><link>https://doi.org/10.1007/s11263-025-02651-9</link><guid>10.1007/s11263-025-02651-9</guid><pubDate>Mon, 12 Jan 2026 04:59:18 +0000</pubDate><dc:creator>Ivan Pereira-Sánchez</dc:creator><dc:creator>Eloi Sans</dc:creator><dc:creator>Julia Navarro</dc:creator><dc:creator>Joan Duran</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02651-9</prism:doi><description>The objective of pansharpening and hypersharpening is to accurately fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (MS) or hyperspectral (HS) image, respectively. Unfolding fusion methods integrate the powerful representation capabilities of deep learning with the robustness of model-based approaches. These techniques usually involve unrolling the steps of the optimization scheme derived from the minimization of a variational energy into a deep learning framework, resulting in efficient and highly interpretable architectures. In this paper, we present a model-based deep unfolded method for satellite image fusion. Our approach relies on a variational formulation that incorporates the classic observation model for MS/HS data, a high-frequency injection constraint, and a general prior. For the unfolding stage, we design upsampling and downsampling layers that leverage geometric information encoded in the PAN image through residual networks. The core of our method is a Multi-Head Attention Residual Network (MARNet), which combines multiple head attentions with residual learning to capture image self-similarities using nonlocal patch-based operators. Additionally, we include a post-processing module based on the MARNet architecture to further enhance the quality of the fused images. Experimental results on PRISMA, QuickBird, and WorldView2 datasets demonstrate the superior performance of our method, both at reduced and full-scale resolutions, along with its ability to generalize across different sensor configurations and varying spatial and spectral resolutions. The source code will be available at https://github.com/TAMI-UIB/MARNet .
Published: 2026-01-12T04:59:18+00:00
Venue: International Journal of Computer Vision
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ivan Pereira-Sánchez; Eloi Sans; Julia Navarro; Joan Duran&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02651-9"&gt;10.1007/s11263-025-02651-9&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;The objective of pansharpening and hypersharpening is to accurately fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (MS) or hyperspectral (HS) image, respectively. Unfolding fusion methods integrate the powerful representation capabilities of deep learning with the robustness of model-based approaches. These techniques usually involve unrolling the steps of the optimization scheme derived from the minimization of a variational energy into a deep learning framework, resulting in efficient and highly interpretable architectures. In this paper, we present a model-based deep unfolded method for satellite image fusion. Our approach relies on a variational formulation that incorporates the classic observation model for MS/HS data, a high-frequency injection constraint, and a general prior. For the unfolding stage, we design upsampling and downsampling layers that leverage geometric information encoded in the PAN image through residual networks. The core of our method is a Multi-Head Attention Residual Network (MARNet), which combines multiple head attentions with residual learning to capture image self-similarities using nonlocal patch-based operators. Additionally, we include a post-processing module based on the MARNet architecture to further enhance the quality of the fused images. Experimental results on PRISMA, QuickBird, and WorldView2 datasets demonstrate the superior performance of our method, both at reduced and full-scale resolutions, along with its ability to generalize across different sensor configurations and varying spatial and spectral resolutions. The source code will be available at https://github.com/TAMI-UIB/MARNet .&lt;/p&gt;</content:encoded></item><item><title>ENTRA: Entropy-Based Redundancy Avoidance in Large Language Model Reasoning</title><link>https://arxiv.org/abs/2601.07123v1</link><guid>http://arxiv.org/abs/2601.07123v1</guid><pubDate>Mon, 12 Jan 2026 01:26:30 +0000</pubDate><dc:creator>Ruichu Cai</dc:creator><dc:creator>Haopeng Du</dc:creator><dc:creator>Qingwen Lin</dc:creator><dc:creator>Yutong Chen</dc:creator><dc:creator>Zijian Li</dc:creator><dc:creator>Boyan Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large Reasoning Models (LRMs) often suffer from overthinking, generating unnecessarily long reasoning chains even for simple tasks. This leads to substantial computational overhead with limited performance gain, primarily due to redundant verification and repetitive generation. While prior work typically constrains output length or optimizes correctness, such coarse supervision fails to guide models toward concise yet accurate inference. In this paper, we propose ENTRA, an entropy-based training framework that suppresses redundant reasoning while preserving performance. ENTRA first estimates the token-level importance using a lightweight Bidirectional Importance Estimation (BIE) method, which accounts for both prediction confidence and forward influence. It then computes a redundancy reward based on the entropy of low-importance tokens, normalized by its theoretical upper bound, and optimizes this reward via reinforcement learning. Experiments on mathematical reasoning benchmarks demonstrate that ENTRA reduces output length by 37% to 53% with no loss-and in some cases, gains-in accuracy. Our approach offers a principled and efficient solution to reduce overthinking in LRMs, and provides a generalizable path toward redundancy-aware reasoning optimization.
Published: 2026-01-12T01:26:30+00:00
Venue: arXiv
Score: 0.767 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruichu Cai; Haopeng Du; Qingwen Lin; Yutong Chen; Zijian Li; Boyan Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.767 (consider)&lt;/p&gt;
&lt;p&gt;Large Reasoning Models (LRMs) often suffer from overthinking, generating unnecessarily long reasoning chains even for simple tasks. This leads to substantial computational overhead with limited performance gain, primarily due to redundant verification and repetitive generation. While prior work typically constrains output length or optimizes correctness, such coarse supervision fails to guide models toward concise yet accurate inference. In this paper, we propose ENTRA, an entropy-based training framework that suppresses redundant reasoning while preserving performance. ENTRA first estimates the token-level importance using a lightweight Bidirectional Importance Estimation (BIE) method, which accounts for both prediction confidence and forward influence. It then computes a redundancy reward based on the entropy of low-importance tokens, normalized by its theoretical upper bound, and optimizes this reward via reinforcement learning. Experiments on mathematical reasoning benchmarks demonstrate that ENTRA reduces output length by 37% to 53% with no loss-and in some cases, gains-in accuracy. Our approach offers a principled and efficient solution to reduce overthinking in LRMs, and provides a generalizable path toward redundancy-aware reasoning optimization.&lt;/p&gt;</content:encoded></item><item><title>How to Build Robust, Scalable Models for GSV-Based Indicators in Neighborhood Research</title><link>https://arxiv.org/abs/2601.06443v1</link><guid>http://arxiv.org/abs/2601.06443v1</guid><pubDate>Sat, 10 Jan 2026 06:00:09 +0000</pubDate><dc:creator>Xiaoya Tang</dc:creator><dc:creator>Xiaohe Yue</dc:creator><dc:creator>Heran Mane</dc:creator><dc:creator>Dapeng Li</dc:creator><dc:creator>Quynh Nguyen</dc:creator><dc:creator>Tolga Tasdizen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>A substantial body of health research demonstrates a strong link between neighborhood environments and health outcomes. Recently, there has been increasing interest in leveraging advances in computer vision to enable large-scale, systematic characterization of neighborhood built environments. However, the generalizability of vision models across fundamentally different domains remains uncertain, for example, transferring knowledge from ImageNet to the distinct visual characteristics of Google Street View (GSV) imagery. In applied fields such as social health research, several critical questions arise: which models are most appropriate, whether to adopt unsupervised training strategies, what training scale is feasible under computational constraints, and how much such strategies benefit downstream performance. These decisions are often costly and require specialized expertise.
  In this paper, we answer these questions through empirical analysis and provide practical insights into how to select and adapt foundation models for datasets with limited size and labels, while leveraging larger, unlabeled datasets through unsupervised training. Our study includes comprehensive quantitative and visual analyses comparing model performance before and after unsupervised adaptation.
Published: 2026-01-10T06:00:09+00:00
Venue: arXiv
Score: 0.766 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiaoya Tang; Xiaohe Yue; Heran Mane; Dapeng Li; Quynh Nguyen; Tolga Tasdizen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (consider)&lt;/p&gt;
&lt;p&gt;A substantial body of health research demonstrates a strong link between neighborhood environments and health outcomes. Recently, there has been increasing interest in leveraging advances in computer vision to enable large-scale, systematic characterization of neighborhood built environments. However, the generalizability of vision models across fundamentally different domains remains uncertain, for example, transferring knowledge from ImageNet to the distinct visual characteristics of Google Street View (GSV) imagery. In applied fields such as social health research, several critical questions arise: which models are most appropriate, whether to adopt unsupervised training strategies, what training scale is feasible under computational constraints, and how much such strategies benefit downstream performance. These decisions are often costly and require specialized expertise.
  In this paper, we answer these questions through empirical analysis and provide practical insights into how to select and adapt foundation models for datasets with limited size and labels, while leveraging larger, unlabeled datasets through unsupervised training. Our study includes comprehensive quantitative and visual analyses comparing model performance before and after unsupervised adaptation.&lt;/p&gt;</content:encoded></item><item><title>Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces</title><link>https://arxiv.org/abs/2601.05913v1</link><guid>http://arxiv.org/abs/2601.05913v1</guid><pubDate>Fri, 09 Jan 2026 16:28:55 +0000</pubDate><dc:creator>Pattarawat Chormai</dc:creator><dc:creator>Ali Hashemi</dc:creator><dc:creator>Klaus-Robert Müller</dc:creator><dc:creator>Grégoire Montavon</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce 'SubDistill', a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.
Published: 2026-01-09T16:28:55+00:00
Venue: arXiv
Score: 0.766 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Pattarawat Chormai; Ali Hashemi; Klaus-Robert Müller; Grégoire Montavon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (consider)&lt;/p&gt;
&lt;p&gt;Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce &amp;#x27;SubDistill&amp;#x27;, a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.&lt;/p&gt;</content:encoded></item></channel></rss>