<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Tue, 27 Jan 2026 02:47:34 +0000</lastBuildDate><item><title>SU-RMT: Toward Bridging Semantic Representation and Structural Detail Modeling for Medical Image Segmentation</title><link>https://doi.org/10.1016/j.inffus.2026.104182</link><guid>10.1016/j.inffus.2026.104182</guid><pubDate>Mon, 26 Jan 2026 06:56:57 +0000</pubDate><dc:creator>Peibo Song</dc:creator><dc:creator>Zihao Wang</dc:creator><dc:creator>Jinshuo Zhang</dc:creator><dc:creator>Shujun Fu</dc:creator><dc:creator>Yunfeng Zhang</dc:creator><dc:creator>Wei Wu</dc:creator><dc:creator>Fangxun Bao</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104182</prism:doi><description>Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .
Published: 2026-01-26T06:56:57+00:00
Venue: Information Fusion
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Peibo Song; Zihao Wang; Jinshuo Zhang; Shujun Fu; Yunfeng Zhang; Wei Wu; Fangxun Bao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104182"&gt;10.1016/j.inffus.2026.104182&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .&lt;/p&gt;</content:encoded></item><item><title>S2I-DiT: Unlocking the Semantic-to-Image Transferability by Fine-tuning Large Diffusion Transformer Models</title><link>https://doi.org/10.1016/j.patcog.2026.113158</link><guid>10.1016/j.patcog.2026.113158</guid><pubDate>Sun, 25 Jan 2026 15:29:56 +0000</pubDate><dc:creator>Gang Li</dc:creator><dc:creator>Enze Xie</dc:creator><dc:creator>Chongjian Ge</dc:creator><dc:creator>Xiang Li</dc:creator><dc:creator>Lingyu Si</dc:creator><dc:creator>Changwen Zheng</dc:creator><dc:creator>Zhenguo Li</dc:creator><prism:publicationName>Pattern Recognition</prism:publicationName><prism:doi>10.1016/j.patcog.2026.113158</prism:doi><description>Denoising Diffusion Probabilistic Models (DDPMs) have made significant progress in image generation. Recent works in semantic-to-image (S2I) synthesis have also shifted from the previously de facto GAN-based methods to DDPMs, yielding better results. However, these works mostly employ a U-Net structure and vanilla training-from-scratch scheme for S2I, unconsciously neglecting the potential benefits offered by task-related pre-training. In this work, we introduce a Transformer-based architecture, namely S2I-DiT, and reconsider the merits of a pre-trained large diffusion model for cross-task adaptation (i.e., from the class-conditional generation to S2I). In S2I-DiT, we propose the integration of semantic embedders within Diffusion Transformers (DiTs) to maximize the utilization of semantic information. The semantic embedder densely encodes semantic layouts to guide the adaptive normalization process. We configure semantic embedders in a layer-wise manner to learn pixel-level correspondence, enabling finer-grained semantic-to-image control. Besides, to fully unleash the cross-task transferability of DDPMs, we introduce a two-stage fine-tuning strategy, which involves initially adapting the semantic embedders in the pixel-level space, followed by fine-tuning the partial/entire model for cross-task adaptation. Notably, S2I-DiT pioneers the application of Large Diffusion Transformers to cross-task fine-tuning. Extensive experiments on four benchmark datasets demonstrate S2I-DiT’s effectiveness, as it achieves state-of-the-art performance in terms of quality (FID) and diversity (LPIPS), while consuming fewer training iterations. This work establishes a new state-of-the-art for semantic-to-image generation and provides valuable insights into cross-task transferability of large generative models.
Published: 2026-01-25T15:29:56+00:00
Venue: Pattern Recognition
Score: 0.799 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Gang Li; Enze Xie; Chongjian Ge; Xiang Li; Lingyu Si; Changwen Zheng; Zhenguo Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Pattern Recognition&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.patcog.2026.113158"&gt;10.1016/j.patcog.2026.113158&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.799 (must_read)&lt;/p&gt;
&lt;p&gt;Denoising Diffusion Probabilistic Models (DDPMs) have made significant progress in image generation. Recent works in semantic-to-image (S2I) synthesis have also shifted from the previously de facto GAN-based methods to DDPMs, yielding better results. However, these works mostly employ a U-Net structure and vanilla training-from-scratch scheme for S2I, unconsciously neglecting the potential benefits offered by task-related pre-training. In this work, we introduce a Transformer-based architecture, namely S2I-DiT, and reconsider the merits of a pre-trained large diffusion model for cross-task adaptation (i.e., from the class-conditional generation to S2I). In S2I-DiT, we propose the integration of semantic embedders within Diffusion Transformers (DiTs) to maximize the utilization of semantic information. The semantic embedder densely encodes semantic layouts to guide the adaptive normalization process. We configure semantic embedders in a layer-wise manner to learn pixel-level correspondence, enabling finer-grained semantic-to-image control. Besides, to fully unleash the cross-task transferability of DDPMs, we introduce a two-stage fine-tuning strategy, which involves initially adapting the semantic embedders in the pixel-level space, followed by fine-tuning the partial/entire model for cross-task adaptation. Notably, S2I-DiT pioneers the application of Large Diffusion Transformers to cross-task fine-tuning. Extensive experiments on four benchmark datasets demonstrate S2I-DiT’s effectiveness, as it achieves state-of-the-art performance in terms of quality (FID) and diversity (LPIPS), while consuming fewer training iterations. This work establishes a new state-of-the-art for semantic-to-image generation and provides valuable insights into cross-task transferability of large generative models.&lt;/p&gt;</content:encoded></item><item><title>VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training</title><link>https://arxiv.org/abs/2601.17830v1</link><guid>http://arxiv.org/abs/2601.17830v1</guid><pubDate>Sun, 25 Jan 2026 13:22:38 +0000</pubDate><dc:creator>Mengmeng Wang</dc:creator><dc:creator>Dengyang Jiang</dc:creator><dc:creator>Liuzhuozheng Li</dc:creator><dc:creator>Yucheng Lin</dc:creator><dc:creator>Guojiang Shen</dc:creator><dc:creator>Xiangjie Kong</dc:creator><dc:creator>Yong Liu</dc:creator><dc:creator>Guang Dai</dc:creator><dc:creator>Jingdong Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.
Published: 2026-01-25T13:22:38+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengmeng Wang; Dengyang Jiang; Liuzhuozheng Li; Yucheng Lin; Guojiang Shen; Xiangjie Kong; Yong Liu; Guang Dai; Jingdong Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.&lt;/p&gt;</content:encoded></item><item><title>$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts</title><link>https://arxiv.org/abs/2601.17680v1</link><guid>http://arxiv.org/abs/2601.17680v1</guid><pubDate>Sun, 25 Jan 2026 03:55:51 +0000</pubDate><dc:creator>Shota Takashiro</dc:creator><dc:creator>Takeshi Kojima</dc:creator><dc:creator>Shohei Taniguchi</dc:creator><dc:creator>Yusuke Iwasawa</dc:creator><dc:creator>Yutaka Matsuo</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.
Published: 2026-01-25T03:55:51+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shota Takashiro; Takeshi Kojima; Shohei Taniguchi; Yusuke Iwasawa; Yutaka Matsuo&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.&lt;/p&gt;</content:encoded></item><item><title>Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity</title><link>https://arxiv.org/abs/2601.17408v1</link><guid>http://arxiv.org/abs/2601.17408v1</guid><pubDate>Sat, 24 Jan 2026 10:51:25 +0000</pubDate><dc:creator>Harsharaj Pathak</dc:creator><dc:creator>Vineeth N Balasubramanian</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.5281/zenodo.17767092</prism:doi><description>Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.
Published: 2026-01-24T10:51:25+00:00
Venue: arXiv
Score: 0.778 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Harsharaj Pathak; Vineeth N Balasubramanian&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.5281/zenodo.17767092"&gt;10.5281/zenodo.17767092&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.778 (must_read)&lt;/p&gt;
&lt;p&gt;Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.&lt;/p&gt;</content:encoded></item><item><title>Boundary and Position Information Mining for Aerial Small Object Detection</title><link>https://arxiv.org/abs/2601.16617v1</link><guid>http://arxiv.org/abs/2601.16617v1</guid><pubDate>Fri, 23 Jan 2026 10:15:12 +0000</pubDate><dc:creator>Rongxin Huang</dc:creator><dc:creator>Guangfeng Lin</dc:creator><dc:creator>Wenbo Zhou</dc:creator><dc:creator>Zhirong Li</dc:creator><dc:creator>Wenhuan Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load.
Published: 2026-01-23T10:15:12+00:00
Venue: arXiv
Score: 0.776 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rongxin Huang; Guangfeng Lin; Wenbo Zhou; Zhirong Li; Wenhuan Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.776 (must_read)&lt;/p&gt;
&lt;p&gt;Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load.&lt;/p&gt;</content:encoded></item><item><title>Proposing and solving olympiad geometry with guided tree search</title><link>https://doi.org/10.1038/s42256-025-01164-x</link><guid>10.1038/s42256-025-01164-x</guid><pubDate>Mon, 26 Jan 2026 10:02:30 +0000</pubDate><dc:creator>Chi Zhang</dc:creator><dc:creator>Jiajun Song</dc:creator><dc:creator>Siyu Li</dc:creator><dc:creator>Yitao Liang</dc:creator><dc:creator>Yuxi Ma</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Yixin Zhu</dc:creator><dc:creator>Song-Chun Zhu</dc:creator><prism:publicationName>Nature Machine Intelligence</prism:publicationName><prism:doi>10.1038/s42256-025-01164-x</prism:doi><description>Mathematics olympiads are prestigious competitions in which both proposing and solving problems are highly honoured. Building artificial intelligence systems capable of addressing these olympiad-level challenges remains an open frontier in automated reasoning, particularly in geometry due to its unique blend of numerical precision and spatial intuition. Here we show that TongGeometry, a neuro-symbolic system using guided tree search, both discovers and proves olympiad-level geometry theorems. Within the same computational budget as existing state-of-the-art systems, TongGeometry establishes a larger repository of geometry theorems: 6.7 billion requiring auxiliary constructions, including 4.1 billion exhibiting geometric symmetry. Among these, three of TongGeometry’s discoveries were selected for regional mathematical olympiads, appearing in a national team qualifying exam in China and a top civil olympiad in the USA. Guided by fine-tuned large language models, TongGeometry solved all International Mathematical Olympiad geometry problems in the IMO-AG-30 benchmark, outperforming average top human competitors on this specific dataset. It also surpasses the existing state of the art across a broader spectrum of olympiad-level problems and requires only consumer-grade computing resources. These results demonstrate that TongGeometry operates as both a mathematical discoverer and a solver, becoming an artificial intelligence system to achieve this dual capability. The deployment of a preliminary system based on TongGeometry demonstrates practical applications and opens fresh possibilities for artificial-intelligence-assisted mathematical research and education. TongGeometry both solves and proposes olympiad-level geometry problems. It uses guided tree search to find hard but concise problems, making advanced mathematical reasoning more accessible.
Published: 2026-01-26T10:02:30+00:00
Venue: Nature Machine Intelligence
Score: 0.775 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chi Zhang; Jiajun Song; Siyu Li; Yitao Liang; Yuxi Ma; Wei Wang; Yixin Zhu; Song-Chun Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Nature Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1038/s42256-025-01164-x"&gt;10.1038/s42256-025-01164-x&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.775 (must_read)&lt;/p&gt;
&lt;p&gt;Mathematics olympiads are prestigious competitions in which both proposing and solving problems are highly honoured. Building artificial intelligence systems capable of addressing these olympiad-level challenges remains an open frontier in automated reasoning, particularly in geometry due to its unique blend of numerical precision and spatial intuition. Here we show that TongGeometry, a neuro-symbolic system using guided tree search, both discovers and proves olympiad-level geometry theorems. Within the same computational budget as existing state-of-the-art systems, TongGeometry establishes a larger repository of geometry theorems: 6.7 billion requiring auxiliary constructions, including 4.1 billion exhibiting geometric symmetry. Among these, three of TongGeometry’s discoveries were selected for regional mathematical olympiads, appearing in a national team qualifying exam in China and a top civil olympiad in the USA. Guided by fine-tuned large language models, TongGeometry solved all International Mathematical Olympiad geometry problems in the IMO-AG-30 benchmark, outperforming average top human competitors on this specific dataset. It also surpasses the existing state of the art across a broader spectrum of olympiad-level problems and requires only consumer-grade computing resources. These results demonstrate that TongGeometry operates as both a mathematical discoverer and a solver, becoming an artificial intelligence system to achieve this dual capability. The deployment of a preliminary system based on TongGeometry demonstrates practical applications and opens fresh possibilities for artificial-intelligence-assisted mathematical research and education. TongGeometry both solves and proposes olympiad-level geometry problems. It uses guided tree search to find hard but concise problems, making advanced mathematical reasoning more accessible.&lt;/p&gt;</content:encoded></item><item><title>MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance</title><link>https://arxiv.org/abs/2601.17866v1</link><guid>http://arxiv.org/abs/2601.17866v1</guid><pubDate>Sun, 25 Jan 2026 15:00:37 +0000</pubDate><dc:creator>Yoonwoo Jeong</dc:creator><dc:creator>Cheng Sun</dc:creator><dc:creator>Yu-Chiang Frank Wang</dc:creator><dc:creator>Minsu Cho</dc:creator><dc:creator>Jaesung Choe</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.
Published: 2026-01-25T15:00:37+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yoonwoo Jeong; Cheng Sun; Yu-Chiang Frank Wang; Minsu Cho; Jaesung Choe&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.&lt;/p&gt;</content:encoded></item><item><title>Masked Depth Modeling for Spatial Perception</title><link>https://arxiv.org/abs/2601.17895v1</link><guid>http://arxiv.org/abs/2601.17895v1</guid><pubDate>Sun, 25 Jan 2026 16:13:49 +0000</pubDate><dc:creator>Bin Tan</dc:creator><dc:creator>Changjiang Sun</dc:creator><dc:creator>Xiage Qin</dc:creator><dc:creator>Hanat Adai</dc:creator><dc:creator>Zelin Fu</dc:creator><dc:creator>Tianxiang Zhou</dc:creator><dc:creator>Han Zhang</dc:creator><dc:creator>Yinghao Xu</dc:creator><dc:creator>Xing Zhu</dc:creator><dc:creator>Yujun Shen</dc:creator><dc:creator>Nan Xue</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as "masked" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.
Published: 2026-01-25T16:13:49+00:00
Venue: arXiv
Score: 0.774 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Bin Tan; Changjiang Sun; Xiage Qin; Hanat Adai; Zelin Fu; Tianxiang Zhou; Han Zhang; Yinghao Xu; Xing Zhu; Yujun Shen; Nan Xue&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.774 (must_read)&lt;/p&gt;
&lt;p&gt;Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as &amp;quot;masked&amp;quot; signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.&lt;/p&gt;</content:encoded></item><item><title>SCID-Net: Few-shot deep-hole defect instance segmentation via multi-grained feature coupling and instance-aware inference decoupling</title><link>https://doi.org/10.1016/j.eswa.2026.131341</link><guid>10.1016/j.eswa.2026.131341</guid><pubDate>Sun, 25 Jan 2026 23:16:10 +0000</pubDate><dc:creator>Zongyang Zhao</dc:creator><dc:creator>Jiehu Kang</dc:creator><dc:creator>Yichen Xu</dc:creator><dc:creator>Jian Liang</dc:creator><dc:creator>Luyuan Feng</dc:creator><dc:creator>Yuqi Ren</dc:creator><dc:creator>Ting Xue</dc:creator><dc:creator>Bin Wu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131341</prism:doi><description>Accurate instance segmentation of deep-hole inner surface defects is critical for ensuring the structural integrity and functional reliability of high-precision industrial components. However, this task remains highly challenging due to the extreme scarcity of annotated data, along with the irregular morphology, weak texture, and dense, random spatial distribution of defects. Existing instance segmentation methods typically rely on large-scale supervision, which is prohibitively expensive and often infeasible in real-world manufacturing. While few-shot learning offers a promising alternative, current models primarily focus on semantic segmentation and fail to delineate individual defect instances with accurate boundaries and counts. Moreover, they lack adaptive mechanisms to model fine-grained morphological variations of defect regions and are susceptible to foreground–background ambiguity induced by incomplete annotations, resulting in classification bias during inspection. To address these limitations, we propose SCID-Net, a novel few-shot defect instance segmentation framework based on multi-granularity feature coupling and instance-aware inference decoupling. Specifically, we introduce a Multi-Grained Coupling Module (GCM) to facilitate hierarchical bi-directional interaction between support and query features, enriching both class-level prototypes and instance-specific representations. Built upon this, the Instance-Aware Inference Decoupling Module (IAM) decouples dense inference into specialized pathways, and further integrates adaptive spatial modulation and prototype-driven semantic alignment to suppress noise from incomplete annotations. Extensive experiments on a proprietary industrial deep-hole defect dataset demonstrate that SCID-Net achieves state-of-the-art performance under few-shot settings. Moreover, evaluations on NEU-Seg and MS COCO further validate the exceptional generalization capability of SCID-Net, highlighting its versatility in both challenging industrial environments and diverse real-world scenarios.
Published: 2026-01-25T23:16:10+00:00
Venue: Expert Systems with Applications
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zongyang Zhao; Jiehu Kang; Yichen Xu; Jian Liang; Luyuan Feng; Yuqi Ren; Ting Xue; Bin Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131341"&gt;10.1016/j.eswa.2026.131341&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Accurate instance segmentation of deep-hole inner surface defects is critical for ensuring the structural integrity and functional reliability of high-precision industrial components. However, this task remains highly challenging due to the extreme scarcity of annotated data, along with the irregular morphology, weak texture, and dense, random spatial distribution of defects. Existing instance segmentation methods typically rely on large-scale supervision, which is prohibitively expensive and often infeasible in real-world manufacturing. While few-shot learning offers a promising alternative, current models primarily focus on semantic segmentation and fail to delineate individual defect instances with accurate boundaries and counts. Moreover, they lack adaptive mechanisms to model fine-grained morphological variations of defect regions and are susceptible to foreground–background ambiguity induced by incomplete annotations, resulting in classification bias during inspection. To address these limitations, we propose SCID-Net, a novel few-shot defect instance segmentation framework based on multi-granularity feature coupling and instance-aware inference decoupling. Specifically, we introduce a Multi-Grained Coupling Module (GCM) to facilitate hierarchical bi-directional interaction between support and query features, enriching both class-level prototypes and instance-specific representations. Built upon this, the Instance-Aware Inference Decoupling Module (IAM) decouples dense inference into specialized pathways, and further integrates adaptive spatial modulation and prototype-driven semantic alignment to suppress noise from incomplete annotations. Extensive experiments on a proprietary industrial deep-hole defect dataset demonstrate that SCID-Net achieves state-of-the-art performance under few-shot settings. Moreover, evaluations on NEU-Seg and MS COCO further validate the exceptional generalization capability of SCID-Net, highlighting its versatility in both challenging industrial environments and diverse real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>HDC-Net:A Multimodal Remote Sensing Semantic Segmentation Network with Hierarchical Dual-Stream Fusion and Cross-Token Interaction</title><link>https://doi.org/10.1016/j.knosys.2026.115416</link><guid>10.1016/j.knosys.2026.115416</guid><pubDate>Sun, 25 Jan 2026 23:15:42 +0000</pubDate><dc:creator>Zhengpeng Li</dc:creator><dc:creator>Yubo Zhang</dc:creator><dc:creator>Jun Hu</dc:creator><dc:creator>Kunyang Wu</dc:creator><dc:creator>Jiawei Miao</dc:creator><dc:creator>Jiansheng Wu</dc:creator><dc:creator>Xiaolin Zhang</dc:creator><dc:creator>Bin Yang</dc:creator><dc:creator>Zhiguo Xia</dc:creator><prism:publicationName>Knowledge-Based Systems</prism:publicationName><prism:doi>10.1016/j.knosys.2026.115416</prism:doi><description>Semantic segmentation of multimodal remote sensing imagery aims to integrate complementary information from different sensors to achieve high-precision land-cover classification, representing a key direction in remote sensing image interpretation. However, most existing approaches adopt homogeneous fusion strategies, such as simple feature concatenation or uniform attention mechanisms, which fail to address the dynamic requirements across different representation levels. In the shallow layers, the lack of precise spatial alignment often leads to the loss of fine details and blurred boundaries, while in the deeper layers, these methods struggle to effectively disentangle cross-modal semantic relationships and model global dependencies. To overcome these limitations, this paper proposes a hierarchical dual-stream fusion and cross-token interaction network (HDC-Net) for multimodal remote sensing semantic segmentation. The network follows a layer-wise heterogeneous design. In the shallow encoding stage, an interactive and shared attention fusion (ISAF) module is introduced to achieve pixel-level spatial alignment and feature enhancement. In the deeper layers, a hierarchical cross-token interaction transformer (HCFormer) is developed for global semantic modeling and cross-modal relationship disentanglement. Additionally, a pyramidal fusion bridge (PFB) is designed to efficiently connect deep and shallow features. Finally, an information-fusion decoder integrates deep semantics, cross-modal bridging features, and shallow spatial details to produce high-fidelity segmentation maps. Extensive experiments on three public benchmark datasets, ISPRS Vaihingen, ISPRS Potsdam, and WHU-OPT-SAR, demonstrate the effectiveness, robustness, and generalization capability of the proposed approach. The implementation is available at https://github.com/lzp-lkd/HDC-Net .
Published: 2026-01-25T23:15:42+00:00
Venue: Knowledge-Based Systems
Score: 0.771 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhengpeng Li; Yubo Zhang; Jun Hu; Kunyang Wu; Jiawei Miao; Jiansheng Wu; Xiaolin Zhang; Bin Yang; Zhiguo Xia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Knowledge-Based Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.knosys.2026.115416"&gt;10.1016/j.knosys.2026.115416&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.771 (must_read)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of multimodal remote sensing imagery aims to integrate complementary information from different sensors to achieve high-precision land-cover classification, representing a key direction in remote sensing image interpretation. However, most existing approaches adopt homogeneous fusion strategies, such as simple feature concatenation or uniform attention mechanisms, which fail to address the dynamic requirements across different representation levels. In the shallow layers, the lack of precise spatial alignment often leads to the loss of fine details and blurred boundaries, while in the deeper layers, these methods struggle to effectively disentangle cross-modal semantic relationships and model global dependencies. To overcome these limitations, this paper proposes a hierarchical dual-stream fusion and cross-token interaction network (HDC-Net) for multimodal remote sensing semantic segmentation. The network follows a layer-wise heterogeneous design. In the shallow encoding stage, an interactive and shared attention fusion (ISAF) module is introduced to achieve pixel-level spatial alignment and feature enhancement. In the deeper layers, a hierarchical cross-token interaction transformer (HCFormer) is developed for global semantic modeling and cross-modal relationship disentanglement. Additionally, a pyramidal fusion bridge (PFB) is designed to efficiently connect deep and shallow features. Finally, an information-fusion decoder integrates deep semantics, cross-modal bridging features, and shallow spatial details to produce high-fidelity segmentation maps. Extensive experiments on three public benchmark datasets, ISPRS Vaihingen, ISPRS Potsdam, and WHU-OPT-SAR, demonstrate the effectiveness, robustness, and generalization capability of the proposed approach. The implementation is available at https://github.com/lzp-lkd/HDC-Net .&lt;/p&gt;</content:encoded></item><item><title>Tiny Object Detection via Normalized Gaussian Label Assignment and Multi-Scale Hybrid Attention</title><link>https://doi.org/10.3390/rs18030396</link><guid>10.3390/rs18030396</guid><pubDate>Mon, 26 Jan 2026 11:14:07 +0000</pubDate><dc:creator>Shihao Lin</dc:creator><dc:creator>Li Zhong</dc:creator><dc:creator>Si Chen</dc:creator><dc:creator>Da-Han Wang</dc:creator><prism:publicationName>Remote Sensing</prism:publicationName><prism:doi>10.3390/rs18030396</prism:doi><description>The rapid development of Convolutional Neural Networks (CNNs) has markedly boosted the performance of object detection in remote sensing. Nevertheless, tiny objects typically account for an extremely small fraction of the total area in remote sensing images, rendering existing IoU-based or area-based evaluation metrics highly sensitive to minor pixel deviations. Meanwhile, classic detection models face inherent bottlenecks in efficiently mining discriminative features for tiny objects, leaving the task of tiny object detection in remote sensing images as an ongoing challenge in this field. To alleviate these issues, this paper proposes a tiny object detection method based on Normalized Gaussian Label Assignment and Multi-scale Hybrid Attention. Firstly, 2D Gaussian modeling is performed on the feature receptive field and the actual bounding box, using Normalized Bhattacharyya Distance for precise similarity measurement. Furthermore, a candidate sample quality ranking mechanism is constructed to select high-quality positive samples. Finally, a Multi-scale Hybrid Attention module is designed to enhance the discriminative feature extraction of tiny objects. The proposed method achieves 25.7% and 27.9% AP on the AI-TOD-v2 and VisDrone2019 datasets, respectively, significantly improving the detection capability of tiny objects in complex remote sensing scenarios.
Published: 2026-01-26T11:14:07+00:00
Venue: Remote Sensing
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Shihao Lin; Li Zhong; Si Chen; Da-Han Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.3390/rs18030396"&gt;10.3390/rs18030396&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;The rapid development of Convolutional Neural Networks (CNNs) has markedly boosted the performance of object detection in remote sensing. Nevertheless, tiny objects typically account for an extremely small fraction of the total area in remote sensing images, rendering existing IoU-based or area-based evaluation metrics highly sensitive to minor pixel deviations. Meanwhile, classic detection models face inherent bottlenecks in efficiently mining discriminative features for tiny objects, leaving the task of tiny object detection in remote sensing images as an ongoing challenge in this field. To alleviate these issues, this paper proposes a tiny object detection method based on Normalized Gaussian Label Assignment and Multi-scale Hybrid Attention. Firstly, 2D Gaussian modeling is performed on the feature receptive field and the actual bounding box, using Normalized Bhattacharyya Distance for precise similarity measurement. Furthermore, a candidate sample quality ranking mechanism is constructed to select high-quality positive samples. Finally, a Multi-scale Hybrid Attention module is designed to enhance the discriminative feature extraction of tiny objects. The proposed method achieves 25.7% and 27.9% AP on the AI-TOD-v2 and VisDrone2019 datasets, respectively, significantly improving the detection capability of tiny objects in complex remote sensing scenarios.&lt;/p&gt;</content:encoded></item><item><title>TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution</title><link>https://arxiv.org/abs/2601.17340v1</link><guid>http://arxiv.org/abs/2601.17340v1</guid><pubDate>Sat, 24 Jan 2026 07:03:41 +0000</pubDate><dc:creator>Haodong He</dc:creator><dc:creator>Xin Zhan</dc:creator><dc:creator>Yancheng Bai</dc:creator><dc:creator>Rui Lan</dc:creator><dc:creator>Lei Sun</dc:creator><dc:creator>Xiangxiang Chu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.
Published: 2026-01-24T07:03:41+00:00
Venue: arXiv
Score: 0.769 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Haodong He; Xin Zhan; Yancheng Bai; Rui Lan; Lei Sun; Xiangxiang Chu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.769 (must_read)&lt;/p&gt;
&lt;p&gt;Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.&lt;/p&gt;</content:encoded></item><item><title>Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning</title><link>https://arxiv.org/abs/2601.17275v1</link><guid>http://arxiv.org/abs/2601.17275v1</guid><pubDate>Sat, 24 Jan 2026 03:18:22 +0000</pubDate><dc:creator>Lianlei Shan</dc:creator><dc:creator>Han Chen</dc:creator><dc:creator>Yixuan Wang</dc:creator><dc:creator>Zhenjie Liu</dc:creator><dc:creator>Wei Li</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.
Published: 2026-01-24T03:18:22+00:00
Venue: arXiv
Score: 0.766 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lianlei Shan; Han Chen; Yixuan Wang; Zhenjie Liu; Wei Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.766 (must_read)&lt;/p&gt;
&lt;p&gt;While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&amp;#x27;&amp;#x27; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&amp;#x27;&amp;#x27; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.&lt;/p&gt;</content:encoded></item><item><title>Feature-Space Generative Models for One-Shot Class-Incremental Learning</title><link>https://arxiv.org/abs/2601.17905v1</link><guid>http://arxiv.org/abs/2601.17905v1</guid><pubDate>Sun, 25 Jan 2026 16:45:11 +0000</pubDate><dc:creator>Jack Foster</dc:creator><dc:creator>Kirill Paramonov</dc:creator><dc:creator>Mete Ozay</dc:creator><dc:creator>Umberto Michieli</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.
Published: 2026-01-25T16:45:11+00:00
Venue: arXiv
Score: 0.765 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jack Foster; Kirill Paramonov; Mete Ozay; Umberto Michieli&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.765 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.&lt;/p&gt;</content:encoded></item><item><title>Power-based Partial Attention: Bridging Linear-Complexity and Full Attention</title><link>https://arxiv.org/abs/2601.17334v1</link><guid>http://arxiv.org/abs/2601.17334v1</guid><pubDate>Sat, 24 Jan 2026 06:30:53 +0000</pubDate><dc:creator>Yufeng Huang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>It is widely accepted from transformer research that "attention is all we need", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0&lt;p&lt;1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.
Published: 2026-01-24T06:30:53+00:00
Venue: arXiv
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yufeng Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;It is widely accepted from transformer research that &amp;quot;attention is all we need&amp;quot;, but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0&amp;lt;p&amp;lt;1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.&lt;/p&gt;</content:encoded></item><item><title>GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing</title><link>https://arxiv.org/abs/2601.17089v1</link><guid>http://arxiv.org/abs/2601.17089v1</guid><pubDate>Fri, 23 Jan 2026 10:12:59 +0000</pubDate><dc:creator>Qigan Sun</dc:creator><dc:creator>Chaoning Zhang</dc:creator><dc:creator>Jianwei Zhang</dc:creator><dc:creator>Xudong Wang</dc:creator><dc:creator>Jiehui Xie</dc:creator><dc:creator>Pengcheng Zheng</dc:creator><dc:creator>Haoyu Wang</dc:creator><dc:creator>Sungyoung Lee</dc:creator><dc:creator>Chi-lok Andy Tai</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Heng Tao Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.
Published: 2026-01-23T10:12:59+00:00
Venue: arXiv
Score: 0.764 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qigan Sun; Chaoning Zhang; Jianwei Zhang; Xudong Wang; Jiehui Xie; Pengcheng Zheng; Haoyu Wang; Sungyoung Lee; Chi-lok Andy Tai; Yang Yang; Heng Tao Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.&lt;/p&gt;</content:encoded></item><item><title>STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation</title><link>https://arxiv.org/abs/2601.17342v1</link><guid>http://arxiv.org/abs/2601.17342v1</guid><pubDate>Sat, 24 Jan 2026 07:07:16 +0000</pubDate><dc:creator>Tong Wang</dc:creator><dc:creator>Xiaodong Zhang</dc:creator><dc:creator>Guanzhou Chen</dc:creator><dc:creator>Jiaqi Wang</dc:creator><dc:creator>Chenxi Liu</dc:creator><dc:creator>Xiaoliang Tan</dc:creator><dc:creator>Wenchao Guo</dc:creator><dc:creator>Xuyang Li</dc:creator><dc:creator>Xuanrui Wang</dc:creator><dc:creator>Zifan Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.
Published: 2026-01-24T07:07:16+00:00
Venue: arXiv
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tong Wang; Xiaodong Zhang; Guanzhou Chen; Jiaqi Wang; Chenxi Liu; Xiaoliang Tan; Wenchao Guo; Xuyang Li; Xuanrui Wang; Zifan Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.&lt;/p&gt;</content:encoded></item><item><title>Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales</title><link>https://arxiv.org/abs/2601.17271v1</link><guid>http://arxiv.org/abs/2601.17271v1</guid><pubDate>Sat, 24 Jan 2026 03:00:45 +0000</pubDate><dc:creator>Kun Huang</dc:creator><dc:creator>Fang-Lue Zhang</dc:creator><dc:creator>Neil Dodgson</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.
Published: 2026-01-24T03:00:45+00:00
Venue: arXiv
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kun Huang; Fang-Lue Zhang; Neil Dodgson&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection&amp;#x27;s 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.&lt;/p&gt;</content:encoded></item><item><title>VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection</title><link>https://arxiv.org/abs/2601.16381v1</link><guid>http://arxiv.org/abs/2601.16381v1</guid><pubDate>Fri, 23 Jan 2026 00:30:24 +0000</pubDate><dc:creator>Yuxin Jiang</dc:creator><dc:creator>Yunkang Cao</dc:creator><dc:creator>Yuqi Cheng</dc:creator><dc:creator>Yiheng Zhang</dc:creator><dc:creator>Weiming Shen</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.
Published: 2026-01-23T00:30:24+00:00
Venue: arXiv
Score: 0.764 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxin Jiang; Yunkang Cao; Yuqi Cheng; Yiheng Zhang; Weiming Shen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.764 (consider)&lt;/p&gt;
&lt;p&gt;Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.&lt;/p&gt;</content:encoded></item><item><title>From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images</title><link>https://arxiv.org/abs/2601.17934v1</link><guid>http://arxiv.org/abs/2601.17934v1</guid><pubDate>Sun, 25 Jan 2026 18:13:48 +0000</pubDate><dc:creator>Vi Vu</dc:creator><dc:creator>Thanh-Huy Nguyen</dc:creator><dc:creator>Tien-Thinh Nguyen</dc:creator><dc:creator>Ba-Thinh Lam</dc:creator><dc:creator>Hoang-Thien Nguyen</dc:creator><dc:creator>Tianyang Wang</dc:creator><dc:creator>Xingjian Li</dc:creator><dc:creator>Min Xu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.
Published: 2026-01-25T18:13:48+00:00
Venue: arXiv
Score: 0.763 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Vi Vu; Thanh-Huy Nguyen; Tien-Thinh Nguyen; Ba-Thinh Lam; Hoang-Thien Nguyen; Tianyang Wang; Xingjian Li; Min Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.763 (consider)&lt;/p&gt;
&lt;p&gt;Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM&amp;#x27;s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.&lt;/p&gt;</content:encoded></item><item><title>Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling</title><link>https://arxiv.org/abs/2601.17259v1</link><guid>http://arxiv.org/abs/2601.17259v1</guid><pubDate>Sat, 24 Jan 2026 02:18:25 +0000</pubDate><dc:creator>Angad Singh Ahuja</dc:creator><dc:creator>Aarush Ram Anandh</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.
Published: 2026-01-24T02:18:25+00:00
Venue: arXiv
Score: 0.762 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Angad Singh Ahuja; Aarush Ram Anandh&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.762 (consider)&lt;/p&gt;
&lt;p&gt;Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.&lt;/p&gt;</content:encoded></item><item><title>Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss</title><link>https://arxiv.org/abs/2601.16645v1</link><guid>http://arxiv.org/abs/2601.16645v1</guid><pubDate>Fri, 23 Jan 2026 11:06:51 +0000</pubDate><dc:creator>Minsu Gong</dc:creator><dc:creator>Nuri Ryu</dc:creator><dc:creator>Jungseul Ok</dc:creator><dc:creator>Sunghyun Cho</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model's generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.
Published: 2026-01-23T11:06:51+00:00
Venue: arXiv
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Minsu Gong; Nuri Ryu; Jungseul Ok; Sunghyun Cho&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model&amp;#x27;s generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.&lt;/p&gt;</content:encoded></item><item><title>GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss</title><link>https://arxiv.org/abs/2601.16885v1</link><guid>http://arxiv.org/abs/2601.16885v1</guid><pubDate>Fri, 23 Jan 2026 16:46:59 +0000</pubDate><dc:creator>Yangfan Xu</dc:creator><dc:creator>Lilian Zhang</dc:creator><dc:creator>Xiaofeng He</dc:creator><dc:creator>Pengdong Wu</dc:creator><dc:creator>Wenqi Wu</dc:creator><dc:creator>Jun Mao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.
Published: 2026-01-23T16:46:59+00:00
Venue: arXiv
Score: 0.761 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yangfan Xu; Lilian Zhang; Xiaofeng He; Pengdong Wu; Wenqi Wu; Jun Mao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.761 (consider)&lt;/p&gt;
&lt;p&gt;Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.&lt;/p&gt;</content:encoded></item><item><title>LongCat-Flash-Thinking-2601 Technical Report</title><link>https://arxiv.org/abs/2601.16725v1</link><guid>http://arxiv.org/abs/2601.16725v1</guid><pubDate>Fri, 23 Jan 2026 13:20:09 +0000</pubDate><dc:creator>Meituan LongCat Team</dc:creator><dc:creator>Anchun Gui</dc:creator><dc:creator>Bei Li</dc:creator><dc:creator>Bingyang Tao</dc:creator><dc:creator>Bole Zhou</dc:creator><dc:creator>Borun Chen</dc:creator><dc:creator>Chao Zhang</dc:creator><dc:creator>Chao Zhang</dc:creator><dc:creator>Chen Gao</dc:creator><dc:creator>Chen Zhang</dc:creator><dc:creator>Chengcheng Han</dc:creator><dc:creator>Chenhui Yang</dc:creator><dc:creator>Chuyu Zhang</dc:creator><dc:creator>Cong Chen</dc:creator><dc:creator>Cunguang Wang</dc:creator><dc:creator>Daoru Pan</dc:creator><dc:creator>Defei Bu</dc:creator><dc:creator>Dengchang Zhao</dc:creator><dc:creator>Di Xiu</dc:creator><dc:creator>Dishan Liu</dc:creator><dc:creator>Dongyu Ru</dc:creator><dc:creator>Dunwei Tu</dc:creator><dc:creator>Fan Wu</dc:creator><dc:creator>Fengcheng Yuan</dc:creator><dc:creator>Fengcun Li</dc:creator><dc:creator>Gang Xu</dc:creator><dc:creator>Guanyu Wu</dc:creator><dc:creator>Guoyuan Lin</dc:creator><dc:creator>Haibin Wang</dc:creator><dc:creator>Hansi Yang</dc:creator><dc:creator>Hao Yang</dc:creator><dc:creator>Haonan Yan</dc:creator><dc:creator>Haoxiang Ma</dc:creator><dc:creator>Haoxing Wen</dc:creator><dc:creator>Hongyan Hao</dc:creator><dc:creator>Hongyin Tang</dc:creator><dc:creator>Hongyu Zang</dc:creator><dc:creator>Hongzhi Ni</dc:creator><dc:creator>Hui Su</dc:creator><dc:creator>Jiacheng Zhang</dc:creator><dc:creator>Jiahong Zhou</dc:creator><dc:creator>Jiahuan Li</dc:creator><dc:creator>Jiaming Wang</dc:creator><dc:creator>Jian Yang</dc:creator><dc:creator>Jianfei Zhang</dc:creator><dc:creator>Jianhao Xu</dc:creator><dc:creator>Jianing Wang</dc:creator><dc:creator>Jiapeng Zhu</dc:creator><dc:creator>Jiaqi Sun</dc:creator><dc:creator>Jiarong Shi</dc:creator><dc:creator>Jiarui Zhao</dc:creator><dc:creator>Jingang Wang</dc:creator><dc:creator>Jinluan Yang</dc:creator><dc:creator>Jinrui Ding</dc:creator><dc:creator>Jinwei Xiao</dc:creator><dc:creator>Jiyuan He</dc:creator><dc:creator>Juncan Xu</dc:creator><dc:creator>Kefeng Zhang</dc:creator><dc:creator>Keheng Wang</dc:creator><dc:creator>Li Wei</dc:creator><dc:creator>Lianhui Ma</dc:creator><dc:creator>Lin Qiu</dc:creator><dc:creator>Lingbing Kong</dc:creator><dc:creator>Lingchuan Liu</dc:creator><dc:creator>Linsen Guo</dc:creator><dc:creator>Mengshen Zhu</dc:creator><dc:creator>Mengxia Shen</dc:creator><dc:creator>Mingyang Zhu</dc:creator><dc:creator>Peiguang Li</dc:creator><dc:creator>Peng Pei</dc:creator><dc:creator>Pengcheng Jia</dc:creator><dc:creator>Pengtao Zhang</dc:creator><dc:creator>Peng Zhao</dc:creator><dc:creator>Qi Gu</dc:creator><dc:creator>Qiong Huang</dc:creator><dc:creator>Qiyuan Duan</dc:creator><dc:creator>Quanchi Weng</dc:creator><dc:creator>Rongxiang Weng</dc:creator><dc:creator>Rongzhi Zhang</dc:creator><dc:creator>Rumei Li</dc:creator><dc:creator>Shanglin Lei</dc:creator><dc:creator>Shengnan An</dc:creator><dc:creator>Shijun Dai</dc:creator><dc:creator>Shuaikang Liu</dc:creator><dc:creator>Shuang Zhou</dc:creator><dc:creator>Shuo Wang</dc:creator><dc:creator>Songyuan Zhao</dc:creator><dc:creator>Tao Liang</dc:creator><dc:creator>Tianhao Hu</dc:creator><dc:creator>Tianze Chen</dc:creator><dc:creator>Wei Liu</dc:creator><dc:creator>Wei Shi</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Weifeng Tang</dc:creator><dc:creator>Wenjie Shi</dc:creator><dc:creator>Wenlong Zhu</dc:creator><dc:creator>Wentao Chen</dc:creator><dc:creator>Wentao Shi</dc:creator><dc:creator>Xi Su</dc:creator><dc:creator>Xiangcheng Liu</dc:creator><dc:creator>Xiandi Ma</dc:creator><dc:creator>Xiangyu Xi</dc:creator><dc:creator>Xiangyuan Liu</dc:creator><dc:creator>Xiangzhou Huang</dc:creator><dc:creator>Xiao Liu</dc:creator><dc:creator>Xiaodong Cai</dc:creator><dc:creator>Xiaolong Chen</dc:creator><dc:creator>Xiaowei Shi</dc:creator><dc:creator>Xiaoyu Li</dc:creator><dc:creator>Xin Chen</dc:creator><dc:creator>Xingchen Liu</dc:creator><dc:creator>Xuan Huang</dc:creator><dc:creator>Xuezhi Cao</dc:creator><dc:creator>Xunliang Cai</dc:creator><dc:creator>Yan Chen</dc:creator><dc:creator>Yang Bai</dc:creator><dc:creator>Yang Liu</dc:creator><dc:creator>Yang Yang</dc:creator><dc:creator>Yang Zheng</dc:creator><dc:creator>Yaoming Wang</dc:creator><dc:creator>Yaoming Zhu</dc:creator><dc:creator>Yaqi Huo</dc:creator><dc:creator>Yanyu Chen</dc:creator><dc:creator>Yaorui Shi</dc:creator><dc:creator>Yerui Sun</dc:creator><dc:creator>Yi Zhang</dc:creator><dc:creator>Yihao Chen</dc:creator><dc:creator>Yi-Kai Zhang</dc:creator><dc:creator>Yifan Lu</dc:creator><dc:creator>Yifan Zhao</dc:creator><dc:creator>Yitao Zhai</dc:creator><dc:creator>Yongjing Yin</dc:creator><dc:creator>Yongwei Zhou</dc:creator><dc:creator>Youshao Xiao</dc:creator><dc:creator>Yuchuan Dai</dc:creator><dc:creator>Yuchen Xie</dc:creator><dc:creator>Yuchen Yu</dc:creator><dc:creator>Yufei Zhang</dc:creator><dc:creator>Yuhuai Wei</dc:creator><dc:creator>Yulei Qian</dc:creator><dc:creator>Yunfan Liang</dc:creator><dc:creator>Yunke Zhao</dc:creator><dc:creator>Yuwei Jiang</dc:creator><dc:creator>Yuxin Bian</dc:creator><dc:creator>Yuxin Chen</dc:creator><dc:creator>Yuxin Liu</dc:creator><dc:creator>Yue Xu</dc:creator><dc:creator>Yueqing Sun</dc:creator><dc:creator>Zeyang Yu</dc:creator><dc:creator>Zhao Yang</dc:creator><dc:creator>Zhengsheng Huang</dc:creator><dc:creator>Zhengyu Chen</dc:creator><dc:creator>Zhijian Liu</dc:creator><dc:creator>Zhikang Xia</dc:creator><dc:creator>Zhimin Lin</dc:creator><dc:creator>Zhiyuan Yao</dc:creator><dc:creator>Zhuofan Chen</dc:creator><dc:creator>Zhuowen Han</dc:creator><dc:creator>Zijian Zhang</dc:creator><dc:creator>Ziran Li</dc:creator><dc:creator>Ziwen Wang</dc:creator><dc:creator>Ziyuan Zhuang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model's strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.
Published: 2026-01-23T13:20:09+00:00
Venue: arXiv
Score: 0.759 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Meituan LongCat Team; Anchun Gui; Bei Li; Bingyang Tao; Bole Zhou; Borun Chen; Chao Zhang; Chao Zhang; Chen Gao; Chen Zhang; Chengcheng Han; Chenhui Yang; Chuyu Zhang; Cong Chen; Cunguang Wang; Daoru Pan; Defei Bu; Dengchang Zhao; Di Xiu; Dishan Liu; Dongyu Ru; Dunwei Tu; Fan Wu; Fengcheng Yuan; Fengcun Li; Gang Xu; Guanyu Wu; Guoyuan Lin; Haibin Wang; Hansi Yang; Hao Yang; Haonan Yan; Haoxiang Ma; Haoxing Wen; Hongyan Hao; Hongyin Tang; Hongyu Zang; Hongzhi Ni; Hui Su; Jiacheng Zhang; Jiahong Zhou; Jiahuan Li; Jiaming Wang; Jian Yang; Jianfei Zhang; Jianhao Xu; Jianing Wang; Jiapeng Zhu; Jiaqi Sun; Jiarong Shi; Jiarui Zhao; Jingang Wang; Jinluan Yang; Jinrui Ding; Jinwei Xiao; Jiyuan He; Juncan Xu; Kefeng Zhang; Keheng Wang; Li Wei; Lianhui Ma; Lin Qiu; Lingbing Kong; Lingchuan Liu; Linsen Guo; Mengshen Zhu; Mengxia Shen; Mingyang Zhu; Peiguang Li; Peng Pei; Pengcheng Jia; Pengtao Zhang; Peng Zhao; Qi Gu; Qiong Huang; Qiyuan Duan; Quanchi Weng; Rongxiang Weng; Rongzhi Zhang; Rumei Li; Shanglin Lei; Shengnan An; Shijun Dai; Shuaikang Liu; Shuang Zhou; Shuo Wang; Songyuan Zhao; Tao Liang; Tianhao Hu; Tianze Chen; Wei Liu; Wei Shi; Wei Wang; Weifeng Tang; Wenjie Shi; Wenlong Zhu; Wentao Chen; Wentao Shi; Xi Su; Xiangcheng Liu; Xiandi Ma; Xiangyu Xi; Xiangyuan Liu; Xiangzhou Huang; Xiao Liu; Xiaodong Cai; Xiaolong Chen; Xiaowei Shi; Xiaoyu Li; Xin Chen; Xingchen Liu; Xuan Huang; Xuezhi Cao; Xunliang Cai; Yan Chen; Yang Bai; Yang Liu; Yang Yang; Yang Zheng; Yaoming Wang; Yaoming Zhu; Yaqi Huo; Yanyu Chen; Yaorui Shi; Yerui Sun; Yi Zhang; Yihao Chen; Yi-Kai Zhang; Yifan Lu; Yifan Zhao; Yitao Zhai; Yongjing Yin; Yongwei Zhou; Youshao Xiao; Yuchuan Dai; Yuchen Xie; Yuchen Yu; Yufei Zhang; Yuhuai Wei; Yulei Qian; Yunfan Liang; Yunke Zhao; Yuwei Jiang; Yuxin Bian; Yuxin Chen; Yuxin Liu; Yue Xu; Yueqing Sun; Zeyang Yu; Zhao Yang; Zhengsheng Huang; Zhengyu Chen; Zhijian Liu; Zhikang Xia; Zhimin Lin; Zhiyuan Yao; Zhuofan Chen; Zhuowen Han; Zijian Zhang; Ziran Li; Ziwen Wang; Ziyuan Zhuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (consider)&lt;/p&gt;
&lt;p&gt;We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model&amp;#x27;s strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.&lt;/p&gt;</content:encoded></item><item><title>Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing</title><link>https://arxiv.org/abs/2601.17673v1</link><guid>http://arxiv.org/abs/2601.17673v1</guid><pubDate>Sun, 25 Jan 2026 03:22:26 +0000</pubDate><dc:creator>Weiyu Zhang</dc:creator><dc:creator>Yuan Hu</dc:creator><dc:creator>Yong Li</dc:creator><dc:creator>Yu Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.
Published: 2026-01-25T03:22:26+00:00
Venue: arXiv
Score: 0.759 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Weiyu Zhang; Yuan Hu; Yong Li; Yu Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.759 (consider)&lt;/p&gt;
&lt;p&gt;Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.&lt;/p&gt;</content:encoded></item><item><title>Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction</title><link>https://arxiv.org/abs/2601.17668v1</link><guid>http://arxiv.org/abs/2601.17668v1</guid><pubDate>Sun, 25 Jan 2026 03:07:54 +0000</pubDate><dc:creator>Jang-Hyun Kim</dc:creator><dc:creator>Dongyoon Han</dc:creator><dc:creator>Sangdoo Yun</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.
Published: 2026-01-25T03:07:54+00:00
Venue: arXiv
Score: 0.757 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jang-Hyun Kim; Dongyoon Han; Sangdoo Yun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.757 (consider)&lt;/p&gt;
&lt;p&gt;Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.&lt;/p&gt;</content:encoded></item><item><title>A Cosine Network for Image Super-Resolution</title><link>https://arxiv.org/abs/2601.16413v1</link><guid>http://arxiv.org/abs/2601.16413v1</guid><pubDate>Fri, 23 Jan 2026 02:58:57 +0000</pubDate><dc:creator>Chunwei Tian</dc:creator><dc:creator>Chengyuan Zhang</dc:creator><dc:creator>Bob Zhang</dc:creator><dc:creator>Zhiwu Li</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><dc:creator>David Zhang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><prism:doi>10.1109/TIP.2025.3645630</prism:doi><description>Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.
Published: 2026-01-23T02:58:57+00:00
Venue: arXiv
Score: 0.755 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunwei Tian; Chengyuan Zhang; Bob Zhang; Zhiwu Li; C. L. Philip Chen; David Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/TIP.2025.3645630"&gt;10.1109/TIP.2025.3645630&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.755 (consider)&lt;/p&gt;
&lt;p&gt;Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.&lt;/p&gt;</content:encoded></item><item><title>Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries</title><link>https://arxiv.org/abs/2601.17535v1</link><guid>http://arxiv.org/abs/2601.17535v1</guid><pubDate>Sat, 24 Jan 2026 17:30:23 +0000</pubDate><dc:creator>Kevin Robbins</dc:creator><dc:creator>Xiaotong Liu</dc:creator><dc:creator>Yu Wu</dc:creator><dc:creator>Le Sun</dc:creator><dc:creator>Grady McPeak</dc:creator><dc:creator>Abby Stylianou</dc:creator><dc:creator>Robert Pless</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.
Published: 2026-01-24T17:30:23+00:00
Venue: arXiv
Score: 0.755 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Kevin Robbins; Xiaotong Liu; Yu Wu; Le Sun; Grady McPeak; Abby Stylianou; Robert Pless&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.755 (consider)&lt;/p&gt;
&lt;p&gt;Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.&lt;/p&gt;</content:encoded></item><item><title>RemEdit: Efficient Diffusion Editing with Riemannian Geometry</title><link>https://arxiv.org/abs/2601.17927v1</link><guid>http://arxiv.org/abs/2601.17927v1</guid><pubDate>Sun, 25 Jan 2026 17:58:57 +0000</pubDate><dc:creator>Eashan Adhikarla</dc:creator><dc:creator>Brian D. Davison</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.
Published: 2026-01-25T17:58:57+00:00
Venue: arXiv
Score: 0.755 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Eashan Adhikarla; Brian D. Davison&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.755 (consider)&lt;/p&gt;
&lt;p&gt;Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold&amp;#x27;s structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.&lt;/p&gt;</content:encoded></item></channel></rss>