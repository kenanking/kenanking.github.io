<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Sun, 18 Jan 2026 02:54:35 +0000</lastBuildDate><item><title>Noise-Tolerant Novel-View SAR Synthesis via Denoising Diffusion</title><link>https://doi.org/10.1109/tgrs.2026.3654542</link><guid>10.1109/tgrs.2026.3654542</guid><pubDate>Fri, 16 Jan 2026 20:49:57 +0000</pubDate><dc:creator>Amir Rahimi</dc:creator><dc:creator>Stella Yu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654542</prism:doi><description>Synthetic Aperture Radar (SAR) enables robust imaging under all weather and lighting conditions, but the scarcity of labeled SAR data limits the use of modern vision models. Novel-view synthesis offers a promising way to augment training data, yet existing methods struggle with speckle noise and radiometric variability inherent to SAR imagery. We introduce a SAR-specific self-supervised representation learning framework based on co-domain augmentations that operate directly on pixel magnitudes. By combining multiplicative Rayleigh speckle and random monotonic intensity remapping, our method learns features that are invariant to speckle realizations while preserving structural and geometric cues. These learned representations are then used to supervise a latent-diffusion novel-view generator adapted from zero-1-to-3 through a projected feature-matching loss, replacing fragile pixel-space comparisons with noise-robust feature-space supervision. Experiments on MSTAR and MSTAR-OOD demonstrate substantial improvements in identity preservation, pose consistency, and perceptual quality for both seen and unseen targets. Although evaluated on object-centric SAR for automatic target recognition, the proposed framework is content-agnostic and naturally extends to scene-level SAR novel-view synthesis.
Published: 2026-01-16T20:49:57+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.844 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Amir Rahimi; Stella Yu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654542"&gt;10.1109/tgrs.2026.3654542&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.844 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic Aperture Radar (SAR) enables robust imaging under all weather and lighting conditions, but the scarcity of labeled SAR data limits the use of modern vision models. Novel-view synthesis offers a promising way to augment training data, yet existing methods struggle with speckle noise and radiometric variability inherent to SAR imagery. We introduce a SAR-specific self-supervised representation learning framework based on co-domain augmentations that operate directly on pixel magnitudes. By combining multiplicative Rayleigh speckle and random monotonic intensity remapping, our method learns features that are invariant to speckle realizations while preserving structural and geometric cues. These learned representations are then used to supervise a latent-diffusion novel-view generator adapted from zero-1-to-3 through a projected feature-matching loss, replacing fragile pixel-space comparisons with noise-robust feature-space supervision. Experiments on MSTAR and MSTAR-OOD demonstrate substantial improvements in identity preservation, pose consistency, and perceptual quality for both seen and unseen targets. Although evaluated on object-centric SAR for automatic target recognition, the proposed framework is content-agnostic and naturally extends to scene-level SAR novel-view synthesis.&lt;/p&gt;</content:encoded></item><item><title>Like Human Rethinking: Contour Transformer AutoRegression for Referring Remote Sensing Interpretation</title><link>https://doi.org/10.1109/tpami.2026.3654392</link><guid>10.1109/tpami.2026.3654392</guid><pubDate>Fri, 16 Jan 2026 20:49:53 +0000</pubDate><dc:creator>Jinming Chai</dc:creator><dc:creator>Licheng Jiao</dc:creator><dc:creator>Xiaoqiang Lu</dc:creator><dc:creator>Lingling Li</dc:creator><dc:creator>Fang Liu</dc:creator><dc:creator>Long Sun</dc:creator><dc:creator>Xu Liu</dc:creator><dc:creator>Wenping Ma</dc:creator><dc:creator>Weibin Li</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3654392</prism:doi><description>Referring remote sensing interpretation holds significant application value in various scenarios such as ecological protection, resource exploration, and emergency management. However, referring remote sensing expression comprehension and segmentation (RRSECS) faces critical challenges, including micro-target localization drift problem caused by insufficient extraction of boundary features in existing paradigms. Moreover, when transferred to remote sensing domains, polygon-based methods encounter issues such as contour-boundary misalignment and multi-task co-optimization conflicts problems. In this paper, we propose SeeFormer, a novel contour autoregressive paradigm specifically designed for RRSECS, which accurately locates and segments micro, irregular targets in remote sensing imagery. We first introduce a brain-inspired feature refocus learning (BIFRL) module that progressively attends to effective object features via a coarse-to-fine scheme, significantly boosting small-object localization and segmentation. Next, we present a language-contour enhancer (LCE) that injects shape-aware contour priors, and a corner-based contour sampler (CBCS) to improve mask-polygon reconstruction fidelity. Finally, we develop an autoregressive dual-decoder paradigm (ARDDP) that preserves sequence consistency while alleviating multi-task optimization conflicts. Extensive experiments on RefDIOR, RRSIS-D, and OPT-RSVG datasets under varying scenarios, scales, and task paradigms demonstrate transformative performance gains: compared to the baseline PolyFormer, our proposed SeeFormer improves oIoU and mIoU by 27.58% and 39.37% for referring image segmentation and by 18.94% and 28.90% for visual grounding on the RefDIOR dataset. The code will be publicly accessible at https://github.com/IPIU-XDU/RSFM.
Published: 2026-01-16T20:49:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.837 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jinming Chai; Licheng Jiao; Xiaoqiang Lu; Lingling Li; Fang Liu; Long Sun; Xu Liu; Wenping Ma; Weibin Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3654392"&gt;10.1109/tpami.2026.3654392&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.837 (must_read)&lt;/p&gt;
&lt;p&gt;Referring remote sensing interpretation holds significant application value in various scenarios such as ecological protection, resource exploration, and emergency management. However, referring remote sensing expression comprehension and segmentation (RRSECS) faces critical challenges, including micro-target localization drift problem caused by insufficient extraction of boundary features in existing paradigms. Moreover, when transferred to remote sensing domains, polygon-based methods encounter issues such as contour-boundary misalignment and multi-task co-optimization conflicts problems. In this paper, we propose SeeFormer, a novel contour autoregressive paradigm specifically designed for RRSECS, which accurately locates and segments micro, irregular targets in remote sensing imagery. We first introduce a brain-inspired feature refocus learning (BIFRL) module that progressively attends to effective object features via a coarse-to-fine scheme, significantly boosting small-object localization and segmentation. Next, we present a language-contour enhancer (LCE) that injects shape-aware contour priors, and a corner-based contour sampler (CBCS) to improve mask-polygon reconstruction fidelity. Finally, we develop an autoregressive dual-decoder paradigm (ARDDP) that preserves sequence consistency while alleviating multi-task optimization conflicts. Extensive experiments on RefDIOR, RRSIS-D, and OPT-RSVG datasets under varying scenarios, scales, and task paradigms demonstrate transformative performance gains: compared to the baseline PolyFormer, our proposed SeeFormer improves oIoU and mIoU by 27.58% and 39.37% for referring image segmentation and by 18.94% and 28.90% for visual grounding on the RefDIOR dataset. The code will be publicly accessible at https://github.com/IPIU-XDU/RSFM.&lt;/p&gt;</content:encoded></item><item><title>LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection</title><link>https://doi.org/10.1109/tcyb.2025.3650459</link><guid>10.1109/tcyb.2025.3650459</guid><pubDate>Fri, 16 Jan 2026 20:50:46 +0000</pubDate><dc:creator>Lei Hao</dc:creator><dc:creator>Lina Xu</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Yanni Dong</dc:creator><prism:publicationName>IEEE Transactions on Cybernetics</prism:publicationName><prism:doi>10.1109/tcyb.2025.3650459</prism:doi><description>Effective deep feature extraction via feature-level fusion is crucial for multimodal object detection. However, previous studies often involve complex training processes that integrate modality-specific features by stacking multiple feature-level fusion units, leading to significant computational overhead. To address this issue, we propose a lightweight attention-guided self-modulation feature fusion network (LASFNet). The LASFNet adopts a single feature-level fusion unit to enable high-performance detection, thereby simplifying the training process. The attention-guided self-modulation feature fusion (ASFF) module in the model adaptively adjusts the responses of fused features at both global and local levels, promoting comprehensive and enriched feature generation. Additionally, a lightweight feature attention transformation module (FATM) is designed at the neck of LASFNet to enhance the focus on fused features and minimize information loss. Extensive experiments on three representative datasets demonstrate that our approach achieves a favorable efficiency–accuracy tradeoff. Compared to state-of-the-art methods, LASFNet reduced the number of parameters and computational cost by as much as 90% and 85%, respectively, while improving detection accuracy mean average precision (mAP) by 1%–3%. The code will be open-sourced at https://github.com/leileilei2000/LASFNet
Published: 2026-01-16T20:50:46+00:00
Venue: IEEE Transactions on Cybernetics
Score: 0.834 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lei Hao; Lina Xu; Chang Liu; Yanni Dong&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Cybernetics&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tcyb.2025.3650459"&gt;10.1109/tcyb.2025.3650459&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.834 (must_read)&lt;/p&gt;
&lt;p&gt;Effective deep feature extraction via feature-level fusion is crucial for multimodal object detection. However, previous studies often involve complex training processes that integrate modality-specific features by stacking multiple feature-level fusion units, leading to significant computational overhead. To address this issue, we propose a lightweight attention-guided self-modulation feature fusion network (LASFNet). The LASFNet adopts a single feature-level fusion unit to enable high-performance detection, thereby simplifying the training process. The attention-guided self-modulation feature fusion (ASFF) module in the model adaptively adjusts the responses of fused features at both global and local levels, promoting comprehensive and enriched feature generation. Additionally, a lightweight feature attention transformation module (FATM) is designed at the neck of LASFNet to enhance the focus on fused features and minimize information loss. Extensive experiments on three representative datasets demonstrate that our approach achieves a favorable efficiency–accuracy tradeoff. Compared to state-of-the-art methods, LASFNet reduced the number of parameters and computational cost by as much as 90% and 85%, respectively, while improving detection accuracy mean average precision (mAP) by 1%–3%. The code will be open-sourced at https://github.com/leileilei2000/LASFNet&lt;/p&gt;</content:encoded></item><item><title>CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection</title><link>https://doi.org/10.1109/tits.2026.3651273</link><guid>10.1109/tits.2026.3651273</guid><pubDate>Fri, 16 Jan 2026 20:51:29 +0000</pubDate><dc:creator>Huiming Yang</dc:creator><dc:creator>Wenzhuo Liu</dc:creator><dc:creator>Yicheng Qiao</dc:creator><dc:creator>Lei Yang</dc:creator><dc:creator>Xianzhu Zeng</dc:creator><dc:creator>Li Wang</dc:creator><dc:creator>Zhiwei Li</dc:creator><dc:creator>Zijian Zeng</dc:creator><dc:creator>Zhiying Jiang</dc:creator><dc:creator>Huaping Liu</dc:creator><dc:creator>Kunfeng Wang</dc:creator><prism:publicationName>IEEE Transactions on Intelligent Transportation Systems</prism:publicationName><prism:doi>10.1109/tits.2026.3651273</prism:doi><description>The sparse cross-modality detector offers more advantages than its counterpart, the Bird’s-Eye-View (BEV) detector, particularly in terms of adaptability for downstream tasks and computational cost savings. However, existing sparse detectors overlook the quality of token representation, leaving it with a sub-optimal foreground quality and limited performance. In this paper, we identify that the geometric structure preserved and the class distribution are the key to improving the performance of the sparse detector, and propose a Sparse Selector (SS). The core module of SS is Ray-Aware Supervision (RAS), which preserves rich geometric information during the training stage, and Class-Balanced Supervision, which adaptively reweights the salience of class semantics, ensuring that tokens associated with small objects are retained during token sampling. Thereby, outperforming other sparse multi-modal detectors in the representation of tokens. Additionally, we design Ray Positional Encoding (Ray PE) to address the distribution differences between the LiDAR modality and the image. Finally, we integrate the aforementioned module into an end-to-end sparse multi-modality detector, dubbed CrossRay3D. Experiments show that, on the challenging nuScenes benchmark, CrossRay3D achieves state-of-the-art performance with 72.4% mAP and 74.7% NDS, while running 1.84 × 1.84 imes faster than other leading methods. Moreover, CrossRay3D demonstrates strong robustness even in scenarios where LiDAR or camera data are partially or entirely missing. The code is available on https://github.com/xuehaipiaoxiang/CrossRay3D
Published: 2026-01-16T20:51:29+00:00
Venue: IEEE Transactions on Intelligent Transportation Systems
Score: 0.820 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huiming Yang; Wenzhuo Liu; Yicheng Qiao; Lei Yang; Xianzhu Zeng; Li Wang; Zhiwei Li; Zijian Zeng; Zhiying Jiang; Huaping Liu; Kunfeng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Intelligent Transportation Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tits.2026.3651273"&gt;10.1109/tits.2026.3651273&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.820 (must_read)&lt;/p&gt;
&lt;p&gt;The sparse cross-modality detector offers more advantages than its counterpart, the Bird’s-Eye-View (BEV) detector, particularly in terms of adaptability for downstream tasks and computational cost savings. However, existing sparse detectors overlook the quality of token representation, leaving it with a sub-optimal foreground quality and limited performance. In this paper, we identify that the geometric structure preserved and the class distribution are the key to improving the performance of the sparse detector, and propose a Sparse Selector (SS). The core module of SS is Ray-Aware Supervision (RAS), which preserves rich geometric information during the training stage, and Class-Balanced Supervision, which adaptively reweights the salience of class semantics, ensuring that tokens associated with small objects are retained during token sampling. Thereby, outperforming other sparse multi-modal detectors in the representation of tokens. Additionally, we design Ray Positional Encoding (Ray PE) to address the distribution differences between the LiDAR modality and the image. Finally, we integrate the aforementioned module into an end-to-end sparse multi-modality detector, dubbed CrossRay3D. Experiments show that, on the challenging nuScenes benchmark, CrossRay3D achieves state-of-the-art performance with 72.4% mAP and 74.7% NDS, while running 1.84 × 1.84 imes faster than other leading methods. Moreover, CrossRay3D demonstrates strong robustness even in scenarios where LiDAR or camera data are partially or entirely missing. The code is available on https://github.com/xuehaipiaoxiang/CrossRay3D&lt;/p&gt;</content:encoded></item><item><title>Spatio-Semantic Enhanced Cascade Swin Detection Network for Infrared Maritime Targets</title><link>https://doi.org/10.1109/tgrs.2026.3654670</link><guid>10.1109/tgrs.2026.3654670</guid><pubDate>Fri, 16 Jan 2026 20:49:57 +0000</pubDate><dc:creator>Dongdong Ma</dc:creator><dc:creator>Shaohua Chen</dc:creator><dc:creator>Lili Dong</dc:creator><dc:creator>Tingkuo Chen</dc:creator><dc:creator>Wenhai Xu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654670</prism:doi><description>To address the issues of high missed detection rates in maritime infrared small and weak target detection łcaused by imbalanced target scales, strong ocean wave interference, and low signal-to-clutter ratiołthis paper proposes a convolutional neural network based on cascade fusion and local feature enhancement. Built on the CSPDarknet53 backbone network, the proposed network incorporates a structural tensor spatiotemporal information enhancement module and a dynamic context-aware attention semantic enhancement module to improve the representational capabilities of shallow spatial features and deep semantic features. Meanwhile, a hierarchical feature bidirectional symmetric fusion network and a scale-aware upsampling operator are designed to achieve effective interaction and reconstruction of multi-scale features. Additionally, Swin transformer is adopted as the detection head to enhance the modeling of long-range semantic dependencies.Experimental results demonstrate that on the self-constructed infrared maritime target dataset, the proposed network achieves a Precision of 0.865, a Recall of 0.875, and an F1-score of 0.870. These performance metrics outperform those of mainstream methods such as RLCM, PSTNN, Faster RCNN, YOLOv5s, YOLOv8s, and AGPCNet. Particularly, the proposed network exhibits high robustness and real-time performance in complex background environments and small/weak target detection tasks.
Published: 2026-01-16T20:49:57+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.819 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Dongdong Ma; Shaohua Chen; Lili Dong; Tingkuo Chen; Wenhai Xu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654670"&gt;10.1109/tgrs.2026.3654670&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.819 (must_read)&lt;/p&gt;
&lt;p&gt;To address the issues of high missed detection rates in maritime infrared small and weak target detection łcaused by imbalanced target scales, strong ocean wave interference, and low signal-to-clutter ratiołthis paper proposes a convolutional neural network based on cascade fusion and local feature enhancement. Built on the CSPDarknet53 backbone network, the proposed network incorporates a structural tensor spatiotemporal information enhancement module and a dynamic context-aware attention semantic enhancement module to improve the representational capabilities of shallow spatial features and deep semantic features. Meanwhile, a hierarchical feature bidirectional symmetric fusion network and a scale-aware upsampling operator are designed to achieve effective interaction and reconstruction of multi-scale features. Additionally, Swin transformer is adopted as the detection head to enhance the modeling of long-range semantic dependencies.Experimental results demonstrate that on the self-constructed infrared maritime target dataset, the proposed network achieves a Precision of 0.865, a Recall of 0.875, and an F1-score of 0.870. These performance metrics outperform those of mainstream methods such as RLCM, PSTNN, Faster RCNN, YOLOv5s, YOLOv8s, and AGPCNet. Particularly, the proposed network exhibits high robustness and real-time performance in complex background environments and small/weak target detection tasks.&lt;/p&gt;</content:encoded></item><item><title>Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs</title><link>https://doi.org/10.1109/tip.2025.3649356</link><guid>10.1109/tip.2025.3649356</guid><pubDate>Fri, 16 Jan 2026 20:52:20 +0000</pubDate><dc:creator>Yunxin Li</dc:creator><dc:creator>Zhenyu Liu</dc:creator><dc:creator>Baotian Hu</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Yuxin Ding</dc:creator><dc:creator>Xiaochun Cao</dc:creator><dc:creator>Min Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3649356</prism:doi><description>Recent advancements in multimodal large language models (MLLMs) have achieved significant multimodal generation capabilities, akin to GPT-4. These models predominantly map visual information into language representation space, leveraging the vast knowledge and powerful text generation abilities of LLMs to produce multimodal instruction-following responses. We could term this method as LLMs for Vision because of its employing LLMs for visual understanding and reasoning, yet observe that these MLLMs neglect the potential of harnessing visual knowledge to enhance the overall capabilities of LLMs, which could be regarded as Vision Enhancing LLMs. In this paper, we propose an approach called MKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage and Sharing in LLMs. Specifically, we introduce Modular Visual Memory (MVM), a component integrated into the internal blocks of LLMs, designed to store open-world visual information efficiently. Additionally, we present a soft Mixture of Multimodal Experts (MoMEs) architecture in LLMs to invoke multimodal knowledge collaboration during text generation. Our comprehensive experiments demonstrate that MKS2 substantially augments the reasoning capabilities of LLMs in contexts necessitating physical or commonsense knowledge. It also delivers competitive results on image-text understanding multimodal benchmarks. The codes will be available at: https://github.com/HITsz-TMG/ MKS2-Multimodal-Knowledge-Storage-and-Sharing.
Published: 2026-01-16T20:52:20+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.815 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yunxin Li; Zhenyu Liu; Baotian Hu; Wei Wang; Yuxin Ding; Xiaochun Cao; Min Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3649356"&gt;10.1109/tip.2025.3649356&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.815 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in multimodal large language models (MLLMs) have achieved significant multimodal generation capabilities, akin to GPT-4. These models predominantly map visual information into language representation space, leveraging the vast knowledge and powerful text generation abilities of LLMs to produce multimodal instruction-following responses. We could term this method as LLMs for Vision because of its employing LLMs for visual understanding and reasoning, yet observe that these MLLMs neglect the potential of harnessing visual knowledge to enhance the overall capabilities of LLMs, which could be regarded as Vision Enhancing LLMs. In this paper, we propose an approach called MKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage and Sharing in LLMs. Specifically, we introduce Modular Visual Memory (MVM), a component integrated into the internal blocks of LLMs, designed to store open-world visual information efficiently. Additionally, we present a soft Mixture of Multimodal Experts (MoMEs) architecture in LLMs to invoke multimodal knowledge collaboration during text generation. Our comprehensive experiments demonstrate that MKS2 substantially augments the reasoning capabilities of LLMs in contexts necessitating physical or commonsense knowledge. It also delivers competitive results on image-text understanding multimodal benchmarks. The codes will be available at: https://github.com/HITsz-TMG/ MKS2-Multimodal-Knowledge-Storage-and-Sharing.&lt;/p&gt;</content:encoded></item><item><title>Dual Perception Detector for Ship Detection in SAR Images</title><link>https://doi.org/10.1109/jstars.2026.3654602</link><guid>10.1109/jstars.2026.3654602</guid><pubDate>Fri, 16 Jan 2026 20:50:16 +0000</pubDate><dc:creator>Ming Tong</dc:creator><dc:creator>Shenghua Fan</dc:creator><dc:creator>Jiu Jiang</dc:creator><dc:creator>Hezhi Sun</dc:creator><dc:creator>Jisan Yang</dc:creator><dc:creator>Chu He</dc:creator><prism:publicationName>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</prism:publicationName><prism:doi>10.1109/jstars.2026.3654602</prism:doi><description>Recently, detectors based on deep learning have boosted the State-of-the-Art (SOTA) of application on ship detection in synthetic aperture radar (SAR) images. However, constructing discriminative feature from scattering of background and distinguishing contour of ship precisely still present challenging subject to the inherent scattering mechanism of SAR. In this article, a dual-branch detection framework with perception of scattering characteristic and geometric contour is introduced to deal with the problem. Firstly, a scattering characteristic perception branch is proposed to fit the scattering distribution of SAR ship through conditional diffusion model, which introduces learnable scattering feature. Secondly, a convex contour perception branch is designed as two-stage coarse-to-fine pipeline to delimit the irregular boundary of ship by learning scattering key points. Finally, a cross-token integration module following Bayesian framework is introduced to couple features of scattering and texture adaptively to learn construction of discriminative feature. Furthermore, comprehensive experiments on three authoritative SAR datasets for oriented ship detection demonstrate the effectiveness of proposed method.
Published: 2026-01-16T20:50:16+00:00
Venue: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ming Tong; Shenghua Fan; Jiu Jiang; Hezhi Sun; Jisan Yang; Chu He&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/jstars.2026.3654602"&gt;10.1109/jstars.2026.3654602&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Recently, detectors based on deep learning have boosted the State-of-the-Art (SOTA) of application on ship detection in synthetic aperture radar (SAR) images. However, constructing discriminative feature from scattering of background and distinguishing contour of ship precisely still present challenging subject to the inherent scattering mechanism of SAR. In this article, a dual-branch detection framework with perception of scattering characteristic and geometric contour is introduced to deal with the problem. Firstly, a scattering characteristic perception branch is proposed to fit the scattering distribution of SAR ship through conditional diffusion model, which introduces learnable scattering feature. Secondly, a convex contour perception branch is designed as two-stage coarse-to-fine pipeline to delimit the irregular boundary of ship by learning scattering key points. Finally, a cross-token integration module following Bayesian framework is introduced to couple features of scattering and texture adaptively to learn construction of discriminative feature. Furthermore, comprehensive experiments on three authoritative SAR datasets for oriented ship detection demonstrate the effectiveness of proposed method.&lt;/p&gt;</content:encoded></item><item><title>Deep-Learning and SIRV-Based Dual-Domain Speckle Suppression for PolSAR Imagery</title><link>https://doi.org/10.1109/tgrs.2026.3654601</link><guid>10.1109/tgrs.2026.3654601</guid><pubDate>Fri, 16 Jan 2026 20:49:57 +0000</pubDate><dc:creator>Lingli Zhao</dc:creator><dc:creator>Nan Jiang</dc:creator><dc:creator>Yexian Ren</dc:creator><dc:creator>Weidong Sun</dc:creator><dc:creator>Zhimin Wang</dc:creator><dc:creator>Lei Shi</dc:creator><dc:creator>Jie Yang</dc:creator><dc:creator>Pingxiang Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2026.3654601</prism:doi><description>Due to the coherent imaging mechanism of synthetic aperture radar (SAR), speckle noise hinders the interpretation of SAR images. Deep learning networks developed for SAR despeckling exhibit promising noise reduction capabilities, but it is difficult to apply the deep-learning-based model on the polarimetric SAR (PolSAR) data directly. Polarimetric information is often distorted by networks due to the complex-valued channels or the normalization applied to PolSAR data. In this paper, we propose a deep learning despeckling network for PolSAR data based on the spherically invariant random vector (SIRV) model which decomposes the polarimetric coherency matrix into a normalized polarimetric coherency matrix and a texture component. Two network models are developed for polarimetric domain and texture domain separately. A block combining Swin-Transformer and convolution is used to extract global and local features of the texture, and a Complex Block was used in polarimetric domain for feature extraction of complex inputs. A new loss function of ratio balanced mean square error (RBMSE) is proposed for the texture domain filtering to help the network handle unnormalized data. PolSAR data in different frequencies and resolutions are used to illustrate the robustness of the proposed method. The results show that the proposed deep-learning and SIRV-based dual-domain filtering algorithm achieves good performance on images acquired by different sensors. It provides a new framework for deep learning based filtering of PolSAR image.
Published: 2026-01-16T20:49:57+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.809 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Lingli Zhao; Nan Jiang; Yexian Ren; Weidong Sun; Zhimin Wang; Lei Shi; Jie Yang; Pingxiang Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2026.3654601"&gt;10.1109/tgrs.2026.3654601&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.809 (must_read)&lt;/p&gt;
&lt;p&gt;Due to the coherent imaging mechanism of synthetic aperture radar (SAR), speckle noise hinders the interpretation of SAR images. Deep learning networks developed for SAR despeckling exhibit promising noise reduction capabilities, but it is difficult to apply the deep-learning-based model on the polarimetric SAR (PolSAR) data directly. Polarimetric information is often distorted by networks due to the complex-valued channels or the normalization applied to PolSAR data. In this paper, we propose a deep learning despeckling network for PolSAR data based on the spherically invariant random vector (SIRV) model which decomposes the polarimetric coherency matrix into a normalized polarimetric coherency matrix and a texture component. Two network models are developed for polarimetric domain and texture domain separately. A block combining Swin-Transformer and convolution is used to extract global and local features of the texture, and a Complex Block was used in polarimetric domain for feature extraction of complex inputs. A new loss function of ratio balanced mean square error (RBMSE) is proposed for the texture domain filtering to help the network handle unnormalized data. PolSAR data in different frequencies and resolutions are used to illustrate the robustness of the proposed method. The results show that the proposed deep-learning and SIRV-based dual-domain filtering algorithm achieves good performance on images acquired by different sensors. It provides a new framework for deep learning based filtering of PolSAR image.&lt;/p&gt;</content:encoded></item><item><title>RTPSeg: A multi-modality dataset for LiDAR point cloud semantic segmentation assisted with RGB-thermal images in autonomous driving</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.008</link><guid>10.1016/j.isprsjprs.2026.01.008</guid><pubDate>Fri, 16 Jan 2026 14:12:44 +0000</pubDate><dc:creator>Yifan Sun</dc:creator><dc:creator>Chenguang Dai</dc:creator><dc:creator>Wenke Li</dc:creator><dc:creator>Xinpu Liu</dc:creator><dc:creator>Yongqi Sun</dc:creator><dc:creator>Ye Zhang</dc:creator><dc:creator>Weijun Guan</dc:creator><dc:creator>Yongsheng Zhang</dc:creator><dc:creator>Yulan Guo</dc:creator><dc:creator>Hanyun Wang</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.008</prism:doi><description>LiDAR point cloud semantic segmentation is crucial for scene understanding in autonomous driving, yet the sparse and textureless characteristics of point clouds cause huge challenges for this task. To address this, numerous studies have explored to leverage the dense color and fine-grained texture from RGB images for multi-modality 3D semantic segmentation. Nevertheless, these methods still encounter certain limitations when facing complex scenarios, as RGB images degrade under poor lighting conditions. In contrast, thermal infrared (TIR) images can provide thermal radiation information of road objects and are robust to illumination change, offering complementary advantages to RGB images. Therefore, in this work we introduce RTPSeg, the first and only multi-modality dataset to simultaneously provide RGB and TIR images for point cloud semantic segmentation. RTPSeg includes over 3000 synchronized frames collected by RGB camera, infrared camera, and LiDAR, providing over 248M pointwise annotations for 18 semantic categories in autonomous driving, involving urban and village scenes during both daytime and nighttime. Based on RTPSeg, we also propose RTPSegNet, a baseline model for point cloud semantic segmentation jointly assisted with RGB and TIR images. Extensive experiments demonstrate that the RTPSeg dataset presents considerable challenges and opportunities to existing point cloud semantic segmentation approaches, and our RTPSegNet exhibits promising effectiveness in jointly leveraging the complementary information between point clouds, RGB images, and TIR images. More importantly, the experimental results also confirm that 3D semantic segmentation can be effectively enhanced by introducing additional TIR image modality, revealing the promising potential of this innovative research and application. We anticipate that the RTPSeg will catalyze in-depth research in this field. Both RTPSeg and RTPSegNet will be released at https://github.com/sssssyf/RTPSeg
Published: 2026-01-16T14:12:44+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yifan Sun; Chenguang Dai; Wenke Li; Xinpu Liu; Yongqi Sun; Ye Zhang; Weijun Guan; Yongsheng Zhang; Yulan Guo; Hanyun Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.008"&gt;10.1016/j.isprsjprs.2026.01.008&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;LiDAR point cloud semantic segmentation is crucial for scene understanding in autonomous driving, yet the sparse and textureless characteristics of point clouds cause huge challenges for this task. To address this, numerous studies have explored to leverage the dense color and fine-grained texture from RGB images for multi-modality 3D semantic segmentation. Nevertheless, these methods still encounter certain limitations when facing complex scenarios, as RGB images degrade under poor lighting conditions. In contrast, thermal infrared (TIR) images can provide thermal radiation information of road objects and are robust to illumination change, offering complementary advantages to RGB images. Therefore, in this work we introduce RTPSeg, the first and only multi-modality dataset to simultaneously provide RGB and TIR images for point cloud semantic segmentation. RTPSeg includes over 3000 synchronized frames collected by RGB camera, infrared camera, and LiDAR, providing over 248M pointwise annotations for 18 semantic categories in autonomous driving, involving urban and village scenes during both daytime and nighttime. Based on RTPSeg, we also propose RTPSegNet, a baseline model for point cloud semantic segmentation jointly assisted with RGB and TIR images. Extensive experiments demonstrate that the RTPSeg dataset presents considerable challenges and opportunities to existing point cloud semantic segmentation approaches, and our RTPSegNet exhibits promising effectiveness in jointly leveraging the complementary information between point clouds, RGB images, and TIR images. More importantly, the experimental results also confirm that 3D semantic segmentation can be effectively enhanced by introducing additional TIR image modality, revealing the promising potential of this innovative research and application. We anticipate that the RTPSeg will catalyze in-depth research in this field. Both RTPSeg and RTPSegNet will be released at https://github.com/sssssyf/RTPSeg&lt;/p&gt;</content:encoded></item><item><title>SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition</title><link>https://arxiv.org/abs/2601.10324v1</link><guid>http://arxiv.org/abs/2601.10324v1</guid><pubDate>Thu, 15 Jan 2026 12:09:49 +0000</pubDate><dc:creator>Yiming Zhang</dc:creator><dc:creator>Weibo Qin</dc:creator><dc:creator>Yuntian Liu</dc:creator><dc:creator>Feng Wang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.
Published: 2026-01-15T12:09:49+00:00
Venue: arXiv
Score: 0.807 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Zhang; Weibo Qin; Yuntian Liu; Feng Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.807 (must_read)&lt;/p&gt;
&lt;p&gt;Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.&lt;/p&gt;</content:encoded></item><item><title>TEDFuse: Task-Driven Equivariant Consistency Decomposition Network for Multi-Modal Image Fusion</title><link>https://doi.org/10.1109/tmm.2026.3654417</link><guid>10.1109/tmm.2026.3654417</guid><pubDate>Fri, 16 Jan 2026 20:50:41 +0000</pubDate><dc:creator>Yiming Sun</dc:creator><dc:creator>Xinyu Cui</dc:creator><dc:creator>Zhen Wang</dc:creator><dc:creator>Hao Cheng</dc:creator><dc:creator>Yongfeng Dong</dc:creator><dc:creator>Pengfei Zhu</dc:creator><dc:creator>Kai Li</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654417</prism:doi><description>Multimodal image fusion integrates infrared and visible images by leveraging their complementary strengths. However, most existing fusion techniques primarily focus on pixel level integration, often neglecting the preservation of semantic consistency between the source and fused images. To address this limitation, we propose TEDFuse, a Task-Driven Equivariant Consistency Decomposition Network that ensures semantic con sistency within the image space and across high-level semantic tasks. TEDFuse incorporates two key components: first, a robust decomposition framework with equivariant consistency, ensuring that the fused image retains consistent transformation properties under shifts, rotations, and reflections, thereby enhancing local detail preservation and global semantic alignment; In addition, a task-driven fusion framework that integrates a segmentation module, reinforcing semantic feature preservation through a semantic loss function and ensuring consistency in downstream tasks such as segmentation and detection. The proposed method not only preserves the semantic coherence of the fused image but also improves performance in high-level tasks, demonstrating superior capability in multimodal fusion for complex visual applications. Extensive experiments validate the effectiveness of TEDFuse by analyzing feature evolution, examining the relationship between fusion quality and task performance, and discussing calibration strategies for infrared-visible image fusion. The code is available at https://github.com/Claire-cxy/TEDFuse.
Published: 2026-01-16T20:50:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yiming Sun; Xinyu Cui; Zhen Wang; Hao Cheng; Yongfeng Dong; Pengfei Zhu; Kai Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654417"&gt;10.1109/tmm.2026.3654417&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal image fusion integrates infrared and visible images by leveraging their complementary strengths. However, most existing fusion techniques primarily focus on pixel level integration, often neglecting the preservation of semantic consistency between the source and fused images. To address this limitation, we propose TEDFuse, a Task-Driven Equivariant Consistency Decomposition Network that ensures semantic con sistency within the image space and across high-level semantic tasks. TEDFuse incorporates two key components: first, a robust decomposition framework with equivariant consistency, ensuring that the fused image retains consistent transformation properties under shifts, rotations, and reflections, thereby enhancing local detail preservation and global semantic alignment; In addition, a task-driven fusion framework that integrates a segmentation module, reinforcing semantic feature preservation through a semantic loss function and ensuring consistency in downstream tasks such as segmentation and detection. The proposed method not only preserves the semantic coherence of the fused image but also improves performance in high-level tasks, demonstrating superior capability in multimodal fusion for complex visual applications. Extensive experiments validate the effectiveness of TEDFuse by analyzing feature evolution, examining the relationship between fusion quality and task performance, and discussing calibration strategies for infrared-visible image fusion. The code is available at https://github.com/Claire-cxy/TEDFuse.&lt;/p&gt;</content:encoded></item><item><title>RECREATE: Supervised contrastive learning and inpainting based hyperspectral image denoising</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.022</link><guid>10.1016/j.isprsjprs.2026.01.022</guid><pubDate>Fri, 16 Jan 2026 12:27:16 +0000</pubDate><dc:creator>Aditya Dixit</dc:creator><dc:creator>Anup Kumar Gupta</dc:creator><dc:creator>Puneet Gupta</dc:creator><dc:creator>Ankur Garg</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.022</prism:doi><description>Hyperspectral image (HSI) contains information at various spectra, making it valuable in several real-world applications such as environmental monitoring, agriculture, and remote sensing. However, the acquisition process often introduces noise, necessitating effective HSI denoising methods to maintain its applicability. Deep Learning (DL) is considered as the de-facto for HSI denoising, but it requires a significant number of training samples to optimize network parameters for effective denoising outcomes. However, obtaining extensive datasets is challenging in HSI, leading to epistemic uncertainties and thereby deteriorating the denoising performance. This paper introduces a novel supervised contrastive learning (SCL) method, RECREATE , to enhance feature learning and mitigate the issue of epistemic uncertainty for HSI denoising. Furthermore, we introduce the exploration of image inpainting as an auxiliary task to enhance the HSI denoising performance. By adding HSI inpainting to CL, our method essentially enhances HSI denoising by increasing training datasets and enforcing improved feature learning. Experimental outcomes on various HSI datasets validate the efficacy of RECREATE , showcasing its potential for integration with existing HSI denoising techniques to enhance their performance, both qualitatively and quantitatively. This innovative method holds promise for addressing the limitations posed by limited training data and thereby advancing the field toward proposing better HSI denoising methods.
Published: 2026-01-16T12:27:16+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aditya Dixit; Anup Kumar Gupta; Puneet Gupta; Ankur Garg&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.022"&gt;10.1016/j.isprsjprs.2026.01.022&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Hyperspectral image (HSI) contains information at various spectra, making it valuable in several real-world applications such as environmental monitoring, agriculture, and remote sensing. However, the acquisition process often introduces noise, necessitating effective HSI denoising methods to maintain its applicability. Deep Learning (DL) is considered as the de-facto for HSI denoising, but it requires a significant number of training samples to optimize network parameters for effective denoising outcomes. However, obtaining extensive datasets is challenging in HSI, leading to epistemic uncertainties and thereby deteriorating the denoising performance. This paper introduces a novel supervised contrastive learning (SCL) method, RECREATE , to enhance feature learning and mitigate the issue of epistemic uncertainty for HSI denoising. Furthermore, we introduce the exploration of image inpainting as an auxiliary task to enhance the HSI denoising performance. By adding HSI inpainting to CL, our method essentially enhances HSI denoising by increasing training datasets and enforcing improved feature learning. Experimental outcomes on various HSI datasets validate the efficacy of RECREATE , showcasing its potential for integration with existing HSI denoising techniques to enhance their performance, both qualitatively and quantitatively. This innovative method holds promise for addressing the limitations posed by limited training data and thereby advancing the field toward proposing better HSI denoising methods.&lt;/p&gt;</content:encoded></item><item><title>AnchorReF: A novel anchor-based visual re-localization framework aided by multi-sensor data fusion</title><link>https://doi.org/10.1016/j.isprsjprs.2026.01.019</link><guid>10.1016/j.isprsjprs.2026.01.019</guid><pubDate>Fri, 16 Jan 2026 12:27:16 +0000</pubDate><dc:creator>Hao Wu</dc:creator><dc:creator>Yu Ran</dc:creator><dc:creator>Xiaoxiang Zhang</dc:creator><dc:creator>Xinying Luo</dc:creator><dc:creator>Li Wang</dc:creator><dc:creator>Teng Zhao</dc:creator><dc:creator>Yongcheng Song</dc:creator><dc:creator>Zhijun Zhang</dc:creator><dc:creator>Huisong Zhang</dc:creator><dc:creator>Jin Liu</dc:creator><dc:creator>Jian Li</dc:creator><prism:publicationName>ISPRS Journal of Photogrammetry and Remote Sensing</prism:publicationName><prism:doi>10.1016/j.isprsjprs.2026.01.019</prism:doi><description>Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.
Published: 2026-01-16T12:27:16+00:00
Venue: ISPRS Journal of Photogrammetry and Remote Sensing
Score: 0.806 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Hao Wu; Yu Ran; Xiaoxiang Zhang; Xinying Luo; Li Wang; Teng Zhao; Yongcheng Song; Zhijun Zhang; Huisong Zhang; Jin Liu; Jian Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; ISPRS Journal of Photogrammetry and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.isprsjprs.2026.01.019"&gt;10.1016/j.isprsjprs.2026.01.019&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.806 (must_read)&lt;/p&gt;
&lt;p&gt;Visual relocalization estimates the precise pose of a query image within a pre-built visual map, serving as a fundamental component for robot navigation, autonomous driving, surveying and mapping, etc. In the past few decades, significant research efforts have been devoted to achieving high relocalization accuracy. However, challenges remain when the query images exhibit significant changes compared to the reference scene. This paper primarily addresses the problem of pose verification and correction of inaccurate pose estimations from the relocalization. We propose a novel anchor-based visual relocalization framework that achieves robust pose estimations through multi-view co-visibility verification. Our approach further utilizes a tightly-coupled multi-sensor data fusion for pose refinement. Comprehensive evaluations on large-scale, real-world urban driving datasets (containing frequent dynamic objects, severe occlusions, and long-term environmental changes) demonstrate that our framework achieves state-of-the-art performance. Specifically, compared to traditional SFM-based and Transformer-based methods under these challenging conditions, our approach reduces the translation error by 46.2% and the rotation error by 8.55%.&lt;/p&gt;</content:encoded></item><item><title>Negative Can Be Positive: A Stable and Noise-Resistant Complementary Contrastive Learning for Cross-Modal Matching</title><link>https://doi.org/10.1016/j.inffus.2026.104156</link><guid>10.1016/j.inffus.2026.104156</guid><pubDate>Fri, 16 Jan 2026 17:09:45 +0000</pubDate><dc:creator>Fangming Zhong</dc:creator><dc:creator>Xinyu He</dc:creator><dc:creator>Haiquan Yu</dc:creator><dc:creator>Xiu Liu</dc:creator><dc:creator>Suhua Zhang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2026.104156</prism:doi><description>Cross-modal matching with noisy correspondence has drawn considerable interest recently, due to the mismatched data imposed inevitably when collecting data from the Internet. Training on such noisy data often leads to severe performance degradation, as conventional methods tend to overfit rapidly to wrongly mismatched pairs. Most of the existing methods focus on predicting more reliable soft correspondence, generating higher weights for the pairs that are more likely to be correct. However, there still remain two limitations: (1) they ignore the informative signals embedded in the negative pairs, and (2) the instability of existing methods due to their sensitivity to the noise ratio. To address these issues, we explicitly take the negatives into account and propose a stable and noise-resistant complementary learning method, named Dual Contrastive Learning (DCL), for cross-modal matching with noisy correspondence. DCL leverages both positive pairs and negative pairs to improve the robustness. With the complementary contrastive learning, the negative pairs also contribute positively to the model optimization. Specifically, to fully explore the potential of mismatched data, we first partition the training data into clean and noisy subsets based on the memorization effect of deep neural networks. Then, we employ vanilla contrastive learning for positive matched pairs in the clean subset. As for negative pairs including the noisy subsets, complementary contrastive learning is adopted. In such doing, whatever the level of noise ratio is, the proposed method is robust to balance the positive information and negative information. Extensive experiments indicate that DCL significantly outperforms the state-of-the-art methods and exhibits remarkable stability with an extremely low variance of R@1. Specifically, the R@1 scores of our DCL are 7% and 9.1% higher than NPC on image-to-text and text-to-image, respectively. The source code is released at https://github.com/hxy2969/dcl .
Published: 2026-01-16T17:09:45+00:00
Venue: Information Fusion
Score: 0.805 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fangming Zhong; Xinyu He; Haiquan Yu; Xiu Liu; Suhua Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2026.104156"&gt;10.1016/j.inffus.2026.104156&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.805 (must_read)&lt;/p&gt;
&lt;p&gt;Cross-modal matching with noisy correspondence has drawn considerable interest recently, due to the mismatched data imposed inevitably when collecting data from the Internet. Training on such noisy data often leads to severe performance degradation, as conventional methods tend to overfit rapidly to wrongly mismatched pairs. Most of the existing methods focus on predicting more reliable soft correspondence, generating higher weights for the pairs that are more likely to be correct. However, there still remain two limitations: (1) they ignore the informative signals embedded in the negative pairs, and (2) the instability of existing methods due to their sensitivity to the noise ratio. To address these issues, we explicitly take the negatives into account and propose a stable and noise-resistant complementary learning method, named Dual Contrastive Learning (DCL), for cross-modal matching with noisy correspondence. DCL leverages both positive pairs and negative pairs to improve the robustness. With the complementary contrastive learning, the negative pairs also contribute positively to the model optimization. Specifically, to fully explore the potential of mismatched data, we first partition the training data into clean and noisy subsets based on the memorization effect of deep neural networks. Then, we employ vanilla contrastive learning for positive matched pairs in the clean subset. As for negative pairs including the noisy subsets, complementary contrastive learning is adopted. In such doing, whatever the level of noise ratio is, the proposed method is robust to balance the positive information and negative information. Extensive experiments indicate that DCL significantly outperforms the state-of-the-art methods and exhibits remarkable stability with an extremely low variance of R@1. Specifically, the R@1 scores of our DCL are 7% and 9.1% higher than NPC on image-to-text and text-to-image, respectively. The source code is released at https://github.com/hxy2969/dcl .&lt;/p&gt;</content:encoded></item><item><title>LLMI3D: MLLM-based 3D Perception from a Single 2D Image</title><link>https://doi.org/10.1109/tmm.2026.3654407</link><guid>10.1109/tmm.2026.3654407</guid><pubDate>Fri, 16 Jan 2026 20:50:41 +0000</pubDate><dc:creator>Fan Yang</dc:creator><dc:creator>Sicheng Zhao</dc:creator><dc:creator>Yanhao Zhang</dc:creator><dc:creator>Hui Chen</dc:creator><dc:creator>Haonan Lu</dc:creator><dc:creator>Jungong Han</dc:creator><dc:creator>Guiguang Ding</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654407</prism:doi><description>Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, especially specialized small models, exhibit poor generalization in open scenarios. On the other hand, multimodal large language models (MLLMs) excel in general capacity but underperform in 3D tasks, due to weak 3D local spatial object perception, poor text-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we develop LLMI3D, and propose the following solutions: Spatial-Enhanced Local Feature Mining for better 3D spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We are the first to adapt an MLLM for image-based 3D perception. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, outperforming other methods by a large margin. We will publicly release our code, models, and dataset.
Published: 2026-01-16T20:50:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fan Yang; Sicheng Zhao; Yanhao Zhang; Hui Chen; Haonan Lu; Jungong Han; Guiguang Ding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654407"&gt;10.1109/tmm.2026.3654407&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, especially specialized small models, exhibit poor generalization in open scenarios. On the other hand, multimodal large language models (MLLMs) excel in general capacity but underperform in 3D tasks, due to weak 3D local spatial object perception, poor text-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we develop LLMI3D, and propose the following solutions: Spatial-Enhanced Local Feature Mining for better 3D spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We are the first to adapt an MLLM for image-based 3D perception. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, outperforming other methods by a large margin. We will publicly release our code, models, and dataset.&lt;/p&gt;</content:encoded></item><item><title>Multiscale Spatial-Frequency Learning for Degradation Decoupling in RS Image Restoration</title><link>https://doi.org/10.1109/tmm.2026.3654414</link><guid>10.1109/tmm.2026.3654414</guid><pubDate>Fri, 16 Jan 2026 20:50:41 +0000</pubDate><dc:creator>Jingwen Zhang</dc:creator><dc:creator>Lingling Li</dc:creator><dc:creator>Licheng Jiao</dc:creator><dc:creator>Xu Liu</dc:creator><dc:creator>Fang Liu</dc:creator><dc:creator>Wenping Ma</dc:creator><dc:creator>Shuyuan Yang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654414</prism:doi><description>Remote sensing (RS) images are prone to various degradations, which poses challenges to downstream tasks. Although existing single-task remote sensing image restoration methods are effective, they lack generalizability across tasks. All-in-one methods can handle multiple degradation tasks, but they usually focus on spatial information, ignoring the physical properties of the degradation information. To address the above limitations, we propose a Multiscale Spatial-Frequency Degradation Decoupling framework for All-in-One remote sensing image restoration (SFD 2 ^{2} IR), which decouples degradation features across different tasks to guide the model in performing task-specific image restoration. Specifically, a task-specific instruction generator (TIG) is proposed first to transform degradation features into task-specific prompts. Then, a multi-scale multi-frequency enhancement (MME) module is designed to decouple degradation effects from both spatial and frequency perspectives, thus enhancing the model's adaptability to various degradation types. Finally, a prompt feature refinement (PFR) module is developed to further refine the model's response to degraded tasks. Extensive experiments demonstrate that the proposed method achieves excellent performance on different RSIR tasks, including cloud removal, deblurring, dehazing, and super-resolution. The source code will be publicly available at SFD 2 ^{2} IR.
Published: 2026-01-16T20:50:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jingwen Zhang; Lingling Li; Licheng Jiao; Xu Liu; Fang Liu; Wenping Ma; Shuyuan Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654414"&gt;10.1109/tmm.2026.3654414&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing (RS) images are prone to various degradations, which poses challenges to downstream tasks. Although existing single-task remote sensing image restoration methods are effective, they lack generalizability across tasks. All-in-one methods can handle multiple degradation tasks, but they usually focus on spatial information, ignoring the physical properties of the degradation information. To address the above limitations, we propose a Multiscale Spatial-Frequency Degradation Decoupling framework for All-in-One remote sensing image restoration (SFD 2 ^{2} IR), which decouples degradation features across different tasks to guide the model in performing task-specific image restoration. Specifically, a task-specific instruction generator (TIG) is proposed first to transform degradation features into task-specific prompts. Then, a multi-scale multi-frequency enhancement (MME) module is designed to decouple degradation effects from both spatial and frequency perspectives, thus enhancing the model&amp;#x27;s adaptability to various degradation types. Finally, a prompt feature refinement (PFR) module is developed to further refine the model&amp;#x27;s response to degraded tasks. Extensive experiments demonstrate that the proposed method achieves excellent performance on different RSIR tasks, including cloud removal, deblurring, dehazing, and super-resolution. The source code will be publicly available at SFD 2 ^{2} IR.&lt;/p&gt;</content:encoded></item><item><title>TextBridge: A Text-Centered Framework for Enhanced Multimodal Integration and Retrieval</title><link>https://doi.org/10.1109/tmm.2026.3654363</link><guid>10.1109/tmm.2026.3654363</guid><pubDate>Fri, 16 Jan 2026 20:50:41 +0000</pubDate><dc:creator>Jie Guo</dc:creator><dc:creator>Wenwei Wang</dc:creator><dc:creator>Haiyang Jing</dc:creator><dc:creator>Bin Song</dc:creator><dc:creator>Minghao Wang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654363</prism:doi><description>Despite significant advancements in multimodal pre-training, effectively integrating and using latent semantic information across multiple modalities remains a challenge. In this paper, we introduce TextBridge, a text-centered framework that uses the text modality as a semantic anchor to guide cross-modal integration and alignment. TextBridge employs frozen encoders from state-of-the-art pre-trained models and introduces an innovative modality bridge module that enhances semantic alignment and reduces redundancy among different modal features. The framework also incorporates a multi-projection text feature fusion method, enhancing the alignment and integration of text features from diverse modalities into a cohesive semantic representation. To optimize the integration of multimodal information, we make the text encoder trainable and use a text-centered contrastive loss function to enhance the model's ability to capture complementary information across modalities. Extensive experiments on the M5Product dataset demonstrate that TextBridge significantly outperforms the SCALE model in mean average precision (mAP) and precision (Prec), underscoring its effectiveness in multimodal retrieval tasks.
Published: 2026-01-16T20:50:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Guo; Wenwei Wang; Haiyang Jing; Bin Song; Minghao Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654363"&gt;10.1109/tmm.2026.3654363&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Despite significant advancements in multimodal pre-training, effectively integrating and using latent semantic information across multiple modalities remains a challenge. In this paper, we introduce TextBridge, a text-centered framework that uses the text modality as a semantic anchor to guide cross-modal integration and alignment. TextBridge employs frozen encoders from state-of-the-art pre-trained models and introduces an innovative modality bridge module that enhances semantic alignment and reduces redundancy among different modal features. The framework also incorporates a multi-projection text feature fusion method, enhancing the alignment and integration of text features from diverse modalities into a cohesive semantic representation. To optimize the integration of multimodal information, we make the text encoder trainable and use a text-centered contrastive loss function to enhance the model&amp;#x27;s ability to capture complementary information across modalities. Extensive experiments on the M5Product dataset demonstrate that TextBridge significantly outperforms the SCALE model in mean average precision (mAP) and precision (Prec), underscoring its effectiveness in multimodal retrieval tasks.&lt;/p&gt;</content:encoded></item><item><title>Efficient Group Attentive Learning for Few-Shot Image Classification</title><link>https://doi.org/10.1016/j.eswa.2026.131245</link><guid>10.1016/j.eswa.2026.131245</guid><pubDate>Fri, 16 Jan 2026 17:03:35 +0000</pubDate><dc:creator>Jiaxing Sun</dc:creator><dc:creator>Keju Huang</dc:creator><dc:creator>Dujia Yang</dc:creator><dc:creator>Hui Liu</dc:creator><prism:publicationName>Expert Systems with Applications</prism:publicationName><prism:doi>10.1016/j.eswa.2026.131245</prism:doi><description>Few-shot image classification is a challenging task that requires novel classes to be recognized with only a few samples. One promising direction is to learn deep representations of support and query samples for similarity comparison. To combat the limited learning capability of representation and metric in few-shot scenarios, recent efforts gather on exploiting dense local features rather than a mixed global vector to represent an image in latent space. Generally, these approaches directly utilize the dense features extracted from backbones to explore semantic correspondence between query samples and each support class, which introduces redundant information from certain local features into the inference process. In this work, we propose to construct simplified group-based representations to adaptively amplify the semantic hierarchical nature of feature learning. Specifically, we transform the dense features of each sample into group-based representations via a learnable self-attention mechanism. This process enables the dense features to become concise and comprehensible via group attentive learning. We thus perform a simplified few-shot classification based on group-based similarities. Extensive experiments are conducted on six widely used few-shot benchmarks in both 5-way 1-shot and 5-way 5-shot scenarios. Experimental results reveal that the proposed method achieves great performance through a novel grouping approach for few-shot classification.
Published: 2026-01-16T17:03:35+00:00
Venue: Expert Systems with Applications
Score: 0.800 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jiaxing Sun; Keju Huang; Dujia Yang; Hui Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Expert Systems with Applications&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.eswa.2026.131245"&gt;10.1016/j.eswa.2026.131245&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.800 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot image classification is a challenging task that requires novel classes to be recognized with only a few samples. One promising direction is to learn deep representations of support and query samples for similarity comparison. To combat the limited learning capability of representation and metric in few-shot scenarios, recent efforts gather on exploiting dense local features rather than a mixed global vector to represent an image in latent space. Generally, these approaches directly utilize the dense features extracted from backbones to explore semantic correspondence between query samples and each support class, which introduces redundant information from certain local features into the inference process. In this work, we propose to construct simplified group-based representations to adaptively amplify the semantic hierarchical nature of feature learning. Specifically, we transform the dense features of each sample into group-based representations via a learnable self-attention mechanism. This process enables the dense features to become concise and comprehensible via group attentive learning. We thus perform a simplified few-shot classification based on group-based similarities. Extensive experiments are conducted on six widely used few-shot benchmarks in both 5-way 1-shot and 5-way 5-shot scenarios. Experimental results reveal that the proposed method achieves great performance through a novel grouping approach for few-shot classification.&lt;/p&gt;</content:encoded></item><item><title>Disentangle Object and Non-object Infrared Features via Language Guidance</title><link>https://arxiv.org/abs/2601.09228v1</link><guid>http://arxiv.org/abs/2601.09228v1</guid><pubDate>Wed, 14 Jan 2026 06:59:54 +0000</pubDate><dc:creator>Fan Liu</dc:creator><dc:creator>Ting Wu</dc:creator><dc:creator>Chuanyi Zhang</dc:creator><dc:creator>Liang Yao</dc:creator><dc:creator>Xing Ma</dc:creator><dc:creator>Yuhui Zheng</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.
Published: 2026-01-14T06:59:54+00:00
Venue: arXiv
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Fan Liu; Ting Wu; Chuanyi Zhang; Liang Yao; Xing Ma; Yuhui Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.&lt;/p&gt;</content:encoded></item><item><title>Isolating Interference Factors for Robust Cloth-Changing Person Re-Identification</title><link>https://doi.org/10.1109/tpami.2026.3655110</link><guid>10.1109/tpami.2026.3655110</guid><pubDate>Fri, 16 Jan 2026 20:49:53 +0000</pubDate><dc:creator>De Cheng</dc:creator><dc:creator>Yubo Li</dc:creator><dc:creator>Chaowei Fang</dc:creator><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Nannan Wang</dc:creator><dc:creator>Xinbo Gao</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2026.3655110</prism:doi><description>Cloth-Changing Person Re-Identification (CC-ReID) aims to recognize individuals across camera views despite clothing variations, a crucial task for surveillance and security systems. Existing methods typically frame it as a cross-modal alignment problem but often overlook explicit modeling of interference factors such as clothing, viewpoints, and pedestrian actions. This oversight can distort their impact, compromising the extraction of robust identity features. To address these challenges, we propose a novel framework that systematically disentangles interference factors from identity features while ensuring the robustness and discriminative power of identity representations. Our approach consists of two key components. First, a dual-stream identity feature learning framework leverages a raw image stream and a cloth-isolated stream, to extract identity representations independent of clothing textures. An adaptive cloth-irrelevant contrastive objective is introduced to mitigate identity feature variations caused by clothing differences. Second, we propose a Text-Driven Conditional Generative Adversarial Interference Disentanglement Network (T-CGAIDN), to further suppress interference factors beyond clothing textures, such as finer clothing patterns, viewpoint, background, and lighting conditions. This network incorporates a multi-granularity interference recognition branch to learn interference-related features, a conditional adversarial module for bidirectional transformation between identity and interference feature spaces, and an interference decoupling objective to eliminate interference dependencies in identity learning. Extensive experiments on public benchmarks demonstrate that our method significantly outperforms state-ofthe- art approaches, highlighting its effectiveness in CC-ReID. Our code is available at https://github.com/yblTech/IIFR-CCReID.
Published: 2026-01-16T20:49:53+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; De Cheng; Yubo Li; Chaowei Fang; Shizhou Zhang; Nannan Wang; Xinbo Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2026.3655110"&gt;10.1109/tpami.2026.3655110&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Cloth-Changing Person Re-Identification (CC-ReID) aims to recognize individuals across camera views despite clothing variations, a crucial task for surveillance and security systems. Existing methods typically frame it as a cross-modal alignment problem but often overlook explicit modeling of interference factors such as clothing, viewpoints, and pedestrian actions. This oversight can distort their impact, compromising the extraction of robust identity features. To address these challenges, we propose a novel framework that systematically disentangles interference factors from identity features while ensuring the robustness and discriminative power of identity representations. Our approach consists of two key components. First, a dual-stream identity feature learning framework leverages a raw image stream and a cloth-isolated stream, to extract identity representations independent of clothing textures. An adaptive cloth-irrelevant contrastive objective is introduced to mitigate identity feature variations caused by clothing differences. Second, we propose a Text-Driven Conditional Generative Adversarial Interference Disentanglement Network (T-CGAIDN), to further suppress interference factors beyond clothing textures, such as finer clothing patterns, viewpoint, background, and lighting conditions. This network incorporates a multi-granularity interference recognition branch to learn interference-related features, a conditional adversarial module for bidirectional transformation between identity and interference feature spaces, and an interference decoupling objective to eliminate interference dependencies in identity learning. Extensive experiments on public benchmarks demonstrate that our method significantly outperforms state-ofthe- art approaches, highlighting its effectiveness in CC-ReID. Our code is available at https://github.com/yblTech/IIFR-CCReID.&lt;/p&gt;</content:encoded></item><item><title>CSP: Channel And Space Pruning for Compressing Deep Convolutional Neural Networks</title><link>https://doi.org/10.1109/tmm.2026.3654455</link><guid>10.1109/tmm.2026.3654455</guid><pubDate>Fri, 16 Jan 2026 20:50:41 +0000</pubDate><dc:creator>Yang Li</dc:creator><dc:creator>Liejun Wang</dc:creator><dc:creator>Minchi Kuang</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654455</prism:doi><description>Pruning is widely researched as one of the effective methods for model compression. However, existing pruning methods still have shortcomings in achieving high model compression rates and maintaining model performance. To this end, we propose a general method called Channel And Space Pruning (CSP) for compressing deep convolutional neural networks (DCNNs). The proposed CSP method comprises Coarse-grained pruning and Fine-grained pruning, which operate on the convolutional kernels from both channel and spatial dimensions to achieve model compression. The Coarse-grained pruning utilizes a Bottleneck-based maximum flow mechanism to ensure maximum flow of inputs and outputs in DCNNs, thereby maintaining model performance. Additionally, a Bottleneck-based dynamic updating mechanism is employed to evaluate the importance of convolutional kernels, allowing for the pruning of redundant kernels while maintaining model performance. The Fine-grained pruning further compresses the model based on Coarse-grained pruning. To ensure that the parameters of the convolutional kernels obtained through Coarse-grained pruning are not spatially redundant, Fine-grained pruning employs the Parameters-free spatial attention mechanism (PSAM) for spatial sparsification of the kernels. The proposed CSP method demonstrates promising results compared to state-of-the-art (SOTA) methods in classic DCNN models.
Published: 2026-01-16T20:50:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yang Li; Liejun Wang; Minchi Kuang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654455"&gt;10.1109/tmm.2026.3654455&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;Pruning is widely researched as one of the effective methods for model compression. However, existing pruning methods still have shortcomings in achieving high model compression rates and maintaining model performance. To this end, we propose a general method called Channel And Space Pruning (CSP) for compressing deep convolutional neural networks (DCNNs). The proposed CSP method comprises Coarse-grained pruning and Fine-grained pruning, which operate on the convolutional kernels from both channel and spatial dimensions to achieve model compression. The Coarse-grained pruning utilizes a Bottleneck-based maximum flow mechanism to ensure maximum flow of inputs and outputs in DCNNs, thereby maintaining model performance. Additionally, a Bottleneck-based dynamic updating mechanism is employed to evaluate the importance of convolutional kernels, allowing for the pruning of redundant kernels while maintaining model performance. The Fine-grained pruning further compresses the model based on Coarse-grained pruning. To ensure that the parameters of the convolutional kernels obtained through Coarse-grained pruning are not spatially redundant, Fine-grained pruning employs the Parameters-free spatial attention mechanism (PSAM) for spatial sparsification of the kernels. The proposed CSP method demonstrates promising results compared to state-of-the-art (SOTA) methods in classic DCNN models.&lt;/p&gt;</content:encoded></item><item><title>LiteEmbed: Adapting CLIP to Rare Classes</title><link>https://arxiv.org/abs/2601.09661v1</link><guid>http://arxiv.org/abs/2601.09661v1</guid><pubDate>Wed, 14 Jan 2026 17:53:11 +0000</pubDate><dc:creator>Aishwarya Agarwal</dc:creator><dc:creator>Srikrishna Karanam</dc:creator><dc:creator>Vineet Gandhi</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.
Published: 2026-01-14T17:53:11+00:00
Venue: arXiv
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aishwarya Agarwal; Srikrishna Karanam; Vineet Gandhi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&amp;#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&amp;#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.&lt;/p&gt;</content:encoded></item><item><title>SPARK-ViT: Pose estimation with adaptive attention and structured reasoning</title><link>https://doi.org/10.1016/j.neucom.2026.132743</link><guid>10.1016/j.neucom.2026.132743</guid><pubDate>Fri, 16 Jan 2026 16:26:52 +0000</pubDate><dc:creator>Henan Hu</dc:creator><dc:creator>Xuan Wu</dc:creator><dc:creator>Ronghua Li</dc:creator><dc:creator>Shiran Zhu</dc:creator><dc:creator>Shange Zhang</dc:creator><dc:creator>Muyu Li</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132743</prism:doi><description>2D human pose estimation in unconstrained environments is challenging due to occlusion and scale variation, where visual cues for joints are incomplete or ambiguous. Transformer-based models improve representational power but suffer from fundamental limitations: their self-attention relies on uniformly partitioned, rigid patch grids that cannot adapt to non-rigid motion or scale changes, whereas independent keypoint regression reduces to an ill-posed inference problem when evidence is missing. We introduce SPARK-ViT (Spatial-Adaptive Reasoning Keypoint ViT), which addresses these issues through two core mechanisms. An Adaptive Deformable Attention Block learns spatial offsets for content-adaptive sampling, providing robustness to deformation and scale. A Spatial-Aware Keypoint Relation Inference module integrates kinematic priors into structured inference, allowing logical recovery of occluded joints. A hybrid detection head further unifies the heatmap and regression outputs to ensure stable predictions. Experiments on COCO and OCHuman show consistent improvements, with SPARK-ViT achieving 77.5 AP and 65.6 APM, surpassing baselines under severe occlusion.
Published: 2026-01-16T16:26:52+00:00
Venue: Neurocomputing
Score: 0.789 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Henan Hu; Xuan Wu; Ronghua Li; Shiran Zhu; Shange Zhang; Muyu Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132743"&gt;10.1016/j.neucom.2026.132743&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.789 (must_read)&lt;/p&gt;
&lt;p&gt;2D human pose estimation in unconstrained environments is challenging due to occlusion and scale variation, where visual cues for joints are incomplete or ambiguous. Transformer-based models improve representational power but suffer from fundamental limitations: their self-attention relies on uniformly partitioned, rigid patch grids that cannot adapt to non-rigid motion or scale changes, whereas independent keypoint regression reduces to an ill-posed inference problem when evidence is missing. We introduce SPARK-ViT (Spatial-Adaptive Reasoning Keypoint ViT), which addresses these issues through two core mechanisms. An Adaptive Deformable Attention Block learns spatial offsets for content-adaptive sampling, providing robustness to deformation and scale. A Spatial-Aware Keypoint Relation Inference module integrates kinematic priors into structured inference, allowing logical recovery of occluded joints. A hybrid detection head further unifies the heatmap and regression outputs to ensure stable predictions. Experiments on COCO and OCHuman show consistent improvements, with SPARK-ViT achieving 77.5 AP and 65.6 APM, surpassing baselines under severe occlusion.&lt;/p&gt;</content:encoded></item><item><title>Selecting and Pruning: A Differentiable Causal Sequentialized State-Space Model for Two-View Correspondence Learning</title><link>https://doi.org/10.1109/tip.2026.3653189</link><guid>10.1109/tip.2026.3653189</guid><pubDate>Fri, 16 Jan 2026 20:52:20 +0000</pubDate><dc:creator>Xiang Fang</dc:creator><dc:creator>Shihua Zhang</dc:creator><dc:creator>Hao Zhang</dc:creator><dc:creator>Xiaoguang Mei</dc:creator><dc:creator>Huabing Zhou</dc:creator><dc:creator>Jiayi Ma</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2026.3653189</prism:doi><description>Two-view correspondence learning aims to discern true and false correspondences between image pairs by recognizing their underlying different information. Previous methods either treat the information equally or require the explicit storage of the entire context, tending to be laborious in real-world scenarios. Inspired by Mamba’s inherent selectivity, we propose CorrMamba, a Correspondence filter leveraging Mamba’s ability to selectively mine information from true correspondences while mitigating interference from false ones, thus achieving adaptive focus at a lower cost. To prevent Mamba from being potentially impacted by unordered keypoints that obscured its ability to mine spatial information, we customize a causal sequential learning approach based on the Gumbel-Softmax technique to establish causal dependencies between features in a fully autonomous and differentiable manner. Additionally, a local-context enhancement module is designed to capture critical contextual cues essential for correspondence pruning, complementing the core framework. Extensive experiments on relative pose estimation, visual localization, and analysis demonstrate that CorrMamba achieves state-of-the-art performance. Notably, in outdoor relative pose estimation, our method surpasses the previous SOTA by 2.58 absolute percentage points in AUC@20°, highlighting its practical superiority. Our code will be publicly available.
Published: 2026-01-16T20:52:20+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiang Fang; Shihua Zhang; Hao Zhang; Xiaoguang Mei; Huabing Zhou; Jiayi Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2026.3653189"&gt;10.1109/tip.2026.3653189&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Two-view correspondence learning aims to discern true and false correspondences between image pairs by recognizing their underlying different information. Previous methods either treat the information equally or require the explicit storage of the entire context, tending to be laborious in real-world scenarios. Inspired by Mamba’s inherent selectivity, we propose CorrMamba, a Correspondence filter leveraging Mamba’s ability to selectively mine information from true correspondences while mitigating interference from false ones, thus achieving adaptive focus at a lower cost. To prevent Mamba from being potentially impacted by unordered keypoints that obscured its ability to mine spatial information, we customize a causal sequential learning approach based on the Gumbel-Softmax technique to establish causal dependencies between features in a fully autonomous and differentiable manner. Additionally, a local-context enhancement module is designed to capture critical contextual cues essential for correspondence pruning, complementing the core framework. Extensive experiments on relative pose estimation, visual localization, and analysis demonstrate that CorrMamba achieves state-of-the-art performance. Notably, in outdoor relative pose estimation, our method surpasses the previous SOTA by 2.58 absolute percentage points in AUC@20°, highlighting its practical superiority. Our code will be publicly available.&lt;/p&gt;</content:encoded></item><item><title>Identity Clue Refinement and Enhancement for Visible-Infrared Person Re-Identification</title><link>https://doi.org/10.1109/tmm.2026.3654447</link><guid>10.1109/tmm.2026.3654447</guid><pubDate>Fri, 16 Jan 2026 20:50:41 +0000</pubDate><dc:creator>Guoqing Zhang</dc:creator><dc:creator>Zhun Wang</dc:creator><dc:creator>Hairui Wang</dc:creator><dc:creator>Zhonglin Ye</dc:creator><dc:creator>Yuhui Zheng</dc:creator><prism:publicationName>IEEE Transactions on Multimedia</prism:publicationName><prism:doi>10.1109/tmm.2026.3654447</prism:doi><description>Visible-Infrared Person Re-Identification (VI-ReID) is a challenging cross-modal matching task due to significant modality discrepancies. While current methods mainly focus on learning modality-invariant features through unified embedding spaces, they often focus solely on the common discriminative semantics across modalities while disregarding the critical role of modality-specific identity-aware knowledge in discriminative feature learning. To bridge this gap, we propose a novel Identity Clue Refinement and Enhancement (ICRE) network to mine and utilize the implicit discriminative knowledge inherent in modality-specific attributes. Initially, we design a Multi-Perception Feature Refinement (MPFR) module that aggregates shallow features from shared branches, aiming to capture modality-specific attributes that are easily overlooked. Then, we propose a Semantic Distillation Cascade Enhancement (SDCE) module, which distills identity-aware knowledge from the aggregated shallow features and guide the learning of modality-invariant features. Finally, an Identity Clues Guided (ICG) Loss is proposed to alleviate the modality discrepancies within the enhanced features and promote the learning of a diverse representation space. Extensive experiments across multiple public datasets clearly show that our proposed ICRE outperforms existing SOTA methods.
Published: 2026-01-16T20:50:41+00:00
Venue: IEEE Transactions on Multimedia
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Guoqing Zhang; Zhun Wang; Hairui Wang; Zhonglin Ye; Yuhui Zheng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Multimedia&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tmm.2026.3654447"&gt;10.1109/tmm.2026.3654447&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Visible-Infrared Person Re-Identification (VI-ReID) is a challenging cross-modal matching task due to significant modality discrepancies. While current methods mainly focus on learning modality-invariant features through unified embedding spaces, they often focus solely on the common discriminative semantics across modalities while disregarding the critical role of modality-specific identity-aware knowledge in discriminative feature learning. To bridge this gap, we propose a novel Identity Clue Refinement and Enhancement (ICRE) network to mine and utilize the implicit discriminative knowledge inherent in modality-specific attributes. Initially, we design a Multi-Perception Feature Refinement (MPFR) module that aggregates shallow features from shared branches, aiming to capture modality-specific attributes that are easily overlooked. Then, we propose a Semantic Distillation Cascade Enhancement (SDCE) module, which distills identity-aware knowledge from the aggregated shallow features and guide the learning of modality-invariant features. Finally, an Identity Clues Guided (ICG) Loss is proposed to alleviate the modality discrepancies within the enhanced features and promote the learning of a diverse representation space. Extensive experiments across multiple public datasets clearly show that our proposed ICRE outperforms existing SOTA methods.&lt;/p&gt;</content:encoded></item><item><title>Exploiting Class-agnostic Visual Prior for Few-shot Keypoint Detection</title><link>https://doi.org/10.1007/s11263-025-02671-5</link><guid>10.1007/s11263-025-02671-5</guid><pubDate>Fri, 16 Jan 2026 05:21:57 +0000</pubDate><dc:creator>Changsheng Lu</dc:creator><dc:creator>Hao Zhu</dc:creator><dc:creator>Piotr Koniusz</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02671-5</prism:doi><description>Abstract Deep learning based keypoint detectors can localize specific object (or body) parts well, but still fall short of general keypoint detection. Instead, few-shot keypoint detection (FSKD) is an underexplored yet more general task of localizing either base or novel keypoints, depending on the prompted support samples. In FSKD, how to build robust keypoint representations is the key to success. To this end, we propose an FSKD approach that models relations between keypoints. As keypoints are located on objects, we exploit a class-agnostic visual prior, i.e ., the unsupervised saliency map or DINO attentiveness map to obtain the region of focus within which we perform relation learning between object patches. The class-agnostic visual prior also helps suppress the background noise largely irrelevant to keypoint locations. Then, we propose a novel Visual Prior guided Vision Transformer (VPViT). The visual prior maps are refined by a bespoke morphology learner to include relevant context of objects. The masked self-attention of VPViT takes the adapted prior map as a soft mask to constrain the self-attention to foregrounds. As robust FSKD must also deal with the low number of support samples and occlusions, based on VPViT, we further investigate i) transductive FSKD to enhance keypoint representations with unlabeled data and ii) FSKD with masking and alignment (MAA) to improve robustness. We show that our model performs well in seven public datasets, and also significantly improves the accuracy in transductive inference and under occlusions. Source codes are available at https://github.com/AlanLuSun/VPViT .
Published: 2026-01-16T05:21:57+00:00
Venue: International Journal of Computer Vision
Score: 0.787 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Changsheng Lu; Hao Zhu; Piotr Koniusz&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02671-5"&gt;10.1007/s11263-025-02671-5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.787 (must_read)&lt;/p&gt;
&lt;p&gt;Abstract Deep learning based keypoint detectors can localize specific object (or body) parts well, but still fall short of general keypoint detection. Instead, few-shot keypoint detection (FSKD) is an underexplored yet more general task of localizing either base or novel keypoints, depending on the prompted support samples. In FSKD, how to build robust keypoint representations is the key to success. To this end, we propose an FSKD approach that models relations between keypoints. As keypoints are located on objects, we exploit a class-agnostic visual prior, i.e ., the unsupervised saliency map or DINO attentiveness map to obtain the region of focus within which we perform relation learning between object patches. The class-agnostic visual prior also helps suppress the background noise largely irrelevant to keypoint locations. Then, we propose a novel Visual Prior guided Vision Transformer (VPViT). The visual prior maps are refined by a bespoke morphology learner to include relevant context of objects. The masked self-attention of VPViT takes the adapted prior map as a soft mask to constrain the self-attention to foregrounds. As robust FSKD must also deal with the low number of support samples and occlusions, based on VPViT, we further investigate i) transductive FSKD to enhance keypoint representations with unlabeled data and ii) FSKD with masking and alignment (MAA) to improve robustness. We show that our model performs well in seven public datasets, and also significantly improves the accuracy in transductive inference and under occlusions. Source codes are available at https://github.com/AlanLuSun/VPViT .&lt;/p&gt;</content:encoded></item><item><title>Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP</title><link>https://arxiv.org/abs/2601.09859v1</link><guid>http://arxiv.org/abs/2601.09859v1</guid><pubDate>Wed, 14 Jan 2026 20:38:36 +0000</pubDate><dc:creator>Anant Mehta</dc:creator><dc:creator>Xiyuan Wei</dc:creator><dc:creator>Xingyu Chen</dc:creator><dc:creator>Tianbao Yang</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.
Published: 2026-01-14T20:38:36+00:00
Venue: arXiv
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anant Mehta; Xiyuan Wei; Xingyu Chen; Tianbao Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.&lt;/p&gt;</content:encoded></item><item><title>A lightweight framework for robust object detection in adverse weather based on dual-teacher feature alignment</title><link>https://doi.org/10.1016/j.neucom.2026.132726</link><guid>10.1016/j.neucom.2026.132726</guid><pubDate>Fri, 16 Jan 2026 16:26:49 +0000</pubDate><dc:creator>Rui Hu</dc:creator><dc:creator>Hanjun Zheng</dc:creator><dc:creator>Shengjie Ye</dc:creator><dc:creator>Linbo Qing</dc:creator><dc:creator>Honggang Chen</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2026.132726</prism:doi><description>Object detection in adverse weather conditions ( e.g. , rain, fog, snow) remains a critical challenge due to degraded image quality and semantic ambiguity. Mainstream approaches attempt to bridge this gap by aligning degraded images with their clear counterparts through two main paradigms: cascading enhancer-detector pipelines and multi-task learning frameworks that jointly optimize restoration and detection objectives. However, these methods often fail to reach their full potential due to the disparity between image restoration and detection tasks. Restoration networks prioritize pixel-level fidelity, while detection networks focus on high-level semantic accuracy, leading to suboptimal performance. In this work, we propose a Dual-Teacher Feature Alignment (DTFA) framework that rethinks the paradigm of “clear image alignment” for adverse weather detection. Instead of directly restoring pixel-level fidelity, we employ clear features as alignment to guide the training of the adverse weather detection student. Specifically, the Invariant Reconstruction Teacher (IRT) from a pre-trained reconstruction network provides weather-invariant priors, and the Semantic Prior Teacher (SPT) from a high-performance detection network offers task-aware semantic information. Two Adaptive Feature Bridging modules dynamically align multi-scale features between the two teachers and the adverse weather detection student, addressing task discrepancy through masked consistency constraints and efficiently enabling the adverse weather detection student to learn complementary information from both the IRT and SPT. During testing, only the adverse weather detection student remains. Therefore, no additional computational costs are incurred. Extensive experiments conducted in rainy, foggy, snowy, and mixed weather conditions demonstrate that our DTFA framework achieves state-of-the-art performance. The source code will be released at https://github.com/huruo1010/DTFA .
Published: 2026-01-16T16:26:49+00:00
Venue: Neurocomputing
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Rui Hu; Hanjun Zheng; Shengjie Ye; Linbo Qing; Honggang Chen&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2026.132726"&gt;10.1016/j.neucom.2026.132726&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Object detection in adverse weather conditions ( e.g. , rain, fog, snow) remains a critical challenge due to degraded image quality and semantic ambiguity. Mainstream approaches attempt to bridge this gap by aligning degraded images with their clear counterparts through two main paradigms: cascading enhancer-detector pipelines and multi-task learning frameworks that jointly optimize restoration and detection objectives. However, these methods often fail to reach their full potential due to the disparity between image restoration and detection tasks. Restoration networks prioritize pixel-level fidelity, while detection networks focus on high-level semantic accuracy, leading to suboptimal performance. In this work, we propose a Dual-Teacher Feature Alignment (DTFA) framework that rethinks the paradigm of “clear image alignment” for adverse weather detection. Instead of directly restoring pixel-level fidelity, we employ clear features as alignment to guide the training of the adverse weather detection student. Specifically, the Invariant Reconstruction Teacher (IRT) from a pre-trained reconstruction network provides weather-invariant priors, and the Semantic Prior Teacher (SPT) from a high-performance detection network offers task-aware semantic information. Two Adaptive Feature Bridging modules dynamically align multi-scale features between the two teachers and the adverse weather detection student, addressing task discrepancy through masked consistency constraints and efficiently enabling the adverse weather detection student to learn complementary information from both the IRT and SPT. During testing, only the adverse weather detection student remains. Therefore, no additional computational costs are incurred. Extensive experiments conducted in rainy, foggy, snowy, and mixed weather conditions demonstrate that our DTFA framework achieves state-of-the-art performance. The source code will be released at https://github.com/huruo1010/DTFA .&lt;/p&gt;</content:encoded></item><item><title>SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3</title><link>https://arxiv.org/abs/2601.09699v1</link><guid>http://arxiv.org/abs/2601.09699v1</guid><pubDate>Wed, 14 Jan 2026 18:52:14 +0000</pubDate><dc:creator>Ruiqi Shen</dc:creator><dc:creator>Chang Liu</dc:creator><dc:creator>Henghui Ding</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.
Published: 2026-01-14T18:52:14+00:00
Venue: arXiv
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Ruiqi Shen; Chang Liu; Henghui Ding&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.&lt;/p&gt;</content:encoded></item><item><title>Large Foundation Model Empowered Region-aware Underwater Image Captioning</title><link>https://doi.org/10.1007/s11263-025-02650-w</link><guid>10.1007/s11263-025-02650-w</guid><pubDate>Sat, 17 Jan 2026 05:47:18 +0000</pubDate><dc:creator>Huanyu Li</dc:creator><dc:creator>Li Li</dc:creator><dc:creator>Hao Wang</dc:creator><dc:creator>Weibo Zhang</dc:creator><dc:creator>Peng Ren</dc:creator><prism:publicationName>International Journal of Computer Vision</prism:publicationName><prism:doi>10.1007/s11263-025-02650-w</prism:doi><description>Underwater image captioning facilitates the transformation from visual perception to semantic understanding in underwater computer vision. Despite advancements in this field, challenges remain in generating high-quality captions for underwater images. These challenges typically stem from (a) ambiguity between object and background regions for feature extraction, and (b) insufficient feature fusion across all regions. To address these challenges, we develop a large foundation model empowered region-aware underwater image captioning framework. Our novel contributions are two-fold: (a) A region-discriminative feature extraction strategy powered by the large foundation segment anything model (SAM) is developed. This strategy accurately delineates object and background regions through segmentation maps, enabling precise extraction of region-discriminative features. (b) A region-guided feature fusion strategy comprehensively fusing regional information throughout an encoding-decoding process is presented. This strategy utilizes a region-guided encoder for the progressive layer-wise fusion of region-discriminative features and grid features, followed by a meshed memory decoder that fuses multi-level encoded features, thereby enhancing the decoded features. Together, these contributions result in the generation of accurate and comprehensive underwater image captions. Experimental evaluations on three datasets demonstrate that our proposed framework achieves state-of-the-art performance for underwater image captioning.
Published: 2026-01-17T05:47:18+00:00
Venue: International Journal of Computer Vision
Score: 0.782 (consider)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huanyu Li; Li Li; Hao Wang; Weibo Zhang; Peng Ren&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; International Journal of Computer Vision&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1007/s11263-025-02650-w"&gt;10.1007/s11263-025-02650-w&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (consider)&lt;/p&gt;
&lt;p&gt;Underwater image captioning facilitates the transformation from visual perception to semantic understanding in underwater computer vision. Despite advancements in this field, challenges remain in generating high-quality captions for underwater images. These challenges typically stem from (a) ambiguity between object and background regions for feature extraction, and (b) insufficient feature fusion across all regions. To address these challenges, we develop a large foundation model empowered region-aware underwater image captioning framework. Our novel contributions are two-fold: (a) A region-discriminative feature extraction strategy powered by the large foundation segment anything model (SAM) is developed. This strategy accurately delineates object and background regions through segmentation maps, enabling precise extraction of region-discriminative features. (b) A region-guided feature fusion strategy comprehensively fusing regional information throughout an encoding-decoding process is presented. This strategy utilizes a region-guided encoder for the progressive layer-wise fusion of region-discriminative features and grid features, followed by a meshed memory decoder that fuses multi-level encoded features, thereby enhancing the decoded features. Together, these contributions result in the generation of accurate and comprehensive underwater image captions. Experimental evaluations on three datasets demonstrate that our proposed framework achieves state-of-the-art performance for underwater image captioning.&lt;/p&gt;</content:encoded></item></channel></rss>