<?xml version='1.0' encoding='utf-8'?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:prism="http://prismstandard.org/namespaces/basic/3.0/" version="2.0"><channel><title>ZotWatch Feed</title><link>https://ehehe.cn/zotwatch/</link><description>AI-assisted literature watch</description><lastBuildDate>Wed, 24 Dec 2025 02:41:52 +0000</lastBuildDate><item><title>Boosting Faithful Multi-modal LLMs via Complementary Visual Grounding</title><link>https://doi.org/10.1109/tip.2025.3644140</link><guid>10.1109/tip.2025.3644140</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Zheren Fu</dc:creator><dc:creator>Zhendong Mao</dc:creator><dc:creator>Lei Zhang</dc:creator><dc:creator>Yongdong Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644140</prism:doi><description>Multimodal Large Language Models (MLLMs) exhibit impressive performance across vision-language tasks, but still face the hallucination challenges, where generated texts are factually inconsistent with visual input. Existing mitigation methods focus on surface symptoms of hallucination and heavily rely on post-hoc corrections, extensive data curation, or costly inference schemes. In this work, we identify two key factors of MLLM hallucination: Insufficient Visual Context, where ambiguous visual contexts lead to language speculation, and Progressive Textual Drift, where model attention strays from visual inputs in longer responses. To address these problems, we propose a novel Complementary Visual Grounding (CVG) framework. CVG exploits the intrinsic architecture of MLLMs, without requiring any external tools, models, or additional data. CVG first disentangles visual context into two complementary branches based on query relevance, then maintains steadfast visual grounding during the auto-regressive generation. Finally, it contrasts the output distributions of two branches to produce a faithful response. Extensive experiments on various hallucination and general benchmarks demonstrate that CVG achieves state-of-the-art performances across MLLM architectures and scales.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.818 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zheren Fu; Zhendong Mao; Lei Zhang; Yongdong Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644140"&gt;10.1109/tip.2025.3644140&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.818 (must_read)&lt;/p&gt;
&lt;p&gt;Multimodal Large Language Models (MLLMs) exhibit impressive performance across vision-language tasks, but still face the hallucination challenges, where generated texts are factually inconsistent with visual input. Existing mitigation methods focus on surface symptoms of hallucination and heavily rely on post-hoc corrections, extensive data curation, or costly inference schemes. In this work, we identify two key factors of MLLM hallucination: Insufficient Visual Context, where ambiguous visual contexts lead to language speculation, and Progressive Textual Drift, where model attention strays from visual inputs in longer responses. To address these problems, we propose a novel Complementary Visual Grounding (CVG) framework. CVG exploits the intrinsic architecture of MLLMs, without requiring any external tools, models, or additional data. CVG first disentangles visual context into two complementary branches based on query relevance, then maintains steadfast visual grounding during the auto-regressive generation. Finally, it contrasts the output distributions of two branches to produce a faithful response. Extensive experiments on various hallucination and general benchmarks demonstrate that CVG achieves state-of-the-art performances across MLLM architectures and scales.&lt;/p&gt;</content:encoded></item><item><title>E
                    &lt;sup&gt;2&lt;/sup&gt;
                    MPL: An Enduring and Efficient Meta Prompt Learning Framework for Few-shot Unsupervised Domain Adaptation</title><link>https://doi.org/10.1109/tip.2025.3645560</link><guid>10.1109/tip.2025.3645560</guid><pubDate>Tue, 23 Dec 2025 18:32:49 +0000</pubDate><dc:creator>Wanqi Yang</dc:creator><dc:creator>Haoran Wang</dc:creator><dc:creator>Wei Wang</dc:creator><dc:creator>Lei Wang</dc:creator><dc:creator>Ge Song</dc:creator><dc:creator>Ming Yang</dc:creator><dc:creator>Yang Gao</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3645560</prism:doi><description>Few-shot unsupervised domain adaptation (FS-UDA) leverages a limited amount of labeled data from a source domain to enable accurate classification in an unlabeled target domain. Despite recent advancements, current approaches of FS-UDA continue to confront a major challenge: models often demonstrate instability when adapted to new FS-UDA tasks and necessitate considerable time investment. To address these challenges, we put forward a novel framework called Enduring and Efficient Meta-Prompt Learning (E2MPL) for FS-UDA. Within this framework, we utilize the pre-trained CLIP model as the backbone of feature learning. Firstly, we design domain-shared prompts, consisting of virtual tokens, which primarily capture meta-knowledge from a wide range of meta-tasks to mitigate the domain gaps. Secondly, we develop a task prompt learning network that adaptively learns task-specific prompts with the goal of achieving fast and stable task generalization. Thirdly, we formulate the meta-prompt learning process as a bilevel optimization problem, consisting of (outer) meta-prompt learner and (inner) task-specific classifier and domain adapter. Also, the inner objective of each meta-task has the closed-form solution, which enables efficient prompt learning and adaptation to new tasks in a single step. Extensive experimental studies demonstrate the promising performance of our framework in a domain adaptation benchmark dataset DomainNet. Compared with state-of-the-art methods, our approach has improved the average accuracy by at least 15 percentage points and reduces the average time by 64.67% in the 5-way 1-shot task; in the 5-way 5-shot task, it achieves at least a 9-percentage-point improvement in average accuracy and reduces the average time by 63.18%. Moreover, our method exhibits more enduring and stable performance than the other methods, i.e., reducing the average IQR value by over 40.80% and 25.35% in the 5-way 1-shot and 5-shot task, respectively.
Published: 2025-12-23T18:32:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wanqi Yang; Haoran Wang; Wei Wang; Lei Wang; Ge Song; Ming Yang; Yang Gao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3645560"&gt;10.1109/tip.2025.3645560&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;Few-shot unsupervised domain adaptation (FS-UDA) leverages a limited amount of labeled data from a source domain to enable accurate classification in an unlabeled target domain. Despite recent advancements, current approaches of FS-UDA continue to confront a major challenge: models often demonstrate instability when adapted to new FS-UDA tasks and necessitate considerable time investment. To address these challenges, we put forward a novel framework called Enduring and Efficient Meta-Prompt Learning (E2MPL) for FS-UDA. Within this framework, we utilize the pre-trained CLIP model as the backbone of feature learning. Firstly, we design domain-shared prompts, consisting of virtual tokens, which primarily capture meta-knowledge from a wide range of meta-tasks to mitigate the domain gaps. Secondly, we develop a task prompt learning network that adaptively learns task-specific prompts with the goal of achieving fast and stable task generalization. Thirdly, we formulate the meta-prompt learning process as a bilevel optimization problem, consisting of (outer) meta-prompt learner and (inner) task-specific classifier and domain adapter. Also, the inner objective of each meta-task has the closed-form solution, which enables efficient prompt learning and adaptation to new tasks in a single step. Extensive experimental studies demonstrate the promising performance of our framework in a domain adaptation benchmark dataset DomainNet. Compared with state-of-the-art methods, our approach has improved the average accuracy by at least 15 percentage points and reduces the average time by 64.67% in the 5-way 1-shot task; in the 5-way 5-shot task, it achieves at least a 9-percentage-point improvement in average accuracy and reduces the average time by 63.18%. Moreover, our method exhibits more enduring and stable performance than the other methods, i.e., reducing the average IQR value by over 40.80% and 25.35% in the 5-way 1-shot and 5-shot task, respectively.&lt;/p&gt;</content:encoded></item><item><title>PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation</title><link>https://doi.org/10.1109/tip.2025.3644785</link><guid>10.1109/tip.2025.3644785</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Mengyuan Liu</dc:creator><dc:creator>Jiajie Liu</dc:creator><dc:creator>Jinyan Zhang</dc:creator><dc:creator>Wenhao Li</dc:creator><dc:creator>Junsong Yuan</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644785</prism:doi><description>The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.814 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Mengyuan Liu; Jiajie Liu; Jinyan Zhang; Wenhao Li; Junsong Yuan&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644785"&gt;10.1109/tip.2025.3644785&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.814 (must_read)&lt;/p&gt;
&lt;p&gt;The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.&lt;/p&gt;</content:encoded></item><item><title>A Comprehensive Survey and Taxonomy of Mamba: Applications, Challenges, and Future Directions</title><link>https://doi.org/10.1016/j.inffus.2025.104094</link><guid>10.1016/j.inffus.2025.104094</guid><pubDate>Mon, 22 Dec 2025 07:49:44 +0000</pubDate><dc:creator>Qiguang Miao</dc:creator><dc:creator>Linxing Jia</dc:creator><dc:creator>Kun Xie</dc:creator><dc:creator>Kaiyuan Fu</dc:creator><dc:creator>Zongkai Yang</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104094</prism:doi><description>Transformer-based architectures have achieved notable success across natural language processing, computer vision, and multimodal learning, yet they face persistent challenges such as high computational complexity and limited adaptability to dynamic environments. State Space Models (SSMs) have emerged as a competitive alternative, offering linear-time complexity and the ability to implicitly capture long-range dependencies. Building on this foundation, the Mamba model introduces time-varying parameterization to dynamically adjust state transitions based on input context, combined with selective state updates, content-aware scanning strategies, and hardware-efficient design. These innovations enable Mamba to maintain linear complexity while delivering higher throughput and significantly reduced memory consumption compared to both Transformer-based and conventional SSM architectures. This survey systematically reviews the theoretical foundations, architectural innovations, and application progress of the Mamba model. First, we trace the evolution of SSMs, highlighting the key design principles that underpin Mamba’s dynamic state transition and selective computation mechanisms. Second, we summarize Mamba’s structural innovations in modeling dynamics and multimodal fusion, categorizing its applications across multiple modalities, including vision, speech, point clouds, and multimodal data. Finally, we evaluate representative applications in medical image analysis, recommendation systems, reinforcement learning, and generative modeling, identifying advantages, limitations, and open challenges. The review concludes by outlining future research directions focused on improving generalization, causal reasoning, interpretability, and computational efficiency. This work aims to provide a concise yet comprehensive reference for researchers and practitioners, promoting further development and deployment of Mamba-based architectures across diverse real-world scenarios.
Published: 2025-12-22T07:49:44+00:00
Venue: Information Fusion
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qiguang Miao; Linxing Jia; Kun Xie; Kaiyuan Fu; Zongkai Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104094"&gt;10.1016/j.inffus.2025.104094&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;Transformer-based architectures have achieved notable success across natural language processing, computer vision, and multimodal learning, yet they face persistent challenges such as high computational complexity and limited adaptability to dynamic environments. State Space Models (SSMs) have emerged as a competitive alternative, offering linear-time complexity and the ability to implicitly capture long-range dependencies. Building on this foundation, the Mamba model introduces time-varying parameterization to dynamically adjust state transitions based on input context, combined with selective state updates, content-aware scanning strategies, and hardware-efficient design. These innovations enable Mamba to maintain linear complexity while delivering higher throughput and significantly reduced memory consumption compared to both Transformer-based and conventional SSM architectures. This survey systematically reviews the theoretical foundations, architectural innovations, and application progress of the Mamba model. First, we trace the evolution of SSMs, highlighting the key design principles that underpin Mamba’s dynamic state transition and selective computation mechanisms. Second, we summarize Mamba’s structural innovations in modeling dynamics and multimodal fusion, categorizing its applications across multiple modalities, including vision, speech, point clouds, and multimodal data. Finally, we evaluate representative applications in medical image analysis, recommendation systems, reinforcement learning, and generative modeling, identifying advantages, limitations, and open challenges. The review concludes by outlining future research directions focused on improving generalization, causal reasoning, interpretability, and computational efficiency. This work aims to provide a concise yet comprehensive reference for researchers and practitioners, promoting further development and deployment of Mamba-based architectures across diverse real-world scenarios.&lt;/p&gt;</content:encoded></item><item><title>Adaptive Cross-Scale Feature Aggregation for Few-shot Object Detection</title><link>https://doi.org/10.1016/j.neucom.2025.132514</link><guid>10.1016/j.neucom.2025.132514</guid><pubDate>Mon, 22 Dec 2025 16:56:56 +0000</pubDate><dc:creator>Anni Wang</dc:creator><dc:creator>Penglin Zhang</dc:creator><dc:creator>Jinhan Li</dc:creator><dc:creator>Jian Chao</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132514</prism:doi><description>In recent years, deep learning algorithms have achieved remarkable success across a wide range of applications. However, high-performance models generally require large quantities of annotated data to achieve optimal results, and in many practical scenarios, obtaining high-quality labeled samples remains a significant challenge, limiting the applicability of deep learning techniques to object detection tasks. Conventional deep learning approaches to object detection heavily rely on visual features extracted from query images to generate region proposals. Consequently, these methods struggle to meet detection requirements in complex environments, especially when confronted with newly introduced categories. To address these limitations, recent research efforts have shifted toward few-shot and zero-shot detection strategies. These emerging approaches enable the recognition of unseen objects in new domains using a relatively small number of annotated examples. Such methods typically employ self-supervised learning mechanisms to extract features without relying on predefined category-specific knowledge, which significantly enhances cross-domain generalization performance. Inspired by this paradigm, this paper proposes a dual-branch cross-domain adaptive object detection algorithm. The proposed method introduces a multi-scale cross-branch feature extraction module designed to enhance the model’s self-supervised learning capabilities. Furthermore, it incorporates a support branch feature aggregation module, which provides effective guidance for both location and category predictions in the query branch. This design enables accurate cross-domain adaptive learning. To validate the effectiveness of the proposed algorithm, comparative experiments were conducted on publicly available datasets using state-of-the-art detection methods as baseline models. The experimental results demonstrate that the proposed approach achieves superior performance on cross-domain object detection tasks.
Published: 2025-12-22T16:56:56+00:00
Venue: Neurocomputing
Score: 0.810 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Anni Wang; Penglin Zhang; Jinhan Li; Jian Chao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132514"&gt;10.1016/j.neucom.2025.132514&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.810 (must_read)&lt;/p&gt;
&lt;p&gt;In recent years, deep learning algorithms have achieved remarkable success across a wide range of applications. However, high-performance models generally require large quantities of annotated data to achieve optimal results, and in many practical scenarios, obtaining high-quality labeled samples remains a significant challenge, limiting the applicability of deep learning techniques to object detection tasks. Conventional deep learning approaches to object detection heavily rely on visual features extracted from query images to generate region proposals. Consequently, these methods struggle to meet detection requirements in complex environments, especially when confronted with newly introduced categories. To address these limitations, recent research efforts have shifted toward few-shot and zero-shot detection strategies. These emerging approaches enable the recognition of unseen objects in new domains using a relatively small number of annotated examples. Such methods typically employ self-supervised learning mechanisms to extract features without relying on predefined category-specific knowledge, which significantly enhances cross-domain generalization performance. Inspired by this paradigm, this paper proposes a dual-branch cross-domain adaptive object detection algorithm. The proposed method introduces a multi-scale cross-branch feature extraction module designed to enhance the model’s self-supervised learning capabilities. Furthermore, it incorporates a support branch feature aggregation module, which provides effective guidance for both location and category predictions in the query branch. This design enables accurate cross-domain adaptive learning. To validate the effectiveness of the proposed algorithm, comparative experiments were conducted on publicly available datasets using state-of-the-art detection methods as baseline models. The experimental results demonstrate that the proposed approach achieves superior performance on cross-domain object detection tasks.&lt;/p&gt;</content:encoded></item><item><title>LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation</title><link>https://arxiv.org/abs/2512.20217v1</link><guid>http://arxiv.org/abs/2512.20217v1</guid><pubDate>Tue, 23 Dec 2025 10:16:33 +0000</pubDate><dc:creator>Xiangxuan Ren</dc:creator><dc:creator>Zhongdao Wang</dc:creator><dc:creator>Pin Tang</dc:creator><dc:creator>Guoqing Wang</dc:creator><dc:creator>Jilai Zheng</dc:creator><dc:creator>Chao Ma</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.
Published: 2025-12-23T10:16:33+00:00
Venue: arXiv
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xiangxuan Ren; Zhongdao Wang; Pin Tang; Guoqing Wang; Jilai Zheng; Chao Ma&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.&lt;/p&gt;</content:encoded></item><item><title>Elaborate Feature Decoupling for Weakly Supervised Fine-Grained Object Detection in Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3647662</link><guid>10.1109/tgrs.2025.3647662</guid><pubDate>Tue, 23 Dec 2025 18:31:04 +0000</pubDate><dc:creator>Xi Yang</dc:creator><dc:creator>Zhongyuan Zhou</dc:creator><dc:creator>Dong Yang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647662</prism:doi><description>Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.
Published: 2025-12-23T18:31:04+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.803 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Xi Yang; Zhongyuan Zhou; Dong Yang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647662"&gt;10.1109/tgrs.2025.3647662&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.803 (must_read)&lt;/p&gt;
&lt;p&gt;Currently, low-resolution fine-grained remote sensing images (RSIs) greatly affect the performance of object detectors. Meanwhile, neither weakly supervised object detection (WSOD) nor fine-grained object detection (FGOD) methods can simultaneously solve the realistic problem of dependency on top-scoring proposals during the detector training faced by WSOD, and the imbalance between fine-grained classification and localization tasks faced by FGOD. To address these issues, this paper proposes a novel Elaborate Feature Decouple Network (EFDNet), which is one of the first end-to-end frameworks to perform weakly supervised fine-grained object detection (WSFGOD) in RSIs. Specifically, a lightweight multi-order degradation (LMD) module is introduced to better simulate complex real-world degradations, thus obtaining high-resolution image features by a modular connection method of multi-stage feature supplementation. Our adaptive contextual perception refinement (ACPR) module aims to adaptively shift the attention of the detection network from the local feature part to the whole object by integrating local and global contextual information. Finally, we propose a feature decoupled head (FDH) module to handle the fine-grained classification and localization tasks by the classification branch (CB) and localization branch (LB), respectively. Among FDH, CB provides rich semantic information for the classification task, while LB provides more detailed texture and edge information to delineate object boundaries accurately. Extensive experiments on the challenging FAIR1M-v1.0 and ShipRSImageNet datasets demonstrate that our proposed method achieves state-of-the-art performance and is highly effective in addressing multi-scale object issues.&lt;/p&gt;</content:encoded></item><item><title>Unlocking Cross-Domain Synergies for Domain Adaptive Semantic Segmentation</title><link>https://doi.org/10.1109/tip.2025.3645599</link><guid>10.1109/tip.2025.3645599</guid><pubDate>Tue, 23 Dec 2025 18:32:49 +0000</pubDate><dc:creator>Qin Xu</dc:creator><dc:creator>Qihang Wu</dc:creator><dc:creator>Bo Jiang</dc:creator><dc:creator>Jiahui Wang</dc:creator><dc:creator>Yuan Chen</dc:creator><dc:creator>Jinhui Tang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3645599</prism:doi><description>Unsupervised domain adaptation semantic segmentation (UDASS) aims to perform dense prediction on the unlabeled target domain by training the model on a labeled source domain. In this field, self-training approaches have demonstrated strong competitiveness and advantages. However, existing methods often rely on additional training data (such as reference datasets or depth maps) to rectify the unreliable pseudo-labels, ignoring the cross-domain interaction between the target and source domains. To address this issue, in this paper, we propose a novel method for unsupervised domain adaptation semantic segmentation, termed Unlocking Cross-Domain Synergies (UCDS). Specifically, in the UCDS network, we design a new Dynamic Self-Correction (DSC) module that effectively transfers source domain knowledge and generates high-confidence pseudolabels without additional training resources. Unlike the existing methods, DSC proposes a Dynamic Noisy Label Detection method for the target domain. To correct the noisy pseudo-labels, we design a Dual Bank mechanism that explores the reliable and unreliable predictions of the source domain, and conducts cross-domain synergy through Weighted Reassignment Self-Correction and Negative Correction Prevention strategies. To enhance the discriminative ability of features and amplify the dissimilarity of different categories, we propose Discrepancy-based Contrastive Learning (DCL). The DCL selects positive and negative samples in the source and target domains based on the semantic discrepancies among different categories, effectively avoiding the numerous false negative samples found in existing methods. Extensive experimental results on three commonly used datasets demonstrate the superiority of the proposed UCDS in comparison with the state-of-the-art methods. The project and code are available at https://github.com/wqh011128/UCDS.
Published: 2025-12-23T18:32:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.801 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Qin Xu; Qihang Wu; Bo Jiang; Jiahui Wang; Yuan Chen; Jinhui Tang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3645599"&gt;10.1109/tip.2025.3645599&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.801 (must_read)&lt;/p&gt;
&lt;p&gt;Unsupervised domain adaptation semantic segmentation (UDASS) aims to perform dense prediction on the unlabeled target domain by training the model on a labeled source domain. In this field, self-training approaches have demonstrated strong competitiveness and advantages. However, existing methods often rely on additional training data (such as reference datasets or depth maps) to rectify the unreliable pseudo-labels, ignoring the cross-domain interaction between the target and source domains. To address this issue, in this paper, we propose a novel method for unsupervised domain adaptation semantic segmentation, termed Unlocking Cross-Domain Synergies (UCDS). Specifically, in the UCDS network, we design a new Dynamic Self-Correction (DSC) module that effectively transfers source domain knowledge and generates high-confidence pseudolabels without additional training resources. Unlike the existing methods, DSC proposes a Dynamic Noisy Label Detection method for the target domain. To correct the noisy pseudo-labels, we design a Dual Bank mechanism that explores the reliable and unreliable predictions of the source domain, and conducts cross-domain synergy through Weighted Reassignment Self-Correction and Negative Correction Prevention strategies. To enhance the discriminative ability of features and amplify the dissimilarity of different categories, we propose Discrepancy-based Contrastive Learning (DCL). The DCL selects positive and negative samples in the source and target domains based on the semantic discrepancies among different categories, effectively avoiding the numerous false negative samples found in existing methods. Extensive experimental results on three commonly used datasets demonstrate the superiority of the proposed UCDS in comparison with the state-of-the-art methods. The project and code are available at https://github.com/wqh011128/UCDS.&lt;/p&gt;</content:encoded></item><item><title>Cross-Frequency Attention and Color Contrast Constraint for Remote Sensing Dehazing</title><link>https://doi.org/10.1109/tip.2025.3644167</link><guid>10.1109/tip.2025.3644167</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Yuxin Feng</dc:creator><dc:creator>Jufeng Li</dc:creator><dc:creator>Tao Huang</dc:creator><dc:creator>Fangfang Wu</dc:creator><dc:creator>Yakun Ju</dc:creator><dc:creator>Chunxu Li</dc:creator><dc:creator>Weisheng Dong</dc:creator><dc:creator>Alex C. Kot</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644167</prism:doi><description>Current deep learning-based methods for remote sensing image dehazing have developed rapidly, yet they still commonly struggle to simultaneously preserve fine texture details and restore accurate colors. The fundamental reason lies in the insufficient modeling of high-frequency information that captures structural details, as well as the lack of effective constraints for color restoration. To address the insufficient modeling of global high-frequency information, we first develop an omni-directional high-frequency feature inpainting mechanism that leverages the wavelet transform to extract multi-directional high-frequency components. While maintaining the advantage of linear complexity, it models global long-range texture dependencies through cross-frequency perception. Then, to further strengthen local high-frequency representation, we design a high-frequency prompt attention module that dynamically injects wavelet-domain optimized high-frequency features as cross-level guidance signals, significantly enhancing the model’s capability in edge sharpness restoration and texture detail reconstruction. Further, to alleviate the problem of inaccurate color restoration, we propose a color contrast loss function based on the HSV color space, which explicitly models the statistical distribution differences of brightness and saturation in hazy regions, guiding the model to generate dehazed images with consistent colors and natural visual appearance. Finally, extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing approaches in both texture detail restoration and color consistency. Further results and code available at: https://github.com/fyxnl/C4RSD.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.796 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuxin Feng; Jufeng Li; Tao Huang; Fangfang Wu; Yakun Ju; Chunxu Li; Weisheng Dong; Alex C. Kot&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644167"&gt;10.1109/tip.2025.3644167&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.796 (must_read)&lt;/p&gt;
&lt;p&gt;Current deep learning-based methods for remote sensing image dehazing have developed rapidly, yet they still commonly struggle to simultaneously preserve fine texture details and restore accurate colors. The fundamental reason lies in the insufficient modeling of high-frequency information that captures structural details, as well as the lack of effective constraints for color restoration. To address the insufficient modeling of global high-frequency information, we first develop an omni-directional high-frequency feature inpainting mechanism that leverages the wavelet transform to extract multi-directional high-frequency components. While maintaining the advantage of linear complexity, it models global long-range texture dependencies through cross-frequency perception. Then, to further strengthen local high-frequency representation, we design a high-frequency prompt attention module that dynamically injects wavelet-domain optimized high-frequency features as cross-level guidance signals, significantly enhancing the model’s capability in edge sharpness restoration and texture detail reconstruction. Further, to alleviate the problem of inaccurate color restoration, we propose a color contrast loss function based on the HSV color space, which explicitly models the statistical distribution differences of brightness and saturation in hazy regions, guiding the model to generate dehazed images with consistent colors and natural visual appearance. Finally, extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing approaches in both texture detail restoration and color consistency. Further results and code available at: https://github.com/fyxnl/C4RSD.&lt;/p&gt;</content:encoded></item><item><title>A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation</title><link>https://doi.org/10.1109/tip.2025.3644789</link><guid>10.1109/tip.2025.3644789</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Jianghao Wu</dc:creator><dc:creator>Xiangde Luo</dc:creator><dc:creator>Yubo Zhou</dc:creator><dc:creator>Lianming Wu</dc:creator><dc:creator>Guotai Wang</dc:creator><dc:creator>Shaoting Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644789</prism:doi><description>Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose A3-TTA, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.794 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jianghao Wu; Xiangde Luo; Yubo Zhou; Lianming Wu; Guotai Wang; Shaoting Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644789"&gt;10.1109/tip.2025.3644789&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.794 (must_read)&lt;/p&gt;
&lt;p&gt;Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose A3-TTA, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.&lt;/p&gt;</content:encoded></item><item><title>CCSFuse: Collaborative Compensation and Selective Fusion for UAV-based RGB-IR Object Detection</title><link>https://doi.org/10.1109/tgrs.2025.3647293</link><guid>10.1109/tgrs.2025.3647293</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Tao Zhang</dc:creator><dc:creator>Ruitao Lu</dc:creator><dc:creator>Xiaogang Yang</dc:creator><dc:creator>Dingwen Zhang</dc:creator><dc:creator>Yansheng Li</dc:creator><dc:creator>Xueli Xie</dc:creator><dc:creator>Yunsong Li</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647293</prism:doi><description>Visible-Infrared (RGB-IR) object detection plays a crucial role in UAV-based vision tasks. However, existing methods still suffer from learning bias caused by imbalanced information distribution and inaccurate fusion due to modal conflicts. Inspired by the human multisensory information processing mechanism, we propose a novel “CompensationFusion” progressive detection framework, CCSFuse, to fully exploit the complementary relationship between modalities while eliminating conflict interference. Specifically, we design a cross-modal feature compensation module, which establishes inter-modal information interaction to achieve mutual complementarity and enhancement during feature extraction, effectively mitigating the issue of imbalanced modal information distribution. Additionally, we introduce an adaptive feature-selection fusion module to address modal conflicts. We employ a cross-modal channel attention to calibrate channel features of different modalities and utilizes a selective fusion strategy to dynamically assess modal importance, thereby achieving adaptive modal fusion. Finally, we validate the effectiveness of CCSFuse on the DroneVehicle and LLVIP datasets. The results confirm that CCSFuse significantly improves the efficiency of feature optimization and integration. In UAV-based object detection scenarios, CCSFuse outperforms state-of-the-art methods in both qualitative and quantitative comparisons, particularly for small objects and low-quality modalities. The code is available at https://github.com/ZhangT-xxl/CCSFuse.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tao Zhang; Ruitao Lu; Xiaogang Yang; Dingwen Zhang; Yansheng Li; Xueli Xie; Yunsong Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647293"&gt;10.1109/tgrs.2025.3647293&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Visible-Infrared (RGB-IR) object detection plays a crucial role in UAV-based vision tasks. However, existing methods still suffer from learning bias caused by imbalanced information distribution and inaccurate fusion due to modal conflicts. Inspired by the human multisensory information processing mechanism, we propose a novel “CompensationFusion” progressive detection framework, CCSFuse, to fully exploit the complementary relationship between modalities while eliminating conflict interference. Specifically, we design a cross-modal feature compensation module, which establishes inter-modal information interaction to achieve mutual complementarity and enhancement during feature extraction, effectively mitigating the issue of imbalanced modal information distribution. Additionally, we introduce an adaptive feature-selection fusion module to address modal conflicts. We employ a cross-modal channel attention to calibrate channel features of different modalities and utilizes a selective fusion strategy to dynamically assess modal importance, thereby achieving adaptive modal fusion. Finally, we validate the effectiveness of CCSFuse on the DroneVehicle and LLVIP datasets. The results confirm that CCSFuse significantly improves the efficiency of feature optimization and integration. In UAV-based object detection scenarios, CCSFuse outperforms state-of-the-art methods in both qualitative and quantitative comparisons, particularly for small objects and low-quality modalities. The code is available at https://github.com/ZhangT-xxl/CCSFuse.&lt;/p&gt;</content:encoded></item><item><title>A Cosine Network for Image Super-Resolution</title><link>https://doi.org/10.1109/tip.2025.3645630</link><guid>10.1109/tip.2025.3645630</guid><pubDate>Tue, 23 Dec 2025 18:32:49 +0000</pubDate><dc:creator>Chunwei Tian</dc:creator><dc:creator>Chengyuan Zhang</dc:creator><dc:creator>Bob Zhang</dc:creator><dc:creator>Zhiwu Li</dc:creator><dc:creator>C. L. Philip Chen</dc:creator><dc:creator>David Zhang</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3645630</prism:doi><description>Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.
Published: 2025-12-23T18:32:49+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.793 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunwei Tian; Chengyuan Zhang; Bob Zhang; Zhiwu Li; C. L. Philip Chen; David Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3645630"&gt;10.1109/tip.2025.3645630&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.793 (must_read)&lt;/p&gt;
&lt;p&gt;Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.&lt;/p&gt;</content:encoded></item><item><title>Learning to Reason in LLMs by Expectation Maximization</title><link>https://arxiv.org/abs/2512.20169v1</link><guid>http://arxiv.org/abs/2512.20169v1</guid><pubDate>Tue, 23 Dec 2025 08:56:49 +0000</pubDate><dc:creator>Junghyun Lee</dc:creator><dc:creator>Branislav Kveton</dc:creator><dc:creator>Sunav Choudhary</dc:creator><dc:creator>Subhojyoti Mukherjee</dc:creator><dc:creator>Anup Rao</dc:creator><dc:creator>Ryan A. Rossi</dc:creator><dc:creator>Alexa Siu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.
Published: 2025-12-23T08:56:49+00:00
Venue: arXiv
Score: 0.792 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Junghyun Lee; Branislav Kveton; Sunav Choudhary; Subhojyoti Mukherjee; Anup Rao; Ryan A. Rossi; Alexa Siu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.792 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.&lt;/p&gt;</content:encoded></item><item><title>DirMixE: Harnessing Test Agnostic Long-tail Recognition with Hierarchical Label Vartiations</title><link>https://doi.org/10.1109/tpami.2025.3647124</link><guid>10.1109/tpami.2025.3647124</guid><pubDate>Tue, 23 Dec 2025 18:31:03 +0000</pubDate><dc:creator>Zhiyong Yang</dc:creator><dc:creator>Qianqian Xu</dc:creator><dc:creator>Sicong Li</dc:creator><dc:creator>Zitai Wang</dc:creator><dc:creator>Xiaochun Cao</dc:creator><dc:creator>Qingming Huang</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647124</prism:doi><description>This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced. We argue that the variation in these distributions can be broken down hierarchically into global and local levels. The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor. Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations. However, the local variations are left unconsidered. To address this issue, we propose a new MoE strategy, \mathsf {DirMixE} \mathsf {DirMixE} , which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations. Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations. This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes. Building on this idea, we develop a general Latent Skill Finetuning (LSF) framework for parameter-efficient finetuning of foundation models. We provide implementations based on LoRA and Adapter. Theoretically, we derive upper bounds on the generalization error for both standard learning and PEFT. Under mild assumptions, we show that the variance-based regularization helps tighten these bounds. Furthermore, we prove that the covering number of the PEFT hypothesis class scales with the number of trainable parameters. Finally, extensive experiments on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist validate the effectiveness of \mathsf {DirMixE} \mathsf {DirMixE} .
Published: 2025-12-23T18:31:03+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Zhiyong Yang; Qianqian Xu; Sicong Li; Zitai Wang; Xiaochun Cao; Qingming Huang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647124"&gt;10.1109/tpami.2025.3647124&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;This paper explores test-agnostic long-tail recognition, a challenging long-tail task where the test label distributions are unknown and arbitrarily imbalanced. We argue that the variation in these distributions can be broken down hierarchically into global and local levels. The global ones reflect a broad range of diversity, while the local ones typically arise from milder changes, often focused on a particular neighbor. Traditional methods predominantly use a Mixture-of-Expert (MoE) approach, targeting a few fixed test label distributions that exhibit substantial global variations. However, the local variations are left unconsidered. To address this issue, we propose a new MoE strategy, \mathsf {DirMixE} \mathsf {DirMixE} , which assigns experts to different Dirichlet meta-distributions of the label distribution, each targeting a specific aspect of local variations. Additionally, the diversity among these Dirichlet meta-distributions inherently captures global variations. This dual-level approach also leads to a more stable objective function, allowing us to sample different test distributions better to quantify the mean and variance of performance outcomes. Building on this idea, we develop a general Latent Skill Finetuning (LSF) framework for parameter-efficient finetuning of foundation models. We provide implementations based on LoRA and Adapter. Theoretically, we derive upper bounds on the generalization error for both standard learning and PEFT. Under mild assumptions, we show that the variance-based regularization helps tighten these bounds. Furthermore, we prove that the covering number of the PEFT hypothesis class scales with the number of trainable parameters. Finally, extensive experiments on CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist validate the effectiveness of \mathsf {DirMixE} \mathsf {DirMixE} .&lt;/p&gt;</content:encoded></item><item><title>TransAUAV: A Transformer-Enhanced RGB-Infrared Fusion Network for Anti-UAV Detection</title><link>https://doi.org/10.1109/taes.2025.3646996</link><guid>10.1109/taes.2025.3646996</guid><pubDate>Tue, 23 Dec 2025 18:32:43 +0000</pubDate><dc:creator>Chenyang Li</dc:creator><dc:creator>Suiping Zhou</dc:creator><dc:creator>Zhiheng Liu</dc:creator><dc:creator>Wenjie Zhang</dc:creator><dc:creator>Ting Wu</dc:creator><prism:publicationName>IEEE Transactions on Aerospace and Electronic Systems</prism:publicationName><prism:doi>10.1109/taes.2025.3646996</prism:doi><description>To improve the accuracy and robustness of anti-UAV detection, this study introduces TransAUAV, a Transformer-based RGB- infrared image fusion detection network. This approach strengthens object feature representation by leveraging multi-modal data fusion and self-attention mechanisms. Specifically, 1) we propose a multi-modal attention fusion module, which enhances the complementarity of RGB and infrared image features through a dual-path attention mechanism; 2) we propose a cross-layer multi-scale Transformer module, which improves detection performance by extracting multi-scale features and facilitating cross-modal information interaction; 3) we propose a texture information focus module, which enhances the representation of local texture details. Additionally, this paper designs a hybrid loss function to improve feature discrimination capability and training efficiency. Experimental results on anti-UAV and Drone-detection datasets show that TransAUAV outperforms state-of-the-art methods on all evaluation metrics.
Published: 2025-12-23T18:32:43+00:00
Venue: IEEE Transactions on Aerospace and Electronic Systems
Score: 0.791 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chenyang Li; Suiping Zhou; Zhiheng Liu; Wenjie Zhang; Ting Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Aerospace and Electronic Systems&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/taes.2025.3646996"&gt;10.1109/taes.2025.3646996&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.791 (must_read)&lt;/p&gt;
&lt;p&gt;To improve the accuracy and robustness of anti-UAV detection, this study introduces TransAUAV, a Transformer-based RGB- infrared image fusion detection network. This approach strengthens object feature representation by leveraging multi-modal data fusion and self-attention mechanisms. Specifically, 1) we propose a multi-modal attention fusion module, which enhances the complementarity of RGB and infrared image features through a dual-path attention mechanism; 2) we propose a cross-layer multi-scale Transformer module, which improves detection performance by extracting multi-scale features and facilitating cross-modal information interaction; 3) we propose a texture information focus module, which enhances the representation of local texture details. Additionally, this paper designs a hybrid loss function to improve feature discrimination capability and training efficiency. Experimental results on anti-UAV and Drone-detection datasets show that TransAUAV outperforms state-of-the-art methods on all evaluation metrics.&lt;/p&gt;</content:encoded></item><item><title>SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction</title><link>https://doi.org/10.1016/j.inffus.2025.104074</link><guid>10.1016/j.inffus.2025.104074</guid><pubDate>Tue, 23 Dec 2025 16:57:18 +0000</pubDate><dc:creator>Yihao Ding</dc:creator><dc:creator>Soyeon Caren Han</dc:creator><dc:creator>Zechuan Li</dc:creator><dc:creator>Hyunsuk Chung</dc:creator><prism:publicationName>Information Fusion</prism:publicationName><prism:doi>10.1016/j.inffus.2025.104074</prism:doi><description>Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.
Published: 2025-12-23T16:57:18+00:00
Venue: Information Fusion
Score: 0.790 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yihao Ding; Soyeon Caren Han; Zechuan Li; Hyunsuk Chung&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Information Fusion&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.inffus.2025.104074"&gt;10.1016/j.inffus.2025.104074&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.790 (must_read)&lt;/p&gt;
&lt;p&gt;Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes SynJAC (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.&lt;/p&gt;</content:encoded></item><item><title>On the Universality of Transformer Architectures; How Much Attention Is Enough?</title><link>https://arxiv.org/abs/2512.18445v1</link><guid>http://arxiv.org/abs/2512.18445v1</guid><pubDate>Sat, 20 Dec 2025 17:31:59 +0000</pubDate><dc:creator>Amirreza Abbasi</dc:creator><dc:creator>Mohsen Hooshmand</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Transformers are crucial across many AI fields, such as large language models, computer vision, and reinforcement learning. This prominence stems from the architecture's perceived universality and scalability compared to alternatives. This work examines the problem of universality in Transformers, reviews recent progress, including architectural refinements such as structural minimality and approximation rates, and surveys state-of-the-art advances that inform both theoretical and practical understanding. Our aim is to clarify what is currently known about Transformers expressiveness, separate robust guarantees from fragile ones, and identify key directions for future theoretical research.
Published: 2025-12-20T17:31:59+00:00
Venue: arXiv
Score: 0.788 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Amirreza Abbasi; Mohsen Hooshmand&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.788 (must_read)&lt;/p&gt;
&lt;p&gt;Transformers are crucial across many AI fields, such as large language models, computer vision, and reinforcement learning. This prominence stems from the architecture&amp;#x27;s perceived universality and scalability compared to alternatives. This work examines the problem of universality in Transformers, reviews recent progress, including architectural refinements such as structural minimality and approximation rates, and surveys state-of-the-art advances that inform both theoretical and practical understanding. Our aim is to clarify what is currently known about Transformers expressiveness, separate robust guarantees from fragile ones, and identify key directions for future theoretical research.&lt;/p&gt;</content:encoded></item><item><title>HCMA-Net: Hierarchical Cross-Modality Aggregation Network for Multimodal Remote Sensing Image Classification</title><link>https://doi.org/10.1109/tgrs.2025.3646806</link><guid>10.1109/tgrs.2025.3646806</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Wenping Ma</dc:creator><dc:creator>Hekai Zhang</dc:creator><dc:creator>Mengru Ma</dc:creator><dc:creator>Boyou Xue</dc:creator><dc:creator>Hao Zhu</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646806</prism:doi><description>While multimodal remote sensing images provide complementary information in different imaging ways, effectively aggregating and jointly learning from these heterogeneous features is non-trivial, primarily due to the inherent modality gap that hinders semantic alignment and feature interoperability. To overcome these issues, we propose a Hierarchical Cross-Modality Aggregation Network (HCMA-Net). It introduces two novel components: the hierarchical feature aggregation and enhancement module (HFAE-Module) and the cross-modality interactive feature extraction module (CMIFE-Module). The HFAE-Module tackles the modality gap and enables cross-scale interaction through its Hierarchical Cross-Modality Feature Aggregation (HCMFA) mechanism, which incorporates Cross-Spectral and Spatial Aggregation Non-Local Attention layers (CSNLA and SANLA) to align features and aggregate contextual information across spectral and spatial dimensions. The CMIFE-Module addresses the optimization conflict by leveraging a dual-attention design; it uses self-attention to reinforce intra-modal coherence and cross-attention to dynamically extract and fuse complementary inter-modal features, thereby maximizing complementarity while avoiding the dilution of discriminative features and preventing negative transfer. Experiments on four real-world datasets (Hohhot, Nanjing, Xi’an, Houston2013) demonstrate that HCMA-Net consistently achieves outstanding classification results. The code is available at: https://github.com/sun740936222/HCMA-Net.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Wenping Ma; Hekai Zhang; Mengru Ma; Boyou Xue; Hao Zhu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646806"&gt;10.1109/tgrs.2025.3646806&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;While multimodal remote sensing images provide complementary information in different imaging ways, effectively aggregating and jointly learning from these heterogeneous features is non-trivial, primarily due to the inherent modality gap that hinders semantic alignment and feature interoperability. To overcome these issues, we propose a Hierarchical Cross-Modality Aggregation Network (HCMA-Net). It introduces two novel components: the hierarchical feature aggregation and enhancement module (HFAE-Module) and the cross-modality interactive feature extraction module (CMIFE-Module). The HFAE-Module tackles the modality gap and enables cross-scale interaction through its Hierarchical Cross-Modality Feature Aggregation (HCMFA) mechanism, which incorporates Cross-Spectral and Spatial Aggregation Non-Local Attention layers (CSNLA and SANLA) to align features and aggregate contextual information across spectral and spatial dimensions. The CMIFE-Module addresses the optimization conflict by leveraging a dual-attention design; it uses self-attention to reinforce intra-modal coherence and cross-attention to dynamically extract and fuse complementary inter-modal features, thereby maximizing complementarity while avoiding the dilution of discriminative features and preventing negative transfer. Experiments on four real-world datasets (Hohhot, Nanjing, Xi’an, Houston2013) demonstrate that HCMA-Net consistently achieves outstanding classification results. The code is available at: https://github.com/sun740936222/HCMA-Net.&lt;/p&gt;</content:encoded></item><item><title>TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning</title><link>https://arxiv.org/abs/2512.20312v1</link><guid>http://arxiv.org/abs/2512.20312v1</guid><pubDate>Tue, 23 Dec 2025 12:30:37 +0000</pubDate><dc:creator>Saisai Yang</dc:creator><dc:creator>Qingyi Huang</dc:creator><dc:creator>Jing Yuan</dc:creator><dc:creator>Liangyu Zha</dc:creator><dc:creator>Kai Tang</dc:creator><dc:creator>Yuhang Yang</dc:creator><dc:creator>Ning Wang</dc:creator><dc:creator>Yucheng Wei</dc:creator><dc:creator>Liyao Li</dc:creator><dc:creator>Wentao Ye</dc:creator><dc:creator>Hao Chen</dc:creator><dc:creator>Tao Zhang</dc:creator><dc:creator>Junlin Zhou</dc:creator><dc:creator>Haobo Wang</dc:creator><dc:creator>Gang Chen</dc:creator><dc:creator>Junbo Zhao</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.
Published: 2025-12-23T12:30:37+00:00
Venue: arXiv
Score: 0.786 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Saisai Yang; Qingyi Huang; Jing Yuan; Liangyu Zha; Kai Tang; Yuhang Yang; Ning Wang; Yucheng Wei; Liyao Li; Wentao Ye; Hao Chen; Tao Zhang; Junlin Zhou; Haobo Wang; Gang Chen; Junbo Zhao&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.786 (must_read)&lt;/p&gt;
&lt;p&gt;Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.&lt;/p&gt;</content:encoded></item><item><title>Can abstract concepts from LLM improve SLM performance?</title><link>https://arxiv.org/abs/2512.19069v1</link><guid>http://arxiv.org/abs/2512.19069v1</guid><pubDate>Mon, 22 Dec 2025 06:17:25 +0000</pubDate><dc:creator>Siddharth Tandon</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\% of accuracy improvement for Qwen3-0.6B.
Published: 2025-12-22T06:17:25+00:00
Venue: arXiv
Score: 0.785 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Siddharth Tandon&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.785 (must_read)&lt;/p&gt;
&lt;p&gt;Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\% of accuracy improvement for Qwen3-0.6B.&lt;/p&gt;</content:encoded></item><item><title>Hypergraph Foundation Model</title><link>https://doi.org/10.1109/tpami.2025.3647504</link><guid>10.1109/tpami.2025.3647504</guid><pubDate>Tue, 23 Dec 2025 18:31:03 +0000</pubDate><dc:creator>Yue Gao</dc:creator><dc:creator>Yifan Feng</dc:creator><dc:creator>Shiquan Liu</dc:creator><dc:creator>Xiangmin Han</dc:creator><dc:creator>Shaoyi Du</dc:creator><dc:creator>Zongze Wu</dc:creator><dc:creator>Han Hu</dc:creator><prism:publicationName>IEEE Transactions on Pattern Analysis and Machine Intelligence</prism:publicationName><prism:doi>10.1109/tpami.2025.3647504</prism:doi><description>Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.
Published: 2025-12-23T18:31:03+00:00
Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
Score: 0.784 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yue Gao; Yifan Feng; Shiquan Liu; Xiangmin Han; Shaoyi Du; Zongze Wu; Han Hu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Pattern Analysis and Machine Intelligence&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tpami.2025.3647504"&gt;10.1109/tpami.2025.3647504&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.784 (must_read)&lt;/p&gt;
&lt;p&gt;Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.&lt;/p&gt;</content:encoded></item><item><title>Local Saliency-Guided Dynamic Matching for Cross-Modal Remote Sensing Image-Text Retrieval</title><link>https://doi.org/10.1109/tgrs.2025.3646809</link><guid>10.1109/tgrs.2025.3646809</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Jie Shao</dc:creator><dc:creator>Yiran Xie</dc:creator><dc:creator>Pengda Wang</dc:creator><dc:creator>Guohao Feng</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3646809</prism:doi><description>Remote sensing cross-modal text-image retrieval (RSCTIR) has emerged as a fundamental task in remote sensing analysis, aiming to bridge the semantic gap between visual and textual modalities under complex scenarios. Despite recent progress, salient information representation and accurate cross-modal alignments remain significant challenges. In this work, we demonstrate that the correlation between visual and textual features is a valuable cue that can be leveraged to enhance saliency analysis and metric learning in cross-modal retrieval. To this end, we propose a Local Saliency Mining module that uses textual attention to guide the extraction of visual salient features. Additionally, we introduce a Multi-granularity Similarity Contrastive Loss and a Dynamic Similarity Matching Loss to enhance semantic alignment. Finally, a graph-based diffusion reranking algorithm is developed to optimize retrieval ranking by leveraging the intrinsic manifold structure of multimodal data, thereby mitigating local optima during inference. Extensive experiments on the RSICD, RSITMD and UCM-Captions datasets confirm the effectiveness and superiority of our method over state-of-the-art approaches.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Jie Shao; Yiran Xie; Pengda Wang; Guohao Feng&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3646809"&gt;10.1109/tgrs.2025.3646809&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Remote sensing cross-modal text-image retrieval (RSCTIR) has emerged as a fundamental task in remote sensing analysis, aiming to bridge the semantic gap between visual and textual modalities under complex scenarios. Despite recent progress, salient information representation and accurate cross-modal alignments remain significant challenges. In this work, we demonstrate that the correlation between visual and textual features is a valuable cue that can be leveraged to enhance saliency analysis and metric learning in cross-modal retrieval. To this end, we propose a Local Saliency Mining module that uses textual attention to guide the extraction of visual salient features. Additionally, we introduce a Multi-granularity Similarity Contrastive Loss and a Dynamic Similarity Matching Loss to enhance semantic alignment. Finally, a graph-based diffusion reranking algorithm is developed to optimize retrieval ranking by leveraging the intrinsic manifold structure of multimodal data, thereby mitigating local optima during inference. Extensive experiments on the RSICD, RSITMD and UCM-Captions datasets confirm the effectiveness and superiority of our method over state-of-the-art approaches.&lt;/p&gt;</content:encoded></item><item><title>CLIP2RS: Leveraging Pretrained Vision-Language Model for Semantic Segmentation of Remote Sensing Images</title><link>https://doi.org/10.1109/tgrs.2025.3647015</link><guid>10.1109/tgrs.2025.3647015</guid><pubDate>Mon, 22 Dec 2025 18:41:29 +0000</pubDate><dc:creator>Yinghui Xing</dc:creator><dc:creator>Dexuan Kong</dc:creator><dc:creator>Shizhou Zhang</dc:creator><dc:creator>Ziyi Li</dc:creator><dc:creator>Qingyi Li</dc:creator><dc:creator>Yanning Zhang</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647015</prism:doi><description>Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.
Published: 2025-12-22T18:41:29+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yinghui Xing; Dexuan Kong; Shizhou Zhang; Ziyi Li; Qingyi Li; Yanning Zhang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647015"&gt;10.1109/tgrs.2025.3647015&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;Semantic segmentation of Remote Sensing (RS) images is a very challenging task due to the complicated characteristics such as diversity, complexity and massiveness. Current research endeavors are predominantly centered on utilizing visual context information exclusively through meticulous architecture design, often overlooking significant semantic details. This oversight limits the efficacy in tackling the challenge of intra-class variations. While in this paper, we propose CLIP2RS which is devised to leverage the pretrained Vision-Language Model (VLM) for semantic segmentation of RS images via the guidance of prior knowledge stored in the pretrained foundation model. Specifically, CLIP2RS utilizes a two-stage training strategy to overcome the domain gap challenge between natural images and remote sensing images. A dual-granularity alignment framework that simultaneously aligns pixel-level local features and image-level global features is designed to alleviate severe class sample imbalance problem. Additionally, a novel prompting mechanism is effectively explored to to fully harness the potential of CLIP textual descriptions. We conduct comprehensive experiments on the iSAID, Potsdam, and Vaihingen datasets, and the experimental results show that our proposed method achieves state-of-the-art performances, demonstrating its superiority.&lt;/p&gt;</content:encoded></item><item><title>A Study of Finetuning Video Transformers for Multi-view Geometry Tasks</title><link>https://arxiv.org/abs/2512.18684v1</link><guid>http://arxiv.org/abs/2512.18684v1</guid><pubDate>Sun, 21 Dec 2025 10:41:11 +0000</pubDate><dc:creator>Huimin Wu</dc:creator><dc:creator>Kwang-Ting Cheng</dc:creator><dc:creator>Stephen Lin</dc:creator><dc:creator>Zhirong Wu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.
Published: 2025-12-21T10:41:11+00:00
Venue: arXiv
Score: 0.783 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Huimin Wu; Kwang-Ting Cheng; Stephen Lin; Zhirong Wu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.783 (must_read)&lt;/p&gt;
&lt;p&gt;This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.&lt;/p&gt;</content:encoded></item><item><title>Bi-Grid Reconstruction for Image Anomaly Detection</title><link>https://doi.org/10.1109/tip.2025.3644787</link><guid>10.1109/tip.2025.3644787</guid><pubDate>Mon, 22 Dec 2025 18:44:25 +0000</pubDate><dc:creator>Aimin Feng</dc:creator><dc:creator>Huichuan Huang</dc:creator><dc:creator>Guangyu Wei</dc:creator><dc:creator>Wenlong Sun</dc:creator><prism:publicationName>IEEE Transactions on Image Processing</prism:publicationName><prism:doi>10.1109/tip.2025.3644787</prism:doi><description>In the domain of image anomaly detection, significant progress has been made in un- and self-supervised methods with datasets containing only normal samples. Although these methods perform well in general industrial anomaly detection scenarios, they often struggle with over- or under-detection when faced with fine-grained anomalies in products. In this paper, we propose GRAD: Bi-Grid Reconstruction for Image Anomaly Detection, which utilizes two continuous grids to detect anomalies from both normal and abnormal perspectives. In this work: 1) Grids are served as feature repositories to assist in the reconstruction task, achieving stronger generalization compared to discrete storage, while also helping to avoid the Identical Shortcut (IS) problem common in general reconstruction methods. 2) An additional grid storing abnormal features is introduced alongside the normal grid storing normal features, which refines the boundaries of normal features, thereby enhancing GRAD’s detection performance for fine-grained defects. 3) The Feature Block Pasting (FBP) module is designed to synthesize a variety of anomalies at the feature level, enabling the rapid deployment of the abnormal grid. Additionally, benefiting from the powerful representation capabilities of grids, GRAD is suitable for a unified task setting, requiring only a single model to be trained for multiple classes. GRAD has been comprehensively tested on classic industrial datasets including MVTecAD, VisA, and the newest GoodsAD dataset, showing significant improvement over current state-of-the-art methods.
Published: 2025-12-22T18:44:25+00:00
Venue: IEEE Transactions on Image Processing
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Aimin Feng; Huichuan Huang; Guangyu Wei; Wenlong Sun&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Image Processing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tip.2025.3644787"&gt;10.1109/tip.2025.3644787&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;In the domain of image anomaly detection, significant progress has been made in un- and self-supervised methods with datasets containing only normal samples. Although these methods perform well in general industrial anomaly detection scenarios, they often struggle with over- or under-detection when faced with fine-grained anomalies in products. In this paper, we propose GRAD: Bi-Grid Reconstruction for Image Anomaly Detection, which utilizes two continuous grids to detect anomalies from both normal and abnormal perspectives. In this work: 1) Grids are served as feature repositories to assist in the reconstruction task, achieving stronger generalization compared to discrete storage, while also helping to avoid the Identical Shortcut (IS) problem common in general reconstruction methods. 2) An additional grid storing abnormal features is introduced alongside the normal grid storing normal features, which refines the boundaries of normal features, thereby enhancing GRAD’s detection performance for fine-grained defects. 3) The Feature Block Pasting (FBP) module is designed to synthesize a variety of anomalies at the feature level, enabling the rapid deployment of the abnormal grid. Additionally, benefiting from the powerful representation capabilities of grids, GRAD is suitable for a unified task setting, requiring only a single model to be trained for multiple classes. GRAD has been comprehensively tested on classic industrial datasets including MVTecAD, VisA, and the newest GoodsAD dataset, showing significant improvement over current state-of-the-art methods.&lt;/p&gt;</content:encoded></item><item><title>Two-Phase Collaborative Model Compression Training for Joint Pruning and Quantization</title><link>https://doi.org/10.1016/j.neunet.2025.108506</link><guid>10.1016/j.neunet.2025.108506</guid><pubDate>Tue, 23 Dec 2025 16:52:34 +0000</pubDate><dc:creator>Chunxiao Fan</dc:creator><dc:creator>Jintao Li</dc:creator><dc:creator>Zhongqian Zhang</dc:creator><dc:creator>Fu Li</dc:creator><dc:creator>Bo Wang</dc:creator><prism:publicationName>Neural Networks</prism:publicationName><prism:doi>10.1016/j.neunet.2025.108506</prism:doi><description>To reduce the storage and computational complexity of neural network models, various model compression techniques have been proposed in recent years, including pruning and quantization. However, due to the lack of interconnection among different type of methods, it is difficult to effectively integrate the advantages of these diverse techniques. This paper proposes a novel two-phase collaborative training framework for joint pruning and quantization to achieve synergistic optimization of multiple compression techniques. This framework combines pruning, quantization operations, consisting of two phases: collaborative constraint pre-compression and post-training compression refinement phases. In the collaborative constraint pre-compression phase, a novel unified constraint loss function is designed to ensure that weights are close to quantization values, and sparse regularization is utilized to automatically learn the network structure for pruning. It can effectively combine pruning and quantization operations, avoiding the potential negative impacts of separately implementing pruning and quantization. By calculating the difference between the current parameter values and the target quantization values, quantization errors are reduced through iterative optimization during the training process, making the parameters closer to the selected 2 n values. The pruned network has a regular structure, and quantization to 2 n values makes it highly suitable for hardware implementation as it can be achieved using a shifter. In the post-training compression refinement phase, joint compression operations including channel pruning and low-bit quantization are completed. Experimental results on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100 show that the framework generates more concise network parameters while maintaining considerable accuracy, demonstrating excellent effectiveness in terms of compression ratio and accuracy. The proposed framework can integrate the complementary aspects of quantization and pruning, and effectively minimize the possible adverse interactions between quantization and pruning.
Published: 2025-12-23T16:52:34+00:00
Venue: Neural Networks
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Chunxiao Fan; Jintao Li; Zhongqian Zhang; Fu Li; Bo Wang&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neural Networks&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neunet.2025.108506"&gt;10.1016/j.neunet.2025.108506&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;To reduce the storage and computational complexity of neural network models, various model compression techniques have been proposed in recent years, including pruning and quantization. However, due to the lack of interconnection among different type of methods, it is difficult to effectively integrate the advantages of these diverse techniques. This paper proposes a novel two-phase collaborative training framework for joint pruning and quantization to achieve synergistic optimization of multiple compression techniques. This framework combines pruning, quantization operations, consisting of two phases: collaborative constraint pre-compression and post-training compression refinement phases. In the collaborative constraint pre-compression phase, a novel unified constraint loss function is designed to ensure that weights are close to quantization values, and sparse regularization is utilized to automatically learn the network structure for pruning. It can effectively combine pruning and quantization operations, avoiding the potential negative impacts of separately implementing pruning and quantization. By calculating the difference between the current parameter values and the target quantization values, quantization errors are reduced through iterative optimization during the training process, making the parameters closer to the selected 2 n values. The pruned network has a regular structure, and quantization to 2 n values makes it highly suitable for hardware implementation as it can be achieved using a shifter. In the post-training compression refinement phase, joint compression operations including channel pruning and low-bit quantization are completed. Experimental results on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100 show that the framework generates more concise network parameters while maintaining considerable accuracy, demonstrating excellent effectiveness in terms of compression ratio and accuracy. The proposed framework can integrate the complementary aspects of quantization and pruning, and effectively minimize the possible adverse interactions between quantization and pruning.&lt;/p&gt;</content:encoded></item><item><title>Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</title><link>https://arxiv.org/abs/2512.19673v1</link><guid>http://arxiv.org/abs/2512.19673v1</guid><pubDate>Mon, 22 Dec 2025 18:51:48 +0000</pubDate><dc:creator>Yuqiao Tan</dc:creator><dc:creator>Minzheng Wang</dc:creator><dc:creator>Shizhu He</dc:creator><dc:creator>Huanxuan Liao</dc:creator><dc:creator>Chengfeng Zhao</dc:creator><dc:creator>Qiunan Lu</dc:creator><dc:creator>Tian Liang</dc:creator><dc:creator>Jun Zhao</dc:creator><dc:creator>Kang Liu</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.
Published: 2025-12-22T18:51:48+00:00
Venue: arXiv
Score: 0.782 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Yuqiao Tan; Minzheng Wang; Shizhu He; Huanxuan Liao; Chengfeng Zhao; Qiunan Lu; Tian Liang; Jun Zhao; Kang Liu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.782 (must_read)&lt;/p&gt;
&lt;p&gt;Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama&amp;#x27;s prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.&lt;/p&gt;</content:encoded></item><item><title>RSRefSeg 2: Decoupling Referring Remote Sensing Image Segmentation with Foundation Models</title><link>https://doi.org/10.1109/tgrs.2025.3647535</link><guid>10.1109/tgrs.2025.3647535</guid><pubDate>Tue, 23 Dec 2025 18:31:04 +0000</pubDate><dc:creator>Keyan Chen</dc:creator><dc:creator>Chenyang Liu</dc:creator><dc:creator>Bowen Chen</dc:creator><dc:creator>Jiafan Zhang</dc:creator><dc:creator>Zhengxia Zou</dc:creator><dc:creator>Zhenwei Shi</dc:creator><prism:publicationName>IEEE Transactions on Geoscience and Remote Sensing</prism:publicationName><prism:doi>10.1109/tgrs.2025.3647535</prism:doi><description>Referring Remote Sensing Image Segmentation (RRSIS) facilitates flexible scene analysis by leveraging vision-language collaborative interpretation. However, conventional coupled frameworks typically perform pixel decoding after cross-modal fusion, conflating target localization (“where”) with boundary delineation (“how”). While recent decoupled approaches utilizing foundation models (e.g., SAM) attempt to separate these tasks, they predominantly rely on reductionist “box-to-mask” or “prompt-to-mask” pipelines. Such simple interfaces compress rich referring expressions into simplistic geometric constraints, severing semantic consistency and failing to leverage open-vocabulary visual-semantic alignment, particularly in multi-entity scenarios. To address these limitations, we propose RSRefSeg 2, an framework that logically reformulates the workflow into sequential subtasks of “slack localization” and “refined segmentation.” Central to our approach is a novel cascaded second-order referring prompter, designed to construct a robust semantic bridge between CLIP’s open-world understanding and SAM’s segmentation capabilities. Specifically, we introduce an orthogonal subspace decomposition mechanism that separates text embeddings into complementary components. This enables implicit cascaded reasoning to isolate target attributes from background noise: first performing slack localization via cross-modal interaction to identify potential regions, and subsequently generating refined prompts to guide SAM’s precise delineation. Furthermore, we incorporate parameter-efficient tuning to align natural image priors with remote sensing domains. Extensive experiments on RefSegRS, RRSIS-D, and RISBench demonstrate that RSRefSeg 2 significantly outperforms state-of-the-art methods, achieving an approximate 3% improvement in gIoU while offering superior diagnostic interpretability. The code is available at: https://github.com/KyanChen/RSRefSeg2.
Published: 2025-12-23T18:31:04+00:00
Venue: IEEE Transactions on Geoscience and Remote Sensing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Keyan Chen; Chenyang Liu; Bowen Chen; Jiafan Zhang; Zhengxia Zou; Zhenwei Shi&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; IEEE Transactions on Geoscience and Remote Sensing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1109/tgrs.2025.3647535"&gt;10.1109/tgrs.2025.3647535&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;Referring Remote Sensing Image Segmentation (RRSIS) facilitates flexible scene analysis by leveraging vision-language collaborative interpretation. However, conventional coupled frameworks typically perform pixel decoding after cross-modal fusion, conflating target localization (“where”) with boundary delineation (“how”). While recent decoupled approaches utilizing foundation models (e.g., SAM) attempt to separate these tasks, they predominantly rely on reductionist “box-to-mask” or “prompt-to-mask” pipelines. Such simple interfaces compress rich referring expressions into simplistic geometric constraints, severing semantic consistency and failing to leverage open-vocabulary visual-semantic alignment, particularly in multi-entity scenarios. To address these limitations, we propose RSRefSeg 2, an framework that logically reformulates the workflow into sequential subtasks of “slack localization” and “refined segmentation.” Central to our approach is a novel cascaded second-order referring prompter, designed to construct a robust semantic bridge between CLIP’s open-world understanding and SAM’s segmentation capabilities. Specifically, we introduce an orthogonal subspace decomposition mechanism that separates text embeddings into complementary components. This enables implicit cascaded reasoning to isolate target attributes from background noise: first performing slack localization via cross-modal interaction to identify potential regions, and subsequently generating refined prompts to guide SAM’s precise delineation. Furthermore, we incorporate parameter-efficient tuning to align natural image priors with remote sensing domains. Extensive experiments on RefSegRS, RRSIS-D, and RISBench demonstrate that RSRefSeg 2 significantly outperforms state-of-the-art methods, achieving an approximate 3% improvement in gIoU while offering superior diagnostic interpretability. The code is available at: https://github.com/KyanChen/RSRefSeg2.&lt;/p&gt;</content:encoded></item><item><title>Towards Minimal Fine-Tuning of VLMs</title><link>https://arxiv.org/abs/2512.19219v1</link><guid>http://arxiv.org/abs/2512.19219v1</guid><pubDate>Mon, 22 Dec 2025 10:02:10 +0000</pubDate><dc:creator>Tiange Luo</dc:creator><dc:creator>Lajanugen Logeswaran</dc:creator><dc:creator>Jaekyeom Kim</dc:creator><dc:creator>Justin Johnson</dc:creator><dc:creator>Honglak Lee</dc:creator><dc:type>preprint</dc:type><prism:publicationName>arXiv</prism:publicationName><description>We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.
Published: 2025-12-22T10:02:10+00:00
Venue: arXiv
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tiange Luo; Lajanugen Logeswaran; Jaekyeom Kim; Justin Johnson; Honglak Lee&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; arXiv&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.&lt;/p&gt;</content:encoded></item><item><title>Seg-LLaVA: A small-scale large vision-language model with external visual prompts</title><link>https://doi.org/10.1016/j.neucom.2025.132423</link><guid>10.1016/j.neucom.2025.132423</guid><pubDate>Mon, 22 Dec 2025 07:44:54 +0000</pubDate><dc:creator>Tianxing Guo</dc:creator><dc:creator>Huanyu Liu</dc:creator><dc:creator>Jiazheng Wen</dc:creator><dc:creator>Junbao Li</dc:creator><prism:publicationName>Neurocomputing</prism:publicationName><prism:doi>10.1016/j.neucom.2025.132423</prism:doi><description>With recent significant advancements in large vision-language models (LVLMs), image-text understanding capabilities have substantially improved. However, a notable gap remains in fine-grained region understanding. Moreover, the resource consumption for training and testing large-scale LVLMs is immense, making them less accessible to researchers with limited resources. In this paper, we propose a small-scale LVLM, Seg-LLaVA, which employs a lightweight visual prompting method that leverages a semantic segmenter and a small-scale large language model (LLM). By integrating fine-grained knowledge generated by a specialized instance segmentation model with the original image into a multi-layer linear model, we enable the model to perceive object boundaries and types in the image without significantly increasing the number of training parameters, thereby greatly enhancing its visual understanding capabilities. Additionally, we adopt an efficient training approach, allowing Seg-LLaVA to achieve outstanding performance while further reducing resource requirements. Experimental results show that our model excels across multiple benchmarks and demonstrates strong fine-grained perception capabilities.
Published: 2025-12-22T07:44:54+00:00
Venue: Neurocomputing
Score: 0.781 (must_read)</description><content:encoded>&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Tianxing Guo; Huanyu Liu; Jiazheng Wen; Junbao Li&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Journal:&lt;/strong&gt; Neurocomputing&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;a href="https://doi.org/10.1016/j.neucom.2025.132423"&gt;10.1016/j.neucom.2025.132423&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Relevance:&lt;/strong&gt; 0.781 (must_read)&lt;/p&gt;
&lt;p&gt;With recent significant advancements in large vision-language models (LVLMs), image-text understanding capabilities have substantially improved. However, a notable gap remains in fine-grained region understanding. Moreover, the resource consumption for training and testing large-scale LVLMs is immense, making them less accessible to researchers with limited resources. In this paper, we propose a small-scale LVLM, Seg-LLaVA, which employs a lightweight visual prompting method that leverages a semantic segmenter and a small-scale large language model (LLM). By integrating fine-grained knowledge generated by a specialized instance segmentation model with the original image into a multi-layer linear model, we enable the model to perceive object boundaries and types in the image without significantly increasing the number of training parameters, thereby greatly enhancing its visual understanding capabilities. Additionally, we adopt an efficient training approach, allowing Seg-LLaVA to achieve outstanding performance while further reducing resource requirements. Experimental results show that our model excels across multiple benchmarks and demonstrates strong fine-grained perception capabilities.&lt;/p&gt;</content:encoded></item></channel></rss>