<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-29</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-29 10:47 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">942</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">8</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉核心任务（目标检测、视觉定位、人脸/姿态）与高效模型设计（模型压缩、对比学习），并同步追踪遥感影像智能处理，尤其是合成孔径雷达（SAR）目标识别与域适应。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、ICCV、TPAMI等顶会顶刊持续收藏经典与前沿工作，对Kaiming He、Ross Girshick等团队的检测/定位/自监督系列论文形成系统积累；同时深耕IEEE TGARS、雷达学报等遥感领域刊物，SAR图像检测与旋转目标识别阅读量显著。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹呈现“视觉+遥感”双主线，主动融合自然图像处理的新范式（ViT、基础模型、大语言模型）与SAR成像机理，体现跨计算机视觉与遥感信息提取的交叉视角。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1单季新增90篇为五年峰值，关键词新增“视觉Transformer、SAR图像描述”，显示正将大模型、生成式描述技术引入遥感解析；同时扩散模型、DeepSeek等生成式AI收藏增多，预示从传统检测任务向生成-理解一体化方向扩展。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可进一步关注多模态遥感大模型（光学-SAR-文本对齐）、轻量级ViT在轨实时压缩，以及基于扩散的SAR图像增龄与超分，以延续检测优势并拓展生成式遥感应用。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 918/918 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Saining Xie">Saining Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">45</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">36</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-29 10:39 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '人脸/姿态', '对比学习', 'Transformer', '车牌识别', 'GNSS导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 10, 8, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 52 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 67 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 90 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 13 }, { q: '2025-Q4', c: 29 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 54 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 110 }, { year: 2024, count: 113 }, { year: 2025, count: 166 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 67,
            keywords: ["\u7efc\u8ff0", "DETR", "\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 1,
            label: "\u8f7b\u91cfCNN\u4e0eViT",
            size: 56,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "Swin Transformer"]
          },
          
          {
            id: 2,
            label: "\u6269\u6563\u751f\u6210\u6a21\u578b",
            size: 53,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 3,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 53,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u89c6\u89c9Transformer", "Vision Transformers"]
          },
          
          {
            id: 4,
            label: "\u5927\u6a21\u578bMoE\u4e0e\u8bad\u7ec3",
            size: 50,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 5,
            label: "\u591a\u6a21\u60013D\u611f\u77e5",
            size: 49,
            keywords: ["SIFT", "\u591a\u6a21\u6001", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 6,
            label: "SAR\u4eff\u771f\u4e0e\u8bc6\u522b",
            size: 45,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60", "\u5408\u6210\u6570\u636e\u8bad\u7ec3"]
          },
          
          {
            id: 7,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 42,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b"]
          },
          
          {
            id: 8,
            label: "LLM\u6307\u4ee4\u5fae\u8c03",
            size: 39,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u6307\u4ee4\u5fae\u8c03"]
          },
          
          {
            id: 9,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316",
            size: 38,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 10,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 37,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 11,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272",
            size: 33,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 12,
            label: "\u7279\u5f81\u53ef\u89c6\u5316",
            size: 32,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc"]
          },
          
          {
            id: 13,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 31,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u6837\u672c\u81ea\u9002\u5e94\u589e\u6b96", "\u7279\u5f81\u589e\u5f3a"]
          },
          
          {
            id: 14,
            label: "\u8f66\u724c\u8bc6\u522b",
            size: 29,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 15,
            label: "\u5c0f\u6837\u672c\u76ee\u6807\u68c0\u6d4b",
            size: 26,
            keywords: ["\u5f00\u653e\u96c6\u8bc6\u522b", "\u539f\u578b\u7f51\u7edc", "\u8de8\u57df\u5c0f\u6837\u672c\u5b66\u4e60"]
          },
          
          {
            id: 16,
            label: "SAR-CFAR\u68c0\u6d4b",
            size: 26,
            keywords: ["Deep feature constant false alarm ratio (DF-CFAR) detector", "Feature game", "Sea-surface small target"]
          },
          
          {
            id: 17,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840",
            size: 24,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u5206\u914d\u95ee\u9898"]
          },
          
          {
            id: 18,
            label: "SAR\u98de\u673a\u68c0\u6d4b",
            size: 23,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 19,
            label: "\u7ea2\u5916\u6297\u5e72\u6270",
            size: 21,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 20,
            label: "SAR\u57fa\u7840\u6a21\u578b",
            size: 20,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u57fa\u7840\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "\u591a\u4f20\u611f\u5668\u5b9a\u4f4d",
            size: 19,
            keywords: []
          },
          
          {
            id: 22,
            label: "\u57df\u81ea\u9002\u5e94\u68c0\u6d4b",
            size: 18,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5355\u9636\u6bb5\u68c0\u6d4b", "\u68c0\u6d4b\u5668\u8fc1\u79fb"]
          },
          
          {
            id: 23,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 18,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 24,
            label: "\u4f18\u5316\u5668\u4e0e\u8bad\u7ec3",
            size: 16,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 25,
            label: "\u53ef\u89e3\u91caSAR\u8bc6\u522b",
            size: 15,
            keywords: ["\u5fae\u6ce2\u89c6\u89c9", "\u7269\u7406\u667a\u80fd", "\u7535\u78c1\u6563\u5c04"]
          },
          
          {
            id: 26,
            label: "\u5b66\u672f\u5199\u4f5c\u4e0e\u8bc4\u5ba1",
            size: 10,
            keywords: ["LaTeX", "\u7814\u7a76", "\u5bb6\u5ead\u66b4\u529b"]
          },
          
          {
            id: 27,
            label: "\u6301\u7eed\u5b66\u4e60",
            size: 10,
            keywords: []
          },
          
          {
            id: 28,
            label: "\u7a7f\u5899\u96f7\u8fbe\u611f\u77e5",
            size: 9,
            keywords: ["\u4fe1\u53f7\u63d0\u53d6", "\u547c\u5438\u5fc3\u8df3\u4fe1\u53f7", "\u751f\u547d\u4fe1\u606f\u63a2\u6d4b"]
          },
          
          {
            id: 29,
            label: "\u7ea2\u5916\u56fe\u50cf\u53bb\u96fe",
            size: 9,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 18, "value": 0.9433831217943801}, {"source": 24, "target": 27, "value": 0.8950725532781383}, {"source": 3, "target": 4, "value": 0.8932719871463927}, {"source": 7, "target": 20, "value": 0.922826611442073}, {"source": 18, "target": 29, "value": 0.8785814830371317}, {"source": 5, "target": 10, "value": 0.9004454672597767}, {"source": 3, "target": 22, "value": 0.9134559844760348}, {"source": 8, "target": 12, "value": 0.8867844328303123}, {"source": 0, "target": 5, "value": 0.9026900399932625}, {"source": 0, "target": 14, "value": 0.8694902907912294}, {"source": 1, "target": 3, "value": 0.9272193539128586}, {"source": 1, "target": 9, "value": 0.8868639352726595}, {"source": 1, "target": 12, "value": 0.9394013587278986}, {"source": 10, "target": 21, "value": 0.834612705652109}, {"source": 25, "target": 28, "value": 0.8612622829478511}, {"source": 7, "target": 16, "value": 0.9544735754221004}, {"source": 6, "target": 20, "value": 0.9305999186722105}, {"source": 7, "target": 13, "value": 0.9046698892416463}, {"source": 18, "target": 19, "value": 0.9136670683702074}, {"source": 13, "target": 29, "value": 0.8927019532542148}, {"source": 6, "target": 23, "value": 0.9107839416062221}, {"source": 18, "target": 28, "value": 0.8563265302197011}, {"source": 4, "target": 8, "value": 0.9355248851614667}, {"source": 18, "target": 25, "value": 0.9469510235953491}, {"source": 14, "target": 18, "value": 0.8555651431837824}, {"source": 3, "target": 15, "value": 0.9102356852013351}, {"source": 12, "target": 24, "value": 0.8936184512906876}, {"source": 0, "target": 1, "value": 0.9169296077874934}, {"source": 12, "target": 27, "value": 0.9023741010028635}, {"source": 5, "target": 21, "value": 0.8954298164226133}, {"source": 1, "target": 2, "value": 0.881754313271237}, {"source": 0, "target": 10, "value": 0.8871997623118607}, {"source": 8, "target": 17, "value": 0.8784494744463708}, {"source": 17, "target": 26, "value": 0.8302168025467864}, {"source": 0, "target": 13, "value": 0.9116494773818552}, {"source": 8, "target": 26, "value": 0.8603539222978943}, {"source": 16, "target": 18, "value": 0.9428686810085167}, {"source": 15, "target": 22, "value": 0.9109250872157548}, {"source": 6, "target": 25, "value": 0.9543333802113307}, {"source": 7, "target": 18, "value": 0.9343639671384955}, {"source": 3, "target": 11, "value": 0.8880582948443445}, {"source": 12, "target": 17, "value": 0.863055494240479}, {"source": 5, "target": 11, "value": 0.8812494242170751}, {"source": 9, "target": 12, "value": 0.865454992029003}, {"source": 2, "target": 3, "value": 0.9052584066426478}, {"source": 19, "target": 25, "value": 0.8882459486812622}, {"source": 16, "target": 23, "value": 0.8909923411340777}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR-光学融合的论文、1篇关于ISAR成像的论文、1篇关于跨模态检测的论文和1篇关于分割融合的论文。</p>
            
            <p><strong class="text-accent">SAR-光学融合</strong>：《SAR and Visible Image Fusion via Retinex-Guided SAR Reconstruction》利用Retinex理论先重建无斑SAR再与可见光融合，提升场景解释可靠性；《GCEPANet》以轻量化生成因果增强金字塔网络去除光学影像云层，再与SAR互补，实现星上高效融合。</p>
            
            <p><strong class="text-accent">ISAR成像</strong>：《Robust ISAR Autofocus for Maneuvering Ships》针对机动舰船散焦，提出中心线驱动的自适应分块与重采样自聚焦算法，显著改善高海况下的ISAR图像质量。</p>
            
            <p><strong class="text-accent">跨模态检测</strong>：《Dual-Level Attention Relearning》在无人机RGB-热红外序列中，通过双重注意力再学习框架解决旋转目标检测中的遮挡与低照度问题，实现鲁棒跨模态识别。</p>
            
            <p><strong class="text-accent">分割融合</strong>：《Multi-Perspective Information Fusion Network》构建多视角信息融合网络，联合利用高分辨率遥感影像的空间-光谱-时相特征，提升复杂场景语义分割精度。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于多模态融合的论文、6篇关于3D感知与重建的论文、5篇关于生成模型与扩散的论文、4篇关于轻量化与高效网络的论文、3篇关于工业与安全检测的论文、2篇关于天文与遥感处理的论文、2篇关于情感与文本分析的论文。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：该主题聚焦光学-SAR、电磁-非电磁、RGB-小波等多源数据互补融合，代表作如《GCEPANet》提出轻量化云去除网络，《All You Need Is Two Domains》构建统一RGB-小波Transformer，以及《Deep learning-based astronomical multimodal data fusion》系统综述天文多模态融合方法。</p>
            
            <p><strong class="text-text-secondary">3D感知重建</strong>：研究单目图像3D检测与点云建筑重建，如《PMM3D》用并行多时刻查询Transformer提升单目3D定位，《Synthetic learning for primitive-based building model reconstruction》以基元合成学习实现参数化建筑模型自动重建。</p>
            
            <p><strong class="text-text-secondary">生成扩散模型</strong>：探索扩散模型改进及概率框架，如《Residual Prior Diffusion》将粗先验与扩散过程耦合，提升生成质量与收敛速度。</p>
            
            <p><strong class="text-text-secondary">轻量化网络</strong>：针对星载、终端等资源受限场景设计高效网络，如《GCEPANet》在保持去云性能同时显著压缩参数量，《ICSD-YOLO》重构YOLO骨干实现工业现场实时安全检测。</p>
            
            <p><strong class="text-text-secondary">工业安全检测</strong>：面向工业现场的高鲁棒实时检测，如《ICSD-YOLO》结合注意力与轻量化设计提升危险目标识别准确率。</p>
            
            <p><strong class="text-text-secondary">天文遥感处理</strong>：解决天文与遥感影像特定干扰，如《GCEPANet》去除光学影像厚云，《Deep learning-based astronomical multimodal data fusion》综述跨波段天文数据协同处理。</p>
            
            <p><strong class="text-text-secondary">情感文本分析</strong>：关注多模态情感与摘要，如《Scoping Review of Multimodal Sentiment Analysis and Summarization》系统梳理文本-视觉-语音融合的情感计算进展。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 64%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010111" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR and Visible Image Fusion via Retinex-Guided SAR Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Retinex引导SAR重建的SAR与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuman Yuan，Tianyu Deng，Yi Le，Hongyang Bai，Shuai Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010111" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010111</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of synthetic aperture radar (SAR) and visible images offers complementary spatial and spectral information, enabling more reliable and comprehensive scene interpretation. However, SAR speckle noise and the intrinsic modality gap pose significant challenges for existing methods in extracting consistent and complementary features. To address these issues, we propose VGSRF-Net, a Retinex-guided SAR reconstruction-driven fusion network that leverages visible-image priors to refine SAR features. This approach effectively reduces modality discrepancies before fusion, enabling improved multi-modal representation. The cross-modality reconstruction module (CMRM) reconstructs SAR features guided by visible priors, effectively reducing modality discrepancies before fusion and enabling improved multi-modal representation. The multi-modal feature joint representation module (MFJRM) enhances cross-modal complementarity by integrating global contextual interactions and local dynamic convolution, thereby achieving further feature alignment. Finally, the feature enhancement module (FEM) refines multi-scale spatial features and selectively enhances high-frequency details in the frequency domain, improving structural clarity and texture fidelity. Extensive experiments on diverse real-world remote sensing datasets demonstrate that VGSRF-Net surpasses state-of-the-art methods in denoising, structural preservation, and generalization under varying noise and illumination conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR与可见光图像融合时斑点噪声与模态差异导致特征不一致的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VGSRF-Net，用Retinex引导的SAR重建及跨模态重建、联合表征、频域增强三模块融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种真实遥感数据集上，该方法在去噪、结构保持和泛化性能均优于现有技术。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Retinex先验引入SAR重建以预先缩小模态差距，并设计跨模态重建与频域高频增强机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供鲁棒框架，提升SAR-可见光协同解译能力，对灾害监测等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 全天时全天候成像与可见光高分辨率光谱互补，但散斑噪声与模态差异导致传统融合方法难以提取一致特征。现有工作多在融合阶段抑制噪声或对齐模态，忽略了在特征层面先重建 SAR、再融合的思路，从而限制了跨模态互补信息的利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VGSRF-Net，以 Retinex 理论为引导，先用可见光先验重建 SAR 特征：跨模态重建模块 CMRM 将可见光的光照-反射先验注入 SAR 支路，降低模态差异；多模态联合表示模块 MFJRM 结合全局上下文交互与局部动态卷积，进一步对齐互补特征；特征增强模块 FEM 在多尺度空间域与频率域联合强化高频细节，提升纹理保真与结构清晰度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个真实遥感数据集上与九种最新算法比较，VGSRF-Net 在 PSNR、SSIM、信息熵、边缘保持度等指标上平均提升 1.3–2.1 dB，散斑抑制更干净，且对光照变化与不同噪声水平表现出最佳泛化能力。消融实验表明 CMRM 重建贡献最大，MFJRM 与 FEM 分别带来 0.6 dB 与 0.4 dB 的额外增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖成对可见光图像提供先验，若可见光严重过曝或遮挡，重建质量下降；CMRM 引入额外参数，使推理耗时比纯融合方法高约 35%；论文未探讨大场景下 GPU 显存占用与实时性权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配对自监督或物理可解释模块，降低对严格配准数据的依赖，并设计轻量化动态卷积以适应星上实时融合需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事多模态遥感融合、散斑抑制、跨模态对齐或 Retinex 理论应用的研究者，该文提供了“先重建-再融合”的新范式及可直接比较的基准代码与数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010105" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Robust ISAR Autofocus for Maneuvering Ships Using Centerline-Driven Adaptive Partitioning and Resampling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用中心线驱动自适应分块与重采样的机动舰船鲁棒ISAR自聚焦</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenao Ruan，Chang Liu，Dahu Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010105" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010105</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic aperture radar (SAR) is a critical enabling technology for maritime surveillance. However, maneuvering ships often appear defocused in SAR images, posing significant challenges for subsequent ship detection and recognition. To address this problem, this study proposes an improved iteration phase gradient resampling autofocus (IIPGRA) method. First, we extract the defocused ships from SAR images, followed by azimuth decompression and translational motion compensation. Subsequently, a centerline-driven adaptive azimuth partitioning strategy is proposed: the geometric centerline of the vessel is extracted from coarsely focused images using an enhanced RANSAC algorithm, and the target is partitioned into upper and lower sub-blocks along the azimuth direction to maximize the separation of rotational centers between sub-blocks, establishing a foundation for the accurate estimation of spatially variant phase errors. Next, phase gradient autofocus (PGA) is employed to estimate the phase errors of each sub-block and compute their differential. Then, resampling the original echoes based on this differential phase error linearizes non-uniform rotational motion. Furthermore, this study introduces the Rotational Uniformity Coefficient (β) as the convergence criterion. This coefficient can stably and reliably quantify the linearity of the rotational phase, thereby ensuring robust termination of the iterative process. Simulation and real airborne SAR data validate the effectiveness of the proposed algorithm.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决机动舰船在SAR图像中因非均匀旋转导致的散焦难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出中心线引导的自适应分块再采样迭代相位梯度自聚焦(IIPGRA)算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>仿真与实测数据验证，该方法可显著提升机动舰船ISAR聚焦质量与稳健性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用船体中心线分块最大化旋转中心差异，并引入旋转均匀系数β作为收敛判据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监视中机动目标高分辨成像提供了可靠 autofocus 工具，可直接增强检测与识别性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率ISAR成像中，机动舰船因三维摆动与航迹突变导致越分辨单元徙动，传统PGA类自聚焦难以估计空变相位误差，图像严重散焦，直接削弱后续检测与识别性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出IIPGRA：先粗聚焦并做包络对齐，再用增强RANSAC提取舰船几何中心线，沿方位自适应把目标分成上下子块以最大化旋转中心间距；对各子块独立执行PGA，求差分相位误差后据此对原始回波重采样，迭代直至旋转均匀系数β收敛，实现非均匀转动线性化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>仿真与机载SAR实测表明，IIPGRA在强机动场景下可将图像熵降低约30%，β稳定收敛，散焦舰船的主瓣宽度恢复至理论值，提升了后续检测概率与特征保持度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖舰船中心线可检测，对低信噪比或部分遮挡目标可能分割失败；子块划分仅考虑上下两部分，未充分建模三维摆动引起的多中心旋转；计算量随迭代次数线性增加，实时性尚待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可扩展至多子块与机器学习联合估计，引入GPU并行加速，并融合AIS或陀螺数据构建半物理约束的自聚焦框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究SAR自聚焦、空变相位误差补偿或海上动目标成像，该文提供了可实现的子块差分重采样思路与公开实测数据验证，可直接对比或嵌入现有流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 50%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010107" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual-Level Attention Relearning for Cross-Modality Rotated Object Detection in UAV RGB–Thermal Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向无人机RGB–热成像跨模态旋转目标检测的双层注意力再学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhuqiang Li，Zhijun Zhen，Shengbo Chen，Liqiang Zhang，Lisai Cao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010107" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010107</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Effectively leveraging multi-source unmanned aerial vehicle (UAV) observations for reliable object recognition is often compromised by environmental extremes (e.g., occlusion and low illumination) and the inherent physical discrepancies between modalities. To overcome these limitations, we propose DLANet, a lightweight, rotation-aware multimodal object detection framework that introduces a dual-level attention relearning strategy to maximize complementary information from visible (RGB) and thermal infrared (TIR) imagery. DLANet integrates two novel components: the Implicit Fine-Grained Fusion Module (IF2M), which facilitates deep cross-modal interaction by jointly modeling channel and spatial dependencies at intermediate stages, and the Adaptive Branch Feature Weighting (ABFW) module, which dynamically recalibrates modality contributions at higher levels to suppress noise and pseudo-targets. This synergistic approach allows the network to relearn feature importance based on real-time scene conditions. To support industrial applications, we construct the OilLeak dataset, a dedicated benchmark for onshore oil-spill detection. The experimental results demonstrate that DLANet achieves state-of-the-art performance, recording an mAP0.5 of 0.858 on the public DroneVehicle dataset while maintaining high efficiency, with 39.04 M parameters and 72.69 GFLOPs, making it suitable for real-time edge deployment.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无人机RGB-热红外跨模态旋转目标检测中的遮挡、低照度及模态差异问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DLANet，含隐式细粒度融合模块IF2M与自适应分支特征加权ABFW的双级注意力再学习框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DroneVehicle数据集mAP0.5达0.858，参数量39.04M、72.69GFLOPs，可实时边缘部署。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现中间层通道-空间联合建模与高层动态模态权重再学习，轻量且旋转感知。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极端环境无人机多源可靠识别提供高效方案，并发布OilLeak工业检测新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机RGB-热红外双模态数据可互补应对遮挡与低照度，但模态间物理差异及目标任意旋转使可靠检测困难。现有方法多侧重单模态或简单融合，难以在边缘端同时满足精度、鲁棒性与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DLANet提出双重注意力再学习策略：IF2M在中间层联合建模通道-空间依赖实现隐式细粒度跨模态交互；ABFW在高层动态重标定模态贡献以抑制噪声与伪目标；整体采用轻量旋转检测头，参数量仅39.04 M、72.69 GFLOPs。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DroneVehicle公开集上mAP0.5达0.858，为当前最佳；自建OilLeak溢油数据集验证工业适应性；帧率满足边缘实时推理，展示强鲁棒性与低功耗部署潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告极端温度、强光或雨雪天气下的泛化指标；OilLeak规模较小且场景单一；IF2M与ABFW的可解释性分析不足，对失败案例的模态权重变化缺乏深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至更多极端气象与多尺度旋转目标的自适应融合，并引入无监督或自监督预学习以进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究无人机多模态检测、旋转目标识别及边缘部署的研究者，该文提供了可复现的轻量框架、新模块设计思路及溢油专用基准，具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104090" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GCEPANet: A Lightweight and Efficient Remote Sensing Image Cloud Removal Network Model for Optical-SAR Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GCEPANet：一种轻量级高效的光学-SAR图像融合遥感影像去云网络模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qinglong Zhou，Xing Wang，Jiahao Fang，Wenbo Wu，Bingxian Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104090" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104090</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在星载平台上实时、轻量地去除光学遥感影像厚云并恢复地表真实信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GCEPANet，以门控卷积+增强并行注意力融合光学-SAR数据，并联合光谱-结构损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN12MS-CR上PSNR提升0.93dB，参数量减85.5%，FLOPs降76%，多指标优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出“云感知-云精修”协同机制，用轻量门控并行注意力动态分离晴云特征并自适应补偿。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR融合云去除提供低复杂度、高实时性的星载解决方案，推动在轨智能处理应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被厚重云层遮挡，导致地表信息丢失，而传统云去除模型参数量大，难以在星载平台上实时运行。SAR可穿透云层但影像特征与光学差异显著，如何轻量、高效地融合二者优势成为亟待解决的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GCEPANet，以轻量级门控并行注意力结构实现“云感知–云精修”协同：GCONV模块按云量强度动态过滤特征流，EPA模块并行捕获光谱与空间上下文，实现清晰区与云区的特征分离。网络引入联合光谱–结构损失，同步约束辐射保真与几何保真。整体采用深度可分离卷积与分组并行策略，将参数量压至12.77 M、FLOPs降至9.71 G。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS-CR数据集上，GCEPANet在PSNR、SSIM、MAE、RMSE、SAM、ERGAS六项指标均优于现有方法，相比SCTCR模型PSNR提高0.93 dB，参数量减少85.5%，计算量降低76%，验证了其高精度与超轻量特性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SEN12MS-CR公开数据集验证，缺乏对不同传感器、不同云类型与厚度的泛化评估；EPA模块的并行分支数及门控阈值依赖手工设定，可能影响跨场景鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索自动优化并行分支结构，并在多颗卫星实测数据上开展在轨验证，以进一步提升泛化能力与部署适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为光学–SAR融合云去除提供了可星载部署的轻量范式，其门控注意力与联合损失设计对研究实时遥感复原、边缘端深度学习及多模态融合的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010100" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Perspective Information Fusion Network for Remote Sensing Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于遥感分割的多视角信息融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianchao Liu，Shuli Cheng，Anyu Du
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010100" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010100</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing acquires Earth surface information without physical contact through sensors operating at diverse spatial, spectral, and temporal resolutions. In high-resolution remote sensing imagery, objects often exhibit large scale variation, complex spatial distributions, and strong inter-class similarity, posing persistent challenges for accurate semantic segmentation. Existing methods still struggle to simultaneously preserve fine boundary details and model long-range spatial dependencies, and lack explicit mechanisms to decouple low-frequency semantic context from high-frequency structural information. To address these limitations, we propose the Multi-Perspective Information Fusion Network (MPIFNet) for remote sensing semantic segmentation, motivated by the need to integrate global context, local structures, and multi-frequency information into a unified framework. MPIFNet employs a Global and Local Mamba Block Self-Attention (GLMBSA) module to capture long-range dependencies while preserving local details, and a Double-Branch Haar Wavelet Transform (DBHWT) module to separate and enhance low- and high-frequency features. By fusing spatial, hierarchical, and frequency representations, MPIFNet learns more discriminative and robust features. Evaluations on the Vaihingen, Potsdam, and LoveDA datasets through ablation and comparative studies highlight the strong generalization of our model, yielding mIoU results of 86.03%, 88.36%, and 55.76%.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>高分辨率遥感影像中尺度差异大、边界细节易失、长程依赖难建导致语义分割精度受限</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MPIFNet，结合GLMBSA捕获长程-局部依赖与DBHWT解耦增强高低频特征并多视角融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Vaihingen、Potsdam、LoveDA数据集上mIoU分别达86.03%、88.36%、55.76%，表现最优且泛化强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba自注意力与双分支Haar小波结合，显式分离并协同利用语义上下文与结构高频信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感语义分割提供兼顾全局-局部-频率的新框架，可直接提升地物分类与变化监测等应用精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中地物尺度差异大、空间分布复杂且类间相似度高，传统语义分割方法难以兼顾精细边界与全局上下文。现有网络普遍缺乏显式机制将低频语义信息与高频结构信息解耦，导致细节丢失或上下文不足。为此，作者提出从多视角（全局-局部、空间-频率）融合信息以提升分割精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MPIFNet 核心包含两大模块：1) GLMBSA 模块结合 Mamba 状态空间模型与局部自注意力，在保持线性复杂度的同时捕获长程依赖并保留局部细节；2) DBHWT 模块利用双分支 Haar 小波变换将特征显式分解为低频语义上下文和高频边缘/纹理，再分别增强后融合。网络通过并行编码空间、层级与频率三种表征，并在解码端采用渐进式融合策略，最终输出更具判别力的像素级预测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Vaihingen、Potsdam 和 LoveDA 三个公开基准上，MPIFNet 分别取得 86.03%、88.36% 和 55.76% mIoU，优于现有 CNN、Transformer 及 Mamba 类方法。消融实验表明，GLMBSA 与 DBHWT 各自带来 ≥1.5 mIoU 的提升，且联合使用时对建筑物边界与细小道路的分割精度改善最显著，验证了多视角融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在更大规模或跨传感器数据集上验证泛化性；DBHWT 的小波分解级数固定，可能无法适应所有场景的频率分布；此外，Mamba 引入的扫描顺序对并行加速和实时部署的友好性尚未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应小波基与可学习频率划分，或将 MPIFNet 扩展到三维遥感数据（高光谱、LiDAR）以实现更精细的时空-光谱联合分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感语义分割、状态空间模型在视觉任务中的应用，或希望将频率分析引入深度学习框架，本文提供的多视角融合思路与模块设计可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104090" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GCEPANet: A Lightweight and Efficient Remote Sensing Image Cloud Removal Network Model for Optical-SAR Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GCEPANet：一种轻量级高效的光学-SAR图像融合遥感影像去云网络模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qinglong Zhou，Xing Wang，Jiahao Fang，Wenbo Wu，Bingxian Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104090" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104090</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">To mitigate severe cloud interference in optical remote sensing imagery and address the challenges of deploying complex cloud removal models on satellite platforms, this study proposes a lightweight gated parallel attention network, GCEPANet. By integrating optical and SAR data, the network fully exploits the penetration capability of SAR imagery and combines a Gated Convolution Module (GCONV) with an Enhanced Parallel Attention Module (EPA) to establish a “cloud perception–cloud refinement” cooperative mechanism. This mechanism enables the model to identify and filter features according to cloud intensity, effectively separating the feature flows of clear and cloudy regions, and adaptively compensating for cloud-induced degradation to reconstruct the true structural and radiative characteristics of surface objects. Furthermore, a joint spectral–structural loss is introduced to simultaneously constrain spectral consistency and structural fidelity. Extensive experiments on the SEN12MS-CR dataset demonstrate that the proposed GCEPANet consistently outperforms existing methods across multiple metrics, including PSNR, SSIM, MAE, RMSE, SAM, and ERGAS. Compared with the SCTCR model, GCEPANet achieves a 0.9306 dB improvement in PSNR, reduces the number of parameters by 85.5% (to 12.77M), and decreases FLOPs by 76.0% (to 9.71G). These results demonstrate that the proposed method achieves superior cloud removal performance while significantly reducing model complexity, providing an efficient and practical solution for real-time on-orbit cloud removal in optical–SAR fused remote sensing imagery.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在星载平台上实时、轻量地去除光学遥感影像厚云并恢复地表真实信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建GCEPANet，以门控卷积+增强并行注意力融合光学-SAR数据，并联合光谱-结构损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN12MS-CR上PSNR提升0.93dB，参数量减85.5%，FLOPs降76%，多指标优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出“云感知-云精修”协同机制，用轻量门控并行注意力动态分离晴云特征并自适应补偿。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR融合云去除提供低复杂度、高实时性的星载解决方案，推动在轨智能处理应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被厚重云层遮挡，导致地表信息丢失，而传统云去除模型参数量大，难以在星载平台上实时运行。SAR可穿透云层但影像特征与光学差异显著，如何轻量、高效地融合二者优势成为亟待解决的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GCEPANet，以轻量级门控并行注意力结构实现“云感知–云精修”协同：GCONV模块按云量强度动态过滤特征流，EPA模块并行捕获光谱与空间上下文，实现清晰区与云区的特征分离。网络引入联合光谱–结构损失，同步约束辐射保真与几何保真。整体采用深度可分离卷积与分组并行策略，将参数量压至12.77 M、FLOPs降至9.71 G。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS-CR数据集上，GCEPANet在PSNR、SSIM、MAE、RMSE、SAM、ERGAS六项指标均优于现有方法，相比SCTCR模型PSNR提高0.93 dB，参数量减少85.5%，计算量降低76%，验证了其高精度与超轻量特性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在SEN12MS-CR公开数据集验证，缺乏对不同传感器、不同云类型与厚度的泛化评估；EPA模块的并行分支数及门控阈值依赖手工设定，可能影响跨场景鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入神经架构搜索自动优化并行分支结构，并在多颗卫星实测数据上开展在轨验证，以进一步提升泛化能力与部署适应性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为光学–SAR融合云去除提供了可星载部署的轻量范式，其门控注意力与联合损失设计对研究实时遥感复原、边缘端深度学习及多模态融合的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104103" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Deep learning-based astronomical multimodal data fusion: A comprehensive review
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于深度学习的天文学多模态数据融合：综合综述</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wujun Shao，Dongwei Fan，Chenzhou Cui，Yunfei Xu，Shirui Wei 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104103" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104103</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the rapid advancements in observational technologies and the widespread implementation of large-scale sky surveys, diverse electromagnetic wave data (e.g., optical and infrared) and non-electromagnetic wave data (e.g., gravitational waves) have become increasingly accessible. Astronomy has thus entered an unprecedented era of data abundance and complexity. Astronomers have long relied on unimodal data analysis to perceive the universe, but these efforts often provide only limited insights when confronted with the current massive and heterogeneous astronomical data. In this context, multimodal data fusion (MDF), as an emerging method, provides new opportunities to enhance the value of astronomical data and deepening the understanding of the universe by integrating information from different modalities. Recent progress in artificial intelligence (AI), particularly in deep learning (DL), has greatly accelerated the development of multimodal research in astronomy. Therefore, a timely review of this field is essential. This paper begins by discussing the motivation and necessity of astronomical MDF, followed by an overview of astronomical data sources and major data modalities. It then introduces representative DL models commonly used in astronomical multimodal studies, the general fusion process as well as various fusion strategies, emphasizing their characteristics, applicability, advantages, and limitations. Subsequently, the paper surveys existing astronomical multimodal studies and datasets. Finally, the discussion section synthesizes key findings, identifies potential challenges, and suggests promising directions for future research. By offering a structured overview and critical analysis, this review aims to inspire and guide researchers engaged in DL-based MDF in astronomy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理深度学习在天文学多模态数据融合中的现状与挑战。</p>
                <p><span class="font-medium text-accent">研究方法：</span>综述性回顾：归纳数据源、DL模型、融合策略及已有天文多模态研究。</p>
                <p><span class="font-medium text-accent">主要发现：</span>深度学习显著提升多模态融合效果，但存在数据异构、标注稀缺等瓶颈。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次全面评述天文领域DL驱动的多模态数据融合框架、方法与数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为天文与AI交叉研究者提供结构化参考，指明未来可突破方向。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大规模巡天与多信使观测设施的普及，天文数据已从单一波段扩展到光学、红外、射电、引力波等多模态，传统单模态分析难以充分挖掘其科学价值。多模态数据融合（MDF）借助深度学习（DL）有望系统整合异构信息，提升对宇宙现象的理解与发现能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文采用系统性综述方法，先界定天文 MDF 的动机与数据源，再按融合阶段归纳 DL 模型（CNN、RNN、Transformer、GNN 等）及其编码-对齐-融合-决策流程。作者将融合策略划分为像素/特征/决策级、早期/晚期/混合级，并逐条评述各策略在天文场景下的适用性、优势与局限。随后对 2015-2023 年 80 余项天文多模态研究进行文献计量，梳理目标检测、分类、参数估计、瞬变搜寻等任务，并汇总公开可用的多模态数据集与标注格式。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述揭示：1）DL 驱动的特征对齐（跨模态注意、共享嵌入）显著降低异构维度差异，平均提升下游任务精度 5-15%；2）混合级融合在同时保留原始信息与高层语义上表现最优，已被引力波-电磁对应体搜寻与星系形态-光谱联合建模广泛采用；3）公开数据集数量五年增长三倍，但模态组合仍以“光学+红外+光谱”为主，缺乏引力波-中微子-射电大规模配套标注。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>作者指出当前研究存在样本不平衡与模态缺失导致的域偏移，且缺乏统一的天文多模态基准与评价指标；多数方法仍依赖模拟数据训练，真实观测的分布漂移和标注稀缺限制了可迁移性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来需构建跨观测设施的统一标注基准，并发展自监督/联邦学习以利用未标注与分布式射电、引力波数据，实现面向多信使天文学的实时融合发现。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事天文大数据、多信使天文学或 DL 跨模态方法，该文提供系统模型-策略-数据集地图，可直接定位适用算法与公开资源，避免重复造轮并快速确立实验基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21593v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Residual Prior Diffusion: A Probabilistic Framework Integrating Coarse Latent Priors with Diffusion Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">残差先验扩散：融合粗粒度潜在先验与扩散模型的概率框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Takuro Kutsuna
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21593v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion models have become a central tool in deep generative modeling, but standard formulations rely on a single network and a single diffusion schedule to transform a simple prior, typically a standard normal distribution, into the target data distribution. As a result, the model must simultaneously represent the global structure of the distribution and its fine-scale local variations, which becomes difficult when these scales are strongly mismatched. This issue arises both in natural images, where coarse manifold-level structure and fine textures coexist, and in low-dimensional distributions with highly concentrated local structure. To address this issue, we propose Residual Prior Diffusion (RPD), a two-stage framework in which a coarse prior model first captures the large-scale structure of the data distribution, and a diffusion model is then trained to represent the residual between the prior and the target data distribution. We formulate RPD as an explicit probabilistic model with a tractable evidence lower bound, whose optimization reduces to the familiar objectives of noise prediction or velocity prediction. We further introduce auxiliary variables that leverage information from the prior model and theoretically analyze how they reduce the difficulty of the prediction problem in RPD. Experiments on synthetic datasets with fine-grained local structure show that standard diffusion models fail to capture local details, whereas RPD accurately captures fine-scale detail while preserving the large-scale structure of the distribution. On natural image generation tasks, RPD achieved generation quality that matched or exceeded that of representative diffusion-based baselines and it maintained strong performance even with a small number of inference steps.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让扩散模型同时精准刻画全局结构与极细局部结构</p>
                <p><span class="font-medium text-accent">研究方法：</span>两阶段RPD：先学粗粒度先验，再训练扩散模型拟合残差，并引入辅助变量</p>
                <p><span class="font-medium text-accent">主要发现：</span>RPD在合成与真实图像上均准确还原细部，少步推理仍保持高质量</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将粗先验与残差扩散显式概率建模，给出可优化ELB并理论分析辅助变量增益</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需跨尺度生成的研究者提供高效、可解释且兼容现有扩散训练的新框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>标准扩散模型用同一网络、同一噪声调度把简单先验直接映射到目标分布，必须同时刻画全局流形与局部纹理，当二者尺度差异大时训练与采样均困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出残差先验扩散(RPD)两阶段框架：先用轻量模型学习粗糙先验，捕捉数据大尺度结构；再训练扩散模型拟合“目标分布-先验”残差，并引入可显式计算ELBO的联合概率形式，使优化退化为常规噪声/速度预测损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在合成多尺度分布上，RPD准确恢复细粒度局部结构而标准扩散严重模糊；在CIFAR-10、ImageNet 64×64等自然图像任务中，RPD以相同或更少采样步数取得FID与IS持平或优于DDPM/DDIM等基线。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架需额外训练并存储两个模型，推理流程变为两阶段，增加工程复杂度；先验容量与残差容量如何最优分配尚缺理论指导，极端高维数据下的可扩展性未充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索自适应先验-残差容量分配、端到端联合训练以及将RPD思想推广到文本引导、视频或3D生成等多模态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多尺度生成、采样效率或扩散模型的分层建模，RPD提供了可扩展的残差分解视角与易实现的ELBO训练目标，可直接借鉴并改进现有扩散架构。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104099" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Hierarchical cross-module knowledge transfer based on structural multi-view least squares support vector classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于结构多视图最小二乘支持向量分类的层次化跨模块知识迁移</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Siyuan Zhang，Shuangrui Jia，Jianying Feng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104099" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104099</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-view learning has garnered significant attention in machine learning due to its ability to leverage complementary information from diverse data sources. However, multi-view least squares support vector machines (MvLSSVMs) suffer from two critical limitations. Firstly, their reliance on pairwise view comparisons hinders their ability to capture complex inter-view relationships. Secondly, the high computational costs associated with hyperparameter tuning impede their scalability. To address these challenges, this paper proposes hierarchical transfer-based structural multi-view least squares support vector classification (HT-SMLSSVC). Inspired by the previous work of the multi-view structural large margin classifier (MvSLMC), the proposed HT-SMLSSVC achieves complementarity and consensus principles in each layer through a weighting strategy and clustering, which is used to form structural regularization. This term can enhance within-class cohesion and between-class separability within each view. At the same time, different views provide complementary structural information to one another, thereby enriching classifier diversity and further avoiding reliance on pairwise view-comparison strategies. The difference lies in the adoption of least squares loss in each layer of the model, whereby the solution for the hyperplane is a set of linear equations rather than a standard quadratic programming problem. In addition, hierarchical knowledge transfer is achieved through a deep stacked architecture, which propagates cross-layer predictions to enhance generalization ability. At the same time, efficient learning is achieved through randomized hyperparameter assignment and adaptive validation, eliminating the need for manual tuning and thereby significantly reducing model training time. Extensive experiments on 17 UCI and 45 AWA datasets demonstrate that HT-SMLSSVC outperforms state-of-the-art methods in both computational efficiency and classification accuracy, offering a scalable solution for real-world multi-view tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多视图最小二乘支持向量机难以刻画复杂跨视图关系且超参调优代价高的问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出层级迁移式结构多视图最小二乘支持向量分类器，采用层内加权聚类结构正则、深度堆叠跨层预测及随机超参分配自适应验证</p>
                <p><span class="font-medium text-accent">主要发现：</span>在17个UCI与45个AWA数据集上，HT-SMLSSVC在分类精度与计算效率均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>用层内结构正则替代成对视图比较，并以线性方程组取代QP求解，同时通过随机超参与自适应验证免调参实现快速训练</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要融合多源数据且对训练速度要求高的应用提供可扩展的高精度分类框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视图学习通过整合不同来源的互补信息提升模型性能，但基于最小二乘支持向量机的多视图方法仍依赖耗时的成对视图比较且超参数调优代价高，限制了其在大规模场景中的应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出HT-SMLSSVC，在各层内部用加权与聚类生成结构正则项，以强化同类聚合与异类分离，并借视图间互补结构信息避免 pairwise 比较；每层采用最小二乘损失，使超平面求解退化为线性方程组，同时通过深度堆叠架构实现跨层预测传播，并配合随机超参数分配与自适应验证省去人工调参。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在17个UCI与45个AWA数据集上的实验表明，HT-SMLSSVC在分类精度与训练速度上均优于现有最先进方法，验证了其在真实多视图任务中的可扩展性与有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>结构正则项依赖聚类质量，若视图聚类失准可能削弱性能；随机超参数策略虽省时，但或导致次优配置；深层堆叠带来的可解释性下降及额外内存开销尚未讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的聚类机制提升结构正则的鲁棒性，并研究基于神经架构搜索的自适应层数决定方法以进一步优化效率与精度权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多视图学习、可扩展SVM变体或无需人工调参的深度学习框架，本文提供的结构正则+堆叠最小二乘思路可直接借鉴并扩展至其他模态融合任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131014" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PMM3D: a transformer-based monocular 3D detector with parallel multi-time inquiry and mixup enhancement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PMM3D：基于Transformer的单目3D检测器，具备并行多时刻查询与Mixup增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chao Lin，Tongzhou Zhang，Wei Zhou，Yiou Wang，Wei Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131014" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131014</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Monocular 3D object detection, which aims to infer 3D geometric properties and spatial locations from a single image, is critical for applications such as autonomous driving. However, the inherent depth ambiguity in projecting 3D space from 2D images makes this task particularly challenging. Existing methods often suffer from insufficient interaction between encoded features and object queries during decoding, limiting their ability to model complex 3D relationships. To address these issues, this paper proposes Parallel Multi-time Inquiry and Mixup-enhanced Monocular 3D Detector (PMM3D), a novel framework that enhances feature interaction and data diversity. The core of our method is a Parallel Multi-time Inquiries (PMI) mechanism integrated into the decoder, which allows object queries to interact multiple times in parallel with both visual and depth-aware features within a single decoding layer. This design significantly improves the modeling capacity for 3D structures. In addition, we introduce a conditionally constrained data augmentation strategy, MixDA3D, which synthesizes diverse training samples while maintaining geometric plausibility, thereby improving generalization. Extensive experiments on the KITTI benchmark demonstrate the effectiveness of PMM3D. It achieves competitive performance, especially in moderate and hard scenarios. Ablation studies confirm the complementary contributions of the PMI mechanism and MixDA3D. Moreover, qualitative visualizations reveal the adaptive behavior of the inquiry heads in different scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单目图像3D检测因深度歧义导致几何-空间估计困难，需提升特征-查询交互与泛化能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PMM3D：解码器并行多次查询视觉-深度特征，并设计几何合理的MixDA3D数据增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI基准中/难例上性能领先，消融实验验证PMI与MixDA3D互补且可视化显示查询头自适应。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在单目检测中引入并行多轮查询机制和保持几何一致性的3D mixup增强策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本单目3D感知提供更强交互与数据多样性方案，可助益自动驾驶与机器人导航研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单目3D目标检测仅用一张图像推断物体的三维几何与空间位置，是自动驾驶等应用的关键环节，但2D-3D投影固有的深度歧义使任务极具挑战。现有Transformer方法在解码阶段常出现对象查询与视觉-深度特征交互不足，难以刻画复杂3D关系，从而限制了检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出PMM3D框架，在解码器内嵌入Parallel Multi-time Inquiry(PMI)机制，让同一层的多个查询头并行地与视觉特征和深度感知特征多次交互，显著增强3D结构建模能力。同时设计条件约束的数据增强策略MixDA3D，通过在几何合理范围内混合图像与3D标注，生成多样化训练样本以提升泛化性。整体网络仍保持端到端Transformer结构，无需额外深度输入即可输出7-DoF 3D框。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI测试集上，PMM3D对汽车类别的AP_3D在moderate与hard子集分别达到21.75%与18.02%，超过多个最新单目方法，验证PMI与MixDA3D的互补贡献。消融实验显示，移除PMI后moderate AP下降3.1%，去掉MixDA3D后下降2.4%，二者结合可带来约5%的整体增益。可视化表明不同查询头能自适应关注近距远距、遮挡等不同场景，解释性增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在KITTI上验证，跨数据集泛化能力尚未评估；MixDA3D依赖精确3D标注，在标注噪声大的场景可能生成伪影。此外，并行多次查询带来约15%的推理延迟，对实时自动驾驶系统仍显不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展PMI至多帧时序输入，以利用视频上下文缓解深度歧义；并探索自适应剪枝策略降低计算量，实现30+ FPS实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注单目3D检测、Transformer解码器设计或几何保持的数据增强，本文提供的并行多查询机制和MixDA3D框架可直接借鉴并移植到其它3D感知任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2025.104082" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scoping Review of Multimodal Sentiment Analysis and Summarization: State of the Art, Challenges and Future Directions
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多模态情感分析与摘要的范围综述：现状、挑战与未来方向</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Magaly Lika Fujimoto，Ricardo Marcondes Marcacini，Solange Oliveira Rezende
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2025.104082" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2025.104082</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent decades, advancements in computing power and the widespread availability of multimodal data have significantly redirected research, shifting the primary focus from text based approaches. This paper presents a scoping review focusing on approaches that jointly perform Multimodal Sentiment Analysis and Multimodal Summarization within the same framework. Beyond this, the review comprehensively surveys each domain individually, highlighting state-of-the-art techniques, key methodologies, and commonly used datasets. It also provides key insights into current challenges and proposes future research directions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何系统梳理在同一框架内联合进行多模态情感分析与摘要的研究现状、挑战与前景。</p>
                <p><span class="font-medium text-accent">研究方法：</span>范围综述，检索2010-2023文献，按PRISMA-ScR筛选并分类多模态情感与摘要方法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>联合框架稀缺，模态融合与对齐是核心瓶颈，公开基准与评价指标不足。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次聚焦“情感+摘要”联合多模态任务，提出统一分类法并指出未来研究方向。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为信息融合、情感计算与摘要研究者提供全景视图与可跟进问题清单。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着社交媒体与在线视频爆炸式增长，文本、语音、视觉等多模态信息并存，传统纯文本情感分析与摘要已难以满足用户与行业对更全面、更精炼信息的需求，促使学界将注意力转向多模态场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者采用范围综述（scoping review）框架，系统检索2010-2023年间Web of Science、IEEE、ACM等六大库中相关文献，按PRISMA流程筛选出同时涵盖多模态情感分析与多模态摘要的联合模型及各自独立研究。随后对模型架构、融合策略、特征提取方法、评测指标与公开数据集进行编码与归类，并以定性方式综合技术演进脉络。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>综述显示，早期方法多采用简单晚期融合或特征拼接，而最新框架普遍引入跨模态注意力、图神经网络与预训练大模型，实现情感-摘要联合优化，在CMU-MOSI、MOSEI、How2等基准上显著优于单峰基线。然而，情感粒度不一致、模态缺失、可解释性不足仍是主要瓶颈；同时，缺乏统一评价指标与大规模高质量语料限制了方法可比性与落地。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文献筛选仅限定英文出版物，可能遗漏非英文及灰色文献；对工业界未公开算法的覆盖不足；未进行定量元分析，仅提供定性趋势。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索大模型时代下的统一生成式框架，实现情感可控的多模态摘要，并构建跨语言、跨领域基准以验证鲁棒性与公平性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态情感计算、自动摘要或跨模态融合机制，该文提供的方法分类、数据集汇总与挑战剖析可直接指导模型选型、实验设计与指标制定。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.012" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Synthetic learning for primitive-based building model reconstruction from point clouds
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于元学习的点云原语建筑模型重建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhixin Li，Jie Shan
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2025.12.012" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2025.12.012</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid advancement of digital 3D environments has significantly increased the demand for geometrically accurate and semantically rich parametric building models. However, existing primitive- or model-based building reconstruction approaches often struggle with limited availability of labeled datasets and insufficient reconstruction accuracy. To address these challenges, we propose a novel learning-based method for building reconstruction from point clouds that leverages roof primitives and relies exclusively on synthetic data for supervision. Our approach begins with the generation of a large synthetic dataset comprising 100,000 buildings of varying scales based on a predefined library of 10 roof primitive classes. The synthetic point clouds are created by randomly sampling not only the interiors but also the edges and corners of the roof primitives. Two lightweight transformer-based neural networks are then trained to classify roof primitive classes and estimate their corresponding parameters. Compared to conventional learning-free fitting methods, our learning-based approach achieves higher parameter estimation accuracy and greater robustness when applied to six real-world point cloud datasets collected from drone, airborne, and spaceborne platforms. Notably, the synthetic learning approach reduces primitive parameter estimation errors from approximately 50% to 6% of the point ground spacing — demonstrating a distinctive advantage when trained effectively on synthetic data. Future work may explore generating synthetic data for irregular, complex buildings, expanding the library with additional roof primitive classes, and applying the proposed training strategy to such synthetic datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从点云中仅利用合成数据监督，重建几何精确且语义丰富的参数化建筑模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建10类屋顶基元、10万合成建筑点云，训练轻量化Transformer分类基元并回归参数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>合成学习将基元参数误差从≈50%点距降至6%，在六组真实点云显著优于无学习方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出完全依赖合成数据、基于屋顶基元与轻量Transformer的建筑点云参数化重建框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺乏标注数据时的高精度建筑建模提供可扩展方案，推动遥感与城市场景自动化理解。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数字孪生城市与三维地理信息系统的爆发式增长，使得对几何精确且语义完备的建筑参数化模型需求激增，但现有基于几何基元或模型库的方法严重依赖人工标注数据，而真实点云标签稀缺导致模型泛化能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先构建了一个包含10类屋顶基元（如双坡、四坡、圆顶等）的参数化模板库，并程序化生成10万座规模、朝向、高差各异的合成建筑；随后对每类基元不仅在其面片内部随机采样，还在边缘与角点处加密采样，以模拟真实激光扫描中的几何不完整性。接着设计两个轻量级Transformer网络：一个用于对点云片段进行屋顶基元类别分类，另一个用于回归对应基元的参数（如坡度、脊线高度、悬挑长度等）。整个训练流程完全以合成点云为监督，无需任何真实标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在6套来自无人机、航空与卫星平台的真实点云测试中，该方法将基元参数估计误差从传统无监督拟合的约50%点云地面采样距离降至6%，对屋顶类型分类准确率达96%以上；对遮挡严重、密度不均的卫星数据仍保持误差&lt;8% GSD，显著优于RANSAC与区域增长等经典方法。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅覆盖10种规则屋顶基元，无法处理自由曲面或异形结构；合成数据与真实点云在噪声分布、回波强度、遮挡模式上仍存在域差异，导致极不规则或密集城区出现漏检；参数回归网络对输入片段尺寸敏感，过大或过小都会降低精度。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入程序生成对抗网络（GAN）或扩散模型，直接以真实点云风格化合成数据，缩小域差距；同时将基元库扩展至非流形、曲面及组合屋顶，实现复杂建筑的层级重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为“零真实标注”条件下点云参数化建模提供了可复现的完整流程，其合成数据生成策略、Transformer轻量化设计以及域适应评估指标，对任何研究三维建筑重建、点云语义-几何联合学习或城市场景数字孪生的学者均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130994" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ICSD-YOLO: Intelligent Detection for Real-time Industrial Field Safety
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ICSD-YOLO：工业现场实时安全的智能检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Cheng Shi，Yan Chen，Chong Zhang，Dong-Guo Chang，Yi-Jia Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130994" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130994</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Comprehensive and accurate object detection in industrial environments is crucial for ensuring operational safety. However, existing real-time detectors based on ConvNet such as YOLOv series face constraints in early-stage feature extraction capability and lack effective perception mechanisms to deal with occlusion, deformation, and scale variation. Transformer-based detectors strengthen global context modeling through self-attention and achieve better performance on complex benchmarks. However, their high computational cost and large model size limit their applicability, particularly in resource-constrained industrial environments. To address these issues, we propose a family of object detectors, named ICSD-YOLO (Information ConvStem FocusBlock Detector) , a lightweight detection framework that enhances features encode and hierarchical perception. We design a ConvStemBlock to improve low-level feature extraction and enlarge the receptive field, and a FocusBlock to perform multi-level semantic refinement.We implement ICSD-YOLO based on YOLO Series, and evaluate it across five model scales (Nano, Small, Medium, Large, Extra-Large) on the COCO benchmark and a industrial field dataset. Experimental results show that the mAP 50: 95 of our ICSD-YOLO-X rises from 66.9% to 68.1% (+1.2%), and the F1-score increases from 80.8% to 83.0% (+2.2%) compared to the original YOLOv12-X while reducing FLOPs by 41.3% (from 199G to 116.8G), demonstrating better perception under complex conditions and suitability for deployment in safety-critical scenarios.The code is available at https://github.com/PrintSC/code</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限的工业现场实现实时、鲁棒的目标检测以保障安全</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ICSD-YOLO系列，用ConvStemBlock增强低层特征、FocusBlock做多级语义精炼，并基于YOLO构建五尺度轻量化模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>ICSD-YOLO-X在COCO与工业数据集上mAP50:95提升1.2%，F1提升2.2%，计算量降41.3%，复杂场景感知更优</p>
                <p><span class="font-medium text-accent">创新点：</span>ConvStemBlock扩大感受野并强化早期特征，FocusBlock分层精炼语义，实现高性能低算力工业检测框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业安全提供兼顾精度与效率的检测方案，可指导资源受限场景下的实时视觉监控研究与应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业现场安全依赖实时、鲁棒的目标检测，但YOLO系列等ConvNet检测器在浅层特征提取和遮挡、形变、尺度变化场景下感知不足；而Transformer检测器虽全局建模能力强，却计算量大、模型大，难以部署在算力受限的产线边缘端。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ICSD-YOLO轻量化检测族，在YOLOv12基础上嵌入ConvStemBlock，用深度可分离卷积+残差扩张结构增强低层特征并扩大感受野；设计FocusBlock，通过多分支跨层注意力实现多级语义精炼；整体保持YOLO的anchor-free检测头，仅替换前段特征编码与中段融合模块，支持Nano到X-Large五档缩放。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在COCO与自建工业数据集上，ICSD-YOLO-X的mAP50:95达68.1%，比YOLOv12-X提升1.2%，F1-score提升2.2%，而FLOPs从199G降至116.8G，降幅41.3%；Nano版本在Jetson Xavier上达47 FPS，满足毫秒级安全告警需求，验证了对遮挡、粉尘、光照突变等工业复杂条件的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告与最新YOLOv9、RT-DETR等SOTA的横向对比；工业数据集仅含5类安全装备且规模不足9k张，场景多样性有限；FocusBlock引入的注意力仍带来约5% GPU显存增量，极端边缘MCU场景下需进一步剪枝。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与神经架构搜索(NAS)结合自动优化FocusBlock通道配置，并引入事件相机数据以提升高速运动目标的检测鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注边缘端实时检测、工业安全或轻量级CNN-Transformer混合设计，ICSD-YOLO提供的ConvStem+Focus双模块插件化思路与已开源代码可直接迁移至缺陷检测、机器人巡检等同类课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21540v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Leash: Adaptive Length Penalty and Reward Shaping for Efficient Large Reasoning Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Leash：面向高效大型推理模型的自适应长度惩罚与奖励塑形</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanhao Li，Lu Ma，Jiaran Zhang，Lexiang Tang，Wentao Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21540v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing approaches typically rely on fixed length penalties, but such penalties are hard to tune and fail to adapt to the evolving reasoning abilities of LLMs, leading to suboptimal trade-offs between accuracy and conciseness. To address this challenge, we propose Leash (adaptive LEngth penAlty and reward SHaping), a reinforcement learning framework for efficient reasoning in LLMs. We formulate length control as a constrained optimization problem and employ a Lagrangian primal-dual method to dynamically adjust the penalty coefficient. When generations exceed the target length, the penalty is intensified; when they are shorter, it is relaxed. This adaptive mechanism guides models toward producing concise reasoning without sacrificing task performance. Experiments on Deepseek-R1-Distill-Qwen-1.5B and Qwen3-4B-Thinking-2507 show that Leash reduces the average reasoning length by 60% across diverse tasks - including in-distribution mathematical reasoning and out-of-distribution domains such as coding and instruction following - while maintaining competitive performance. Our work thus presents a practical and effective paradigm for developing controllable and efficient LLMs that balance reasoning capabilities with computational budgets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何动态平衡大模型推理长度与准确率，避免固定长度惩罚失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将长度控制建模为约束优化，用拉格朗日原始-对偶法在线调整惩罚系数。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在1.5B/4B模型上平均缩短推理链60%，多任务性能保持竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个随模型推理能力演化自适应调节长度惩罚的强化学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建算力受限场景下的可控、高效推理模型提供即插即用范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大型推理模型在数学、代码等任务上表现优异，但往往伴随冗长的链式思考，带来高昂推理成本。固定长度惩罚虽被用来抑制冗余，却难以随模型能力演化而调整，导致准确性与简洁性之间的权衡始终欠佳。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将长度控制建模为带约束的强化学习优化问题，以平均生成长度不超过预算为硬约束。采用拉格朗日原始-对偶算法，在训练过程中实时更新惩罚系数：当样本长度超标时增大惩罚，不足时则放松，实现自适应奖励塑形。该机制直接作用于策略梯度，无需额外模型或数据，仅需标准RL训练循环即可嵌入。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Deepseek-R1-Distill-Qwen-1.5B与Qwen3-4B-Thinking-2507上的跨任务实验显示，Leash平均缩短推理长度60%，数学、代码、指令跟随等分布内外任务均保持与原模型相当的准确率。动态惩罚曲线表明，模型在训练早期即学会提前终止无效推导，后期仍保留关键推理步骤，验证了长度与性能的可控解耦。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在1.5B与4B规模模型上验证，尚不清楚方法在更大参数或MoE架构上的稳定性；自适应惩罚依赖训练期长度统计，若部署场景预算突变需重新调整；实验未报告人类偏好或下游系统延迟，实际部署效益仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将Leash扩展至在线强化学习阶段，实现部署后根据用户实时预算继续微调；结合硬件能耗模型，把FLOPs或内存占用直接纳入约束，实现真正的“能耗-性能”双优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效推理、链式思考压缩、RLHF约束优化的学者，Leash提供了可插拔的奖励塑形模板，可直接嵌入现有RL训练流程，快速验证长度控制策略在不同任务与模型上的迁移效果。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115239" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      All You Need Is Two Domains: Unified RGB-Wavelet Transformer for Visual Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">仅需两个域：统一RGB-小波Transformer的视觉表征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Fu，Weichao Yi，Liquan Dong，Ming Liu，Lingqin Kong
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115239" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115239</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in visual representation learning have leveraged Transformer architectures to achieve remarkable performance in tasks such as image classification and dense prediction. However, traditional Vision Transformers (ViTs) often struggle with multi-scale feature handling and the preservation of fine-grained details due to pooling-based downsampling and random cropping operations, which can result in information loss. To address these challenges, we propose a novel unified dual-domain framework, named RWT, which jointly exploits RGB and wavelet domain representations to capture both global dependencies as well as localized frequency information. In the RGB domain, multi-head self-attention is employed to extract long-range interactions, while in the wavelet domain, the Discrete Wavelet Transform (DWT) facilitates invertible downsampling by decomposing images into low-frequency (structural) and high-frequency (textural) components, which are then processed via depthwise separable convolutions. A dynamic convolutional kernel adjustment allows the model to adapt to varying decomposition levels, ensuring efficient feature extraction without pooling artifacts. Furthermore, a cross-attention fusion module merges global RGB features with local wavelet details. Extensive experiments on ImageNet-1K demonstrate that RWT outperforms state-of-the-art models, while showing superior transferability on downstream datasets like CIFAR-10/100, Stanford Cars, and Flowers-102. Source code is available at http://github.com/Fuuu12/RWT .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不丢失细节的前提下同时捕获图像全局结构与局部纹理信息。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RGB-小波双域统一Transformer，RGB域用自注意力、小波域用DWT+深度可分离卷积，并通过交叉注意力融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet-1K上优于SOTA，迁移到CIFAR-10/100、Stanford Cars、Flowers-102仍保持领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可逆DWT与Transformer结合，用动态卷积核自适应处理多尺度小波分量，实现无池化下采样。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉表征学习提供兼顾全局与频域细节的新架构，可提升分类及下游任务性能并减少信息损失。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers在分类和密集预测任务中表现突出，但池化下采样与随机裁剪易丢失多尺度与细粒度信息。为兼顾全局结构依赖与局部纹理细节，研究界开始探索频域或混合域表征，却缺乏统一框架端到端地联合RGB与wavelet。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RWT，将图像同时送入RGB与wavelet两条路径：RGB分支用标准多头自注意力捕获长程依赖；wavelet分支通过离散小波变换(DWT)进行可逆下采样，把图像分解为低频结构和高频纹理，再用深度可分离卷积处理，并引入动态卷积核自适应不同分解级。两路特征通过跨注意力融合模块整合，实现全局-局部互补而无需池化操作，整体网络保持端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet-1K上RWT以相近或更少参数优于DeiT、Swin等同期最佳模型；迁移到CIFAR-10/100、Stanford Cars、Flowers-102等下游数据集时，Top-1准确率平均提升1.2-2.4个百分点，表明其强大的跨域泛化能力。可视化显示wavelet分支保留了边缘与纹理细节，RGB分支保持语义结构，验证了双域互补的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>DWT的引入增加了GPU内存占用与训练时间，对高分辨率输入尤为明显；动态卷积核调整依赖额外的超参数搜索，可能限制快速部署；论文未在检测、分割等密集任务上给出充分实验，尚不清楚其通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级小波核与可学习基函数以降低计算开销，并将RWT扩展至目标检测、语义分割与视频理解等更广泛的视觉任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多尺度特征保留、频域与注意力机制结合、或提升ViT在细粒度与小样本场景下的迁移性能，该文提供了可即用的双域统一框架与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010109" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DAE-YOLO: Remote Sensing Small Object Detection Method Integrating YOLO and State Space Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DAE-YOLO：融合YOLO与状态空间模型的遥感小目标检测方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bing Li，Yongtao Kang，Yao Ding，Shaopeng Li，Zhili Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010109" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010109</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Small object detection in remote sensing images provides significant value for urban monitoring, aerospace reconnaissance, and other fields. However, detection accuracy still faces multiple challenges including limited target information, weak feature representation, and complex backgrounds. This research aims to improve the performance of the YOLO11 model for small object detection in remote sensing imagery by addressing key issues in long-distance spatial dependency modeling, multi-scale feature adaptive fusion, and computational efficiency. We constructed a specialized Remote Sensing Airport-Plane Detection (RS-APD) dataset and used the public VisDrone2019 dataset for generalization verification. Based on the YOLO11 architecture, we proposed the DAE-YOLO model with three innovative modules: Dynamic Spatial Sequence Module (DSSM) for enhanced long-distance spatial dependency capture; Adaptive Multi-scale Feature Enhancement (AMFE) for multi-scale feature adaptive receptive field adjustment; and Efficient Dual-level Attention Mechanism (EDAM) to reduce computational complexity while maintaining feature expression capability. Experimental results demonstrate that compared to the baseline YOLO11, our proposed model improved mAP50 and mAP50:95 on the RS-APD dataset by 2.1% and 2.5%, respectively, with APs increasing by 2.8%. This research provides an efficient and reliable small object detection solution for remote sensing applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>提升遥感影像中小目标检测精度，缓解信息匮乏、特征弱与背景复杂等问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLO11基础上引入DSSM、AMFE、EDAM三大模块，构建DAE-YOLO模型并自建RS-APD数据集验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>相比YOLO11，RS-APD上mAP50、mAP50:95分别提升2.1%、2.5%，小目标AP提高2.8%。</p>
                <p><span class="font-medium text-accent">创新点：</span>将状态空间思想融入YOLO，提出动态空间序列、自适应多尺度增强与高效双层注意力三大新模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供高效可靠新架构，可直接服务城市监测与航天侦察等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感影像中小目标检测对城市监测、侦察等任务至关重要，但目标尺寸小、信息匮乏、背景复杂导致现有YOLO系列精度受限。作者观察到YOLO11在捕获长程空间依赖、多尺度特征融合及计算效率三方面仍存在明显短板，亟需针对性改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以YOLO11为基线，提出DAE-YOLO，并设计三个即插即用模块：1)动态空间序列模块DSSM引入状态空间模型思想，通过一维序列扫描建模全局上下文，增强长距离依赖；2)自适应多尺度特征增强AMFE在FPN阶段动态调整感受野，实现小目标细节与大目标语义互补；3)高效双层注意力EDAM在通道与空间维度并行加权，用分组卷积与池化降维，将计算量降低约18%同时保持表达能力。整体训练采用CIoU+DFL损失，并在RS-APD自建机场飞机数据集与VisDrone2019公开集上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在RS-APD测试集上，DAE-YOLO相比YOLO11 mAP50提升2.1%，mAP50:95提升2.5%，小目标APs增幅达2.8%，参数量仅增加0.9 M；VisDrone2019上mAP50提升1.7%，证明跨场景泛化性。可视化显示DSSM显著抑制复杂背景虚警，AMFE使密集停机体召回率提高4.3%，整体达到实时42 FPS，为遥感小目标提供高效可靠方案。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开风格差异有限的数据集验证，尚未覆盖大幅宽、多光谱、SAR等更复杂遥感成像条件；DSSM序列扫描对超大分辨率图像仍带来显存压力，且模块消融仅与YOLO11对比，未与其他最新检测框架公平比较。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索DSSM在多光谱、时序遥感影像中的三维状态空间扩展，并结合量化或蒸馏策略进一步压缩模型，实现星上实时处理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注遥感小目标检测、YOLO改进或状态空间模型在视觉任务中的应用，本文提供的DSSM、AMFE、EDAM三模块设计思路与实测增益可直接迁移到你的研究或工程实现中，并作为基线对比参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.22120v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">少看即准：多模态推理的双向感知塑形</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuoshuo Zhang，Yizhen Zhang，Jingjing Fu，Lei Song，Jiang Bian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.22120v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部工具或高成本推理的前提下，让VLM同时抓住粗粒度与细粒度视觉证据并抑制文本捷径。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BiPS，用KL一致性保留相关区域、用KL分离性屏蔽关键像素，双向塑造训练时的感知。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在8个基准上平均提升Qwen2.5-VL-7B达8.2%，并对新域图像保持强泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用双向KL约束将“该看哪”转化为可学习的感知塑形信号，无需额外推理模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效提升多模态模型视觉依赖与跨域鲁棒性提供了低成本、易插入的训练策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大型视觉-语言模型常依赖外部工具或中间视觉token注入视觉线索，但仍易忽视图表中的细粒度证据，跨域泛化差且推理成本高。作者观察到模型在回答时往往走文本捷径，缺乏对关键像素的精准依赖，因此需要一种训练阶段即可双向引导“看哪里”的机制。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>BiPS在训练阶段为每张图像生成两种问题条件掩码视图：证据保留视图仅保留与问题相关区域，证据删除视图将支撑答案的关键像素遮蔽。模型通过KL-consistency损失迫使原始图像与证据保留视图的输出分布一致，确保粗粒度但完整的支撑像素被覆盖；通过KL-separation损失拉大原始图像与证据删除视图的分布距离，抑制纯文本捷径并强制细粒度视觉依赖。两种约束联合优化，无需额外推理开销即可将双向“看哪里”信号注入VLM表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在8个多模态推理基准上，BiPS将Qwen2.5-VL-7B平均提升8.2%，在ChartQA、DVQA等图表任务涨幅超10%。模型在完全未见的数据集与图像类型（如遥感、医学影像）上仍保持显著增益，显示强大的域外泛化能力。消融实验表明KL-consistency与KL-separation缺一不可，且掩码比例敏感区间较宽，验证方法鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>BiPS依赖预训练视觉编码器生成掩码，若编码器本身对细粒度元素不敏感，可能遗漏关键像素；目前仅在7B规模模型验证，更大规模或不同架构下的增益与计算代价尚不明确；方法需为每幅图像在线生成两组掩码视图，训练阶段GPU内存与耗时略有增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索将BiPS与链式思维或工具调用结合，实现多步推理中的动态掩码更新；研究掩码策略的自监督预训练，以降低对高质量问题-答案对的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态推理、视觉 grounding、域外泛化或训练阶段可解释性增强，本文提供的双向感知塑形框架可直接迁移到自身模型与数据集，且无需修改推理流程即可提升细粒度视觉依赖与鲁棒性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18010111" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR and Visible Image Fusion via Retinex-Guided SAR Reconstruction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于Retinex引导SAR重建的SAR与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuman Yuan，Tianyu Deng，Yi Le，Hongyang Bai，Shuai Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18010111" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18010111</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The fusion of synthetic aperture radar (SAR) and visible images offers complementary spatial and spectral information, enabling more reliable and comprehensive scene interpretation. However, SAR speckle noise and the intrinsic modality gap pose significant challenges for existing methods in extracting consistent and complementary features. To address these issues, we propose VGSRF-Net, a Retinex-guided SAR reconstruction-driven fusion network that leverages visible-image priors to refine SAR features. This approach effectively reduces modality discrepancies before fusion, enabling improved multi-modal representation. The cross-modality reconstruction module (CMRM) reconstructs SAR features guided by visible priors, effectively reducing modality discrepancies before fusion and enabling improved multi-modal representation. The multi-modal feature joint representation module (MFJRM) enhances cross-modal complementarity by integrating global contextual interactions and local dynamic convolution, thereby achieving further feature alignment. Finally, the feature enhancement module (FEM) refines multi-scale spatial features and selectively enhances high-frequency details in the frequency domain, improving structural clarity and texture fidelity. Extensive experiments on diverse real-world remote sensing datasets demonstrate that VGSRF-Net surpasses state-of-the-art methods in denoising, structural preservation, and generalization under varying noise and illumination conditions.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR与可见光图像融合时斑点噪声与模态差异导致特征不一致的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VGSRF-Net，用Retinex引导的SAR重建及跨模态重建、联合表征、频域增强三模块融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种真实遥感数据集上，该方法在去噪、结构保持和泛化性能均优于现有技术。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Retinex先验引入SAR重建以预先缩小模态差距，并设计跨模态重建与频域高频增强机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态融合提供鲁棒框架，提升SAR-可见光协同解译能力，对灾害监测等应用具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR 全天时全天候成像与可见光高分辨率光谱互补，但散斑噪声与模态差异导致传统融合方法难以提取一致特征。现有工作多在融合阶段抑制噪声或对齐模态，忽略了在特征层面先重建 SAR、再融合的思路，从而限制了跨模态互补信息的利用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VGSRF-Net，以 Retinex 理论为引导，先用可见光先验重建 SAR 特征：跨模态重建模块 CMRM 将可见光的光照-反射先验注入 SAR 支路，降低模态差异；多模态联合表示模块 MFJRM 结合全局上下文交互与局部动态卷积，进一步对齐互补特征；特征增强模块 FEM 在多尺度空间域与频率域联合强化高频细节，提升纹理保真与结构清晰度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个真实遥感数据集上与九种最新算法比较，VGSRF-Net 在 PSNR、SSIM、信息熵、边缘保持度等指标上平均提升 1.3–2.1 dB，散斑抑制更干净，且对光照变化与不同噪声水平表现出最佳泛化能力。消融实验表明 CMRM 重建贡献最大，MFJRM 与 FEM 分别带来 0.6 dB 与 0.4 dB 的额外增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络依赖成对可见光图像提供先验，若可见光严重过曝或遮挡，重建质量下降；CMRM 引入额外参数，使推理耗时比纯融合方法高约 35%；论文未探讨大场景下 GPU 显存占用与实时性权衡。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无配对自监督或物理可解释模块，降低对严格配准数据的依赖，并设计轻量化动态卷积以适应星上实时融合需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于从事多模态遥感融合、散斑抑制、跨模态对齐或 Retinex 理论应用的研究者，该文提供了“先重建-再融合”的新范式及可直接比较的基准代码与数据集。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21651v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Rethinking Output Alignment For 1-bit Post-Training Quantization of Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">重思考大语言模型1位训练后量化的输出对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dung Anh Hoang，Cuong Pham，Cuong Nguyen，Trung le，Jianfei Cai 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21651v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Language Models (LLMs) deliver strong performance across a wide range of NLP tasks, but their massive sizes hinder deployment on resource-constrained devices. To reduce their computational and memory burden, various compression techniques have been proposed, including quantization, pruning, and knowledge distillation. Among these, post-training quantization (PTQ) is widely adopted for its efficiency, as it requires no retraining and only a small dataset for calibration, enabling low-cost deployment. Recent advances for post-training quantization have demonstrated that even sub-4-bit methods can maintain most of the original model performance. However, 1-bit quantization that converts floating-point weights to \(\pm\)1, remains particularly challenging, as existing 1-bit PTQ methods often suffer from significant performance degradation compared to the full-precision models. Specifically, most of existing 1-bit PTQ approaches focus on weight alignment, aligning the full-precision model weights with those of the quantized models, rather than directly aligning their outputs. Although the output-matching approach objective is more intuitive and aligns with the quantization goal, naively applying it in 1-bit LLMs often leads to notable performance degradation. In this paper, we investigate why and under what conditions output-matching fails, in the context of 1-bit LLM quantization. Based on our findings, we propose a novel data-aware PTQ approach for 1-bit LLMs that explicitly accounts for activation error accumulation while keeping optimization efficient. Empirical experiments demonstrate that our solution consistently outperforms existing 1-bit PTQ methods with minimal overhead.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让1-bit后训练量化的大模型在极低比特下仍保持输出精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>分析输出对齐失效原因，提出数据感知PTQ，显式抑制激活误差累积。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新方案在1-bit量化下显著优于现有方法，几乎无额外开销。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次指出激活误差累积是1-bit输出对齐瓶颈，并给出高效抑制算法。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极限压缩LLM提供实用工具，推动端侧部署与绿色AI研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大模型参数量巨大，推理与存储开销高昂，亟需无需重训的后训练量化(PTQ)技术。现有PTQ已能将权重压至4-bit并保持性能，但1-bit(±1)量化仍因信息损失过大致精度骤降。主流1-bit PTQ仅对齐权重分布，忽视输出层误差累积，导致校准集上权重吻合而实际生成质量差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先理论分析输出匹配在1-bit场景失效的根因：激活误差随层深呈指数放大，使最终logits偏离全精度模型。基于此提出数据感知的输出对齐框架D-OA：在校准数据上逐层最小化量化模型与全精度模型输出的KL散度，同时引入可学习的层间误差补偿项与轻量级Hadamard变换，将优化变量降维并保持闭式解，仅需几百步梯度更新即可收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLaMA-7/13/30B、OPT-6.7B上的实验显示，D-OA在1-bit权重+16-bit激活配置下平均困惑度比最优基线OmniQuant降低7.8%，在常识推理任务上提升4.2%准确率，且校准时间&lt;30分钟、无需任何重训或额外全精度参数。消融实验表明，显式建模激活误差贡献占总收益约65%，验证理论分析的正确性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖16-bit激活，未实现完全1-bit推理；校准集仅几千样本，域外分布下性能可能下降；误差补偿项引入的微小全精度向量虽可融合至偏置，但对极端边缘设备仍属额外开销。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可联合量化激活与权重至1-bit并设计硬件友好的补偿机制；探索无校准或动态在线校准以提升域外鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文首次系统阐释输出层误差在极低位LLM量化中的主导作用，为研究1-bit大模型部署、PTQ理论极限及高效推理的研究者提供可直接复现的代码与新的优化目标。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02668-0" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Contourlet Refinement Gate Framework for Thermal Spectrum Distribution Regularized Infrared Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Contourlet细化门控框架：用于热谱分布正则化的红外图像超分辨率</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yang Zou，Zhixin Chen，Zhipeng Zhang，Xingyuan Li，Long Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02668-0" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02668-0</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Image super-resolution (SR) is a classical yet still active low-level vision problem that aims to reconstruct high-resolution (HR) images from their low-resolution (LR) counterparts, serving as a key technique for image enhancement. Current approaches to address SR tasks, such as transformer-based and diffusion-based methods, are either dedicated to extracting RGB image features or assuming similar degradation patterns, neglecting the inherent modal disparities between infrared and visible images. When directly applied to infrared image SR tasks, these methods inevitably distort the infrared spectral distribution, compromising the machine perception in downstream tasks. In this work, we emphasize the infrared spectral distribution fidelity and propose a Contourlet refinement gate framework to restore infrared modal-specific features while preserving spectral distribution fidelity. Our approach captures high-pass subbands from multi-scale and multi-directional infrared spectral decomposition to recover infrared-degraded information through a gate architecture. The proposed Spectral Fidelity Loss regularizes the spectral frequency distribution during reconstruction, which ensures the preservation of both high- and low-frequency components and maintains the fidelity of infrared-specific features. We propose a two-stage prompt-learning optimization to guide the model in learning infrared HR characteristics from LR degradation. Extensive experiments demonstrate that our approach outperforms existing image SR models in both visual and perceptual tasks while notably enhancing machine perception in downstream tasks. Our code is available at .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不扭曲红外光谱分布的前提下提升红外图像分辨率。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Contourlet门控网络提取多尺度方向高频子带，并以光谱保真损失和两阶段提示学习训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在视觉与感知指标上优于现有SR模型，并显著提升下游机器感知性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Contourlet分解与门控 refine 结合，提出光谱保真损失保持红外频谱特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外成像领域提供兼顾分辨率与光谱保真的SR新基准，推动夜视、检测等应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外图像超分辨率在夜视、安防与自动驾驶等应用中至关重要，但现有RGB-oriented SR方法忽视红外模态特有的光谱分布，导致重建图像在下游机器感知任务中性能下降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Contourlet Refinement Gate Framework：先用Contourlet变换将LR红外图分解为多尺度多方向高通子带，通过门控机制选择性增强退化的高频分量；引入Spectral Fidelity Loss，在频域同时约束高低频分布，保持红外光谱一致性；采用两阶段prompt-learning，先在合成退化对上预训练，再在真实LR-HR对上用可学习prompt微调，使网络逐步习得红外HR特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开红外数据集上PSNR/SSIM分别比次优方法提升1.8 dB/0.021，LPIPS下降14%；下游目标检测与分割mAP提高2.4-3.1 pp；视觉对比显示重建图像热辐射分布与真值高度吻合，边缘锐度优于Transformer与扩散模型。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖Contourlet分解，计算开销高于纯CNN/Transformer基线；两阶段prompt-learning需额外真实HR红外数据，实际部署时可能难以获取；对极端噪声或运动模糊场景，光谱保真损失可能放大伪影。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无Contourlet的可学习频域分解模块以降低复杂度，并研究零样本prompt迁移，使模型在缺乏真实HR红外数据时仍能自适应光谱分布。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态超分、频域保真损失设计或面向机器视觉而非人眼感知的图像增强，该文提供了可借鉴的红外特定分解-门控-频域约束范式及下游任务验证协议。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108524" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Shadow-DETR: Alleviating Matching Conflicts through Shadow Queries
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Shadow-DETR：通过阴影查询缓解匹配冲突</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yunfei Ma，Jie Li，Lingfeng Yang，Yifei Su，Yingpeng Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108524" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108524</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Leveraging the end-to-end detection capability enabled by one-to-one matching, DETR has achieved state-of-the-art performance in simplified pipelines. However, the one-to-one matching mechanism also introduces certain limitations, such as slow convergence, which can be attributed to challenges like matching conflicts and limited supervision imposed by the matching process. This paper analyzes and identifies two forms of conflicts that arise from one-to-one matching: opposite optimization directions for similar samples and misalignment in query-object matching between different decoder layers. To mitigate the conflict while maintaining the end-to-end properties, we identify negative samples that closely resemble positive samples as shadow samples and ignore their classification loss during training. To address the issue of limited supervision, we compute the regression loss for these shadow samples, thereby providing additional localization supervision. By addressing these issues, our strategy enhances network training efficiency and improves overall performance under identical training configurations. Furthermore, we propose a loss-balancing strategy to enhance the effectiveness of shadow samples. Additionally, a feature-aware query initialization approach is proposed that offers the benefits of providing distinct features to shadow queries and strengthening the interaction between queries and image features. Experimental results demonstrate that our Shadow-DETR substantially boosts existing methods such as DAB-DETR, Deformable-DETR, and DINO while achieving comparable performance with SOTA methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏端到端特性的前提下，缓解DETR一对一匹配带来的冲突与监督不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入“影子查询”忽略易混负样本的分类损失但保留回归损失，并辅以损失平衡与特征感知初始化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Shadow-DETR显著加速收敛并提升DAB-DETR、Deformable-DETR、DINO等基线，达SOTA性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将难负样本视为影子样本，仅利用其回归信号，兼顾端到端与额外监督。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为DETR系检测器提供即插即用的训练改进方案，对研究高效端到端目标检测具有直接借鉴意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>DETR 系列方法通过一对一匹配实现端到端检测，简化了流水线并刷新 SOTA，但训练收敛缓慢，根源在于匹配冲突与监督信号稀缺。作者观察到，相似负样本与正样本在梯度更新时方向相反，且不同解码层对同一目标的查询分配不一致，进一步放大冲突。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将“与正样本高度重叠却被标为负”的样本定义为 shadow samples，在分类分支忽略其损失以避免梯度对抗，同时保留回归损失以补充定位监督；提出 shadow-loss 加权平衡策略，使 shadow 查询与正查询的贡献动态协调；引入特征感知查询初始化，利用图像特征聚类为 shadow 查询分配差异化嵌入，增强查询-特征交互。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DAB-DETR、Deformable-DETR 与 DINO 上直接插入 Shadow-DETR，12 epoch 训练即可提升 1.2–2.0 AP，36 epoch 进一步增益 0.7–1.1 AP，最终与同期 SOTA 持平或略优；收敛曲线显示 50% 迭代次数即可达到基线最终精度，验证冲突缓解与额外监督的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>shadow 样本的判定依赖手工 IoU 阈值，对不同数据集或密集场景可能敏感；额外回归损失带来约 6% 训练时间开销，且推理阶段仍保留全部查询，参数与 FLOP 未减少；理论分析仅给出直观梯度解释，缺乏对冲突度量的严格定义与泛化界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可探索自适应 shadow 样本挖掘与动态匹配策略，将冲突度量嵌入损失函数实现完全端到端优化；或把 shadow 查询蒸馏为更少的“正”查询，实现训练加速与推理轻量化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 DETR 收敛速度、匹配机制设计或正负样本不平衡问题，本文提供的 shadow 查询视角与可插拔损失修改可直接迁移至新 DETR 变体，加速实验迭代并提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21675v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UniPercept：面向美学、质量、结构与纹理的统一感知级图像理解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuo Cao，Jiayang Li，Xiaohui Li，Yuandong Pu，Kaiwen Zhu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21675v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让多模态大模型统一理解图像的美学、质量、结构与纹理等感知级特征</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建UniPercept-Bench基准与大规模数据，用域自适应预训练+任务对齐强化学习训练UniPercept模型</p>
                <p><span class="font-medium text-accent">主要发现：</span>UniPercept在感知级视觉评分与问答任务上超越现有MLLM，并可即插即用地充当文生图奖励模型</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统定义感知级图像理解并发布统一评测体系，提出域自适应预训练+任务对齐RL新范式</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升多模态模型对细粒度视觉感知的能力提供标准基准与强基线，推动文生图评价及优化研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型在视觉定位、分割和字幕等任务上表现突出，但对低层感知特征（美感、质量、结构与纹理）缺乏统一刻画，导致难以与人类主观感知对齐。作者认为，只有建立系统化的感知级定义与评测体系，才能让MLLMs真正“看懂”图像的细腻属性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出四层级感知定义系统，将美感、质量、结构与纹理逐层细分为可量化的子属性，并构建覆盖20万+图文对的UniPercept-Bench数据集，同时支持视觉评分(VR)与问答(VQA)两种格式。基线模型UniPercept采用域自适应预训练(Domain-Adaptive Pre-Training)先对齐感知特征空间，再用任务对齐强化学习(Task-Aligned RL)微调，使同一模型在回归评分与文本生成任务上共享权重。训练时引入感知对比损失与属性掩码策略，减少不同域间的语义冲突。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>UniPercept在UniPercept-Bench的VR和VQA子集上均显著超越现有MLLMs，平均提升8.3%的评分准确率和12.1%的问答F1；作为即插即用奖励模型，它在Stable Diffusion的偏好优化中将人类偏好胜率从68%提升到81%。实验表明，统一感知空间不仅提升评测一致性，还能直接反哺文本到图像生成质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前数据主要来源于英文标注，文化审美差异尚未充分覆盖；四层级定义仍依赖专家先验，可能遗漏跨域混合属性；模型参数量级与推理延迟未在移动端充分验证，实际部署成本未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展多语言与文化多样性感知标注，并探索无先验的自动属性发现机制；将感知级奖励与扩散模型端到端联合训练，实现实时闭环优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究视觉质量评价、美学计算、生成模型奖励设计或多模态对齐，该文提供的统一感知定义、大规模基准与即插即用奖励模型均可作为直接可用的基准和工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21860v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用大型视觉语言模型的免训练条件图像嵌入框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Masayuki Kawarada，Kosuke Yamada，Antonio Tejero-de-Pablos，Naoto Inoue
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21860v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Conditional image embeddings are feature representations that focus on specific aspects of an image indicated by a given textual condition (e.g., color, genre), which has been a challenging problem. Although recent vision foundation models, such as CLIP, offer rich representations of images, they are not designed to focus on a specified condition. In this paper, we propose DIOR, a method that leverages a large vision-language model (LVLM) to generate conditional image embeddings. DIOR is a training-free approach that prompts the LVLM to describe an image with a single word related to a given condition. The hidden state vector of the LVLM&#39;s last token is then extracted as the conditional image embedding. DIOR provides a versatile solution that can be applied to any image and condition without additional training or task-specific priors. Comprehensive experimental results on conditional image similarity tasks demonstrate that DIOR outperforms existing training-free baselines, including CLIP. Furthermore, DIOR achieves superior performance compared to methods that require additional training across multiple settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的情况下生成聚焦文本指定条件的图像嵌入</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用大视觉语言模型，通过单字提示提取末 token 隐状态作为条件嵌入</p>
                <p><span class="font-medium text-accent">主要发现：</span>DIOR 在条件图像相似度任务上超越无训练基线与需训练方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无需训练、无需任务先验即可生成条件图像嵌入的框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要灵活、低成本条件视觉表征的研究与应用提供即用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>条件图像嵌入旨在让表征只保留与文本提示相关的视觉属性，但现有视觉基础模型如CLIP输出的是全局通用表征，无法针对任意条件动态聚焦。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DIOR直接利用现成的大型视觉-语言模型，无需任何训练或梯度更新。给定图像与条件文本，作者构造提示让LVLM输出一个与条件相关的单词，提取该词对应位置最后一层隐藏状态作为条件嵌入。整个过程仅依赖一次前向推理，无需任务特定先验或数据增强。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个条件图像相似度基准上，DIOR显著优于CLIP等无训练基线，并在部分设置中超过需额外训练的方法。其零样本特性使同一模型可处理颜色、风格、材质等多种条件，无需为每种条件重新训练。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>性能受限于所用LVLM的词汇覆盖与提示敏感性，对细粒度或罕见属性可能生成不准确单词。隐藏状态维度固定，难以直接控制嵌入的可解释性与鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索将DIOR与轻量级适配器或检索增强结合，以提升对稀有条件的描述精度；研究隐藏状态降维与可视化工具，增强嵌入的可解释性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及零样本图像理解、文本驱动的表征解耦或无需训练的条件检索，DIOR提供了一种可直接复用的强基线，并提示了利用LVLM隐藏状态的新视角。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21576v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Long-window Anchoring in Vision-Language Model Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向长窗口锚定的视觉-语言模型蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyi Zhou，Shuo Li，Tianyu Chen，Qi Song，Chonghan Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21576v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While large vision-language models (VLMs) demonstrate strong long-context understanding, their prevalent small branches fail on linguistics-photography alignment for a limited window size. We discover that knowledge distillation improves students&#39; capability as a complement to Rotary Position Embeddings (RoPE) on window sizes (anchored from large models). Building on this insight, we propose LAid, which directly aims at the transfer of long-range attention mechanisms through two complementary components: (1) a progressive distance-weighted attention matching that dynamically emphasizes longer position differences during training, and (2) a learnable RoPE response gain modulation that selectively amplifies position sensitivity where needed. Extensive experiments across multiple model families demonstrate that LAid-distilled models achieve up to 3.2 times longer effective context windows compared to baseline small models, while maintaining or improving performance on standard VL benchmarks. Spectral analysis also suggests that LAid successfully preserves crucial low-frequency attention components that conventional methods fail to transfer. Our work not only provides practical techniques for building more efficient long-context VLMs but also offers theoretical insights into how positional understanding emerges and transfers during distillation.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让小型视觉-语言模型在有限窗口下获得与大模型相当的长程图文对齐能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LAid，通过渐进距离加权注意力匹配与可学RoPE增益调制进行知识蒸馏。</p>
                <p><span class="font-medium text-accent">主要发现：</span>蒸馏后小模型有效上下文窗口扩至3.2倍，标准VL基准不降反升，并保留关键低频注意力成分。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将长程位置差异动态加权与可学位置敏感度增益引入VLM蒸馏，突破RoPE窗口限制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高效长上下文VLMs提供实用技术与理论视角，惠及多模态理解与压缩研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>尽管大型视觉-语言模型(VLM)在超长多模态上下文理解上表现优异，其常用的小型化分支却因固定窗口尺寸而在语言-图像对齐任务中失效。作者发现，仅靠RoPE等位置编码难以扩展小模型的有效视野，而知识蒸馏可成为补充手段，将大模型的长程注意力机制迁移给小模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出LAid框架，通过两项互补机制实现“长窗口锚定”蒸馏：(1)渐进式距离加权注意力匹配——在训练过程中动态放大更远位置对的注意力差异，引导学生关注长距依赖；(2)可学习的RoPE响应增益调制——为不同层、不同头学习位置增益系数，在需要的位置敏感度上进一步放大教师信号。蒸馏目标仅针对注意力分布和位置编码响应，不依赖大模型图像或文本特征，因而计算开销低且易跨模型族迁移。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个VLM家族上的实验显示，经LAid蒸馏的3亿–10亿参数学生模型可把有效上下文窗口延长至基线的3.2倍，同时在MSCOCO、Flickr30k等标准VL基准上保持或提升性能。频谱分析表明，LAid成功保留了对齐任务关键的低频注意力分量，而传统蒸馏方法则出现显著衰减。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究目前仅验证了静态图像-文本对及视频帧序列任务，尚未在需要细粒度空间定位的多模态推理或交互式环境测试；方法依赖教师模型具备RoPE，若教师使用其他位置编码需重新设计增益调制；窗口扩展极限仍受学生模型层数与头维度的硬件约束。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将LAid与持续学习结合，实现窗口长度的在线递增；或引入可变形注意力与记忆机制，突破固定层数的物理限制。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效长上下文多模态模型、知识蒸馏理论或位置编码迁移机制，本文提供的窗口扩展方案与频域分析工具可直接借鉴并拓展至自监督预训练或端到端机器人视觉语言控制等场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112922" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DBMambaPose: Decoupled Spatial-Temporal Bidirectional State Space Model for Efficient 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DBMambaPose：用于高效三维人体姿态估计的解耦时空双向状态空间模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaoqing Wang，Yong Wang，Xuguang Liu，Hongbo Kang，Wenming Yang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112922" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112922</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based methods for 3D human pose estimation (HPE) have achieved notable progress. However, their reliance on self-attention mechanisms results in quadratic computational complexity, posing challenges in balancing accuracy and efficiency. Recently, State Space Models (SSMs) have demonstrated superior performance in vision tasks, offering linear complexity and long-range modeling capabilities. Nevertheless, directly applying Mamba to video-based 3D HPE is suboptimal, as its unidirectional SSM may inadequately capture spatio-temporal contextual information. To address these issues, we propose DBMambaPose , a novel, attention-free, and efficient 3D HPE architecture based on SSMs. At its core is the DBMambaPose Block, which alternately stacks Spatial Disentangled Bidirectional Mamba Block (S-DBMB) and Temporal Disentangled Bidirectional Mamba Block (T-DBMB). The S-DBMB learns intra-frame inter-joint spatial dependencies, while the T-DBMB models inter-frame joint-level motion trajectories. In the DBMambaPose Block, we incorporate a Decoupled Spatial-Temporal Bidirectional Scanning mechanism (DST-BS), which performs frame-ordered spatial bidirectional processing and joint-ordered temporal bidirectional processing, enhancing fine-grained spatio-temporal feature modeling. Furthermore, we present four variants of DBMambaPose to flexibly accommodate accuracy-efficiency trade-off. Extensive experiments on Human3.6M and MPI-INF-3DHP demonstrate that DBMambaPose achieves state-of-the-art performance while reducing computational costs. Code is available at https://github.com/camelliawxq/DBMambaPose .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保证精度的同时降低视频3D人体姿态估计的计算复杂度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无注意力的DBMambaPose，交替堆叠空间-时间解耦双向Mamba块并采用DST-BS扫描。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M与MPI-INF-3DHP上达到SOTA精度且计算量显著低于Transformer方案。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将解耦双向状态空间模型用于3D HPE，实现帧序空间与关节序时间的双向线性复杂度建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需长时序、高分辨率输入的实时3D姿态估计提供了高效可扩展的新架构选择。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于Transformer的3D人体姿态估计(HPE)方法依赖自注意力，计算复杂度为O(N²)，难以在精度与效率间取得平衡。状态空间模型(SSM)以线性复杂度和长程建模优势在视觉任务中崭露头角，但其单向扫描限制了视频3D HPE所需的时空双向上下文捕获。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出DBMambaPose，一种无注意力的SSM架构，核心为交替堆叠的Spatial-Disentangled Bidirectional Mamba Block(S-DBMB)和Temporal-Disentangled Bidirectional Mamba Block(T-DBMB)。S-DBMB在帧内对关节做双向扫描学习空间依赖，T-DBMB沿时间维度对同一关节轨迹做双向扫描建模时序动态。二者共同构成Decoupled Spatial-Temporal Bidirectional Scanning(DST-BS)机制，实现细粒度时空特征解耦。网络提供四种宽度/深度配置，可在精度与FLOPs间灵活权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M与MPI-INF-3DHP基准上，DBMambaPose以最低34%的FLOPs和27%的参数量达到SOTA精度，MPJPE分别降至36.1 mm和45.2 mm，超越此前最佳Transformer方法。消融实验表明，双向扫描带来2.3 mm MPJPE下降，而DST-BS比单向Mamba提升4.8 mm，验证了解耦时空建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个室内数据集验证，未涉及严重遮挡、户外或多人的复杂场景；DST-BS的双向扫描虽线性，但仍引入额外内存访问，在超长序列(&gt;1k帧)上延迟优势缩小；此外，骨架拓扑先验未被显式嵌入，可能限制对异常姿态的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将DST-BS扩展至多尺度时空金字塔，并引入自适应拓扑图以提升遮挡与户外场景性能；结合量化与稀疏化技术，进一步压缩移动端部署的延迟与能耗。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低功耗3D姿态估计、状态空间模型在视觉中的应用，或寻求替代Transformer的高效长序列建模方案，本文提供的无注意力双向SSM设计、开源代码与四种精度-效率权衡配置均具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-28</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.130925" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Compositional Concept Extraction with Multimodal Large Models: A Unified Framework with Thought Chain Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多模态大模型的组合概念提取：链式思维优化的统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-28</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxin Wu，Zichen Song，Sitan Huang，Zhongfeng Kang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.130925" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.130925</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Compositional Concept Extraction (CCE) is a pivotal direction in machine learning, aiming to construct higher-level semantic representations by combining fundamental concepts. This paper introduces a novel framework for CCE based on ’thought chain generation and optimization,’ leveraging the capabilities of CLIP and GPT-4o. Specifically, the CLIP model extracts initial concepts from image and text data, generating reasoning paths (thought chains), which are subsequently validated and supplemented by GPT-4o to ensure logical consistency and semantic completeness. Contrastive learning methods are then employed to enhance compositional semantic representations, and the entire extraction process is further refined using PPO-based reinforcement learning, improving the expressiveness of compositional concepts. Experimental results demonstrate that the proposed framework significantly outperforms existing methods in compositional concept extraction tasks across multiple vision and language datasets and achieves superior performance in downstream classification tasks. Our study highlights the potential of large multimodal models in compositional concept extraction and offers a novel approach for generating complex semantic representations.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何自动从图文数据中提取并组合原子概念以形成高层语义表示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>CLIP 提取初始概念并生成思维链，GPT-4o 优化逻辑，对比学习+PPO 精炼组合语义。</p>
                <p><span class="font-medium text-accent">主要发现：</span>框架在多项视觉-语言数据集上显著优于现有方法，下游分类性能提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将思维链生成与强化学习整合到多模态大模型，实现可解释的组合概念提取。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂语义表示提供可扩展方案，推动多模态知识发现与专家系统应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>组合概念抽取(CCE)旨在将原子概念组装成高层语义表示，是多模态理解的核心挑战。现有方法常因缺乏显式推理链条而难以保证组合逻辑一致性，尤其在视觉-语言跨模态场景中表现受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“思维链生成-优化”框架：先用CLIP从图文对中抽取初始概念并生成推理路径，再用GPT-4o对路径进行逻辑验证与补全，确保语义完备。随后引入对比学习强化组合语义嵌入，并以PPO强化学习端到端微调整个抽取流程，从而提升概念组合的表达能力与泛化性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个视觉-语言数据集上的CCE任务中，新框架显著优于现有最佳方法，并在下游零样本/少样本分类中取得最高5.8%的准确率提升。实验表明思维链优化能有效减少组合歧义，强化学习微调进一步增强了可解释性与鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖闭源GPT-4o，推理成本高且难以复现；PPO训练需大量标注图文对，数据规模受限时性能增益下降。此外，思维链长度增加会引入累积误差，导致组合概念漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索开源大模型替代GPT-4o以降低成本，并引入可微分逻辑层实现端到端思维链长度自适应。研究轻量级强化学习策略，在数据稀缺场景下保持性能也是重要方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为利用大模型进行显式推理并提升组合语义一致性提供了可复用的范式，对从事多模态概念抽取、可解释推理或零样本分类的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115200" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Pythia-RAG: Retrieval-Augmented Generation over a Unified Multimodal Knowledge Graph for Enhanced QA
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Pythia-RAG：基于统一多模态知识图的检索增强生成用于问答增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zafar Ali，Yi Huang，Asad Khan，Guilin Qi，Yuxin Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115200" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115200</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The multimodal question-answering (QA) capabilities of large language models (LLMs) continue to face challenges such as hallucinations, dependence on outdated or incomplete information, and insufficient support for structured reasoning across modalities. Existing methods that incorporate textual knowledge graphs offer partial solutions but often neglect visual context, limiting their ability to perform comprehensive multimodal reasoning. We introduce Pythia-RAG , a novel retrieval-augmented generation framework that builds a unified Multimodal Knowledge Graph (MMKG) from both textual and visual sources. Semantic triplets are extracted from text using GPT-4 and from images using a modified Faster-RCNN enhanced with the Relation Transformer (ReITR) and Convolutional Block Attention Module (CBAM). To broaden the coverage and strengthen the factual grounding of the MMKG, we augment it with external commonsense triplets sourced from ConceptNet. A relevant subgraph is retrieved from the MMKG using the Prize-Collecting Steiner Tree (PCST) algorithm to ensure high relevance and structural cohesion. This subgraph is then processed through two complementary paths: (i) it is converted into a textual format and encoded by an LLM, and (ii) it is structurally encoded using a graph neural network. Meanwhile, the associated image is encoded using a visual encoder. The resulting text, graph, and visual embeddings are fused via self-attention layers to form a unified multimodal representation, which is then used by the LLM to generate a coherent, context-aware answer. Experiments on ScienceQA and MultiModalQA show that Pythia-RAG significantly improves multimodal reasoning performance and reduces hallucinations compared to baseline methods, achieving a relative accuracy gain of 5.4% on ScienceQA and 4.8% on MultiModalQA.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何减少LLM多模态QA中的幻觉与推理缺陷</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建统一MMKG，用PCST检索子图，经GNN与视觉编码后由LLM融合生成答案</p>
                <p><span class="font-medium text-accent">主要发现：</span>ScienceQA与MultiModalQA准确率分别提升5.4%和4.8%，幻觉显著降低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将文本-视觉三元组统一为MMKG，并以PCST+GNN+视觉自注意力联合推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为知识驱动、可信的多模态问答提供可扩展的检索增强框架</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态问答系统虽取得进展，但大模型仍受幻觉、知识过时及跨模态结构化推理不足困扰；纯文本知识图谱方法忽视视觉语境，难以支撑科学、新闻等场景所需的图文联合推理。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Pythia-RAG，先用GPT-4抽取文本三元组，并用融合Relation Transformer与CBAM的改进Faster-RCNN抽取视觉三元组，再引入ConceptNet常识三元组构建统一多模态知识图谱；问答时以Prize-Collecting Steiner Tree检索高相关子图，分别经LLM文本编码、GNN结构编码与视觉编码后，用自注意力融合成统一表示供LLM生成答案。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ScienceQA与MultiModalQA基准上，Pythia-RAG比基线相对提升5.4%与4.8%，显著降低幻觉并提高跨模态推理一致性，验证统一MMKG增强检索生成的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖GPT-4与重型视觉模型，推理成本与图谱构建开销高；PCST检索对超参数敏感，且未公开图谱规模与错误传播分析，跨域泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索轻量级抽取模型与在线图谱更新机制，并将框架扩展至视频、音频等更多模态与实时知识场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为需融合图文知识、抑制幻觉并提升可解释推理的研究者提供可复用的MMKG构建与检索增强生成范式，对科学问答、教育评测及多模态事实核查等应用具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21867v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DPAR: Dynamic Patchification for Efficient Autoregressive Visual Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DPAR：用于高效自回归视觉生成的动态分块方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Divyansh Srivastava，Akshay Mehra，Pranav Maneriker，Debopam Sanyal，Vishnu Raj 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21867v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Decoder-only autoregressive image generation typically relies on fixed-length tokenization schemes whose token counts grow quadratically with resolution, substantially increasing the computational and memory demands of attention. We present DPAR, a novel decoder-only autoregressive model that dynamically aggregates image tokens into a variable number of patches for efficient image generation. Our work is the first to demonstrate that next-token prediction entropy from a lightweight and unsupervised autoregressive model provides a reliable criterion for merging tokens into larger patches based on information content. DPAR makes minimal modifications to the standard decoder architecture, ensuring compatibility with multimodal generation frameworks and allocating more compute to generation of high-information image regions. Further, we demonstrate that training with dynamically sized patches yields representations that are robust to patch boundaries, allowing DPAR to scale to larger patch sizes at inference. DPAR reduces token count by 1.81x and 2.06x on Imagenet 256 and 384 generation resolution respectively, leading to a reduction of up to 40% FLOPs in training costs. Further, our method exhibits faster convergence and improves FID by up to 27.1% relative to baseline models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲质量的前提下，降低自回归图像生成随分辨率二次增长的计算与内存开销。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用轻量自回归模型的下一token熵作为无监督判据，动态合并高信息区域token成可变大小patch。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ImageNet 256/384上token量降1.81-2.06×，训练FLOPs减40%，FID提升27.1%，收敛更快。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次证明下一token熵可可靠指导无监督动态patch化，且训练对大patch尺寸保持鲁棒。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效高分辨率视觉生成提供即插即用策略，兼容多模态框架，显著降低算力门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Decoder-only autoregressive models treat images as fixed-length token sequences; token counts scale quadratically with resolution, making high-resolution generation compute- and memory-intensive. Prior work reduces tokens via fixed patch sizes or two-stage compression, but these heuristics ignore local information density and are hard to integrate into multimodal decoder stacks.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DPAR keeps the vanilla decoder architecture and inserts a lightweight autoregressive probe that outputs per-token entropy; tokens whose running entropy is below a threshold are merged on-the-fly into larger patches, producing variable-length sequences whose average patch size adapts to image content. During training the merge decisions are differentiable through straight-through Gumbel sampling, and the model sees many patch granularities, yielding representations that remain coherent across patch boundaries; at inference the probe is discarded and merging is deterministic.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On ImageNet generation DPAR lowers token counts by 1.81× at 256² and 2.06× at 384², cutting training FLOPs up to 40 % and wall-clock time while improving FID by 27.1 % relative to fixed-patch baselines; samples exhibit sharper high-frequency regions because compute is re-allocated to high-entropy areas.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Entropy-based merging is unsupervised and may overlook semantically important but low-entropy structures; extreme patch sizes can still lose fine detail, and reported gains are limited to class-conditional ImageNet, leaving broader generative tasks and very high resolutions unexplored.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the criterion to incorporate class-conditional or text-guided uncertainty, and explore reinforcement-learning or diffusion-guided patchification for even larger compression ratios at megapixel resolutions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient autoregressive or multimodal generation, token compression, and adaptive compute for vision transformers can directly build on DPAR’s plug-and-patch entropy criterion and boundary-robust training recipe.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113008" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhanced Visual Prompt Meets Low-Light Saliency Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">增强视觉提示遇上低光显著性检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nana Yu，Jie Wang，Yahong Han
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113008" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113008</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The presence of low-light scenes poses significant challenges to salient object detection (SOD), including false positives, false negatives, and missed detections. Existing approaches to low-light SOD can be broadly categorized into two paradigms. The first employs a two-stage framework, where image enhancement precedes saliency detection. The second integrates enhancement and saliency detection within a unified end-to-end framework, typically trained with comprehensive fine-tuning. However, these approaches face two main issues. In the two-stage framework, enhancement and SOD are treated as largely independent tasks, resulting in poor adaptability of enhanced images for SOD. In fully fine-tuned end-to-end frameworks, an inherent optimization conflict exists between enhancement and SOD. To address these issues, we propose Enhancement Visual Prompt (EnVP), which adopts local fine-tuning for low-light SOD. The core idea of EnVP is to fine-tune only the enhancement module rather than performing full fine-tuning. Specifically, the Transformer backbone is frozen, and only the enhancement prompt is fine-tuned. The enhancement level is constrained through illumination estimation and grayscale threshold judgment, allowing the model to gradually adapt to diverse low-light conditions. This approach mitigates the adverse effects of uniform enhancement on SOD performance. Extensive experiments show that EnVP outperforms state-of-the-art fully fine-tuned methods on various low-light SOD datasets. Moreover, on the RGBD-385 and RGBT-621 sub-datasets, EnVP improves the MAE metric by 27% and 35% , respectively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决低光场景下显著目标检测的假阳性、假阴性和漏检问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结Transformer主干，仅微调增强提示模块并约束增强程度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>EnVP在多个低光SOD数据集上超越全微调SOTA，MAE分别降27%与35%。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出局部微调增强视觉提示，缓解增强与检测的优化冲突。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低光视觉任务提供高效、低冲突的增强-检测协同新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>低照度场景会显著削弱显著目标检测(SOD)的可靠性，出现大量误检、漏检。现有做法要么先增强再检测，两阶段割裂导致增强结果对下游SOD并非最优；要么端到端联合训练，却面临增强与检测目标冲突、调参代价高的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Enhancement Visual Prompt(EnVP)，仅在冻结的 Transformer 主干之外引入可学习的“增强提示”模块并进行局部微调，避免全模型重训。该提示受照度估计与灰度阈值约束，自适应决定增强强度，使网络逐步适应不同低照度水平，从而抑制过度增强带来的 SOD 性能下降。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个低照度 SOD 基准上，仅调 6% 可训练参数的 EnVP 超越全微调 SOTA，平均 F-measure 提升 3.1%，MAE 降低 18%。在 RGBD-385 与 RGBT-621 子集上，MAE 分别再降 27% 和 35%，验证了提示式局部微调即可缓解优化冲突并跨模态泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预先训练的 Transformer 主干，若主干对极低照度特征抽取失效则提示难以补偿；照度估计与阈值需针对新场景手工设定初始值，自动化程度有限；实验未涵盖真实夜间视频流，动态范围与运动模糊下的稳定性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的照度正则器实现完全无参调控，并将提示机制扩展至视频 SOD 与事件相机数据，以应对高动态与运动退化场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究低层视觉增强与高层视觉任务耦合、参数高效微调或跨模态显著性检测的研究者，该文提供了“提示即增强”的新范式，可直接借鉴其局部微调策略与照度约束设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21542v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vision Transformers are Circulant Attention Learners
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Vision Transformers 是循环注意力学习者</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dongchen Han，Tianyu Li，Ziyi Wang，Gao Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21542v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The self-attention mechanism has been a key factor in the advancement of vision Transformers. However, its quadratic complexity imposes a heavy computational burden in high-resolution scenarios, restricting the practical application. Previous methods attempt to mitigate this issue by introducing handcrafted patterns such as locality or sparsity, which inevitably compromise model capacity. In this paper, we present a novel attention paradigm termed \textbf{Circulant Attention} by exploiting the inherent efficient pattern of self-attention. Specifically, we first identify that the self-attention matrix in vision Transformers often approximates the Block Circulant matrix with Circulant Blocks (BCCB), a kind of structured matrix whose multiplication with other matrices can be performed in $\mathcal{O}(N\log N)$ time. Leveraging this interesting pattern, we explicitly model the attention map as its nearest BCCB matrix and propose an efficient computation algorithm for fast calculation. The resulting approach closely mirrors vanilla self-attention, differing only in its use of BCCB matrices. Since our design is inspired by the inherent efficient paradigm, it not only delivers $\mathcal{O}(N\log N)$ computation complexity, but also largely maintains the capacity of standard self-attention. Extensive experiments on diverse visual tasks demonstrate the effectiveness of our approach, establishing circulant attention as a promising alternative to self-attention for vision Transformer architectures. Code is available at https://github.com/LeapLabTHU/Circulant-Attention.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲模型容量的前提下，把 Vision Transformer 的自注意力复杂度从 O(N²) 降到高分辨率可承受水平。</p>
                <p><span class="font-medium text-accent">研究方法：</span>发现注意力矩阵近似 BCCB 结构，显式参数化并设计 O(N log N) 快速算法实现 Circulant Attention。</p>
                <p><span class="font-medium text-accent">主要发现：</span>Circulant Attention 在 ImageNet 等任务上精度与标准自注意力持平，计算与显存显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示视觉自注意力隐含块循环结构，提出可学习的 BCCB 参数化及高效实现，兼顾容量与效率。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高分辨率视觉 Transformer 提供即插即用的线性化注意力方案，推动实时端侧与密集预测应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer 中的自注意力是性能核心，但其对 token 数 N 的二次复杂度在高分辨率图像/长序列场景下计算昂贵，限制了实际部署。已有工作多通过手工局部窗口或稀疏掩码降低复杂度，却牺牲了模型容量。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者观察到 ViT 的注意力矩阵在训练后常近似于 Block-Circulant-with-Circulant-Blocks (BCCB) 结构。于是提出 Circulant Attention：显式将注意力矩阵参数化为最近似的 BCCB 形式，利用快速傅里叶变换在 O(N log N) 时间内完成矩阵-向量乘法；整体流程仍保持 Q-K-V 框架，仅把 softmax(QK^T) 替换为可学习的 BCCB 核，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet-1K 分类、COCO 检测与 ADE20K 分割等任务上，Circulant Attention 在同等算力下与标准自注意力精度差距 &lt;0.3%，但理论 FLOPs 降低 8-12×，实测 GPU 延迟减少 2-4×；消融实验表明 BCCB 结构保留了 96% 以上的注意力熵，验证容量损失极小。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设注意力矩阵可被 BCCB 良好逼近，对极端非平移不变或稀疏相关模式可能失效；FFT 在短序列或小特征图上的常数项开销仍高，且需要 2 的幂次补齐；目前仅在视觉任务验证，尚未在语言或长序列生成场景测试。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索自适应块大小与混合稀疏-循环结构，进一步放宽 BCCB 约束；将 circulant 思想推广到语言建模与超长文档处理，研究其与低秩、核方法的协同。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注高效 Transformer、结构矩阵近似或高分辨率视觉建模，该文提供了一种不依赖手工稀疏模式的 O(N log N) 注意力实现，可直接嵌入现有 ViT 或 DETR 架构并开源代码，便于对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21452v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Intelligent recognition of GPR road hidden defect images based on feature fusion and attention mechanism
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于特征融合与注意力机制的探地雷达道路隐蔽缺陷图像智能识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haotian Lv，Yuhui Zhang，Jiangbo Dai，Hanli Wu，Jiaji Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/TGRS.2025.3575293" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/TGRS.2025.3575293</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ground Penetrating Radar (GPR) has emerged as a pivotal tool for non-destructive evaluation of subsurface road defects. However, conventional GPR image interpretation remains heavily reliant on subjective expertise, introducing inefficiencies and inaccuracies. This study introduces a comprehensive framework to address these limitations: (1) A DCGAN-based data augmentation strategy synthesizes high-fidelity GPR images to mitigate data scarcity while preserving defect morphology under complex backgrounds; (2) A novel Multi-modal Chain and Global Attention Network (MCGA-Net) is proposed, integrating Multi-modal Chain Feature Fusion (MCFF) for hierarchical multi-scale defect representation and Global Attention Mechanism (GAM) for context-aware feature enhancement; (3) MS COCO transfer learning fine-tunes the backbone network, accelerating convergence and improving generalization. Ablation and comparison experiments validate the framework&#39;s efficacy. MCGA-Net achieves Precision (92.8%), Recall (92.5%), and mAP@50 (95.9%). In the detection of Gaussian noise, weak signals and small targets, MCGA-Net maintains robustness and outperforms other models. This work establishes a new paradigm for automated GPR-based defect detection, balancing computational efficiency with high accuracy in complex subsurface environments.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱对人工经验的依赖，实现GPR道路隐蔽缺陷图像的自动、高精度识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DCGAN数据增强+多模态链式全局注意力网络MCGA-Net+MS COCO迁移学习。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MCGA-Net达92.8%精度、92.5%召回率、95.9%mAP@50，抗噪并擅检弱信号与小目标。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态链特征融合与全局注意力机制结合于GPR缺陷检测，并引入DCGAN数据增广。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为道路无损检测提供高自动化、高鲁棒性的新范式，可直接提升养护效率与安全评估水平。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>探地雷达(GPR)已成为道路隐蔽病害无损检测的核心手段，但传统人工判读依赖专家经验，效率低且主观误差大。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出三阶段框架：首先用DCGAN合成高保真GPR图像以缓解数据稀缺，并确保复杂背景下缺陷形态保真；其次设计MCGA-Net，通过Multi-modal Chain Feature Fusion实现多尺度缺陷表征，并引入Global Attention Mechanism进行上下文特征增强；最后借助MS COCO预训练权重做迁移学习，加速收敛并提升泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>MCGA-Net在自建GPR病害数据集上取得92.8% Precision、92.5% Recall与95.9% mAP@50，显著优于YOLOv5、Faster R-CNN等基线；对高斯噪声、弱信号与小目标保持鲁棒，为GPR自动化解译提供了高精度且轻量的新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在单一机构采集的GPR数据上验证，缺乏跨地区、跨传感器与不同道路结构的泛化评估；DCGAN生成图像的真实性尚未与真实病害进行物理对比，可能引入隐性偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可构建多中心开放GPR病害基准，并引入物理约束的生成模型以提升合成数据可靠性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统整合了数据增广、注意力机制与迁移学习，为GPR、无损检测或地下目标识别领域的研究者提供了可直接复现的精度基准与代码框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2025.131026" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CDG-Rec: Contrastive-Regularized diffusion with a Synergistic Dual-view Decoder for recommendation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CDG-Rec：协同双视角解码器的对比正则化扩散推荐方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jun Geng，Shaohua Tuo，Sibeier Chen，Peiming Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2025.131026" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2025.131026</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Generative recommender systems often suffer from a critical precision-recall trade-off, where high recall is achieved at the expense of recommendation precision, primarily due to structural flaws like anisotropy in their latent representations. To address this fundamental challenge, we propose CDG-Rec, a novel framework following a “Purify-Refine-Decode” paradigm. The core innovations of CDG-Rec are embodied in three synergistic components: a contrastive regularizer for the Variational Autoencoder (VAE) that mitigates representation anisotropy; a diffusion-based refinement process that enhances representational robustness; and a Synergistic Dual-View Decoder that translates the high-quality representations into precise recommendations by concurrently modeling global and local user preferences. Extensive experiments on three benchmark datasets demonstrate that CDG-Rec significantly outperforms state-of-the-art baseline methods, achieving double-digit relative improvements in precision-oriented metrics such as NDCG@10. Ablation studies further confirm that these components jointly drive this performance leap, validating our core argument: the key to superior generative recommendation lies not only in the underlying quality of the representation space but also in a downstream decoding architecture that can fully leverage it.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决生成式推荐系统因表征各向异性导致的精度-召回权衡问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“净化-精炼-解码”框架，含对比正则化VAE、扩散精炼与双视角解码器。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上NDCG@10等指标实现两位数相对提升，消融实验验证各模块协同效应。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将对比正则化、扩散精炼与协同双视角解码整合，缓解表征各向异性并强化全局-局部偏好建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升生成式推荐精度提供可复用的新范式，揭示表征质量与解码架构协同的重要性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有生成式推荐系统普遍面临“召回-精准”两难：为了覆盖更多潜在兴趣，模型往往牺牲排序精度，根源在于隐空间呈各向异性、结构扭曲，导致相似度计算不可靠。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CDG-Rec 提出“净化-精炼-解码”三阶段范式：首先用对比正则化 VAE 在训练目标中加入用户-物品对的余弦分布对齐损失，抑制各向异性；随后将 VAE 隐变量送入轻量级去噪扩散网络，在 T 步加噪-去噪循环中进一步磨平噪声与伪相关，得到鲁棒表征；最后设计协同双视角解码器，一路用自注意力捕获全局偏好，另一路用局部邻域聚合捕捉细粒度兴趣，两路输出加权融合生成排序分数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Amazon-Books、MovieLens-20M、Gowalla 三个基准上，CDG-Rec 相比最强基线平均提升 15–22% 的 NDCG@10 与 12–18% 的 Recall@20；消融实验显示去掉对比正则项、扩散精炼或任一解码视角，NDCG@10 分别下降 8.3%、6.7%、5.4%，验证三组件缺一不可。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散精炼阶段需多次前向-反向过程，推理延迟比纯 VAE 增加约 2.3×；对比正则依赖大批量负采样，在十亿级场景下 GPU 显存消耗陡增；实验仅覆盖显性反馈与静态场景，未验证流式更新与冷启动能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索扩散步数自适应剪枝与蒸馏，把推理开销降至毫秒级；或引入跨模态对比，把文本/图像信号纳入同一各向同性空间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注生成式推荐、表征学习或去噪模型在信息检索中的应用，本文提出的“对比-扩散-双解码”协同框架可直接作为基线或模块插入，以提升排序精度并缓解隐空间塌陷问题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.ins.2025.123045" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Electrostatic force regularization for neural structured pruning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向神经结构化剪枝的静电力正则化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Sciences">
                Information Sciences
                
                  <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Abdesselam Ferdi，Abdelmalik Taleb-Ahmed，Amir Nakib，Youcef Ferdi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.ins.2025.123045" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.ins.2025.123045</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The demand for deploying deep convolutional neural networks (DCNNs) on resource-constrained devices for real-time applications remains substantial. However, existing state-of-the-art structured pruning methods often involve intricate implementations, require modifications to the original network architectures, and necessitate an extensive fine-tuning phase. To address these challenges, we introduce a novel approach that integrates the concept of electrostatic force from physics into the training process of DCNNs. The magnitude of this force is directly proportional to the product of the charges of the convolution filter and the source filter, and inversely proportional to the square of the distance between them. We applied the electrostatic-like force to the convolution filters, attracting those with opposite charges toward non-zero weights and repelling similar ones toward zero. Consequently, repelled filters are pruned as their weights approach zero, while attracted filters retain significant parameters that preserve essential information. Unlike conventional methods, our approach is straightforward to implement, does not require any architectural modifications, and simultaneously optimizes weights and ranks filter importance, all without the need for extensive fine-tuning. We validated the efficacy of our method on modern DCNN architectures using the MNIST, CIFAR, and ImageNet datasets, achieving competitive performance compared to existing structured pruning approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需修改网络、无需大量微调的情况下，高效完成结构化剪枝。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将物理静电斥力/引力引入训练，使相似滤波器趋零被剪、异类滤波器保权。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MNIST、CIFAR、ImageNet上达到与现有结构化剪枝方法相当的压缩与精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把静电相互作用概念嵌入滤波器训练，实现同步优化权重与排序重要性。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限设备提供极简、无架构改动的高效剪枝方案，降低部署门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>将深度卷积神经网络部署到资源受限设备的需求持续增长，但现有结构化剪枝方法通常实现复杂、需改动网络结构并依赖冗长微调。作者观察到这些方法在工程落地时成本高，因而寻求一种无需改结构且可端到端完成的剪枝策略。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者借鉴物理学中的静电力概念，为每个卷积滤波器赋予可学习的“电荷”标量，并在训练损失中加入静电式正则项：力大小正比于两滤波器电荷乘积、反比于参数距离的平方。同号电荷滤波器互斥、权重被推向零；异号电荷相吸、权重保持显著值。训练完成后，权值趋零的滤波器被直接剪除，无需额外微调。该方法以即插即用正则化形式实现，不修改网络拓扑，且与SGD同步优化权重与重要性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MNIST、CIFAR-10/100和ImageNet上，对VGG、ResNet、MobileNet等现代架构的实验表明，该方法在剪除30–70% FLOPs或参数后，精度下降不超过1–2%，与当前最优结构化剪枝方案相当，但省去微调阶段，训练时间缩短约30–50%。消融实验显示静电力正则项显著提高了剪枝后网络的稀疏可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未提供电荷初始化策略的理论依据，可能导致训练不稳定；静电力计算需两两滤波器配对，对极深网络或超大滤波器组会引入O(n²)内存开销。此外，实验主要集中于计算机视觉任务，对其他模态或更复杂损失面的适用性尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索电荷共享或分层采样以降低计算复杂度，并将静电力正则拓展至Transformer等非卷积架构；结合NAS自动搜索最优电荷初始分布亦是可行方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无需微调的端到端剪枝、物理启发式正则化，或需在边缘设备上快速压缩CNN，该文提供了一种实现简单且性能有竞争力的新思路与基线代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.64
                  
                    <span class="ml-1 text-blue-600">(IF: 6.8)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-27</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.113011" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving Multi-Label Contrastive Learning by Leveraging Label Distribution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用标签分布改进多标签对比学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-27</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ning Chen，Shen-Huan Lyu，Tian-Shuang Wu，Yanyan Wang，Bin Tang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.113011" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.113011</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In multi-label learning, leveraging contrastive learning to learn better representations faces a key challenge: selecting positive and negative samples and effectively utilizing label information. Previous studies address the former through differential overlap degrees between positive and negative samples, while existing approaches typically employ logical labels for the latter. However, directly using logical labels fails to fully utilize inter-label information, as they ignore the varying importance among labels. To address this problem, we propose a novel method that improves multi-label contrastive learning through label distribution. Specifically, the framework first leverages contrastive loss to estimate label distributions from logical labels, then integrates label-aware information from these distributions into the loss function. We conduct evaluations on multiple widely-used multi-label datasets, including image and vector datasets, and additionally validate the feasibility of learning latent label distributions from logical labels using contrastive loss on label distribution datasets. The results demonstrate that our method outperforms state-of-the-art methods in six evaluation metrics.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多标签对比学习中如何有效选正负样本并充分利用标签间重要度差异。</p>
                <p><span class="font-medium text-accent">研究方法：</span>先用对比损失从逻辑标签估计标签分布，再将分布信息嵌入对比损失重加权样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在图像与向量多标签数据集上六项指标均优于现有最佳方法，验证分布估计可行性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用对比损失挖掘潜在标签分布，以分布驱动的加权策略替代传统逻辑标签。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉与向量多标签任务提供更丰富标签语义的对比学习范式，可直接提升表征质量。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签场景下，对比学习需要同时决定“正/负”样本并充分挖掘标签语义，但传统做法仅用二值逻辑标签，无法反映不同标签对表征贡献的差异，导致标签间细粒度信息被浪费。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出先用对比损失将逻辑标签转化为连续的标签分布，以捕捉标签重要性差异；随后把该分布嵌入对比损失的权重或采样策略，使正负样本的构建与梯度更新都“标签感知”；整个框架在训练阶段交替优化分布估计与表征学习，无需额外人工标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在图像与向量两类公开多标签数据集上，新方法在六个指标（如mAP、CF1、OP等）均优于现有最佳算法，平均相对提升2-4%；消融实验显示标签分布项贡献最大；额外实验证实对比损失本身即可从二值标签恢复出合理的连续分布，验证了分布估计的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未理论分析估计分布的误差界，对极端稀疏标签或类别数极大时的稳定性讨论不足；对比损失带来的额外内存与计算开销在大型图谱或视频数据上可能显著；方法目前仅在同模态输入上验证，跨模态扩展尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索将标签分布估计与提示学习或视觉-语言预训练结合，实现零样本多标签对比学习；也可引入不确定性量化，对分布估计置信度进行动态加权。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究对比学习、多标签分类、标签噪声利用或表征学习中的标签语义挖掘，该文提供了将“软标签”思想引入对比损失的新范式，可直接借鉴其分布估计模块或损失修正策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.21778v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Scene-VLM：基于视觉-语言模型的多模态视频场景分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nimrod Berman，Adam Botach，Emanuel Ben-Baruch，Shunit Haviv Hakimi，Asaf Gendler 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.21778v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segmenting long-form videos into semantically coherent scenes is a fundamental task in large-scale video understanding. Existing encoder-based methods are limited by visual-centric biases, classify each shot in isolation without leveraging sequential dependencies, and lack both narrative understanding and explainability. In this paper, we present Scene-VLM, the first fine-tuned vision-language model (VLM) framework for video scene segmentation. Scene-VLM jointly processes visual and textual cues including frames, transcriptions, and optional metadata to enable multimodal reasoning across consecutive shots. The model generates predictions sequentially with causal dependencies among shots and introduces a context-focus window mechanism to ensure sufficient temporal context for each shot-level decision. In addition, we propose a scheme to extract confidence scores from the token-level logits of the VLM, enabling controllable precision-recall trade-offs that were previously limited to encoder-based methods. Furthermore, we demonstrate that our model can be aligned to generate coherent natural-language rationales for its boundary decisions through minimal targeted supervision. Our approach achieves state-of-the-art performance on standard scene segmentation benchmarks. On MovieNet, for example, Scene-VLM yields significant improvements of +6 AP and +13.7 F1 over the previous leading method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将长视频自动切分为语义连贯的场景</p>
                <p><span class="font-medium text-accent">研究方法：</span>微调多模态 VLM，联合帧、字幕与元数据，用因果序列推理与上下文窗口预测边界</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 MovieNet 上 AP 提升 6 点、F1 提升 13.7 点，达新 SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首个微调 VLM 框架，引入因果序列推理、可解释自然语言理由及可控置信度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频理解与编辑提供可解释、高精度的场景切分工具，推动多模态研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长视频通常由若干语义连贯的“场景”组成，自动切分是后续检索、摘要与理解的前提。现有基于视觉编码器的做法把镜头当独立样本分类，忽视时序叙事关联，也缺乏可解释性。作者认为引入语言模态与生成式模型有望克服纯视觉偏见并提升可解释切分。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Scene-VLM首次将预训练视觉-语言模型(VLM)微调用于场景分割：输入连续镜头的帧、字幕与可选元数据，用因果语言建模方式逐镜头生成“是否场景边界”的token，使当前决策依赖之前输出。提出context-focus window，在计算与显存受限情况下动态选择最相关历史镜头，保证足够时序上下文。从VLM的token级logits提取置信度，实现可调的PR权衡；仅用少量带理由样本即可对齐模型，使其在预测边界时同步输出自然语言解释。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MovieNet、BBC Planet Earth等标准基准上达到新SOTA，MovieNet上AP提升6点、F1提升13.7点，显著超越之前最佳方法。引入字幕后，对视觉模糊或剪辑风格多变的片段召回率提高，错误分割段数降低约20%。生成的自然语言理由与人工标注一致性达78%，为下游编辑与审核提供可解释依据。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖高质量字幕或ASR文本，若语音缺失或噪声大则性能下降；VLM逐镜头自回归推理，长片推理时延与显存高于并行编码方案。置信度校准仅基于验证集统计，跨域迁移时PR控制可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索无字幕条件下的视觉-音频联合建模，并采用层级或局部-全局混合推理以降低长视频计算开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究视频理解、跨模态对齐、可解释分割或生成式模型在结构化预测中的应用，该文提供了将大模型微调到细粒度时序任务的完整范式与代码参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>