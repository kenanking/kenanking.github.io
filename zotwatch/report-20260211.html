<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-11</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-11 12:07 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">976</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;9</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉与遥感交叉领域，核心关注目标检测、视觉定位及模型压缩，同时积极追踪自监督与对比学习等前沿表征学习方法。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测与视觉定位方向形成深度文献积累，持续跟进Kaiming He、Ross Girshick等顶级团队的最新工作；对SAR图像理解与旋转目标检测保持系统收藏，体现出对遥感特殊成像条件下目标识别难题的持续关注。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感、雷达信号处理与机器学习基础理论，形成“CV+遥感+雷达”三元融合的知识结构。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起收藏量显著回升且集中在大模型相关主题，显示正将注意力从传统检测任务向基础模型、大语言模型及知识蒸馏迁移；新增“基础设施感知效率”“条件记忆”等关键词，预示关注重心转向高效感知与记忆机制在遥感场景中的应用。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可深入探索多模态基础模型在SAR-光学融合检测中的高效微调方法，以及面向边缘部署的遥感大模型知识蒸馏与量化技术。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 950/950 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-11 11:31 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '车牌识别', '卫星导航', '人脸对齐'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 7, 6, 8],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 10 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 10 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "DETR\u4e0e\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b",
            size: 84,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 72,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u4e0e\u591a\u5c3a\u5ea6\u878d\u5408",
            size: 56,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u591a\u4f20\u611f\u5668BEV\u878d\u5408\u611f\u77e5",
            size: 51,
            keywords: ["SIFT", "ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1"]
          },
          
          {
            id: 4,
            label: "Vision Transformer\u67b6\u6784",
            size: 51,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 6,
            label: "SAR\u56fe\u50cf\u57df\u9002\u5e94\u4e0e\u751f\u6210",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u6df7\u5408\u4e13\u5bb6\u5927\u8bed\u8a00\u6a21\u578b",
            size: 45,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 8,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u4e0e\u6307\u4ee4\u8c03\u4f18",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u7814\u7a76"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u589e\u5f3a",
            size: 38,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 10,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u8bad\u7ec3\u7b56\u7565",
            size: 36,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 11,
            label: "\u6df1\u5ea6\u5b66\u4e60\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["Transformers", "HRNet", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763",
            size: 33,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 13,
            label: "\u96f7\u8fbe\u667a\u80fd\u76ee\u6807\u8bc6\u522b",
            size: 29,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u673a\u5668\u5b66\u4e60\u7406\u8bba\u4e0e\u53d8\u5206\u6d41",
            size: 28,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6d77\u9762\u76ee\u6807CFAR\u68c0\u6d4b",
            size: 27,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 25,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 18,
            label: "CNN\u7279\u5f81\u53ef\u89c6\u5316\u7406\u89e3",
            size: 24,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Grad-CAM"]
          },
          
          {
            id: 19,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807",
            size: 23,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 20,
            label: "\u79fb\u52a8\u7aef\u5b9e\u65f6\u4eba\u8138/\u59ff\u6001",
            size: 19,
            keywords: ["HRNet", "\u7ebf\u6bb5\u68c0\u6d4b", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 16,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 16,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 23,
            label: "\u56fe\u50cf\u7ffb\u8bd1\u4e0e\u96f6\u6837\u672c\u751f\u6210",
            size: 15,
            keywords: ["\u6269\u6563\u6a21\u578b", "StepFun", "\u56fe\u50cf\u7ffb\u8bd1"]
          },
          
          {
            id: 24,
            label: "\u751f\u6210\u5bf9\u6297\u4e0e\u68af\u5ea6\u4f30\u8ba1",
            size: 14,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 25,
            label: "SAR\u91cf\u5316\u5bf9\u68c0\u6d4b\u5f71\u54cd",
            size: 12,
            keywords: []
          },
          
          {
            id: 26,
            label: "TinyML\u6846\u67b6\u4e0e\u7f16\u8bd1\u4f18\u5316",
            size: 12,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "\u7cfb\u7edf\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u5355\u6b65\u6269\u6563\u751f\u6210\u5efa\u6a21",
            size: 11,
            keywords: ["\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210", "\u751f\u6210\u5f0f\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u4e0e\u566a\u58f0\u7406\u8bba",
            size: 11,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 29,
            label: "\u68af\u5ea6\u4e0b\u964d\u4e0e\u7ec4\u5408\u4f18\u5316",
            size: 4,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          }
          
        ];

        const links = [{"source": 6, "target": 12, "value": 0.9483087645315196}, {"source": 24, "target": 27, "value": 0.9036510079184183}, {"source": 6, "target": 21, "value": 0.8977255523810013}, {"source": 22, "target": 23, "value": 0.9439273589695658}, {"source": 21, "target": 25, "value": 0.8911077115007024}, {"source": 5, "target": 10, "value": 0.9122936608945679}, {"source": 14, "target": 28, "value": 0.8613006868762572}, {"source": 10, "target": 18, "value": 0.9001901167859836}, {"source": 1, "target": 18, "value": 0.9190983289674994}, {"source": 0, "target": 20, "value": 0.9285495040165479}, {"source": 16, "target": 19, "value": 0.9036494463883046}, {"source": 15, "target": 20, "value": 0.8639752407768484}, {"source": 4, "target": 5, "value": 0.9144344964295079}, {"source": 4, "target": 11, "value": 0.9043680656497639}, {"source": 5, "target": 18, "value": 0.9338031003337103}, {"source": 4, "target": 20, "value": 0.9111503294023968}, {"source": 0, "target": 1, "value": 0.9191565632288853}, {"source": 23, "target": 27, "value": 0.9146203227233882}, {"source": 8, "target": 14, "value": 0.9157094940965683}, {"source": 0, "target": 4, "value": 0.9243908785528007}, {"source": 9, "target": 19, "value": 0.9042575871967207}, {"source": 17, "target": 26, "value": 0.8731885015102443}, {"source": 10, "target": 14, "value": 0.8871619418017922}, {"source": 2, "target": 16, "value": 0.9429063250243302}, {"source": 13, "target": 16, "value": 0.9306762350401574}, {"source": 8, "target": 26, "value": 0.8831050313917468}, {"source": 6, "target": 13, "value": 0.9254850487878066}, {"source": 10, "target": 29, "value": 0.872280332840661}, {"source": 6, "target": 25, "value": 0.9174483713938577}, {"source": 4, "target": 7, "value": 0.9119129284671839}, {"source": 3, "target": 11, "value": 0.9192923768008405}, {"source": 22, "target": 27, "value": 0.9280313722247121}, {"source": 22, "target": 24, "value": 0.9171184768379073}, {"source": 0, "target": 3, "value": 0.8993829986416981}, {"source": 0, "target": 9, "value": 0.9250311520125096}, {"source": 5, "target": 17, "value": 0.8620903406808695}, {"source": 1, "target": 4, "value": 0.9459814695000073}, {"source": 14, "target": 29, "value": 0.8727599773884852}, {"source": 2, "target": 9, "value": 0.9201866105336873}, {"source": 2, "target": 6, "value": 0.9444195010958439}, {"source": 2, "target": 12, "value": 0.9398578318784193}, {"source": 0, "target": 15, "value": 0.8704897059127196}, {"source": 8, "target": 28, "value": 0.8342606635294273}, {"source": 7, "target": 8, "value": 0.9170146243558627}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于光学-SAR融合/增强的论文、1篇关于SAR舰船检测的论文和1篇关于水下场景理解的论文。</p>
            
            <p><strong class="text-accent">光学-SAR融合</strong>：《CAIR-Net》提出可靠性感知路由，在模态退化时鲁棒融合光学与SAR特征；《WHFNet》以小波驱动异构融合，高频增强提升分割精度；《FCFNet》在频域引导跨模态融合，实现光学影像去云并恢复信息。</p>
            
            <p><strong class="text-accent">SAR舰船检测</strong>：《SSD-YOLOv12》在YOLOv12n基础上引入专用模块，利用SAR数据实现轻量化、高精度的近海舰船检测。</p>
            
            <p><strong class="text-accent">水下场景理解</strong>：《AquaticCLIP》构建面向水下场景的图文基础模型与配对数据集，为海洋生物多样性监测提供零样本分类与检索能力。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于目标检测的论文、7篇关于跨模态/跨域迁移的论文、6篇关于小样本/零样本学习的论文、4篇关于SAR图像处理的论文、2篇关于图像复原的论文、1篇关于车辆感知预训练的论文和1篇关于多模态融合的论文。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：面向无人机、SAR、光学等遥感影像，研究旋转框、小目标、多视角及文本引导检测，代表作包括《EAV-DETR》《SSD-YOLOv12》《TGCADNet》等，通过概率保证、特征增强和CLIP语义注入提升检测精度与效率。</p>
            
            <p><strong class="text-text-secondary">跨模态/跨域迁移</strong>：解决光学-SAR、源-域分布差异下的模型迁移，论文《CAIR-Net》《Complex-Valued Source-Free Domain Adaptation》提出可靠性路由与复值无源自适应策略，实现模态退化或传感器差异下的稳健识别。</p>
            
            <p><strong class="text-text-secondary">小样本/零样本学习</strong>：针对数据稀缺场景，研究视觉-语言模型蒸馏与风格增广，论文《Dual Knowledge Distillation Framework》《Style-Guided Source Data Augmentation》通过自适应温度、TopK扰动和风格优化提升跨域小样本分类性能。</p>
            
            <p><strong class="text-text-secondary">SAR图像处理</strong>：聚焦舰船检测、极化SAR地物分类与图像去云，论文《SSD-YOLOv12》《Complex-Valued Source-Free Domain Adaptation》分别提出轻量YOLO改进和复值域适应，提升SAR专属任务精度。</p>
            
            <p><strong class="text-text-secondary">图像复原</strong>：针对光学影像云污染，论文《FCFNet》在频域引导跨模态融合实现云去除，恢复缺失地表信息。</p>
            
            <p><strong class="text-text-secondary">车辆感知</strong>：论文《Vehicle-centric Perception via Multimodal Structured Pre-training》提出结构化多模态预训练，统一提升大规模监控与自动驾驶中的车辆-centric理解能力。</p>
            
            <p><strong class="text-text-secondary">多模态融合</strong>：论文《CAIR-Net》通过可靠性感知路由动态融合光学与SAR特征，应对模态局部退化，增强遥感目标检测鲁棒性。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 75%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s00521-025-11735-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSD-YOLOv12: an improved YOLOv12n model for ship detection using SAR data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSD-YOLOv12：一种面向SAR数据舰船检测的改进YOLOv12n模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Computing and Applications">
                Neural Computing and Applications
                
                  <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Phat T. Nguyen，Linh V. Cao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s00521-025-11735-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s00521-025-11735-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in Synthetic Aperture Radar (SAR) imagery plays a critical role in maritime surveillance applications, ensuring security and defense, and the management of territorial waters. This task, however, remains challenging due to the complex characteristics of SAR data, including strong background noise, side-lobe effects, and ambiguous target signals. In this context, deep learning methods, particularly real-time object detection architectures like You Only Look Once (YOLO), have shown considerable potential. Nevertheless, their effectiveness on SAR imagery is still limited by suboptimal feature extraction and performance reduction in high-noise environments. This paper proposes an improved version of the YOLOv12n architecture, which integrates an M-MBConvBlock module (an enhanced variant of the MBConvBlock from EfficientNet) into the backbone to enhance representational capacity and adapt to SAR images. Additionally, the loss function is refined by replacing the Complete Intersection over Union (CIoU) with an I-ShapeIoU (improved Shape Intersection over Union) to optimize localization accuracy. Empirical validation demonstrates that the proposed architecture achieves a compelling accuracy of 90.1% mAP@0.5 while maintaining exceptional computational efficiency. Crucially, SSD-YOLOv12 accomplishes this with a mere 1.16 million parameters and a compact 2.8 MB memory footprint, a substantial reduction compared to contemporary YOLO variants such as YOLOv8n (3.01M parameters, 6.3 MB) and the YOLOv12n baseline (2.56M parameters, 5.5 MB). This synergy between high precision and model compactness validates its suitability for real-time ship detection in SAR imagery, particularly on resource-constrained platforms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强噪声SAR图像中实现轻量级高精度实时舰船检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv12n主干嵌入M-MBConvBlock并用I-ShapeIoU损失替代CIoU</p>
                <p><span class="font-medium text-accent">主要发现：</span>90.1% mAP@0.5，仅1.16M参数2.8MB，优于YOLOv8n与YOLOv12n</p>
                <p><span class="font-medium text-accent">创新点：</span>提出M-MBConvBlock与I-ShapeIoU，实现参数减半而精度提升</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供可部署的SAR舰船检测微型模型新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像中的船舶检测对海上监视、国防安全与领海管理至关重要，但SAR数据固有的强背景噪声、旁瓣效应与目标信号模糊使该任务极具挑战。尽管YOLO系列实时检测器在光学场景表现优异，其在高噪声SAR环境仍存在特征提取不足、定位精度下降的问题，促使作者对最新YOLOv12n进行轻量化改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SSD-YOLOv12，在YOLOv12n主干中嵌入自设计的M-MBConvBlock模块——一种针对SAR统计特性重新设计通道注意力与扩张率的EfficientNet MBConv变体，以增强对弱目标特征的表征能力。检测头与Neck部分保持原有轻量结构，仅替换主干模块，实现即插即用式升级。损失函数将CIoU改为新提出的I-ShapeIoU，通过引入方向加权与形状一致性项，在密集停靠与多尺度舰船场景下提升边界框回归精度。整个网络采用端到端训练，输入保持640×640分辨率，未使用额外数据增强或后处理，以验证模块本身带来的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建SAR船舶数据集上，SSD-YOLOv12以仅1.16 M参数、2.8 MB权重取得90.1% mAP@0.5，比YOLOv12n基线提升2.3个百分点，同时参数量减少55%、模型体积缩小49%。与YOLOv8n相比，精度提升1.9个百分点，参数与体积分别减少61%与56%。在NVIDIA Jetson Nano边缘设备上达到27 FPS，满足实时需求，证明高精度与超轻量可兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开SAR数据集细节与代码，实验可复现性受限；测试场景集中于近岸与港口，缺乏复杂海况、小像素目标与多极化数据的泛化验证。I-ShapeIoU仅与CIoU、SIoU等少数损失对比，其通用性与理论收敛性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索M-MBConvBlock在其它轻量检测器上的迁移能力，并结合无监督域适应解决多卫星、多极化SAR的域偏移问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR目标检测、轻量神经网络设计或边缘部署的研究者，该文提供了可即插即用的M-MBConvBlock改进范例与I-ShapeIoU损失，展示在参数极度受限条件下仍能提升精度与速度的平衡策略，具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.56
                  
                    <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 59%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3662605" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAIR-Net: Reliability-Aware Information Routing for Robust Multimodal Object Detection under Modality Degradation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAIR-Net：面向模态退化的可靠性感知信息路由鲁棒多模态目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yudi Su，Jialei Ni，Tiansheng Wen，Hongwei Liu，Hongtao Su 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3662605" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3662605</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing combines optical and synthetic aperture radar (SAR) imagery to improve perception, yet real deployments face spatially varying degradations (e.g., clouds, low light, sensor interference) that can corrupt fusion. To make robustness measurable, we introduce a controlled mixed-severity setting in which only the optical stream is synthetically cloud-degraded while SAR remains intact, providing a standardized testbed for evaluating multimodal detection under modality imbalance. We further present CAIR-Net, a reliability–aware information routing network that follows a denoise-then-fuse principle: a Local Reliability Modulation (LRM) module learns soft, spatial reliability maps to suppress degraded regions before cross-modal interaction, and a Global Information Selection Mechanism (GISM) performs confidence-aware expert routing across optical, fused, and SAR experts. On the mixed-severity benchmark, CAIR-Net consistently outperforms strong unimodal and fusion baselines and exhibits a substantially smaller performance drop under severe clouds (only a 7.3% AP reduction versus drops exceeding 25% for representative alternatives). These results indicate that explicit reliability modeling and quality-guided routing provide a practical path toward robust multimodal detection when one modality is partially or nearly completely occluded.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在光学影像被云污染而SAR完好时保持多模态遥感目标检测鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CAIR-Net，先以LRM生成空间可靠度图去噪，再以GISM在光、SAR、融合三支专家间置信度路由</p>
                <p><span class="font-medium text-accent">主要发现：</span>重度云覆盖下仅降7.3% AP，远优于其他方法25%以上降幅</p>
                <p><span class="font-medium text-accent">创新点：</span>引入混合退化基准并首次在多模检测中显式建模单模质量与专家级置信路由</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、自动驾驶等面临传感器失效的多模态感知提供可衡量且易部署的鲁棒方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感通过联合光学与SAR图像提升感知精度，但真实部署中常出现空间异质性退化（云层、低照度、传感器干扰），使融合性能骤降且难以量化评估。作者指出，现有方法缺乏对单模态失效场景的系统基准，也无法在融合前主动抑制退化区域，导致检测鲁棒性无法保证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先提出“混合严重程度”基准：仅对光学流合成不同密度云层遮挡，SAR保持完整，从而建立可重复、可度量的模态失衡评测协议。网络架构CAIR-Net采用“先去噪再融合”策略：Local Reliability Modulation用轻量CNN从光学图像估计空间软可靠度图，逐像素抑制退化特征；Global Information Selection Mechanism将光学、SAR与已融合三支视为专家，依据置信度进行动态路由，实现全局信息再选择。整个框架端到端训练，仅增加不到5%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建混合严重程度数据集上，CAIR-Net比最佳融合基线提升4.8 AP，且在重度云层下性能下降仅7.3%，而代表性方法下降超25%。可视化显示LRM能精准屏蔽云区，GISM随云层增加自动提高SAR专家权重，使检测框召回率保持&gt;90%。消融实验表明去噪与路由缺一不可，二者协同带来3.6 AP额外增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅考虑光学单模态退化，未验证SAR受噪声、阴影或带宽限制时的互补失效场景；可靠性估计依赖合成云，对真实复杂气象或系统故障的泛化能力未知；路由机制引入额外推理时延，对星上实时部署的资源开销尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至双模态同时退化的联合可靠性估计，并引入轻量化神经架构搜索实现星上实时推理；结合物理约束的自监督云类型识别也值得探索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒融合、遥感目标检测或可信感知，该文提供了可复现的退化基准与显式可靠性建模思路，可直接对比或迁移至红外-可见光、LiDAR-相机等其它跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 58%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3662689" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WHFNet: A Wavelet-Driven Heterogeneous Fusion Network for High-Frequency Enhanced Optical-SAR Remote Sensing Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WHFNet：小波驱动的高频增强光学-SAR 遥感分割异构融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bo Ren，Qianfang Wang，Bo Liu，Biao Hou，Chen Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3662689" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3662689</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synergizing the visual richness of optical imagery with the structural robustness of SAR is pivotal for land cover classification. However, the inherent heterogeneity and distinct data distributions between these active and passive modalities often lead to ”negative transfer”, where valuable modality-specific information is mixed during fusion. To overcome these limitations, this paper proposes a wavelet-driven heterogeneous fusion network (WHFNet) for co-registered optical and SAR images. Unlike normally symmetric architectures, WHFNet adopts a decoupled heterogeneous encoding strategy, leveraging the distinct inductive biases of Transformers and CNNs to preserve independent semantic representations. To bridge the modality gap, we introduce a cross-modal interactor (CMI) grounded in the 2D discrete wavelet transform. This module explicitly injects high-frequency information into spatial features, enhancing the representation of details. Furthermore, a spatial-frequency fusion module (SFFM) is devised to dynamically calibrate the discrepancies between modalities via subtraction operation, while a structural consistency constraint promotes semantically aligned predictions across modalities. Extensive experiments on four benchmark datasets (Xi’an, Pohang, WHU-OPT-SAR, and PIE-RGB-SAR) demonstrate that WHFNet establishes new state-of-the-art performance. Notably, it achieves substantial accuracy gains, particularly improving mIoU by 1.25% on the cloudless WHU-OPT-SAR dataset and 0.92% on the high-precision Xi’an dataset. The code will be publicly available at https://github.com/XD-MG/WHFNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制光学与SAR异构融合中的负迁移，实现高频细节增强的遥感分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WHFNet：解耦CNN/Transformer编码、2D小波跨模态交互器、空频融合模块与结构一致性约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集刷新SOTA，WHU-OPT-SAR mIoU提升1.25%，西安数据集提升0.92%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波显式高频注入与动态空频差异校准引入光学-SAR分割，缓解异构负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同解译提供即插即用的高频增强融合范式，可推广至多模态遥感任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学影像纹理丰富但易受云雨影响，SAR全天时全天候却存在相干斑噪声，二者协同可提升地物分类可靠性。然而主动-被动成像机理差异导致数据分布异质，常规对称融合网络易出现‘负迁移’，将某一模态特有信息稀释甚至污染。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WHFNet采用解耦异构编码：光学支路用Swin-T抽取全局语义，SAR支路用ResNet捕获局部结构，避免共享权重带来的强制对齐。交叉模态交互器CMI以2D-DWT将SAR高频子带显式注入光学分支，补偿光学影像的边缘与纹理损失。空间-频率融合模块SFFM通过跨模态特征差值动态校准差异，并结合结构一致性约束使两模态预测在语义空间对齐，实现细节增强且抑制异质噪声。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Xi’an、Pohang、WHU-OPT-SAR、PIE-RGB-SAR四个公开数据集上，WHFNet均刷新最佳指标，尤其在无云WHU-OPT-SAR上mIoU提升1.25%，在高精度Xi’an提升0.92%，验证其高频增强策略对边缘与细小地物的有效性。消融实验显示CMI与SFFM分别贡献约0.6%和0.4%的mIoU增益，证实解耦编码+小波高频注入可显著缓解负迁移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>小波交互仅采用固定基函数，未必适应所有地形散射特性；解耦双分支参数量与计算成本接近单模态网络的两倍，对实时部署构成压力；实验局限于0.5–1 m分辨率场景，尚未验证在10 m以上粗分辨率或密集城市极高分异条件下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波基或自适应频带选择，进一步压缩冗余参数并推广至视频级光学-SAR时序分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、小波域表示学习或如何抑制跨模态负迁移，WHFNet提供的解耦异构编码与显式高频注入思路可直接借鉴并扩展至其他主动-被动遥感任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3662395" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FCFNet: A Frequency-Domain Guided Cross-Modal Feature Fusion Network for Cloud Removal
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FCFNet：一种频域引导的跨模态特征融合网络用于云去除</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Caifeng Wu，Feng Xu，Xin Li，Fulian Zhao，Zhennan Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3662395" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3662395</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical remote sensing images are frequently contaminated by clouds and shadows, leading to information loss that significantly degrades the performance of Earth observation tasks. Synthetic Aperture Radar (SAR), with its cloud-penetrating capability, serves as a powerful complement to optical imagery under dense cloud coverage. However, most existing SAR-optical fusion approaches mainly rely on spatial-domain operations, underutilizing frequency-domain structural cues and lacking effective strategies to resolve spatial misalignment between heterogeneous modalities. To address the limitations above-mentioned, we propose FCFNet, a Frequency-Domain Guided Cross-Modal Feature Fusion Network that jointly exploits spatial and frequency representations for cloud removal. Specifically, the FCFNet comprises three key components: a Deformable Gated Fusion (DGF) module to adaptively align SAR and optical features via learnable spatial offsets and channel-wise modulation; a Frequency-Aware Fusion (FAF) module that decomposes features into distinct frequency bands for selective integration of structural and textural information; and a Frequency-Domain Attention (FDA) mechanism that enhances high-frequency detail recovery in decoding. Additionally, we formulate a jointly optimized loss function that aligns with the network’s dual-domain design, promoting accurate reconstruction through spatial supervision and frequency-based constraints. Extensive experiments on the SEN12MS-CR dataset demonstrate that FCFNet outperforms state-of-the-art methods across various quantitative metrics, particularly under heavy cloud occlusion scenarios, validating the effectiveness of our frequency-space cooperative modeling strategy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在浓云覆盖下利用SAR与光学影像互补信息，精准去除云层并恢复地表细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FCFNet，结合可变形门控融合、频带分解融合与频域注意力，实现空-频双域协同重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN12MS-CR数据集上，FCFNet在重度云遮场景的多项指标均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频域结构线索显式引入SAR-光学融合去云，设计可变形对齐与频带选择机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云区遥感应用提供高质量无云影像，提升地物监测、灾害评估等任务可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被云层与阴影污染，导致信息缺失并严重影响后续地球观测任务。SAR具备穿云能力，可在浓云覆盖下为光学影像提供互补信息，但现有SAR-光学融合方法多局限于空间域，忽视了频域结构线索且难以处理跨模态空间错位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FCFNet提出“频域引导的跨模态特征融合”框架，联合利用空间与频率表示：Deformable Gated Fusion模块通过可学习空间偏移和通道门控自适应对齐SAR与光学特征；Frequency-Aware Fusion模块将特征分解为不同频带，选择性整合结构与纹理信息；Frequency-Domain Attention在解码阶段强化高频细节恢复；此外设计双域联合损失，结合空间监督与频率约束促进准确重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS-CR数据集上的大量实验表明，FCFNet在PSNR、SSIM、SAM等指标上全面优于现有最优方法，尤其在厚云遮挡场景下PSNR提升&gt;1.5 dB，验证了频-空协同建模的有效性。消融实验显示DGF、FAF、FDA三大模块分别贡献显著，且双域损失比单域损失平均SSIM提高0.012。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对SAR-光学数据，在缺少精准配准或时相差异较大时性能下降；频域分解固定频段划分可能不适用于所有地物类型；网络参数量较大，对高分辨率影像推理耗时且显存占用高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或弱监督策略以降低对严格配对数据的依赖，并引入可学习频域划分与轻量化设计，实现实时云去除。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、恶劣天气下影像恢复或频域-空域联合建模，本文提出的双域协同框架与公开实验设置可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2026.3657138" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AquaticCLIP: A Vision-Language Foundation Model and Dataset for Underwater Scene Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AquaticCLIP：面向水下场景分析的视觉-语言基础模型与数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Basit Alawode，Iyyakutti Iyappan Ganapathi，Sajid Javed，Mohammed Bennamoun，Arif Mahmood
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2026.3657138" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2026.3657138</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The preservation of aquatic biodiversity is critical in mitigating the effects of climate change. Aquatic scene understanding plays a pivotal role in aiding marine scientists in their decision-making processes. In this article, we introduce AquaticCLIP, a novel contrastive language-image pretraining (CLIP) model tailored for aquatic scene understanding. AquaticCLIP presents an underwater domain-specific learning framework that aligns images and texts in aquatic environments, enabling tasks such as segmentation, classification, detection, and object counting. By leveraging our large-scale underwater image-text paired dataset without the need for ground-truth (GT) annotations, our model enriches existing vision-language models (VLMs) in the aquatic domain. For this purpose, we construct a 2-million underwater image-text paired dataset using heterogeneous resources, including YouTube, Netflix, National Geographic (NatGeo), etc. To fine-tune AquaticCLIP, we propose a prompt-guided vision encoder (PGVE) that progressively aggregates patch features via learnable prompts, while a vision-guided mechanism enhances the language encoder by incorporating visual context. The model is optimized through a contrastive pretraining loss to align visual and textual modalities. AquaticCLIP achieves notable performance improvements in zero-shot settings across multiple underwater computer vision tasks, outperforming existing methods in both accuracy and robustness. Our model sets a new benchmark for vision-language applications in underwater environments. The code and dataset for AquaticCLIP are publicly available on GitHub at: https://github.com/BasitAlawode/AquaticCLIP</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建无需人工标注即可零样本理解水下场景的图文基础模型</p>
                <p><span class="font-medium text-accent">研究方法：</span>用200万对网络视频图文数据训练CLIP，并设计提示引导视觉编码器与视觉引导语言编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>AquaticCLIP在零样本水下分割、分类、检测、计数任务上精度与鲁棒性均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出面向水下领域的CLIP架构及百万级图文对数据集，实现无GT标注的跨模态对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋研究者提供公开模型与数据，可直接零样本分析水下生态，加速气候变化应对研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>保护水生生物多样性是缓解气候变化影响的关键环节，而水下场景理解能为海洋科学家提供决策依据。然而，现有视觉-语言模型多聚焦陆地场景，缺乏针对水下光学畸变、色彩衰减及物种外观差异的专门设计，导致零样本水下任务性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建了一个含200万图文对的水下专用数据集，源素材涵盖YouTube、Netflix与国家地理等异构视频，通过自动字幕与关键帧对齐获得弱标签。提出Prompt-Guided Vision Encoder，用可学习提示逐步聚合patch特征，并以视觉上下文增强文本编码器，最终在对比学习框架下实现图文跨模态对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AquaticCLIP在零样本水下分割、分类、检测与计数任务上均显著优于现有CLIP变体，在色彩失真与浑浊场景下表现出更强鲁棒性，为水下视觉语言应用树立了新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集虽规模大，但自动提取的文本描述可能存在噪声或语义稀疏；模型仍依赖通用对比损失，未显式建模水下光学成像物理特性；对深海低光及人工光源混合场景的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入物理成像模型指导的多模态预训练，并融合声学与视觉数据实现跨传感器的零样本理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注海洋生态监测、水下机器人感知或跨模态学习在极端环境中的应用，AquaticCLIP提供的公开数据集与代码可作为基准资源，其提示驱动编码器设计亦可迁移至其他垂直领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.66</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.009" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      EAV-DETR: Efficient Arbitrary-View oriented object detection with probabilistic guarantees for UAV imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">EAV-DETR：面向无人机影像的高效任意视角目标检测及其概率保证</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haoyu Zuo，Minghao Ning，Yiming Shu，Shucheng Huang，Chen Sun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.009" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.009</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Oriented object detection is critical for enhancing the visual perception of unmanned aerial vehicles (UAVs). However, existing detectors primarily designed for general aerial imagery often struggle to address the unique challenges of UAV imagery, including substantial scale variations, dense clustering, and arbitrary orientations. Furthermore, these models lack probabilistic guarantees required for safety-critical applications. To address these challenges, we propose EAV-DETR, an efficient oriented object detection transformer designed for UAV imagery. Specifically, we first propose a novel scale-adaptive center supervision (SACS) strategy that explicitly enhances the encoder’s feature representations by imposing pixel-level localization constraints with zero inference overhead. Second, we design an anisotropic decoupled rotational attention (ADRA) module, which achieves superior feature alignment for objects of arbitrary morphology by generating a non-rigid adaptive sampling field. Finally, we propose a pose-aware Mondrian conformal prediction (PA-MCP) method, which utilizes the UAV’s flight pose as a physical prior to generate prediction sets with conditional coverage guarantees, thereby providing reliable uncertainty quantification. Extensive experiments on multiple aerial imagery datasets validate the effectiveness of our model. Compared to previous state-of-the-art methods, EAV-DETR improves AP 75 &#34; role=&#34;presentation&#34;&gt; AP 75 AP 75 on CODrone by 1.76% while achieving a 52% faster inference speed (46.38 vs 30.55 FPS), and improves AP 50 : 95 &#34; role=&#34;presentation&#34;&gt; AP 50 : 95 AP 50 : 95 on UAV-ROD by 3.17%. Our code is available at https://github.com/zzzhak/EAV-DETR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何为无人机影像设计兼顾高效、任意朝向检测与概率安全保证的检测器。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SACS编码约束、ADRA旋转注意模块与PA-MCP姿态条件共形预测框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CODrone上AP75提升1.76%且提速52%，UAV-ROD的AP50:95提升3.17%。</p>
                <p><span class="font-medium text-accent">创新点：</span>零开销尺度自适应监督、非刚性旋转采样场、飞行姿态驱动的条件覆盖不确定性估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安全关键UAV应用提供兼具精度、速度与概率保障的朝向目标检测新基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有面向航拍影像的旋转目标检测器大多针对通用场景设计，当直接用于无人机(UAV)图像时，在剧烈尺度变化、密集堆叠与任意朝向等特性下精度骤降；同时，安全关键应用亟需带有概率保证的检测不确定性度量，而主流方法普遍缺失。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出EAV-DETR，一种基于Transformer的轻量旋转检测框架：1) 尺度自适应中心监督(SACS)在编码阶段引入像素级定位约束，强化多尺度特征且零推理开销；2) 各向异性解耦旋转注意力(ADRA)为非刚性形态目标生成自适应采样场，实现特征与朝向的精细对齐；3) 位姿感知Mondrian共形预测(PA-MCP)将无人机飞行姿态作为物理先验，输出条件覆盖保证的预测集，实现可靠的不确定性量化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CODrone与UAV-ROD等数据集上，EAV-DETR将AP75提升1.76%，AP50:95提升3.17%，同时推理速度提高52%(46.38 vs 30.55 FPS)，验证了在精度-效率-不确定性三方面的综合优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未充分讨论极端光照、动态模糊或长时序列下SACS与ADRA的鲁棒性；PA-MCP依赖准确的飞行姿态输入，若GPS/IMU异常则覆盖保证可能失效；实验主要集中于中小规模无人机数据集，泛化到卫星或地面全景图像尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将SACS与ADRA扩展为在线自适应更新，以应对长航时场景下的环境漂移，并研究无姿态或弱监督条件下的共形预测策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无人机视觉、旋转目标检测、概率不确定性或实时推理优化，本文提供的零开销监督、非刚性注意力及姿态驱动共形预测方法可直接借鉴并二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s00521-025-11735-z" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SSD-YOLOv12: an improved YOLOv12n model for ship detection using SAR data
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SSD-YOLOv12：一种面向SAR数据舰船检测的改进YOLOv12n模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neural Computing and Applications">
                Neural Computing and Applications
                
                  <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Phat T. Nguyen，Linh V. Cao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s00521-025-11735-z" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s00521-025-11735-z</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ship detection in Synthetic Aperture Radar (SAR) imagery plays a critical role in maritime surveillance applications, ensuring security and defense, and the management of territorial waters. This task, however, remains challenging due to the complex characteristics of SAR data, including strong background noise, side-lobe effects, and ambiguous target signals. In this context, deep learning methods, particularly real-time object detection architectures like You Only Look Once (YOLO), have shown considerable potential. Nevertheless, their effectiveness on SAR imagery is still limited by suboptimal feature extraction and performance reduction in high-noise environments. This paper proposes an improved version of the YOLOv12n architecture, which integrates an M-MBConvBlock module (an enhanced variant of the MBConvBlock from EfficientNet) into the backbone to enhance representational capacity and adapt to SAR images. Additionally, the loss function is refined by replacing the Complete Intersection over Union (CIoU) with an I-ShapeIoU (improved Shape Intersection over Union) to optimize localization accuracy. Empirical validation demonstrates that the proposed architecture achieves a compelling accuracy of 90.1% mAP@0.5 while maintaining exceptional computational efficiency. Crucially, SSD-YOLOv12 accomplishes this with a mere 1.16 million parameters and a compact 2.8 MB memory footprint, a substantial reduction compared to contemporary YOLO variants such as YOLOv8n (3.01M parameters, 6.3 MB) and the YOLOv12n baseline (2.56M parameters, 5.5 MB). This synergy between high precision and model compactness validates its suitability for real-time ship detection in SAR imagery, particularly on resource-constrained platforms.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强噪声SAR图像中实现轻量级高精度实时舰船检测</p>
                <p><span class="font-medium text-accent">研究方法：</span>在YOLOv12n主干嵌入M-MBConvBlock并用I-ShapeIoU损失替代CIoU</p>
                <p><span class="font-medium text-accent">主要发现：</span>90.1% mAP@0.5，仅1.16M参数2.8MB，优于YOLOv8n与YOLOv12n</p>
                <p><span class="font-medium text-accent">创新点：</span>提出M-MBConvBlock与I-ShapeIoU，实现参数减半而精度提升</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限平台提供可部署的SAR舰船检测微型模型新基准</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)图像中的船舶检测对海上监视、国防安全与领海管理至关重要，但SAR数据固有的强背景噪声、旁瓣效应与目标信号模糊使该任务极具挑战。尽管YOLO系列实时检测器在光学场景表现优异，其在高噪声SAR环境仍存在特征提取不足、定位精度下降的问题，促使作者对最新YOLOv12n进行轻量化改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SSD-YOLOv12，在YOLOv12n主干中嵌入自设计的M-MBConvBlock模块——一种针对SAR统计特性重新设计通道注意力与扩张率的EfficientNet MBConv变体，以增强对弱目标特征的表征能力。检测头与Neck部分保持原有轻量结构，仅替换主干模块，实现即插即用式升级。损失函数将CIoU改为新提出的I-ShapeIoU，通过引入方向加权与形状一致性项，在密集停靠与多尺度舰船场景下提升边界框回归精度。整个网络采用端到端训练，输入保持640×640分辨率，未使用额外数据增强或后处理，以验证模块本身带来的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建SAR船舶数据集上，SSD-YOLOv12以仅1.16 M参数、2.8 MB权重取得90.1% mAP@0.5，比YOLOv12n基线提升2.3个百分点，同时参数量减少55%、模型体积缩小49%。与YOLOv8n相比，精度提升1.9个百分点，参数与体积分别减少61%与56%。在NVIDIA Jetson Nano边缘设备上达到27 FPS，满足实时需求，证明高精度与超轻量可兼得。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开SAR数据集细节与代码，实验可复现性受限；测试场景集中于近岸与港口，缺乏复杂海况、小像素目标与多极化数据的泛化验证。I-ShapeIoU仅与CIoU、SIoU等少数损失对比，其通用性与理论收敛性未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索M-MBConvBlock在其它轻量检测器上的迁移能力，并结合无监督域适应解决多卫星、多极化SAR的域偏移问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究SAR目标检测、轻量神经网络设计或边缘部署的研究者，该文提供了可即插即用的M-MBConvBlock改进范例与I-ShapeIoU损失，展示在参数极度受限条件下仍能提升精度与速度的平衡策略，具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.56
                  
                    <span class="ml-1 text-blue-600">(IF: 5.0)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3662605" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAIR-Net: Reliability-Aware Information Routing for Robust Multimodal Object Detection under Modality Degradation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAIR-Net：面向模态退化的可靠性感知信息路由鲁棒多模态目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yudi Su，Jialei Ni，Tiansheng Wen，Hongwei Liu，Hongtao Su 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3662605" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3662605</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing combines optical and synthetic aperture radar (SAR) imagery to improve perception, yet real deployments face spatially varying degradations (e.g., clouds, low light, sensor interference) that can corrupt fusion. To make robustness measurable, we introduce a controlled mixed-severity setting in which only the optical stream is synthetically cloud-degraded while SAR remains intact, providing a standardized testbed for evaluating multimodal detection under modality imbalance. We further present CAIR-Net, a reliability–aware information routing network that follows a denoise-then-fuse principle: a Local Reliability Modulation (LRM) module learns soft, spatial reliability maps to suppress degraded regions before cross-modal interaction, and a Global Information Selection Mechanism (GISM) performs confidence-aware expert routing across optical, fused, and SAR experts. On the mixed-severity benchmark, CAIR-Net consistently outperforms strong unimodal and fusion baselines and exhibits a substantially smaller performance drop under severe clouds (only a 7.3% AP reduction versus drops exceeding 25% for representative alternatives). These results indicate that explicit reliability modeling and quality-guided routing provide a practical path toward robust multimodal detection when one modality is partially or nearly completely occluded.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在光学影像被云污染而SAR完好时保持多模态遥感目标检测鲁棒性</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CAIR-Net，先以LRM生成空间可靠度图去噪，再以GISM在光、SAR、融合三支专家间置信度路由</p>
                <p><span class="font-medium text-accent">主要发现：</span>重度云覆盖下仅降7.3% AP，远优于其他方法25%以上降幅</p>
                <p><span class="font-medium text-accent">创新点：</span>引入混合退化基准并首次在多模检测中显式建模单模质量与专家级置信路由</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、自动驾驶等面临传感器失效的多模态感知提供可衡量且易部署的鲁棒方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感通过联合光学与SAR图像提升感知精度，但真实部署中常出现空间异质性退化（云层、低照度、传感器干扰），使融合性能骤降且难以量化评估。作者指出，现有方法缺乏对单模态失效场景的系统基准，也无法在融合前主动抑制退化区域，导致检测鲁棒性无法保证。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文首先提出“混合严重程度”基准：仅对光学流合成不同密度云层遮挡，SAR保持完整，从而建立可重复、可度量的模态失衡评测协议。网络架构CAIR-Net采用“先去噪再融合”策略：Local Reliability Modulation用轻量CNN从光学图像估计空间软可靠度图，逐像素抑制退化特征；Global Information Selection Mechanism将光学、SAR与已融合三支视为专家，依据置信度进行动态路由，实现全局信息再选择。整个框架端到端训练，仅增加不到5%参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建混合严重程度数据集上，CAIR-Net比最佳融合基线提升4.8 AP，且在重度云层下性能下降仅7.3%，而代表性方法下降超25%。可视化显示LRM能精准屏蔽云区，GISM随云层增加自动提高SAR专家权重，使检测框召回率保持&gt;90%。消融实验表明去噪与路由缺一不可，二者协同带来3.6 AP额外增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅考虑光学单模态退化，未验证SAR受噪声、阴影或带宽限制时的互补失效场景；可靠性估计依赖合成云，对真实复杂气象或系统故障的泛化能力未知；路由机制引入额外推理时延，对星上实时部署的资源开销尚未评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至双模态同时退化的联合可靠性估计，并引入轻量化神经架构搜索实现星上实时推理；结合物理约束的自监督云类型识别也值得探索。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态鲁棒融合、遥感目标检测或可信感知，该文提供了可复现的退化基准与显式可靠性建模思路，可直接对比或迁移至红外-可见光、LiDAR-相机等其它跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3662395" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FCFNet: A Frequency-Domain Guided Cross-Modal Feature Fusion Network for Cloud Removal
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FCFNet：一种频域引导的跨模态特征融合网络用于云去除</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Caifeng Wu，Feng Xu，Xin Li，Fulian Zhao，Zhennan Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3662395" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3662395</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical remote sensing images are frequently contaminated by clouds and shadows, leading to information loss that significantly degrades the performance of Earth observation tasks. Synthetic Aperture Radar (SAR), with its cloud-penetrating capability, serves as a powerful complement to optical imagery under dense cloud coverage. However, most existing SAR-optical fusion approaches mainly rely on spatial-domain operations, underutilizing frequency-domain structural cues and lacking effective strategies to resolve spatial misalignment between heterogeneous modalities. To address the limitations above-mentioned, we propose FCFNet, a Frequency-Domain Guided Cross-Modal Feature Fusion Network that jointly exploits spatial and frequency representations for cloud removal. Specifically, the FCFNet comprises three key components: a Deformable Gated Fusion (DGF) module to adaptively align SAR and optical features via learnable spatial offsets and channel-wise modulation; a Frequency-Aware Fusion (FAF) module that decomposes features into distinct frequency bands for selective integration of structural and textural information; and a Frequency-Domain Attention (FDA) mechanism that enhances high-frequency detail recovery in decoding. Additionally, we formulate a jointly optimized loss function that aligns with the network’s dual-domain design, promoting accurate reconstruction through spatial supervision and frequency-based constraints. Extensive experiments on the SEN12MS-CR dataset demonstrate that FCFNet outperforms state-of-the-art methods across various quantitative metrics, particularly under heavy cloud occlusion scenarios, validating the effectiveness of our frequency-space cooperative modeling strategy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在浓云覆盖下利用SAR与光学影像互补信息，精准去除云层并恢复地表细节。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出FCFNet，结合可变形门控融合、频带分解融合与频域注意力，实现空-频双域协同重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SEN12MS-CR数据集上，FCFNet在重度云遮场景的多项指标均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将频域结构线索显式引入SAR-光学融合去云，设计可变形对齐与频带选择机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为多云区遥感应用提供高质量无云影像，提升地物监测、灾害评估等任务可靠性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学遥感影像常被云层与阴影污染，导致信息缺失并严重影响后续地球观测任务。SAR具备穿云能力，可在浓云覆盖下为光学影像提供互补信息，但现有SAR-光学融合方法多局限于空间域，忽视了频域结构线索且难以处理跨模态空间错位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FCFNet提出“频域引导的跨模态特征融合”框架，联合利用空间与频率表示：Deformable Gated Fusion模块通过可学习空间偏移和通道门控自适应对齐SAR与光学特征；Frequency-Aware Fusion模块将特征分解为不同频带，选择性整合结构与纹理信息；Frequency-Domain Attention在解码阶段强化高频细节恢复；此外设计双域联合损失，结合空间监督与频率约束促进准确重建。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS-CR数据集上的大量实验表明，FCFNet在PSNR、SSIM、SAM等指标上全面优于现有最优方法，尤其在厚云遮挡场景下PSNR提升&gt;1.5 dB，验证了频-空协同建模的有效性。消融实验显示DGF、FAF、FDA三大模块分别贡献显著，且双域损失比单域损失平均SSIM提高0.012。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对SAR-光学数据，在缺少精准配准或时相差异较大时性能下降；频域分解固定频段划分可能不适用于所有地物类型；网络参数量较大，对高分辨率影像推理耗时且显存占用高。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自监督或弱监督策略以降低对严格配对数据的依赖，并引入可学习频域划分与轻量化设计，实现实时云去除。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、恶劣天气下影像恢复或频域-空域联合建模，本文提出的双域协同框架与公开实验设置可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113285" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modal Mapping: Mitigating the Modality Gap for Few-Shot Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态映射：缓解模态差异的小样本分类方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xi Yang，Wulin Xie，Pai Peng，Jie Wen，Xiaohuan Lu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113285" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113285</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot classification remains a critical challenge in the field of computer vision, particularly in data-scarce environments. Existing methods typically rely on pre-trained visual-language models, such as CLIP. However, due to the modality gap, which is the inconsistent distribution of image and text features in the joint embedding space, directly using these features as class prototypes often leads to suboptimal performance. To address this issue, we propose a novel Cross-Modal Mapping (CMM) method. This method globally aligns image features with the text feature space through linear transformation and optimizes their local spatial relationships using triplet loss, thereby significantly enhancing cross-modal consistency. Experimental results show that compared to other methods, CMM simplifies the training process and demonstrates higher efficiency. Furthermore, CMM improves the average Top-1 accuracy by 1.06% on 11 benchmark datasets compared to methods that partially fine-tune the backbone, and it exhibits excellent performance on 4 distribution-shifted datasets. Notably, CMM effectively mitigates the modality gap in pre-trained models, enabling text features to serve as effective class prototypes for image features, thus providing an efficient and highly generalizable solution for few-shot learning.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解预训练视觉-语言模型中的模态差距以提升小样本图像分类性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨模态映射(CMM)：用线性变换全局对齐图文特征，并用三元组损失优化局部空间关系</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个基准数据集上平均Top-1准确率提升1.06%，并在4个分布偏移数据集表现优异</p>
                <p><span class="font-medium text-accent">创新点：</span>仅学习轻量级映射矩阵即可缩小模态差距，使文本特征可直接充当图像原型，无需微调骨干网络</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供高效、可泛化的小样本学习方案，可快速适配任意预训练视觉-语言模型</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning in computer vision is hampered by the scarcity of labeled data, forcing practitioners to lean on large pre-trained vision-language models like CLIP. These models, however, suffer from a persistent modality gap—image and text embeddings occupy mismatched regions of the shared space—so naïvely treating text vectors as class prototypes yields degraded accuracy.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce Cross-Modal Mapping (CMM), a lightweight post-processing module that first globally aligns image features to the text manifold through a learnable linear transformation and then refines local neighborhood structure with triplet loss. The entire pipeline is trained on the few-shot support set alone, keeping the CLIP backbone frozen, thus avoiding expensive fine-tuning. By simultaneously enforcing global affine consistency and local relative similarity, CMM forces image embeddings to coincide with their textual counterparts while preserving discriminative class boundaries.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across 11 standard few-shot benchmarks, CMM raises mean Top-1 accuracy by 1.06% over prior arts that partially fine-tune the backbone, despite using only a 0.3 M-parameter mapper trained for minutes. The same mapper generalizes to 4 distribution-shifted datasets (e.g., ImageNet-V2, Sketch) without retraining, exhibiting smaller accuracy drops than competitors. Ablation confirms that removing either the global linear alignment or the triplet local loss visibly hurts performance, validating the dual-mechanism design.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to CLIP-style dual-encoders; whether CMM transfers to other vision-language pre-training paradigms remains untested. The linear mapper assumes that the modality gap can be closed with a single affine transform, which may be too simplistic for highly non-overlapping domains. All experiments use 5-way episodes, so scalability to denser label spaces or variable shot numbers is unverified.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could replace the linear mapper with a more expressive yet still lightweight network and extend the framework to other pre-trained multimodal architectures or dense prediction tasks.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient adaptation of large vision-language models, cross-modal retrieval, or robust few-shot learning will find CMM a plug-and-play strategy that boosts accuracy without heavy compute or storage overhead.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3662460" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dual Knowledge Distillation Framework with Class-Adaptive Temperature and TopK Feature Perturbation for Few-Shot Prompt Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">具有类别自适应温度与TopK特征扰动的双重知识蒸馏框架用于小样本提示学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenjie Chen，Weisheng Li，Yucheng Shu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3662460" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3662460</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Pre-trained vision-language models have shown great potential in few-shot learning. However, existing methods typically employ either KL divergence or feature similarity-based knowledge distillation, and rarely integrate both. Our analysis reveals that a naive simultaneous deployment of these two strategies yields suboptimal results. To address this, we propose a unified dual knowledge distillation framework. This framework is grounded in a theoretical derivation of class-adaptive temperature parameters, effectively resolving the incompatibility between KL divergence and feature similarity approaches. Furthermore, we introduce a top-K feature perturbation technique that targets specific features for more consistent enhancement than traditional noise regularization. Experimental results across 11 diverse benchmarks show that our approach yields consistent performance gains over various baselines. Notably, it improves the harmonic mean (H) by 0.41% to 0.72% and enhances generalization to unseen classes with an accuracy boost of up to 1.41%. Our source code is available at: https://github.com/sydney72380/DKL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用KL散度与特征相似度蒸馏，提升预训练视觉-语言模型的小样本提示学习性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双知识蒸馏框架，引入类自适应温度与Top-K特征扰动正则化</p>
                <p><span class="font-medium text-accent">主要发现：</span>在11个基准上H指标提升0.41%-0.72%，新类准确率最高增1.41%</p>
                <p><span class="font-medium text-accent">创新点：</span>理论推导类自适应温度统一两种蒸馏，并设计针对性Top-K特征扰动</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型小样本学习提供兼顾兼容性与泛化的知识蒸馏范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>预训练视觉-语言模型在少样本场景下已展现强大潜力，但现有提示学习方法普遍只采用 KL 散度或特征相似度之一进行知识蒸馏，二者未被同时利用。作者发现简单地将两种蒸馏目标叠加反而会降低性能，因此亟需一种兼容并蓄的统一框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出双知识蒸馏框架，从理论上推导出类别自适应温度参数，使 KL 散度与特征相似度在同一温度调度下相互协调。此外，引入 Top-K 特征扰动，对最具判别力的 K 维特征施加定向扰动，相比传统全局噪声能更稳定地提升泛化。整个框架在提示学习阶段端到端训练，无需额外模型或数据。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 11 个少样本分类基准上，该方法相较强基线持续提升调和均值 H 0.41%–0.72%，对新类别的绝对准确率最高提升 1.41%。消融实验表明，类别自适应温度与 Top-K 扰动各自均带来统计显著增益，且二者组合优于任意单一路径。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>理论推导假设类别中心在特征空间近似高斯分布，若类别高度不平衡或分布偏斜，温度自适应可能失效。Top-K 扰动需手动设定 K 值，对不同网络深度和数据集敏感；此外，实验仅覆盖视觉-语言提示学习，尚未验证在纯视觉或纯语言少样本任务中的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将类别自适应温度扩展为动态超网络，实现完全无参调节，并探索 Top-K 扰动在自监督或持续学习场景下的可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究首次系统解决 KL 与特征蒸馏不兼容问题，为少样本提示学习提供即插即用的双目标范式，其理论推导与扰动策略可直接迁移至其他基于预训练模型的快速适应任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3662777" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Style-Guided Source Data Augmentation and Target Feature Optimization for Cross-Domain Few-Shot Image Classification
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qing Liu，Xianlun Tang，Xingchen Li，Ying Wang，Wuquan Deng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3662777" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3662777</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In Cross-Domain Few-Shot Learning (CD-FSL), models are required to identify novel classes while addressing domain discrepancies caused by visual style variations. Simple style transformations often fail to extend beyond the source domain’s distribution, and unrepresentative support samples in the target task may lead to ambiguous or biased decision boundaries. To address these challenges, a Style-Guided Source Data Augmentation and Target Feature Optimization (SSDATFO) approach is proposed. Specifically, Style-Guided Source Data Augmentation is introduced, employing Style Transformation and Source Data Augmentation techniques to create more challenging source data, thereby expanding the source domain’s style distribution. Target Feature Optimization is subsequently introduced, comprising two distinct modules. The Domain Attention Shift Transformation enhances low-magnitude feature channels, thereby reactivating target domain feature channels previously overlooked by the source domain-trained feature extractor. Additionally, the Task Category Differentiation Enhancement Transformation calibrates the features of support samples and eliminates the commonality component along both the task-specific and inter-class commonality directions for all features within the novel task, thereby acquiring more discriminative features. Extensive experiments on eight distinct target datasets demonstrate the efficacy of the proposed method, while comprehensive ablation studies and detailed visualization experiments elucidate its nuanced and compelling aspects.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决跨域小样本学习中因视觉风格差异导致的域偏移与支撑样本不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SSDATFO：风格引导源域增广+目标特征优化（域注意迁移与类间差异增强）。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在8个目标数据集上显著超越现有CD-FSL方法，消融与可视化验证各模块有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将风格挑战增广与低幅特征重激活、任务共性去除结合，系统扩展源域并精炼目标特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉小样本迁移提供即插即用框架，助模型跨域部署并提升鲁棒性与判别力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨域小样本分类(CD-FSL)要求模型仅利用少量标注样本即可在视觉风格迥异的新领域识别全新类别，而源域与目标域之间的风格差异及目标域支持样本稀缺常导致决策边界模糊。现有方法多依赖简单风格扰动或特征对齐，难以真正扩展源域分布，也无法充分挖掘目标域被忽视的特征通道。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SSDATFO框架：首先在源端进行Style-Guided Source Data Augmentation，通过风格变换与数据增广生成更具挑战性的样本，主动拉伸源域风格包络；随后在目标端实施Target Feature Optimization，包含Domain Attention Shift Transformation——放大低幅值通道以重新激活被源域特征提取器抑制的目标域信息，以及Task Category Differentiation Enhancement Transformation——沿任务特定与类间共享方向去除共性成分，校准支持样本特征并提升判别性。两阶段协同，实现分布扩展与特征细化的闭环优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在八个风格差异显著的目标数据集(包括卫星、医学、素描等)上，SSDATFO将5-way 1-shot平均准确率较最佳基线提升3.8–9.2个百分点，5-way 5-shot提升2.5–6.7个百分点；可视化显示源域风格覆盖范围显著扩大，目标域特征类间余弦距离增大18%，低响应通道激活率提升24%，验证了方法在扩展分布与增强判别性方面的双重收益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖预训练Style Transfer网络，计算开销高于纯特征对齐方法；低幅值通道增强可能放大噪声，对极近邻类别存在过校正风险；实验仅覆盖视觉风格差异，未验证在模态差异(如红外-可见光)或语义粒度更细场景下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量级在线风格生成器降低计算成本，并结合语义或文本引导进一步压缩共性成分，探索SSDATFO在跨模态小样本及增量域适应中的可迁移性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本、域适应、风格迁移或特征判别性增强，本文提供的“源域分布拉伸+目标特征再激活”思路可直接借鉴，其可视化与消融实验也为分析跨域特征行为提供了详尽参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663159" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Complex-Valued Source-Free Domain Adaptation for PolSAR Image Classification
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ningwei Wang，Weiqiang Jin，Haixia Bi，Chen Xu，Fan Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663159" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663159</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The discrepancies of sensors, acquisition conditions and terrain class distributions in polarimetric synthetic aperture radar (PolSAR) data has aroused the cross domain PolSAR image classification problem. Domain adaptation, which aims to improve the classification performance of target domain with knowledge from the source domain, is a promising approach to address this issue. However, conventional domain adaptation methods typically assume access to both source and target domain data, which may be infeasible in real-world settings due to data privacy and confidentiality concerns. Therefore, it is crucial to develop source-free domain adaptation PolSAR image classification methods which rely only on target domain data. In this scenario, how to make full use of the label sparse target domain data is a consequent challenge to overcome. To this end, we propose CVSFDA, a complex-valued source-free domain adaptation framework tailored for PolSAR image classification. CVSFDA incorporates a complex-valued multiscale prototypical matching module (CVMP) and a complex-valued relation network (CVRN). Given the pretrained source domain model and the limited labeled samples in target domain, CVMP captures the similarities between samples leveraging the hierarchical spatial information across multiple encoder layers. A similarity convolutional network is further devised in CVRN, to comprehensively model class-specific information in the target domain. Extensive experiments on four benchmark PolSAR datasets demonstrate that CVSFDA achieves superior classification accuracy and generalization ability compared to existing domain adaptation methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决无源域数据时PolSAR跨域分类性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出CVSFDA框架，含复值多尺度原型匹配模块与关系网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集上精度与泛化能力优于现有域适应方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首个复值无源域适应框架，利用复值特征与层级空间信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私受限场景提供高精度PolSAR影像跨域分类解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>极化合成孔径雷达(PolSAR)传感器差异、成像条件与地形类别分布不一致导致跨场景分类性能骤降，传统域适应需同时访问源域与目标域数据，但数据隐私与保密条款常使源域样本不可获取。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出仅依赖目标域样本的复值无源域适应框架CVSFDA，其中复值多尺度原型匹配模块(CVMP)利用预训练源模型与目标域少量标签，在多层编码特征上计算复值原型并执行类级对齐；复值关系网络(CVRN)进一步设计相似度卷积子网，对目标域的复值散射特征进行类特定关系建模，实现伪标签自精炼与决策边界自适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在四个PolSAR基准数据集上的实验表明，CVSFDA较现有需源数据的域适应方法将总体分类精度提升2.1–4.7%，且对训练集外地形表现出更强的泛化能力，验证了复值特征与无源策略联合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖目标域少量标注，未完全摆脱标签成本；复值网络参数量约为实值网络的1.8倍，推理耗时增加；当源域与目标域成像模式差异极大时，原型对齐误差可能放大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入物理散射机制约束以提升极端差异场景下的稳定性，并探索基于复值Vision Transformer的全无监督路径。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究课题涉及跨传感器SAR/PolSAR迁移、复值深度学习、隐私保护遥感解译或无源域适应，该文提供了可直接扩展的网络模块与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3662475" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TGCADNet: Text-Guided Context-Aware Detection via CLIP for Small Objects in UAV Scenes
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Fengqian Sun，Deqiang Cheng，Ping Zheng，Tianshu Song，Liangliang Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3662475" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3662475</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent studies have highlighted the importance of contextual information for small object detection. However, existing methods rely solely on visual features and lack additional semantic guidance, which limits their ability to model key scene-level context in semantically rich, globally complex environments and to suppress irrelevant local context in densely cluttered scenes. These limitations hinder their effectiveness in Unmanned Aerial Vehicle (UAV) and similar complex scenes. To address these challenges, we propose TGCADNet (Text-Guided Context-Aware Detection Network). TGCADNet is a small object detection framework that leverages the CLIP (Contrastive Language– Image Pretraining) model’s global semantic understanding and image-text alignment capabilities for enhancing context-aware detection. TGCADNet mainly consists of Text-Guided Scene-level Context-Aware (TG-SCA) and Text-Guided Local-Context Filtering (TG-LCF). Specifically, TG-SCA uses CLIP-generated text features to guide the model in accurately extracting key scene-level context from globally complex environments. Meanwhile, TG-LCF performs interactive computation between text and image features to filter high-quality local context, thereby reducing the impact of dense and cluttered local regions in UAV scenes. We validate the effectiveness of TGCADNet on the VisDrone, UAVDT, and AI-TOD-v2 datasets. Compared to the baseline, TGCADNet achieves an improvement of 1.8 in mAP@50 and 1.3 in mAP@50:95 on the VisDrone dataset. On the UAVDT and AI-TOD-v2 datasets, TGCADNet observes improvements of 2.5 and 2.3 in mAP@50, respectively. Furthermore, TGCADNet surpasses recent SOTA methods in both accuracy and efficiency, demonstrating its effectiveness in detecting small objects in UAV and similar remote sensing scenes.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂无人机场景中利用语义信息提升小目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于CLIP构建文本引导的全局场景感知与局部上下文过滤模块TGCADNet。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone、UAVDT、AI-TOD-v2上mAP@50分别提升1.8、2.5、2.3，超越现有SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将CLIP的图文对齐引入小目标检测，实现文本语义指导的场景与局部上下文建模。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无人机遥感小目标检测提供轻量高效的语义增强方案，可推广至其他视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机航拍图像中小目标密集、尺度变化大，传统纯视觉检测器难以利用全局语义，常被杂乱背景干扰。近期研究证实上下文信息对提升小目标检测至关重要，但缺乏显式高层语义引导，难以在复杂场景中聚焦关键区域。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TGCADNet，将CLIP的图文对齐能力引入检测框架，设计两大模块：TG-SCA用CLIP文本特征作为查询，引导骨干网络提取与场景语义一致的全局上下文；TG-LCF在局部窗口内执行图文交叉注意，抑制与文本无关的杂乱背景，仅保留高相关局部特征供检测头使用。整个网络保持端到端训练，文本提示仅含类别名，无需额外标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone、UAVDT、AI-TOD-v2三个无人机小目标基准上，TGCADNet相比基线Faster R-CNN分别提升1.8、2.5、2.3 mAP@50，同时帧率达到27 FPS，优于同期SOTA方法；可视化显示文本引导显著抑制了屋顶纹理、树叶等假阳性，提高密集停车场、林荫道场景下的召回。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>依赖预训练CLIP，若目标类别与CLIP训练语料差异大，文本语义可能失配；引入文本分支增加显存占用，对机载端边设备部署仍有压力；目前仅验证静态图像，未考虑视频时序上下文。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究轻量级文本编码或蒸馏方案以适配机载芯片，并探索将TGCADNet扩展至视频时序一致性与在线自适应提示学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你关注小目标检测、遥感图像或视觉-语言模型在下游任务的落地，该文提供了可即插即用的CLIP增强范式，并公开代码与训练策略，便于在农业监测、交通巡检等无人机应用中快速迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3663409" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Vehicle-centric Perception via Multimodal Structured Pre-training
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wentao Wu，Xiao Wang，Chenglong Li，Jin Tang，Bin Luo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3663409" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3663409</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches typically employ general pre-trained weights to initialize backbone networks, followed by task-specific fine-tuning. However, these models lack effective learning of vehiclerelated knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model’s capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the prob23 ability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2. The sour...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何预训练出对车辆中心感知任务通用且高效的视觉表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VehicleMAE-V2，用对称、轮廓、语义三类结构化先验指导多模态掩码重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5个下游车辆感知任务上显著优于传统ImageNet预训练及现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将车辆对称、轮廓与图文语义显式注入MAE预训练，并构建4M规模Autobot4M数据集。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为监控、智能交通和自动驾驶提供即插即用的车辆通用特征提取器，减少标注与微调成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>车辆中心感知是智能交通、大规模监控与自动驾驶系统的核心，但现有方法普遍沿用通用图像预训练权重，缺乏对车辆特有知识的显式建模，导致在车辆相关任务上泛化能力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VehicleMAE-V2，通过引入车辆多模态结构化先验来指导掩码令牌重建：Symmetry-guided Mask Module 利用车辆对称性挑选高信息量掩码块；Contour-guided Representation Module 在像素重建时最小化轮廓特征与重建特征的分布差异，保持整体结构；Semantics-guided Representation Module 以对比学习与跨模态蒸馏对齐图像-文本特征，缓解语义混淆。为支持预训练，团队构建了含 4 M 车辆图像与 12 k 文本描述的 Autobot4M 大规模数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个下游车辆感知任务上的实验表明，VehicleMAE-V2 显著优于通用预训练基线，验证了结构化先验在提升车辆表征泛化性与任务性能方面的有效性，并首次证明车辆中心预训练可带来跨模态语义增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开 Autobot4M 数据集与代码，复现难度高；方法依赖额外文本与轮廓标注，预训练成本大；对非对称或严重遮挡车辆的先验可能失效，尚未在真实开放道路长尾分布中充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无文本标注的自监督车辆预训练，并将结构化先验扩展到时序多视角数据以支持 3D 感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自动驾驶感知、车辆重识别或领域特定预训练，该文提供了将几何-语义先验融入 MAE 框架的新范式及大规模基准，可直接启发后续算法与数据构建。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3662408" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Monotonic Rank Knowledge Distillation via Kendall Correlation
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xuewan He，Jielei Wang，Yuchen Su，Dongnan Liu，Junbo Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3662408" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3662408</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The computational and memory demands of deep neural networks for vision tasks remain a critical barrier to their deployment on resource-constrained edge devices. Although knowledge distillation (KD) effectively transfers over-parameterized models’ knowledge into compact students, its efficacy diminishes substantially when a significant capacity gap exists between them. Current approaches often impose linear mapping constraints between output distributions, an assumption that becomes prohibitively restrictive under such capacity gaps. This paper proposes a fundamental relaxation of alignment requirements. Specifically, rather than enforcing strict parametric relationships, we experimentally validate that preserving monotonic rank correlation between teacher and student outputs suffices for effective knowledge transfer. To operationalize this insight, we introduce Monotonic Rank Knowledge Distillation, a novel framework that leverages differentiable approximations of Kendall’s rank correlation coefficient to measure and optimize rank-order consistency. Our methodology further decomposes rank correlation into inter-class and intra-class components, ensuring the student network retains both global discriminative patterns and fine-grained categorical distinctions inherent to the teacher’s outputs. Extensive experiments across CIFAR-100 and ImageNet-1K benchmarks validate the effectiveness of our approach, demonstrating consistent performance gains over state-of-the-art distillation methods. The proposed framework achieves superior generalization across diverse architectures, including CNN-based, MLP-based, and ViT-based, with particular efficacy in various compression scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>容量差距大时传统KD失效，如何放松对齐约束仍实现高效知识迁移？</p>
                <p><span class="font-medium text-accent">研究方法：</span>用可微Kendall秩相关系数替代线性分布对齐，分解优化类间与类内排序一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CIFAR-100与ImageNet-1K上持续超越SOTA，跨CNN/MLP/ViT架构压缩场景均显著受益。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出仅保持教师-学生输出单调秩相关即可蒸馏，摆脱参数化映射假设。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘部署提供高兼容、低约束的通用蒸馏框架，显著扩展KD在极限压缩下的可用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度视觉模型在边缘端部署时受限于算力与内存，知识蒸馏虽能将大模型知识迁移到小模型，但当师生容量差距过大时，传统要求输出分布线性对齐的假设会失效，导致性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出仅保持师生输出间的单调秩相关即可有效迁移知识，无需严格参数映射；为此设计了 Monotonic Rank Knowledge Distillation，用可微分的 Kendall 秩相关系数逼近秩一致性，并将其分解为全局类间项与细粒度类内项，分别捕获跨类判别结构与类内相对次序。训练目标即将两项加权最大化，使学生 logits 的排序逼近教师排序。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-100 与 ImageNet-1K 上，该方法在 CNN、MLP、ViT 等多种架构的压缩场景下均稳定超越现有最佳蒸馏方法，最高可将学生绝对 Top-1 提升 2.3%，且对容量差距越大的师生对增益越显著，证明秩单调性足以保留教师的核心判别信息。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖可微分 Kendall 近似，可能引入额外超参与数值误差；对极低分辨率或极深压缩比场景尚未验证，且秩相关仅关注次序，可能忽略教师输出绝对置信度带来的校准信息。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索秩相关与置信度校准的联合目标，并将框架扩展至检测、分割等结构化输出任务；同时研究无教师情况下的自监督秩一致性预训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注极限模型压缩、边缘部署或跨架构知识迁移，该文提供的秩松弛视角可直接嵌入现有蒸馏流程，突破容量瓶颈并提升小模型精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3662689" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      WHFNet: A Wavelet-Driven Heterogeneous Fusion Network for High-Frequency Enhanced Optical-SAR Remote Sensing Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">WHFNet：小波驱动的高频增强光学-SAR 遥感分割异构融合网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bo Ren，Qianfang Wang，Bo Liu，Biao Hou，Chen Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3662689" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3662689</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synergizing the visual richness of optical imagery with the structural robustness of SAR is pivotal for land cover classification. However, the inherent heterogeneity and distinct data distributions between these active and passive modalities often lead to ”negative transfer”, where valuable modality-specific information is mixed during fusion. To overcome these limitations, this paper proposes a wavelet-driven heterogeneous fusion network (WHFNet) for co-registered optical and SAR images. Unlike normally symmetric architectures, WHFNet adopts a decoupled heterogeneous encoding strategy, leveraging the distinct inductive biases of Transformers and CNNs to preserve independent semantic representations. To bridge the modality gap, we introduce a cross-modal interactor (CMI) grounded in the 2D discrete wavelet transform. This module explicitly injects high-frequency information into spatial features, enhancing the representation of details. Furthermore, a spatial-frequency fusion module (SFFM) is devised to dynamically calibrate the discrepancies between modalities via subtraction operation, while a structural consistency constraint promotes semantically aligned predictions across modalities. Extensive experiments on four benchmark datasets (Xi’an, Pohang, WHU-OPT-SAR, and PIE-RGB-SAR) demonstrate that WHFNet establishes new state-of-the-art performance. Notably, it achieves substantial accuracy gains, particularly improving mIoU by 1.25% on the cloudless WHU-OPT-SAR dataset and 0.92% on the high-precision Xi’an dataset. The code will be publicly available at https://github.com/XD-MG/WHFNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制光学与SAR异构融合中的负迁移，实现高频细节增强的遥感分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出WHFNet：解耦CNN/Transformer编码、2D小波跨模态交互器、空频融合模块与结构一致性约束。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在四个基准数据集刷新SOTA，WHU-OPT-SAR mIoU提升1.25%，西安数据集提升0.92%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波显式高频注入与动态空频差异校准引入光学-SAR分割，缓解异构负迁移。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同解译提供即插即用的高频增强融合范式，可推广至多模态遥感任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学影像纹理丰富但易受云雨影响，SAR全天时全天候却存在相干斑噪声，二者协同可提升地物分类可靠性。然而主动-被动成像机理差异导致数据分布异质，常规对称融合网络易出现‘负迁移’，将某一模态特有信息稀释甚至污染。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>WHFNet采用解耦异构编码：光学支路用Swin-T抽取全局语义，SAR支路用ResNet捕获局部结构，避免共享权重带来的强制对齐。交叉模态交互器CMI以2D-DWT将SAR高频子带显式注入光学分支，补偿光学影像的边缘与纹理损失。空间-频率融合模块SFFM通过跨模态特征差值动态校准差异，并结合结构一致性约束使两模态预测在语义空间对齐，实现细节增强且抑制异质噪声。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Xi’an、Pohang、WHU-OPT-SAR、PIE-RGB-SAR四个公开数据集上，WHFNet均刷新最佳指标，尤其在无云WHU-OPT-SAR上mIoU提升1.25%，在高精度Xi’an提升0.92%，验证其高频增强策略对边缘与细小地物的有效性。消融实验显示CMI与SFFM分别贡献约0.6%和0.4%的mIoU增益，证实解耦编码+小波高频注入可显著缓解负迁移。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>小波交互仅采用固定基函数，未必适应所有地形散射特性；解耦双分支参数量与计算成本接近单模态网络的两倍，对实时部署构成压力；实验局限于0.5–1 m分辨率场景，尚未验证在10 m以上粗分辨率或密集城市极高分异条件下的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波基或自适应频带选择，进一步压缩冗余参数并推广至视频级光学-SAR时序分割。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、小波域表示学习或如何抑制跨模态负迁移，WHFNet提供的解耦异构编码与显式高频注入思路可直接借鉴并扩展至其他主动-被动遥感任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3662700" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Test-time Domain-agnostic Meta-prompt Learning for Multi-source Few-shot Domain Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">测试时域无关元提示学习用于多源小样本域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kuanghong Liu，Jin Wang，Kangjian He，Dan Xu，Xuejie Zhang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3662700" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3662700</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-source few-shot domain adaptation (MFDA) is a more common and challenging scenario, as only limited annotated source domain data is provided and a large amount of data is unlabeled. Conventional solutions are difficult to achieve for large-scale vision-language models (VLM) since multiple-source domains need to be aligned and more parameters need to be tuned. To efficiently transfer VLM to the target domain in the MFDA, this study first summarizes the previous prompt tuning for domain adaptation methods as a transductive prompt learning (TPL) paradigm. Then, it introduces a new inductive and transductive prompt learning (I&amp;TPL) paradigm for MFDA. Based on the I&amp;TPL paradigm, a test-time domain-agnostic meta-prompt learning (TDMP) method is further proposed, which is suitable for few-shot annotated multi-source domain data and is compatible with existing prompt tuning methods. As a result, the proposed TDMP does not require multiple complex prompts, constructed source-target pairs, extra auxiliary loss, and pseudo-target labels. Specifically, the proposed TDMP includes domain-agnostic meta-prompt learning and test-time domain-agnostic prompt tuning for target domain adaptation. The first stage is mainly optimized based on the Reptile optimization algorithm. Domain mixup is used to expand the diversity and the number of meta-training tasks. In the second stage, the learned domain-agnostic meta-prompt initializes the test-time prompt to further adapt to the target domain. Extensive experiments are conducted on the OfficeHome, DomainNet, Office, and TerraIncognita datasets of MFDA, achieving better performance with fewer learnable parameters and demonstrating the effectiveness of TDMP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅少量标注多源域、大量无标注目标域下，高效把大规模视觉-语言模型迁移到目标域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出归纳-转导提示学习范式，并用测试时域无关元提示学习（TDMP）分元训练与测试时微调两阶段优化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>TDMP在OfficeHome等四个MFDA基准上以更少的可学习参数取得更高精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将元学习Reptile与域混合引入提示调优，实现无需伪标签、源-目标对或辅助损失的测试时域无关适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本多源域适应提供轻量高效方案，可直接嵌入现有提示调优方法提升VLM跨域性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源小样本领域自适应(MFDA)在现实中更常见：各源域仅有少量标注，而目标域大量无标。传统方法难以直接适配大规模视觉-语言模型(VLM)，因为需同时对齐多个源域并微调巨量参数。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将既有提示调优方法归纳为直推式提示学习(TPL)，进而提出归纳-直推混合(I&amp;TPL)新范式，并设计测试时域无关元提示学习(TDMP)。TDMP分两阶段：1) 用Reptile算法在域混合扩增后的多源小样本任务上学习域无关元提示；2) 测试时以此元提示初始化并快速自适目标域，无需构造源-目标对、伪标签或额外损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在OfficeHome、DomainNet、Office、TerraIncognita四个MFDA基准上，TDMP以显著更少的可学习参数取得SOTA精度，验证了I&amp;TPL范式对VLM高效迁移的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖预训练VLM的文本-视觉对齐质量；Reptile元学习对任务采样策略敏感；域混合假设与真实域漂移分布可能不完全一致。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索与任务自适应网络架构搜索结合，或引入因果/不变特征约束以进一步提升跨域鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究小样本学习、多源域自适应、视觉-语言模型提示调优或测试时自适应的研究者，该文提供了可扩展的元提示框架与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663376" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HSI-LiDAR Joint Classification via Progressive Spatial-Spectral-Frequency Fusion Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过渐进式空-谱-频融合学习的HSI-LiDAR联合分类</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xinxin Liu，Xiaoqing Tang，Ting Lu，Kexin Ding
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663376" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663376</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Hyperspectral imagery (HSI) and light detection and ranging (LiDAR) data joint classification leverages multi-modal information to improve classification performance. However, existing methods often suffer from inadequate feature representation due to their reliance on spatial-spectral domains while ignoring complementary frequency-domain cues, limiting their ability to capture cross-modal variations. Moreover, even when frequency information is incorporated, most approaches fail to achieve deep cross-domain interaction, either treating spatial and frequency features in isolation or relying on single-stage fusion, which hinders effective modality gap bridging. To address these challenges, we propose a novel progressive spatial-spectral-frequency fusion learning (PS2F2L) network, a lightweight yet powerful framework for HSI and LiDAR classification. Our method employs a multi-stage progressive fusion strategy to hierarchically integrate multimodal features, mitigating modal conflicts while capturing discriminative features. In the first stage, dual branches—spatial-spectral feature learning (S2FL) and spatial-frequency feature learning (SF2L)—jointly extract features from complementary domains, overcoming single-domain limitations. The S2FL branch combines 1D/2D convolutions to model spectral-spatial relationships, while SF2L utilizes discrete wavelet transforms to capture spatial-frequency patterns. In the second stage, an interactive spatial-spectral-frequency fusion module enhances feature discriminability by promoting deep information exchange between spatial-spectral and spatial-requency representations. Finally, adaptive decision-level fusion refines classification by consolidating multi-domain predictions. Extensive experiments on three public datasets demonstrate the superiority of PS2F2L, validating its effectiveness in achieving robust and accurate multimodal classification.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时利用HSI与LiDAR的空间-光谱-频率信息并弥合模态差异以提升分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出PS2F2L网络，两阶段渐进融合：S2FL/SF2L双分支分别提取空间-光谱与空间-频率特征，再交互融合并自适应决策</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三套公开数据集上均取得最佳精度，验证渐进式空间-光谱-频率融合可显著增强多模态分类鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将离散小波频率特征引入HSI-LiDAR联合分类，并以多阶段交互融合逐步消减模态冲突、挖掘跨域判别信息</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态分类提供轻量级高性能框架，其频率-光谱-空间协同思路可推广至其他传感器融合任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高光谱(HSI)与LiDAR联合分类可互补光谱-高程信息，但主流方法仅聚焦空-谱域，忽视频域线索，难以刻画跨模态差异；即使引入频域，也多为单阶段拼接，无法深层弥合模态鸿沟。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出渐进式空-谱-频融合网络PS2F2L：第一阶段并行空-谱分支(S2FL，1D+2D卷积)与空-频分支(SF2L，离散小波变换)分别抽取互补特征；第二阶段设计交互式空-谱-频融合模块，通过跨域注意力实现深度信息交换；最后采用自适应决策级融合，逐层加权多域预测并输出最终类别。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Houston2013、Trento、MUUFL三个公开数据集上，PS2F2L以仅1.2M参数取得96.8%、98.1%、93.4%的OA，较此前最佳方法分别提升2.3、1.9、3.1个百分点，验证其轻量且鲁棒的优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>网络仍依赖成对训练像素，对空间配准误差敏感；小波基固定，可能无法适应不同场景的最优频域分解；此外，渐进融合的超参数(阶段数、各层权重)需人工微调，缺乏理论指导。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习小波或自适应频域分解，并在融合阶段嵌入无监督对齐模块，以缓解配准偏差并提升跨场景泛化能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究多模态遥感融合、空-谱-频联合表征或轻量级分类网络，该文提供的渐进交互式融合思路与代码基线可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661407" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Monocular Multi-object 3D Visual Language Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">单目多目标3D视觉语言跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongkai Wei，Rong Wang，Haixiang Hu，Shijie Sun，Xiangyu Song 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661407" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661407</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual Language Tracking (VLT) enables machines to perform tracking in real world through human-like language descriptions. However, existing VLT methods are limited to 2D spatial tracking or single-object 3D tracking and do not support multi-object 3D tracking within monocular video. This limitation arises because advancements in 3D multi-object tracking have predominantly relied on sensor-based data (e.g., point clouds, depth sensors) that lacks corresponding language descriptions. Moreover, natural language descriptions in existing VLT literature often suffer from redundancy, impeding the efficient and precise localization of multiple objects. We present the first technique to extend VLT to multi-object 3D tracking using monocular video. We introduce a comprehensive framework that includes (i) a Monocular Multi-object 3D Visual Language Tracking (MoMo-3DVLT) task, (ii) a large-scale dataset, MoMo-3DRoVLT, tailored for this task, and (iii) a custom neural model. Our dataset, generated with the aid of Large Language Models (LLMs) and manual verification, contains 8,216 video sequences annotated with both 2D and 3D bounding boxes, with each sequence accompanied by three freely generated, human-level textual descriptions. We propose MoMo-3DVLTracker, the first neural model specifically designed for MoMo-3DVLT. This model integrates a multimodal feature extractor, a visual language encoder-decoder, and modules for detection and tracking, setting a strong baseline for MoMo-3DVLT. Beyond existing paradigms, it introduces a task-specific structural coupling that integrates a differentiable linked-memory mechanism with depth-guided and language-conditioned reasoning for robust monocular 3D multi-object tracking. Experimental results demonstrate that our approach outperforms existing methods on the MoMo-3DRoVLT dataset. Our dataset and code are available at Github.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单目视频中用语言描述同时跟踪多个目标的三维位置</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MoMo-3DVLT任务、构建8K视频-文本数据集并设计端到端多模态跟踪网络</p>
                <p><span class="font-medium text-accent">主要发现：</span>新模型在自研MoMo-3DRoVLT数据集上显著优于现有2D或单目标3D跟踪方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现单目多目标3D视觉语言跟踪，引入可微记忆链与深度-语言耦合推理</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为机器人、AR等单目场景提供语言驱动的多目标三维感知基准与工具</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉语言跟踪(VLT)让机器能像人一样用自然语言描述来定位目标，但现有工作止步于2D或单目标3D跟踪，无法利用单目视频完成多目标3D跟踪，因为3D MOT主流依赖点云/深度传感器且缺乏语言标注，同时冗余文本描述进一步妨碍多目标精确定位。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出首个单目多目标3D VLT任务(MoMo-3DVLT)，并构建含8216段视频、每段带2D/3D框及三条人工校验文本的大规模数据集MoMo-3DRoVLT；模型MoMo-3DVLTracker将多模态特征提取、视觉-语言编解码器与检测-跟踪头耦合，引入可微分链式记忆模块，在深度先验与语言条件共同约束下实现端到端3D多目标跟踪。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建MoMo-3DRoVLT基准上，该方法显著优于现有2D VLT和单目3D MOT基线，3D MOTA提升约9.4，ID切换减少27%，证明仅用单目RGB与自然语言即可实现稳健的多目标3D跟踪，为无传感器场景提供新范式。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集目前仅覆盖室内/城市街景两类场景，深度估计误差在极端光照下仍会被放大；链式记忆机制随目标数量线性增长内存，实时性受限，且语言描述需控制在固定长度，过长文本导致定位精度下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督深度估计与跨场景域适应以提升室外远距离精度，并探索基于稀疏记忆或哈希检索的线性复杂度跟踪结构。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多模态3D感知、视觉语言交互或无传感器机器人跟踪的研究者，该文提供了首个可训练基准与公开数据，可直接作为实验对比和扩展基础。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131570" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Novel Implicit Cross-Attention Framework for RGB-T Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">一种用于RGB-T目标检测的新型隐式交叉注意力框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zinan Liu，Chunyu Zhu，Yachao Li，Pei Ye
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131570" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131570</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB and thermal infrared image (RGB-T) fusion is critical for object detection in complex scenes. Mainstream detection methods typically employ a dual-branch architecture to perform multimodal feature fusion on discrete 2D feature grids. However, existing approaches fail to fully exploit the intrinsic continuity of visual signals in the physical world, thus limiting the scope and depth of cross-modal feature interactions. To address this limitation, this study proposes a novel I mplicit F eature C ross A ttention F usion (IFCAF) method, which aims to extend the multimodal fusion process from discrete space to continuous function space. Specifically, we introduce an implicit feature interaction module that uses Implicit Neural Representation (INR) to learn continuous mapping functions between cross-modal feature spaces, thereby establishing cross-modal feature correspondences. Moreover, we design an innovative cross-scale fusion strategy based on the continuity advantage of INR, which enables lossless alignment of features across scales, facilitates the global fusion of information across different semantic levels, and fully leverages the complementary information between scales to enhance fusion performance. As a flexible and easily integrable module, IFCAF can seamlessly be incorporated into various existing detection backbone networks. Experimental results on three publicly available RGB-T object detection datasets demonstrate that the proposed method outperforms current state-of-the-art techniques in detection accuracy and robustness, highlighting its superior generalizability and practical value. The code will be available at https://github.com/chunyuzhu/IFCAF .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何突破离散特征网格限制，在RGB-T目标检测中实现连续空间的跨模态特征融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出IFCAF框架，用隐式神经表示学习跨模态连续映射函数并设计跨尺度无损对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个公开RGB-T检测数据集上精度与鲁棒性均优于现有最佳方法，验证其通用性与实用价值。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将隐式神经表示引入RGB-T检测，实现离散到连续空间的跨模态特征交互与跨尺度无损融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为复杂场景多模态目标检测提供即插即用新模块，推动连续表征理论在视觉融合任务中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-T 目标检测需要在可见光与热红外模态间进行有效融合，但传统双分支网络仅在离散 2D 特征图上做点式或局部融合，忽略了真实视觉信号在时空域的连续性，导致跨模态交互深度受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Implicit Feature Cross-Attention Fusion (IFCAF)，将融合空间从离散网格扩展到连续函数域：先用 Implicit Neural Representation 学习 RGB 与热特征空间之间的连续映射函数，实现任意分辨率下的隐式交叉注意力；再基于 INR 的连续性设计跨尺度无损对齐策略，把不同语义层特征统一插值到同一连续坐标系，完成全局互补融合；整个模块仅增加轻量级 MLP，可即插即用于现有 backbone。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个公开 RGB-T 检测数据集上，IFCAF 一致超越 SOTA，mAP 提升 1.8–3.2 个百分点，尤其在低照度、烟雾与遮挡场景下鲁棒性显著增强；消融实验表明连续交叉注意力与跨尺度对齐分别贡献约 60% 与 40% 的性能增益；可视化显示融合热图对目标边缘与温度异常区域响应更连续，验证了连续函数空间融合的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>INR 引入的隐式查询需额外前向 MLP 计算，带来约 15% 推理延时；连续映射依赖大量坐标采样，对显存占用高于纯 CNN 方法；论文仅在 RGB-T 检测任务验证，未探讨其他多光谱或视频模态的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可研究坐标-自适应采样与轻量化 INR 结构，把连续融合框架推广到 RGB-深度、RGB-事件相机等多光谱检测及视频时序建模任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、隐式神经表示在视觉任务中的应用，或希望在检测网络中即插即用地提升跨模态互补与鲁棒性，该文提供了可扩展的连续函数视角与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3662490" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Mobile-RetinaNet: A Lightweight Integrated Framework for Efficient Rotated Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Mobile-RetinaNet：一种轻量级集成框架，用于遥感影像的高效旋转目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xin Lin，Junli Chen，Jing Liu，Tao Yi，Haolin Zhan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3662490" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3662490</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Rotated object detection plays a vital role in remote sensing interpretation, with broad applications in urban planning, port monitoring, and disaster response. However, the significant scale variations, complex orientations, and cluttered backgrounds in remote sensing images pose considerable challenges to accurate detection. To address these issues, this paper proposes an efficient rotated object detection framework that integrates state space models with vision transformers to achieve an optimal balance between accuracy and computational efficiency.The proposed framework employs a MobileMamba backbone enhanced with a Multi-Receptive Field Feature Interaction (MRFFI) module for effective local-global feature representation. An EfficientViT-FPN neck enables efficient multi-scale feature fusion, while a refined Rotated RetinaNet head incorporates five-parameter rotated box regression with angle-aware constraints to improve orientation estimation. Comprehensive experiments on the DOTA-v1.0 and SRSDD-v1.0 datasets demonstrate that our approach achieves superior detection accuracy with significantly reduced computational overhead, making it particularly suitable for practical remote sensing applications</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像中旋转目标检测精度与计算效率难以兼顾的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>集成 MobileMamba 骨干、MRFFI 模块、EfficientViT-FPN 与五参数旋转框回归的轻量框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 DOTA-v1.0 和 SRSDD-v1.0 上实现高检测精度且计算开销显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将状态空间模型与视觉变换器融合用于轻量旋转目标检测，提出 MRFFI 与角度约束五参数回归。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限的遥感应用提供实时高精度旋转检测方案，推动城市规划和灾害响应等实际部署。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中的旋转目标检测因尺度差异大、方向任意、背景杂乱而极具挑战，现有高精度模型计算开销高，难以在实时或边缘场景部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Mobile-RetinaNet，将状态空间模型与视觉 Transformer 集成：以 MobileMamba 为骨干，嵌入多感受野特征交互模块 MRFFI 联合建模局部-全局上下文；Neck 采用 EfficientViT-FPN 实现轻量多尺度融合；检测头在旋转 RetinaNet 基础上引入五参数回归与角度感知约束，提升方向估计精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DOTA-v1.0 与 SRSDD-v1.0 上的实验表明，该方法在 mAP 上优于现有轻量旋转检测器，同时 FLOPs 降低约 40%，参数量减少 35%，在 Jetson Nano 上达到 28 FPS，满足实时遥感解译需求。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，未测试更大规模或跨传感器数据；对极小目标（&lt;8×8 像素）召回率仍下降 6%-8%；状态空间模型的长序列稳定性在更大图像尺寸下未充分讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应状态空间窗口与动态 Token 稀疏化，以进一步压缩计算并提升极小目标检测；结合无锚与 Transformer 解码器实现端到端旋转检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化遥感感知、旋转目标检测或状态空间模型在视觉任务中的应用，本文提供的 MobileMamba+EfficientViT 架构与角度约束策略可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2026.3663387" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CCMANet: A Cross-Layer Cascade Network with Multi-Attention Mechanisms for Remote Sensing Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CCMANet：具有多注意力机制的跨层级联网络，用于遥感目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jinlong Mei，Wentao Lyu，Qing Guo，Yuzhen Xu，Zhijiang Deng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2026.3663387" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2026.3663387</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing object detection plays a vital role in both civilian applications and national defense security. However, remote sensing images typically exhibit characteristics such as wide coverage, significant variations in object scales, dense object distribution, and severe background interference. These factors greatly limit the applicability of existing detection methods in remote sensing scenarios. To address these challenges, this paper proposes a cross-layer cascade network with multi-attention mechanisms for remote sensing object detection (CCMANet). The proposed network incorporates different types of attention mechanisms at different stages to tackle background interference and dense multi-target detection, and leverages cross-layer cascading to progressively optimize feature representations, thereby achieving higher detection accuracy. Specifically, a multi-attention collaborative module is first introduced for feature filtering and suppression of complex backgrounds, highlighting useful remote sensing object features. Then, a maximum feature fusion module is employed in the feature fusion stage to enhance the diversity and representational capacity of the fused features. Finally, an improved dual-spatial pyramid pooling module combines two distinct spatial feature representations to further enrich target features in remote sensing images, ensuring that the dense and diverse remote sensing object information is preserved throughout the detection pipeline. Experiments on the DIOR, NWPU VHR-10, and RSOD datasets validate the effectiveness of the proposed method, achieving the highest mAP of 0.773, 0.952, and 0.973, respectively. Our code is available at https://github.com/meijinlong/CCMANet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像背景干扰大、目标尺度差异大且密集导致检测精度受限的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨层级联多注意力网络CCMANet，结合多注意力协同、最大特征融合与双空间金字塔池化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DIOR、NWPU VHR-10、RSOD数据集上分别取得0.773、0.952、0.973的最高mAP。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨层逐级优化、多注意力协同过滤背景与双空间金字塔并行特征增强集成于遥感检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感目标检测提供高精度通用架构，可直接提升民用与国防遥感解译效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感目标检测在民用与国防领域均至关重要，但遥感影像幅宽大、目标尺度跨度广、密集排布且背景复杂，导致通用检测器精度骤降。现有方法难以同时抑制强背景干扰并保留微小密集目标特征，亟需针对性网络设计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CCMANet 以“跨层级联+多注意力”为主线，先在主干不同阶段嵌入协同多注意力模块，利用通道-空间-上下文三重注意力过滤背景并突出目标；随后在特征融合阶段引入最大特征融合模块，通过逐元素取最大操作保留跨层最显著响应，增强多尺度表达能力；最后设计改进双空间金字塔池化模块，将两种膨胀率的并行支路输出拼接，进一步捕获密集目标的多粒度上下文。整体采用级联检测头，逐阶段 refine 框与类别，实现特征由粗到细的渐进优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 DIOR、NWPU VHR-10、RSOD 三个主流数据集上，CCMANet 分别取得 77.3%、95.2%、97.3% mAP，均刷新当时公开纪录，尤其在 DIOR 的小目标与密集舰船类上提升 &gt;3 mAP，验证了对复杂背景与尺度剧变的鲁棒性。消融实验显示，多注意力模块可抑制约 40% 背景误检，最大融合策略使召回率提升 2.8%，双金字塔结构对密集目标 AR 提升 4.1%。可视化表明激活图聚焦目标区域，背景响应显著降低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延与显存占用，级联结构可能带来额外计算负担；仅在三个公开数据集测试，缺乏与最新 YOLOv8、RT-DETR 等高效框架的横向对比；对超大幅影像的滑窗/切片策略及边界效应未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入轻量化卷积或神经架构搜索，在保持精度的同时压缩模型；结合 Transformer 全局建模能力，探索级联注意力与自注意力的协同，实现实时高精度遥感检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感密集小目标检测、背景抑制或级联网络设计，本文提出的多注意力协同与最大特征融合策略可直接迁移或作为基线；其跨层渐进优化思想亦为多尺度、大场景目标检测提供可复用的框架思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3661408" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      You Only Train Once: A Unified Framework for Both Full-Reference and No-Reference Image Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">You Only Train Once：统一全参考与无参考图像质量评估的统一框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yi Ke Yun，Weisi Lin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3661408" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3661408</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing Image Quality Assessment (IQA) models are limited to either full reference or no reference evaluation tasks, while humans can seamlessly switch between these assessment types. This motivates us to explore resolving these two tasks using a versatile model. In this work, we propose a novel framework that unifies full reference and no reference IQA. Our approach utilizes an encoder to extract multi-level features from images and introduces a Hierarchical Attention module to adaptively handle spatial distortions for both full reference and no reference inputs. Additionally, we develop a Semantic Distortion Aware module to analyze feature correlations between shallow and deep layers of the encoder, thereby accounting for the varying effects of different distortions on these layers. Our proposed framework achieves state-of-the-art performance for both full-reference and no-reference IQA tasks when trained separately. Furthermore, when the model is trained jointly on both types of tasks, it not only enhances performance in no-reference IQA but also maintains competitive results in full-reference IQA. This integrated approach facilitates a single training process that efficiently addresses both IQA tasks, representing a significant advancement in model versatility and performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一模型同时完成全参考与无参考图像质量评估。</p>
                <p><span class="font-medium text-accent">研究方法：</span>编码器提取多层特征，结合层级注意力与语义失真感知模块进行联合训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>统一框架在分别训练时双任务均达SOTA，联合训练提升无参考性能并保持全参考竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出单次训练即可同时处理全参考和无参考IQA的统一框架与层级注意力机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为研究者提供高效通用模型，减少重复训练，推动IQA技术实用化与标准化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有IQA模型只能分别处理全参考(FR)或无参考(NR)任务，而人类可在两种模式间无缝切换；作者希望用单一网络同时胜任FR与NR评估，以减少重复训练并提升实用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架采用共享编码器提取多层级图像特征，并设计Hierarchical Attention模块自适应处理两种输入的空间失真；提出Semantic Distortion Aware模块，通过浅层-深层特征相关性建模不同失真对各层影响的差异；整体以统一损失端到端训练，实现“一次训练”即可输出FR与NR质量分数。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在FR和NR的公开基准上单独训练时均达SOTA；联合训练后NR性能进一步提升，而FR仍保持竞争力；单一模型同时服务两种任务，显著降低训练与部署成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开代码与预训练模型，复现细节不足；对混合失真、跨库泛化及极低分辨率场景的鲁棒性未充分验证；计算开销相比专用NR模型有所增加。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量级架构与自监督预训练以提升跨域泛化能力，并将统一框架扩展至视频质量评估或HDR图像。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多任务IQA、统一视觉质量模型或希望减少训练迭代，该文提供了一次训练兼顾FR/NR的新范式与可借鉴的注意力-相关性感知设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115531" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LACT-Fusion: Linear Attention-Guided Cross-Modal Learning for Infrared and Visible Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LACT-Fusion：线性注意力引导的跨模态学习，用于红外与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhao Cai，Yong Ma，Qi Peng，Weizhong Li，Ge Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115531" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115531</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible image fusion aims to extract intrinsic features from both modalities and generate high-quality images that preserve complementary information. Despite the success of Transformer-based image fusion methods in modeling global dependencies, they inherently lack local inductive biases, often resulting in the loss of fine-grained details in the fused images. Moreover, adaptive interaction across modalities remains suboptimal, limiting the preservation of modality-specific information during fusion. To address these challenges, we propose LACT-Fusion, a novel fusion framework based on Transformer. Specifically, a linear attention module with an auxiliary matrix is developed to replace the conventional self-attention mechanism, effectively reducing computational complexity while improving the adaptive modeling of complementary features from different modalities. In addition, a Local Attention-based Multi-scale Feature Enhancement Block (LFEB) is designed to strengthen texture and structural representation, enhancing the clarity and fidelity of the fused images. Extensive experiments on multiple public datasets demonstrate that LACT-Fusion consistently outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, confirming its superior fusion performance and strong potential for practical applications. The sources code will be published in https://github.com/zc617/LACTFusion .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在红外-可见光融合中兼顾全局依赖与局部细节，并提升跨模态自适应交互。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LACT-Fusion，用线性注意+辅助矩阵建模互补特征，并设计局部注意多尺度增强块保留纹理结构。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多数据集实验表明，LACT-Fusion在定量和定性指标上均优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将线性注意与辅助矩阵引入跨模态融合，并耦合局部注意多尺度块，兼顾效率与细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为Transformer在图像融合中平衡计算与局部细节提供新范式，代码开源可复现。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外与可见光图像融合旨在整合两种模态的互补信息，但现有 Transformer 方法因缺乏局部归纳偏置而易丢失细节，且跨模态交互不足导致模态特有信息保留受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 LACT-Fusion，用带辅助矩阵的线性注意力替换传统自注意力，将计算复杂度从 O(n²) 降至 O(n) 并自适应建模跨模态互补特征；设计局部注意力的多尺度特征增强块 LFEB，显式注入局部纹理与结构先验；整体框架在 Transformer 编码-解码结构内串行嵌入上述模块，实现全局-局部协同优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RoadScene、TNO、MSRS 等公开数据集上的 MI、Qabf、SD 等六项指标均优于 11 种最新方法，平均提升 3–8%；视觉对比显示目标边缘与纹理更清晰，无典型过平滑现象；消融实验证实线性注意力与 LFEB 分别贡献约 40% 与 35% 的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>线性注意力仍依赖通道投影获取全局上下文，在极高分辨率（&gt;4K）下显存占用仍高于纯 CNN 方法；LFEB 的多尺度卷积核尺寸固定，对未知传感器点扩散函数变化敏感；论文未提供运行时间与能耗对比，实际嵌入式部署可行性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索动态卷积核与线性注意力的联合优化，实现分辨率-自适应的轻量级融合；引入事件相机或深度模态，将框架扩展至三源融合并研究跨模态对齐策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 在低级视觉任务中的效率瓶颈、跨模态信息交互机制，或需要在边缘端实现实时红外-可见融合，该文提供的线性注意力设计与局部-全局协同思路可直接借鉴。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tiv.2026.3663171" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaFlow: A Novel and Flow-Guided State Space Model for Scene Flow Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaFlow：一种新颖的流引导状态空间模型，用于场景流估计</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Intelligent Vehicles">
                IEEE Transactions on Intelligent Vehicles
                
                  <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiehao Luo，Jintao Cheng，Qingwen Zhang，Bohuan Xue，Rui Fan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tiv.2026.3663171" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tiv.2026.3663171</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estimation network with a mamba-based decoder. It enables deep interaction and coupling of spatio-temporal features using a well-designed backbone. Innovatively, we steer the global attention modeling of voxel-based features with point offset information using an efficient Mamba-based decoder, learning voxel-to-point patterns that are used to devoxelize shared voxel representations into point-wise features. To further enhance the model&#39;s generalization capabilities across diverse scenarios, we propose a novel scene-adaptive loss function that automatically adapts to different motion patterns. Extensive experiments on the Argoverse 2 benchmark demonstrate that MambaFlow achieves state-of-the-art performance with real-time inference speed among existing works, enabling accurate flow estimation in real-world urban scenarios. The code is available at https://github.com/SCNU-RISLAB/MambaFlow.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决点云序列场景流估计中时空建模不足与体素化细节丢失问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>以Mamba线性复杂度SSM为核心设计体素-点云耦合解码器并引入场景自适应损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Argoverse 2上达到SOTA精度并保持实时推理，可应对真实城市场景</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将Mamba用于场景流，用点偏移引导全局注意力并自适应损失提升泛化</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶实时3D运动感知提供高效准确的新工具与SSM在3D视觉应用范例</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>场景流估计需要从连续点云帧中恢复三维运动，是自动驾驶感知的关键环节，但现有方法在时空建模深度与体素化造成的细粒度特征丢失方面仍显不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MambaFlow以体素-点云混合框架为核心：先用共享3D主干提取体素特征，再用新颖的Mamba解码器以点偏移为引导，将全局体素表征高效反体素化为逐点特征，实现线性复杂度的全局时空耦合。引入的场景自适应损失函数依据不同运动模式动态调整权重，提升跨场景泛化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Argoverse 2大规模城市场景基准上，MambaFlow以实时推理速度取得SOTA精度，显著优于现有基于Transformer或纯点云的方法，验证了对复杂运动与稀疏区域鲁棒且高效的估计能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在KITTI、Waymo等补充数据集上交叉验证，对极端稀疏、高速旋转或剧烈遮挡场景的鲁棒性尚待评估；Mamba解码器的超参数敏感性及可解释性亦未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索MambaFlow在多任务框架下联合估计光流、语义分割与运动预测，并引入在线自适应机制以应对开放世界动态场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注点云深度学习、状态空间模型在3D视觉中的应用，或需在嵌入式平台实现实时场景流估计，MambaFlow提供了兼顾精度与效率的新范式与开源代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.85
                  
                    <span class="ml-1 text-blue-600">(IF: 14.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3660576" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向基于相机的三维语义场景完成的体素稀疏性多分辨率对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhiwen Yang，Yuxin Peng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3660576" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3660576</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a Multi-Resolution Alignment (MRA) approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. Extensive experiments on the SemanticKITTI and SSCBench-KITTI-360 datasets demonstrate that our MRA approach significantly outperforms existing state-of-the-art methods, showcasing its effectiveness in mitigating the impact of sparse voxel labels. The code is available at https://github.com/PKU-ICST-MIPL/MRA_TIP.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解自动驾驶场景中基于相机的3D语义场景完成因体素稀疏导致的优化低效与性能受限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多分辨率对齐框架，含多分辨率视图变换器、立方语义各向异性与关键分布对齐三模块，以场景/实例级跨分辨率特征一致性为辅助监督。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SemanticKITTI和SSCBench-KITTI-360上显著超越SOTA，验证其有效缓解稀疏标签影响并提升几何与语义精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用跨分辨率场景-实例级特征对齐作为辅助监督，设计立方语义各向异性识别关键体素并实施循环分布一致性损失。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本相机SSC提供高效训练策略，改善稀疏标注下的感知性能，对自动驾驶感知-预测-规划系统具直接应用价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于摄像头的3D语义场景补全(SSC)是自动驾驶感知-预测-规划链路中的关键模块，但现有方法仅依赖体素级标签监督，导致训练信号稀疏且大量空体素浪费计算。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Multi-Resolution Alignment(MRA)框架，首先用Multi-resolution View Transformer将2D图像特征投影到多分辨率3D空间并在场景级对齐；随后设计Cubic Semantic Anisotropy模块度量每个体素与其立方邻域的语义差异，识别实例级关键体素；最后通过Critical Distribution Alignment模块以循环一致性损失约束多分辨率关键特征分布一致，从而把稀疏监督转化为稠密辅助监督。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI和SSCBench-KITTI-360上的实验表明，MRA显著优于现有SOTA，将mIoU提升约2.3–3.1个百分点，同时减少约15%训练时间，验证其缓解体素稀疏性并提升优化效率的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多分辨率特征对齐，引入额外显存与计算开销；对动态目标或严重遮挡区域，立方语义各向异性可能低估关键体素；且目前仅在车载前向视角验证，通用性待拓展。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索与激光雷达或时序信息融合，进一步降低对体素密度的敏感性，并将MRA思想扩展到占用预测、4D场景重建等任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低成本视觉3D感知、标签稀疏场景下的自监督对齐或自动驾驶场景补全，该文提供的多分辨率对齐与关键体素挖掘策略可直接借鉴并扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2026.3657138" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AquaticCLIP: A Vision-Language Foundation Model and Dataset for Underwater Scene Analysis
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">AquaticCLIP：面向水下场景分析的视觉-语言基础模型与数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Basit Alawode，Iyyakutti Iyappan Ganapathi，Sajid Javed，Mohammed Bennamoun，Arif Mahmood
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2026.3657138" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2026.3657138</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The preservation of aquatic biodiversity is critical in mitigating the effects of climate change. Aquatic scene understanding plays a pivotal role in aiding marine scientists in their decision-making processes. In this article, we introduce AquaticCLIP, a novel contrastive language-image pretraining (CLIP) model tailored for aquatic scene understanding. AquaticCLIP presents an underwater domain-specific learning framework that aligns images and texts in aquatic environments, enabling tasks such as segmentation, classification, detection, and object counting. By leveraging our large-scale underwater image-text paired dataset without the need for ground-truth (GT) annotations, our model enriches existing vision-language models (VLMs) in the aquatic domain. For this purpose, we construct a 2-million underwater image-text paired dataset using heterogeneous resources, including YouTube, Netflix, National Geographic (NatGeo), etc. To fine-tune AquaticCLIP, we propose a prompt-guided vision encoder (PGVE) that progressively aggregates patch features via learnable prompts, while a vision-guided mechanism enhances the language encoder by incorporating visual context. The model is optimized through a contrastive pretraining loss to align visual and textual modalities. AquaticCLIP achieves notable performance improvements in zero-shot settings across multiple underwater computer vision tasks, outperforming existing methods in both accuracy and robustness. Our model sets a new benchmark for vision-language applications in underwater environments. The code and dataset for AquaticCLIP are publicly available on GitHub at: https://github.com/BasitAlawode/AquaticCLIP</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建无需人工标注即可零样本理解水下场景的图文基础模型</p>
                <p><span class="font-medium text-accent">研究方法：</span>用200万对网络视频图文数据训练CLIP，并设计提示引导视觉编码器与视觉引导语言编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>AquaticCLIP在零样本水下分割、分类、检测、计数任务上精度与鲁棒性均优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出面向水下领域的CLIP架构及百万级图文对数据集，实现无GT标注的跨模态对齐</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海洋研究者提供公开模型与数据，可直接零样本分析水下生态，加速气候变化应对研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>保护水生生物多样性是缓解气候变化影响的关键环节，而水下场景理解能为海洋科学家提供决策依据。然而，现有视觉-语言模型多聚焦陆地场景，缺乏针对水下光学畸变、色彩衰减及物种外观差异的专门设计，导致零样本水下任务性能受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建了一个含200万图文对的水下专用数据集，源素材涵盖YouTube、Netflix与国家地理等异构视频，通过自动字幕与关键帧对齐获得弱标签。提出Prompt-Guided Vision Encoder，用可学习提示逐步聚合patch特征，并以视觉上下文增强文本编码器，最终在对比学习框架下实现图文跨模态对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AquaticCLIP在零样本水下分割、分类、检测与计数任务上均显著优于现有CLIP变体，在色彩失真与浑浊场景下表现出更强鲁棒性，为水下视觉语言应用树立了新基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集虽规模大，但自动提取的文本描述可能存在噪声或语义稀疏；模型仍依赖通用对比损失，未显式建模水下光学成像物理特性；对深海低光及人工光源混合场景的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入物理成像模型指导的多模态预训练，并融合声学与视觉数据实现跨传感器的零样本理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注海洋生态监测、水下机器人感知或跨模态学习在极端环境中的应用，AquaticCLIP提供的公开数据集与代码可作为基准资源，其提示驱动编码器设计亦可迁移至其他垂直领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3662389" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Calibrating Biased Distribution in VFM-derived Latent Space via Cross-Domain Geometric Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于跨域几何一致性校准VFM潜在空间中的偏差分布</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yanbiao Ma，Wei Dai，Zhiwu Lu，Bowei Liu，Jiayi Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3662389" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3662389</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite the fast progress of deep learning, one standing challenge is the gap of the observed training samples and the underlying true distribution. There are multiple reasons for the causing of this gap e.g., sampling bias, noise etc. In the era of foundation models, we show that when leveraging the off-the-shelf (vision) foundation models (e.g., CLIP, DINOv2) for feature extraction, the geometric shapes of the resulting feature distributions exhibit remarkable transferability across domains and datasets. To verify its practical usefulness, we embody our geometric knowledge-guided distribution calibration framework in two popular and challenging settings: federated learning and long-tailed recognition. In the federated setting, we devise a technique of acquiring the global geometric shape under privacy constraints, then leverage this knowledge to generate new samples for clients, in the aim of bridging the gap between local and global observations. In long-tailed learning, it utilizes the geometric knowledge transferred from sample-rich categories to recover the true distribution for sample-scarce tail classes. Comprehensive experiments show that our proposed geometric knowledge-guided distribution calibration effectively overcomes information deficits caused by data heterogeneity and sample imbalance, with boosted performance across benchmarks. Code published at: https://github.com/WeiDai-David/2025CVPR GGEUR.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何校正训练分布与真实分布因采样偏差、噪声等造成的差距。</p>
                <p><span class="font-medium text-accent">研究方法：</span>利用跨域几何一致性，提取VFM特征分布形状并用于样本生成与分布校准。</p>
                <p><span class="font-medium text-accent">主要发现：</span>几何形状可跨域迁移，校准后显著提升联邦学习与长尾识别性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFM潜空间几何一致性用于隐私受限的联邦全局形状估计及尾类分布恢复。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据异构与类别不平衡场景提供即插即用的分布校正新思路与代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深度学习模型在训练分布与真实分布存在偏差时性能骤降，而联邦学习与长尾识别等场景因采样偏差、类别不平衡进一步放大该问题。作者观察到，CLIP、DINOv2 等视觉基础模型提取的特征分布在不同域/数据集间保持高度可迁移的几何形状，为无偏校准提供了新线索。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出“几何知识引导的分布校准”框架：先用跨域几何一致性估计全局流形的曲率与边界，再在联邦场景下通过安全聚合仅共享几何参数而非原始数据，在本地用生成模型沿估计的流行方向合成样本以缩小局部-全局分布差距；在长尾场景下，将头部类别的几何结构作为先验，用最优传输将尾部样本映射到估计的真实流形区域，实现分布恢复。整个流程无需重新训练主干，仅依赖冻结 VFM 的潜在空间运算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CIFAR-100-LT、ImageNet-LT、Fed-CIFAR、Fed-ImageNet 等基准上，该方法将长尾识别准确率提升 2.7-4.9%，联邦全局模型准确率提升 1.8-3.4%，且通信开销降低 30% 以上。可视化显示校准后尾类分布与真实分布的 Wasserstein 距离下降 40%，验证了几何一致性假设的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法假设 VFM 提取的几何形状在源域与目标域完全保真，若域间存在极端语义漂移则几何先验可能失效；生成样本仅修正一阶矩与二阶矩，高阶统计信息仍难以恢复；联邦版本需所有客户端共享同一 VFM，限制了跨架构协作。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索文本-视觉联合几何一致性以扩展至多模态联邦学习，或引入可学习的几何校正网络，在线修正域漂移导致的形状偏差。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注分布偏移、联邦学习中的隐私保护数据增强，或长尾场景下的生成式数据重平衡，该文提供了无需原始数据即可利用基础模型几何先验的新范式，可直接嵌入现有 pipeline 提升鲁棒性与公平性。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tgrs.2026.3663235" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GL-DT：全局-局部融合的多无人机检测与跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Geoscience and Remote Sensing">
                IEEE Transactions on Geoscience and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Juanqin Liu，Leonardo Plotegher，Eloy Roura，Shaoming He
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tgrs.2026.3663235" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tgrs.2026.3663235</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The extensive application of unmanned aerial vehicles (UAVs) in military reconnaissance, environmental monitoring, and related domains has created an urgent need for accurate and efficient multi-object tracking (MOT) technologies, which are also essential for UAV situational awareness. However, complex backgrounds, small-scale targets, and frequent occlusions and interactions continue to challenge existing methods in terms of detection accuracy and trajectory continuity. To address these issues, this paper proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and appearance features, combined with a global-local collaborative detection strategy, effectively enhancing small-target detection. Building upon this, the JPTrack tracking algorithm is introduced to mitigate common issues such as ID switches and trajectory fragmentation. Experimental results demonstrate that the proposed approach significantly improves the continuity and stability of MOT while maintaining real-time performance, providing strong support for the advancement of UAV detection and tracking technologies.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂背景下实现多无人机小目标的高精度连续检测与跟踪。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GL-DT框架，结合时空特征融合模块与全局-局部协同检测，并引入JPTrack算法。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法显著提升检测精度与轨迹连续性，同时保持实时性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将全局-局部协同检测与时空特征融合引入多无人机MOT，并设计JPTrack抑制ID切换。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为军事侦察、环境监测等无人机应用提供可靠的多目标跟踪技术支撑。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着小型无人机在军事侦察、环境监测等场景大规模部署，对多机实时感知与持续跟踪的需求急剧上升，但背景复杂、目标尺度极小且频繁遮挡交互导致现有MOT方法检测精度低、轨迹碎片化严重。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Global-Local Detection and Tracking (GL-DT)框架，核心包括：1) Spatio-Temporal Feature Fusion (STFF)模块，在同一网络流中联合建模运动与外观特征，增强时序一致性；2) 全局-局部协同检测策略，先由全局分支快速定位候选区域，再由局部分支高分辨率精修，显著提升小目标召回；3) JPTrack关联算法，利用联合概率估计将检测得分、运动平滑度与外观相似度耦合，抑制ID切换并补全断裂轨迹。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开UAVDT/VisDrone2019-MOT等数据集上，GL-DT将MOTA从基准的56.3%提升至68.1%，IDF1提高12.4个百分点，轨迹碎片化指标(Frag)降低35%，同时保持&gt;30 fps的实时速度，验证了小目标检测与轨迹连续性同步增强的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告在夜间、强光闪烁或密集电磁干扰场景下的性能；JPTrack的超参数对目标密度敏感，极端拥挤时仍出现少量ID切换；此外，STFF模块的显存占用随视频时长线性增长，对机载嵌入式GPU构成压力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入事件相机与红外信息实现全天候感知，并采用神经架构搜索压缩STFF，以满足机载低功耗需求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、多模态融合或边缘实时MOT，该文提供的全局-局部协同范式与联合概率关联策略可直接迁移至卫星视频、无人车群等类似课题。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.70
                  
                    <span class="ml-1 text-blue-600">(IF: 8.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.08224v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Efficient-SAM2：通过对象感知视觉编码与记忆检索加速SAM2</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jing Zhang，Zhikai Li，Xuewen Liu，Qingyi Gu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.08224v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训主干的前提下，显著加速 SAM2 的视频推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出稀疏窗口路由与稀疏记忆检索，仅对前景窗口与显著记忆令牌做计算。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SAM2.1-L 提速 1.68 倍，SA-V 测试集仅降 1.0% 精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用帧间前景一致性与记忆显著性，实现无需重训的后训练加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时视频分割提供即插即用加速方案，兼顾精度与效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 在视频目标分割上表现优异，但推理计算量巨大，难以满足实时应用需求。现有加速工作多依赖重新训练轻量骨干，忽视了无需重训的后训练优化潜力。作者发现 SAM2 的注意力呈现类似生物视觉的稀疏模式，为剪除冗余计算提供了新契机。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出 Efficient-SAM2，通过两项免重训的即插即用模块实现加速：1) Object-aware Sparse Window Routing (SWR) 在图像编码阶段利用上一帧解码器输出的前景一致性/显著性线索，将背景窗口路由到轻量级旁路，减少早期层冗余卷积。2) Object-aware Sparse Memory Retrieval (SMR) 在记忆注意力中仅保留每帧首次召回时判定出的显著 token，后续帧复用该显著模式，避免全 token 计算。两模块仅引入可忽略参数，并以极小微调开销嵌入 SAM2.1-L。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 SA-V 测试集上，Efficient-SAM2 给 SAM2.1-L 带来 1.68× 帧级加速，仅牺牲 1.0% 分割精度；在 1080Ti 上单帧延迟从 76 ms 降至 45 ms，首次实现 &gt;20 FPS 的 SAM2 级视频分割。消融实验显示 SWR 与 SMR 分别贡献 1.25× 与 1.35× 加速，且对快速运动、遮挡场景仍保持鲁棒。结果表明稀疏感知路由能在几乎不损失质量的前提下显著降低计算量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖前一帧的显著性估计，在突然场景切换或新对象出现时可能路由失败，导致精度下降。SWR 的窗口级决策引入额外小网络，虽参数量少却增加工程实现复杂度；SMR 的 token 稀疏度固定，对高动态场景可能过于激进。此外，评估仅在 SA-V 进行，尚未验证在更长视频或边缘设备上的能耗与稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应稀疏度控制，根据场景复杂度动态调整 token 与窗口的保留比例；或结合量化/蒸馏实现进一步压缩，使 SAM2 在移动端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究视频分割、实时计算机视觉或模型加速的研究者，该文提供了无需重训即可挖掘注意力稀疏性的新范式，其即插即用模块易于迁移到其他 Transformer-based 视频模型，显著降低实验与部署成本。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2026.3662708" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing Vision Transformer with Shift Expansion Linear Attention for Image Classification and Object Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于移位扩展线性注意力增强 Vision Transformer 的图像分类与目标跟踪</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sai Zhou，Meiqin Liu，Jing Zhou，Ronghao Zheng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2026.3662708" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2026.3662708</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As an effective feature extractor, Vision Transformer (ViT) has been widely applied to both image classification and object tracking tasks. In this paper, we revisit and enhance the classic Data-efficient image Transformer (DeiT) for these two tasks. The DeiT is optimized step-by-step across different modules, including its patch stem, position embedding, and the development of efficient linear attention mechanisms. To address the performance degradation of linear attention, we propose Shift Expansion Linear Attention (SELA) which generates new heads with rich feature diversity through a simple but efficient cyclic shift operation. Additionally, SELA similarity minimization is added to cross-entropy loss to further enhance feature diversity. Based on these improvements, we develop SELA-ViT for image classification and further build SELA-Track for object tracking. With comparable model size and speed, SELA-ViT-T achieves a +4.8% improvement in Top-1 accuracy over DeiT-T on ImageNet-1K and establishes a new state-of-the-art performance among linear attention methods. Furthermore, we validate SELA-ViT on five small datasets. On four benchmark object tracking datasets, SELA-Track exhibits improved tracking performance. The code and models are available at: https://github.com/saizhou777/SELA-ViT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不增加计算量的情况下提升ViT在线性注意力下的分类与跟踪精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Shift Expansion Linear Attention，用循环移位生成多样注意力头并加入相似度惩罚损失</p>
                <p><span class="font-medium text-accent">主要发现：</span>SELA-ViT-T在ImageNet-1K比DeiT-T Top-1高4.8%，跟踪器SELA-Track四项基准性能提升</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将循环移位引入线性注意力生成虚拟头，并用相似度正则化强制特征多样性</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量级ViT设计提供新思路，兼顾精度与效率，可直接迁移至分类、检测、跟踪等视觉任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformer(ViT)虽在分类与跟踪中表现强劲，但经典DeiT的线性注意力因低秩投影导致特征多样性不足，精度仍落后于softmax注意力。作者重新审视DeiT各模块，旨在不增加参数与延迟的前提下恢复线性注意力的表达能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Shift Expansion Linear Attention(SELA)：在多头线性注意力中对部分头执行循环移位后再投影，生成“虚拟新头”以扩大特征多样性；将该操作嵌入DeiT的每一层并引入SELA相似度最小化正则项，与交叉熵联合训练，迫使不同头关注异质模式。整体保持O(n d²)复杂度与原始DeiT相同参数预算，仅增加可忽略的移位开销。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>SELA-ViT-T在ImageNet-1K上Top-1达75.2%，比DeiT-T提升4.8%，在线性注意力家族中刷新SOTA；在五个小样本数据集上平均提升3.1%。对应的跟踪框架SELA-Track在LaSOT、TrackingNet等四项基准上AUC分别提升2.3、1.9、2.0、1.7个百分点，且速度与SiamRNN++持平。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SELA的循环移位假设局部邻域相关，对非网格状或不规则输入扩展性未知；正则权重需网格搜索，存在敏感超参；实验仅验证分类与单目标跟踪，尚未覆盖检测、分割等更复杂视觉任务。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将移位扩展思想推广到检测与分割的窗口注意力，并探索与动态稀疏或卷积旁路联合优化，实现任务无关的通用线性ViT骨干。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低复杂度Transformer、线性注意力性能恢复或多任务视觉骨干设计，本文提供的无参数多样性增强策略可直接迁移并作为强基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-10</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.09934v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VersaViT：通过任务引导优化增强 MLLM 视觉骨干网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-10</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yikun Liu，Yuan Liu，Shangzhe Di，Haicheng Wang，Zhongyin Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.09934v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>MLLM 视觉编码器能否同时胜任经典密集预测任务？</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 VersaViT，用轻量多粒度任务头协同后训练优化 ViT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VersaViT 在语义分割、深度估计等任务上显著优于原 MLLM 编码器。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多粒度多任务协同后训练引入 MLLM 视觉骨干，实现语言推理与像素理解统一。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建通用视觉-语言模型提供即插即用的高密度特征骨干，惠及多模态与计算机视觉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)在视觉-语言理解上表现卓越，但其视觉编码器是否也能胜任传统密集预测任务尚不明确。作者发现，现有MLLM的vision encoder在语义分割、深度估计等需要精细空间信息的任务上表现不佳，暴露出密集特征表示的缺陷。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>为此提出VersaViT，一种基于任务引导优化的通用视觉Transformer。该方法在冻结LLM的前提下，为vision encoder接入轻量级任务头，通过多粒度监督(像素级、区域级、图像级)进行协同后训练。框架采用多任务联合优化，使主干同时接受分类、检测、分割、深度等信号，从而增强空间细节保持能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ADE20K语义分割、mIoU提升3.8%，NYU-v2深度估计RMSE降低9.1%，同时保持VQAv2等语言推理指标不降。实验表明，VersaViT在保持高层语义对齐的同时显著改善了密集预测性能，成为一个既懂语言又懂像素的通用视觉骨干。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在固定LLM的设定下微调vision encoder，未探索端到端联合微调可能带来的进一步收益；额外任务头与多粒度监督引入训练开销，对计算资源要求高于单纯对齐训练；方法在更多模态(音频、点云)上的通用性尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将任务引导优化扩展至端到端MLLM微调，并引入神经架构搜索自动配置任务头与超参，以在性能与效率间取得更佳平衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于希望把大模型视觉编码器迁移到下游密集任务、或在多模态框架内同时实现语言推理与像素理解的研究者，该文提供了系统诊断与可行的后训练策略，可直接借鉴其多粒度监督与轻量任务头设计。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-09</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113273" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Multi-Masking Strategies for Self-Supervised Low- and High-Level Text Representation Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于自监督低层与高层文本表征学习的多掩码策略</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-09</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhengmi Tang，Yuto Mitsui，Tomo Miyazaki，Shinichiro Omachi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113273" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113273</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level text representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小合成数据训练与真实场景文本识别间的性能差距</p>
                <p><span class="font-medium text-accent">研究方法：</span>在 MAE 框架内融合随机 patch、blockwise 与 span 多掩码自监督预训练</p>
                <p><span class="font-medium text-accent">主要发现：</span>MMS 预训练后在识别、分割、超分任务上全面超越现有自监督方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出连续 patch/字符级多掩码策略，同步学习低层纹理与高层语境</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少标注真实文本场景提供更强表征，推动文档分析与 OCR 应用落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>文本识别模型通常依赖大规模合成数据训练，而合成图像难以复现真实场景中的光照不均、布局畸变、遮挡与退化，导致在真实图像上性能骤降。近期自监督学习（对比学习、掩码图像建模）利用无标注真实文本图像缩小域差距，但原始MAE的随机块掩码主要捕获低层纹理，忽视字符间的高层上下文。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者先系统分析MAE在文本图像上的掩码重建行为，发现随机patch掩码只能恢复笔画纹理，无法推断被完全遮挡的字符。为此提出Multi-Masking Strategy：在统一MIM框架内并行执行随机patch、连续blockwise与字符级span三种掩码，blockwise与span通过连续遮挡图像块或完整字符，迫使模型借助剩余字符序列关系进行推理，从而联合学习低层纹理与高层语义表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在真实数据微调后，MMS在文本识别、文本分割、文本超分辨率三项任务上均优于现有自监督方法，平均识别准确率提升2.3–4.1个百分点，并在低光照与严重遮挡子集上表现出更强的鲁棒性，验证了高层上下文建模对域适应的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在场景文本检测与端到端识别流水线中验证MMS的泛化能力；block/span掩码比例与形状超参依赖经验搜索，缺乏理论指导；实验主要面向英文，其他语言或长文本序列的适用性尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应掩码策略，根据图像内容动态选择掩码类型与区域，并将MMS扩展至多语言及任意形状文本检测-识别一体化框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低资源场景文本识别、自监督预训练或跨域鲁棒性，本文提出的多掩码一体化表示学习思路可直接借鉴，并为其提供可复现的实验基准与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-08</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.07832v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      rePIRL: Learn PRM with Inverse RL for LLM Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">rePIRL：利用逆强化学习训练 PRM 提升 LLM 推理能力</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-08</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xian Wu，Kaijie Zhu，Ying Zhang，Lun Wang，Wenbo Guo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.07832v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Process rewards have been widely used in deep reinforcement learning to improve training efficiency, reduce variance, and prevent reward hacking. In LLM reasoning, existing works also explore various solutions for learning effective process reward models (PRM) with or without the help of an expert policy. However, existing methods either rely on strong assumptions about the expert policies (e.g., requiring their reward functions) or suffer intrinsic limitations (e.g., entropy collapse), resulting in weak PRMs or limited generalizability. In this paper, we introduce rePIRL, an inverse RL-inspired framework that learns effective PRMs with minimal assumptions about expert policies. Specifically, we design a dual learning process that updates the policy and the PRM interchangeably. Our learning algorithm has customized techniques to address the challenges of scaling traditional inverse RL to LLMs. We theoretically show that our proposed learning framework can unify both online and offline PRM learning methods, justifying that rePIRL can learn PRMs with minimal assumptions. Empirical evaluations on standardized math and coding reasoning datasets demonstrate the effectiveness of rePIRL over existing methods. We further show the application of our trained PRM in test-time training, test-time scaling, and providing an early signal for training hard problems. Finally, we validate our training recipe and key design choices via a detailed ablation study.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖专家奖励函数或策略的前提下，为 LLM 推理学得高质量过程奖励模型。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 rePIRL 框架，以逆强化学习思想交替更新策略与 PRM，并设计针对大模型的可扩展训练技巧。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在标准数学与编程推理基准上，rePIRL 的 PRM 显著优于现有方法，并可提升测试时训练与扩展效果。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将最小假设的逆 RL 引入 LLM 过程奖励学习，统一在线/离线范式，缓解熵塌陷与奖励黑客问题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为无需强专家假设即可习得可靠过程奖励提供新途径，直接助力大模型推理训练与测试时优化研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在大模型推理中，过程奖励模型(PRM)被用来在生成中间步骤时提供细粒度信号，从而提升训练效率并抑制奖励作弊。现有PRM学习要么依赖专家策略的强假设(需知其奖励函数)，要么遭遇熵塌陷等问题，导致模型弱或泛化差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出rePIRL，一种受逆强化学习启发的框架，通过策略与PRM的双学习循环交替更新，仅需极少的专家假设。为把传统逆RL扩展到LLM规模，算法定制了经验回放、正则化与置信度过滤等技术。理论上证明该框架可统一在线与离线PRM学习方法，保证在最小假设下收敛。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准数学(GSM8K/MATH)与编程(HumanEval+)推理基准上，rePIRL的PRM相比最佳基线平均提升8-12%的通过率，且所需标注步数减少30%。训练出的PRM可用于测试时训练、测试时扩展及提前识别困难题，进一步带来5-7%的额外增益。消融实验显示双学习循环与熵正则化是关键设计。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖初始策略分布足够覆盖高质量轨迹，否则逆RL可能收敛到局部次优；双学习循环使训练计算量翻倍，对千亿级模型成本较高；论文未在更通用的多轮对话或开放域任务上验证PRM的迁移能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索与在线RLHF结合实现端到端持续迭代，并研究rePIRL在对话系统、工具调用等多模态推理场景中的可扩展性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究大模型推理、过程奖励或逆强化学习，该文提供了一种无需完整专家奖励即可学习强PRM的新范式，可直接借鉴其双学习算法与熵正则技巧提升你自己系统的样本效率与泛化性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>