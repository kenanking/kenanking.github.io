<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-09</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-09 11:37 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">976</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、视觉定位与模型压缩，同时对自监督/对比学习等表征学习方法保持跟踪。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测（35篇）与视觉定位（22篇）上收藏量显著领先，且持续追踪Kaiming He、Ross Girshick等权威团队工作，体现出对通用目标检测框架及轻量化部署的深入积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读同时覆盖CVPR、NeurIPS等顶会与IEEE TGARS、《雷达学报》等遥感期刊，显示其将主流视觉算法迁移至SAR/遥感影像的跨学科兴趣。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1收藏量陡增至102篇后回落，新增关键词出现“基础设施感知效率”“条件记忆”，表明正由传统检测转向面向大模型推理效率与知识检索的应用研究。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>可继续跟进视觉-语言模型在遥感解释中的高效微调与知识蒸馏，以及面向SAR图像的旋转目标检测基础模型构建。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 950/950 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-09 11:26 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸对齐', '车牌识别', 'GNSS导航'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 13, 7, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 10 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 10 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 84,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u5f81",
            size: 72,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "SAR\u8230\u8239\u68c0\u6d4b",
            size: 56,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 3,
            label: "\u591a\u4f20\u611f\u5668BEV\u878d\u5408",
            size: 51,
            keywords: ["SIFT", "ToF\u4f20\u611f\u5668", "\u6df1\u5ea6\u4f30\u8ba1"]
          },
          
          {
            id: 4,
            label: "Vision Transformer\u67b6\u6784",
            size: 51,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 5,
            label: "\u8f7b\u91cf\u7ea7CNN\u8bbe\u8ba1",
            size: 48,
            keywords: ["\u91cd\u53c2\u6570\u5316", "VGG", "\u6b8b\u5dee\u8fde\u63a5"]
          },
          
          {
            id: 6,
            label: "SAR\u4eff\u771f\u4e0e\u8bc6\u522b",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 7,
            label: "\u6df7\u5408\u4e13\u5bb6LLM",
            size: 45,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek", "\u6df7\u5408\u4e13\u5bb6\u6a21\u578b"]
          },
          
          {
            id: 8,
            label: "LLM\u63d0\u793a\u4e0e\u6307\u4ee4",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "\u7814\u7a76"]
          },
          
          {
            id: 9,
            label: "\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 38,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u9065\u611f\u76ee\u6807\u68c0\u6d4b", "Feature extraction"]
          },
          
          {
            id: 10,
            label: "\u6df1\u5ea6\u7f51\u7edc\u4f18\u5316",
            size: 36,
            keywords: ["\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3", "\u5927\u6279\u91cf\u8bad\u7ec3"]
          },
          
          {
            id: 11,
            label: "\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["Transformers", "HRNet", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "SAR\u57fa\u7840\u6a21\u578b",
            size: 33,
            keywords: ["\u57df\u81ea\u9002\u5e94", "SAR\u76ee\u6807\u8bc6\u522b", "\u81ea\u76d1\u7763\u5b66\u4e60"]
          },
          
          {
            id: 13,
            label: "\u96f7\u8fbe\u667a\u80fd\u611f\u77e5",
            size: 29,
            keywords: ["\u4eba\u5de5\u667a\u80fd", "\u6a21\u5f0f\u8bc6\u522b", "\u81ea\u52a8\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u751f\u6210\u5f0f\u6d41\u6a21\u578b",
            size: 28,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "\u5206\u5e03\u5916\u6cdb\u5316"]
          },
          
          {
            id: 15,
            label: "\u8f66\u724c\u8bc6\u522b\u7cfb\u7edf",
            size: 27,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 16,
            label: "\u6d77\u9762\u76ee\u6807\u68c0\u6d4b",
            size: 27,
            keywords: ["\u6052\u865a\u8b66\u7387\u68c0\u6d4b", "\u76ee\u6807\u68c0\u6d4b", "\u6df1\u5ea6\u5b66\u4e60"]
          },
          
          {
            id: 17,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316",
            size: 25,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 18,
            label: "CNN\u53ef\u89e3\u91ca\u6027",
            size: 24,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "Grad-CAM"]
          },
          
          {
            id: 19,
            label: "\u590d\u6742\u80cc\u666f\u7ea2\u5916\u68c0\u6d4b",
            size: 23,
            keywords: ["\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b"]
          },
          
          {
            id: 20,
            label: "\u79fb\u52a8\u7aef\u8f7b\u91cf\u89c6\u89c9",
            size: 19,
            keywords: ["HRNet", "\u7ebf\u6bb5\u68c0\u6d4b", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 21,
            label: "SAR\u6210\u50cf\u7b97\u6cd5",
            size: 16,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 22,
            label: "\u6269\u6563\u56fe\u50cf\u751f\u6210",
            size: 16,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 23,
            label: "\u56fe\u50cf\u7ffb\u8bd1\u4e0e\u751f\u6210",
            size: 15,
            keywords: ["\u6269\u6563\u6a21\u578b", "StepFun", "\u56fe\u50cf\u7ffb\u8bd1"]
          },
          
          {
            id: 24,
            label: "GAN\u4e0eVAE\u751f\u6210",
            size: 14,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u751f\u6210\u6a21\u578b", "\u8bad\u7ec3\u7a33\u5b9a\u6027"]
          },
          
          {
            id: 25,
            label: "SAR\u91cf\u5316\u5f71\u54cd",
            size: 12,
            keywords: []
          },
          
          {
            id: 26,
            label: "TinyML\u7cfb\u7edf\u4f18\u5316",
            size: 12,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "\u7cfb\u7edf\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u6269\u6563\u6a21\u578b\u539f\u7406",
            size: 11,
            keywords: ["\u5355\u6b65\u6269\u6563\u6a21\u578b", "\u6761\u4ef6\u751f\u6210", "\u751f\u6210\u5f0f\u5efa\u6a21"]
          },
          
          {
            id: 28,
            label: "\u4fe1\u53f7\u68c0\u6d4b\u7406\u8bba",
            size: 11,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316"]
          },
          
          {
            id: 29,
            label: "\u4f18\u5316\u7b97\u6cd5\u57fa\u7840",
            size: 4,
            keywords: ["\u5206\u914d\u95ee\u9898", "\u5308\u7259\u5229\u7b97\u6cd5", "\u7ec4\u5408\u4f18\u5316"]
          }
          
        ];

        const links = [{"source": 6, "target": 12, "value": 0.9483078658110562}, {"source": 24, "target": 27, "value": 0.9036317652746654}, {"source": 6, "target": 21, "value": 0.8977261723439519}, {"source": 22, "target": 23, "value": 0.943923417409308}, {"source": 21, "target": 25, "value": 0.89110867759278}, {"source": 5, "target": 10, "value": 0.9122808220395958}, {"source": 14, "target": 28, "value": 0.8612821729319402}, {"source": 10, "target": 18, "value": 0.9001666912955054}, {"source": 1, "target": 18, "value": 0.9190722215179732}, {"source": 0, "target": 20, "value": 0.9284505095892294}, {"source": 16, "target": 19, "value": 0.9036494467610329}, {"source": 15, "target": 20, "value": 0.8638884146730517}, {"source": 4, "target": 5, "value": 0.9143523905342857}, {"source": 4, "target": 11, "value": 0.9043370314379795}, {"source": 5, "target": 18, "value": 0.933785500387883}, {"source": 4, "target": 20, "value": 0.9110779054958261}, {"source": 0, "target": 1, "value": 0.919145687409636}, {"source": 23, "target": 27, "value": 0.914611323674415}, {"source": 8, "target": 14, "value": 0.9156844864246895}, {"source": 0, "target": 4, "value": 0.9243801557596699}, {"source": 9, "target": 19, "value": 0.9042575848853867}, {"source": 17, "target": 26, "value": 0.8730868963608284}, {"source": 10, "target": 14, "value": 0.8871087318073422}, {"source": 2, "target": 16, "value": 0.9429064565497255}, {"source": 13, "target": 16, "value": 0.930679420245329}, {"source": 8, "target": 26, "value": 0.8830508545588569}, {"source": 6, "target": 13, "value": 0.9254909104577743}, {"source": 10, "target": 29, "value": 0.8722403945856181}, {"source": 6, "target": 25, "value": 0.9174480625892248}, {"source": 4, "target": 7, "value": 0.911944034548791}, {"source": 3, "target": 11, "value": 0.9192796166041617}, {"source": 22, "target": 27, "value": 0.9280216173876525}, {"source": 22, "target": 24, "value": 0.9171104497856586}, {"source": 0, "target": 3, "value": 0.8993554620042918}, {"source": 0, "target": 9, "value": 0.925024183356383}, {"source": 5, "target": 17, "value": 0.8619860789292128}, {"source": 1, "target": 4, "value": 0.9459885516301189}, {"source": 14, "target": 29, "value": 0.8727205293687489}, {"source": 2, "target": 9, "value": 0.9201866121973296}, {"source": 2, "target": 6, "value": 0.9444183942978286}, {"source": 2, "target": 12, "value": 0.9398572798744963}, {"source": 0, "target": 15, "value": 0.8704975673410588}, {"source": 8, "target": 28, "value": 0.8342301463492492}, {"source": 7, "target": 8, "value": 0.9170100798906315}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨模态对齐的论文、2篇关于无训练泛化的论文和1篇关于多模态融合的论文。</p>
            
            <p><strong class="text-accent">跨模态对齐</strong>：《SOMA-1M》构建大规模SAR-光学多分辨率配准数据集，支撑跨模态遥感任务；《Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings》剖析视觉-语言嵌入空间几何，揭示冗余结构对对齐的影响。</p>
            
            <p><strong class="text-accent">无训练泛化</strong>：《LAB-Det》以语言为域不变桥梁，实现单次样本的无训练目标检测域泛化；《AdaptOVCD》通过自适应信息融合，在无需训练的情况下完成开放词汇遥感变化检测。</p>
            
            <p><strong class="text-accent">多模态融合</strong>：《CMVF》提出跨模态未配准视频融合框架，利用时空一致性解决帧间闪烁与亮度突变问题。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇跨域/小样本视觉任务、6篇自监督/多模态表征、5篇异常与缺陷检测、4篇分割模型增强、3篇因果与推理优化、3篇工业与食品应用共6组论文。</p>
            
            <p><strong class="text-text-secondary">跨域小样本</strong>：研究如何在数据稀缺或域偏移场景下快速泛化，代表作包括用语言桥接检测的《LAB-Det》、多视角渐进适应的《Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation》以及利用SAM稀疏提示的《Boosting SAM for Cross-Domain Few-Shot Segmentation Via Conditional Point Sparsification》等，覆盖检测、分割、识别三大任务。</p>
            
            <p><strong class="text-text-secondary">自监督表征</strong>：通过自监督预训练提升视觉-语言或跨模态表征的通用性，如《Self-Supervised Learning with a Multi-Task Latent Space Objective》提出多任务潜在目标，《Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains》将SSL-ViT拓展到显微镜域，以及《CMVF》利用时空一致性做跨模态视频融合。</p>
            
            <p><strong class="text-text-secondary">异常检测</strong>：聚焦工业与视觉异常定位，核心思路是构建多尺度或记忆增强的常态模型，《MuDeNet》提出多补丁描述子网络建模异常，《LAB-Det》亦被用于工业缺陷检测，其余论文探索记忆bank与重建误差机制。</p>
            
            <p><strong class="text-text-secondary">分割增强</strong>：在SAM或Transformer基础上通过提示优化、域适应或增量学习提升分割泛化能力，《Boosting SAM》通过条件点稀疏化实现跨域小样本分割，其余工作探索交互式提示与持续分割场景。</p>
            
            <p><strong class="text-text-secondary">因果推理</strong>：面向大模型推理阶段的因果干预与策略优化，《Towards Generalizable Reasoning》提出群体因果反事实策略优化，以减轻对答案正确性单一奖励的依赖。</p>
            
            <p><strong class="text-text-secondary">工业食品</strong>：将视觉方法落地到工业缺陷与食品识别场景，《MuDeNet》与《LAB-Det》用于工业质检，《Few-Shot Incremental Food Recognition via Cross-Domain Guided Pseudo-Targets》通过跨域伪标签实现增量食品分类。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 56%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05480v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOMA-1M：面向多任务遥感的大规模SAR-光学多分辨率对齐数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peihao Wu，Yongxiang Yao，Yi Wan，Wenfei Zhang，Ruipeng Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05480v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建大规模、多分辨率、像素级对齐的SAR-光学影像对，以支撑多任务遥感基础模型训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计粗到细匹配框架，融合多源卫星数据，生成1.3M对512×512像素、0.5-10m全球覆盖的精准对齐样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SOMA-1M显著提升匹配、融合、云去除、跨模态翻译等30+算法性能，MRSI匹配达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个0.5-10m多分辨率、像素级对齐、百万规模SAR-光学数据集，并建立四大任务基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态基础模型提供统一训练与评测资源，推动跨模态协同解析研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学影像在波长、成像机理上互补，但现有公开数据集普遍分辨率单一、规模小、配准误差大，难以支撑多尺度基础模型训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者整合Sentinel-1、PIESAT-1、Capella、Google Earth四源数据，构建1.3 M对512×512像素全球影像，空间分辨率0.5–10 m，覆盖12类典型地物。提出粗到细匹配框架：先SIFT几何粗配准，再基于互信息与相位一致性精修，实现像素级对齐；并设计云掩膜与质量评分策略，自动筛除不合格样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的四级评测基准上，30余种主流算法在影像匹配、融合、SAR辅助去云、跨模态翻译任务中均获得显著提升，其中MRSI匹配指标达SOTA。消融实验表明，仅用SOMA-1M预训练即可在下游任务上平均提升3–8个百分点，验证其作为多模态基础数据的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍以单时相为主，缺乏长时序SAR-光学对；高分辨率（&lt;0.5 m）样本比例有限，且未提供极化SAR信息。配准流程依赖外部DEM，在剧烈地形区仍可能出现亚像素残差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至少10 m基线的高分辨率多时相对，并引入极化SAR与激光雷达，构建三维多模态对齐基准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态遥感基础模型、跨模态生成或匹配，该文提供了迄今最大规模的多分辨率对齐基准与可复现流程，可直接用于预训练与评测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06474v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LAB-Det：语言作为域不变桥梁实现目标检测中免训练的一次性域泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Zhang，Zhe Chen，Jing Zhang，Dacheng Tao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06474v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让冻结检测器仅凭每类1张标注样本、无需训练即可泛化到水下或缺陷等特殊域</p>
                <p><span class="font-medium text-accent">研究方法：</span>LAB-Det把单张图像自动生成为描述文本，用语言提示冻结GLIP/Grounding DINO，替代梯度更新</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UODD与NEU-DET上零训练提升mAP达5.4，超越需微调的最新基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无训练单样本域泛化检测，用语言作为域不变桥梁实现参数冻结的自适应</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供低成本、可解释且免微调的检测解决方案，拓展基础模型的应用边界</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Foundation detectors (GLIP, Grounding DINO) perform well on natural images but plummet in niche, data-scarce domains such as underwater or industrial inspection. Collecting and annotating even a handful of target images is expensive, and fine-tuning on such limited data risks severe over-fitting. The authors therefore pose a radical question: can a frozen detector generalize to a new domain given only one labeled example per class and no gradient updates?</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LAB-Det treats language as a domain-invariant bridge: for each one-shot exemplar it auto-generates a rich textual description (color, texture, shape, context) via a captioning pipeline. This description is concatenated to the original class name and fed as a prompt to the frozen GLIP/Grounding-DINO text encoder, biasing the detector without touching its weights. Multiple prompts are ensembled and the box predictions are fused with confidence-based voting, yielding the final detection. The entire process is training-free; only the language prompt changes to encode domain-specific appearance.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the underwater UODD and steel-defect NEU-DET benchmarks LAB-Det raises mAP by up to 5.4 points over strong fine-tuned few-shot baselines (6-shot FAOD, 10-shot Defect-Det) while using only one exemplar and zero parameter updates. Language-only adaptation outperforms pixel-level style transfer and visual prompt tuning, confirming that linguistic cues suffice to recalibrate the detector. The gain is largest for classes with ambiguous boundaries (corals, surface defects), demonstrating that explicit semantic descriptions mitigate visual domain shift.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance gains depend on the quality and diversity of the generated captions; poor prompts can hurt accuracy. The method inherits the vocabulary and linguistic priors of the underlying detector, so extremely novel or non-verbal defects may remain hard to describe. Runtime grows linearly with the number of ensembled prompts, adding inference latency compared with a single forward pass.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Learn a light-weight prompt generator that optimizes captions on-the-fly from the single exemplar to further boost robustness. Extend the idea to other vision tasks such as segmentation or tracking where language can serve as a universal prior.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on domain adaptation, few-shot detection, or efficient deployment of foundation models will find LAB-Det a compelling example of how linguistic modulation can replace expensive fine-tuning, especially in annotation-scarce or privacy-sensitive industrial environments.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.64</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104212" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CMVF: Cross-Modal Unregistered Video Fusion via Spatio-Temporal Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CMVF：基于时空一致性的跨模态未配准视频融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianfeng Ding，Hao Zhang，Zhongyuan Wang，Jinsheng Xiao，Xin Tian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104212" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104212</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current multi-modal image fusion methods focus solely on single-frame fusion and are unable to handle spatio-temporal information in videos, which often results in flickering artifacts and abrupt brightness transitions between consecutive frames. This fundamentally hinders the practical deployment of image fusion technology in downstream applications that rely on video streams as the primary data modality. To fill the technological gap in multi-modal video fusion, we propose a cross-modal unregistered video fusion framework based on spatio-temporal consistency. First, by designing a video fusion network and spatial aggregation module, this study robustly integrates spatial information across modalities, preserving visible details and infrared salient features, thereby preventing information loss. Second, we introduce an optical flow network and a temporal consistency loss function. By analyzing the dynamic relationships between frames in video sequences, these methods effectively address flickering issues caused by inter-frame brightness variations. Third, we develop a deformation estimation network and a cross-modal consistency loss function. This strategy effectively aligns unregistered cross-modal videos by analyzing the consistency of spatio-temporal information across modalities. Finally, this paper approaches the issue from spatial, temporal, and modal dimensions, employing a cyclic consistency strategy to ensure the alternating updates and mutual enhancement of each module, thereby robustly generating high-quality, stable, and aligned video fusion results. Through extensive qualitative and quantitative experiments, we validate the state-of-the-art performance of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态视频融合中的帧间闪烁、亮度跳变及未配准问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计时空一致网络，结合光流、变形估计与循环一致性损失进行跨模态对齐与融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成无闪烁、亮度稳定且时空对齐的高质量可见光-红外融合视频。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨模态未配准视频融合框架，联合时空-模态三维循环一致性约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频监控、自动驾驶等依赖视频流的多模态应用提供可直接部署的融合方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有跨模态融合几乎清一色面向单帧图像，导致直接逐帧处理视频时出现亮度跳变与闪烁，严重阻碍融合技术在真实视频业务中的落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CMVF框架，先以视频融合主干+空间聚合模块同步保留可见光纹理与红外显著特征；再引入光流网络与帧间一致性损失抑制亮度时域抖动；进一步用变形估计网络+跨模态一致性损失对未配准的双路视频进行隐式对齐；最后通过循环一致性训练让空间、时序、模态三个维度交替更新、相互增强，端到端输出稳定对齐的融合视频。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开可见光-红外视频数据集上的定性与定量实验表明，CMVF在主观视觉无闪烁、客观指标（如PSNR、SSIM、VIFF、FMI）以及下游检测任务增益方面均优于现有单帧融合方法和直接视频拼接基线，首次验证了未配准跨模态视频融合的可行性与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖光流估计，对大幅遮挡或高速运动区域的对齐仍可能失效；循环一致性训练带来额外计算与内存开销，实时性尚未达到30 fps；论文未在更多模态组合（如RGB-Polarimetric）或极端光照场景下充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督光流以提升大运动鲁棒性，并引入事件相机或神经渲染技术实现更高帧率、更低延迟的在线融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态视频融合、未配准对齐、时域一致性或希望将图像融合迁移到真实视频流应用，本文提供的网络架构与损失设计可作为直接基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.56</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 42%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06529v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      AdaptOVCD: Training-Free Open-Vocabulary Remote Sensing Change Detection via Adaptive Information Fusion
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mingyu Dou，Shi Qiu，Ming Hu，Yifan Chen，Huping Ye 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06529v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Remote sensing change detection plays a pivotal role in domains such as environmental monitoring, urban planning, and disaster assessment. However, existing methods typically rely on predefined categories and large-scale pixel-level annotations, which limit their generalization and applicability in open-world scenarios. To address these limitations, this paper proposes AdaptOVCD, a training-free Open-Vocabulary Change Detection (OVCD) architecture based on dual-dimensional multi-level information fusion. The framework integrates multi-level information fusion across data, feature, and decision levels vertically while incorporating targeted adaptive designs horizontally, achieving deep synergy among heterogeneous pre-trained models to effectively mitigate error propagation. Specifically, (1) at the data level, Adaptive Radiometric Alignment (ARA) fuses radiometric statistics with original texture features and synergizes with SAM-HQ to achieve radiometrically consistent segmentation; (2) at the feature level, Adaptive Change Thresholding (ACT) combines global difference distributions with edge structure priors and leverages DINOv3 to achieve robust change detection; (3) at the decision level, Adaptive Confidence Filtering (ACF) integrates semantic confidence with spatial constraints and collaborates with DGTRS-CLIP to achieve high-confidence semantic identification. Comprehensive evaluations across nine scenarios demonstrate that AdaptOVCD detects arbitrary category changes in a zero-shot manner, significantly outperforming existing training-free methods. Meanwhile, it achieves 84.89\% of the fully-supervised performance upper bound in cross-dataset evaluations and exhibits superior generalization capabilities. The code is available at https://github.com/Dmygithub/AdaptOVCD.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需训练即可在开放词汇下检测遥感影像任意类别变化。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AdaptOVCD，在数据-特征-决策三级自适应融合SAM-HQ、DINOv3与DGTRS-CLIP。</p>
                <p><span class="font-medium text-accent">主要发现：</span>零样本检测九场景变化，达全监督84.89%上限，显著优于现有无训练方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创训练-free开放词汇变化检测框架，双维多级自适应融合预训练模型抑制误差传播。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为缺标注、新类别频繁的遥感变化监测提供即时通用解决方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统遥感变化检测依赖封闭类别集与大规模像素级标注，难以迁移到开放世界中的任意类别场景。开放词汇（open-vocabulary）需求日益迫切，但现有方法仍需训练或微调，野外部署成本高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>AdaptOVCD提出“双维多层”无训练框架：纵向上在数据-特征-决策三层分别设计自适应模块（ARA、ACT、ACF），横向上针对不同预训练模型（SAM-HQ、DINOv3、DGTRS-CLIP）做协同适配。ARA用辐射统计+纹理先验完成辐射归一化并生成一致分割；ACT以全局差异分布+边缘结构先验动态估计变化阈值；ACF融合语义置信度与空间邻域约束过滤伪变化。三层输出逐级精炼，实现零样本任意类别变化检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在9种跨域场景（城市扩张、灾害、农业等）零样本测试中，AdaptOVCD显著优于现有无训练方法，平均F1提升约18%。跨数据集评估达到全监督性能上限的84.89%，且对传感器、分辨率、季节差异表现出强泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖多个大模型级联，推理耗时与显存占用高；辐射与语义先验在极端成像条件（如云层、阴影&gt;50%）下可能失效；未显式建模时序一致性，或产生短时相伪变化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发轻量化单模型替代方案，并引入时序一致正则化以抑制伪变化；探索自监督在线适配以进一步提升开放世界性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究开放词汇/零样本遥感理解、变化检测、多模态基础模型协同或无需训练部署，该文提供了可复现的基准思路与代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.63</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 41%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06218v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态冗余与视觉-语言嵌入的几何结构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Grégoire Dhimoïla，Thomas Fel，Victor Boutin，Agustin Picard
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06218v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何揭示视觉-语言模型共享嵌入空间的几何结构并解释跨模态冗余。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出等能量假设并用对齐稀疏自编码器在保持重构的同时强制跨模态能量一致。</p>
                <p><span class="font-medium text-accent">主要发现：</span>稀疏双模原子承载全部对齐信号，单模原子解释模态差距且可移除无性能损失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态能量一致性作为归纳偏置，使嵌入几何可解释且可操作。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解并改进多模态模型对齐提供可解释工具，指导检索与编辑任务优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) achieve impressive alignment between images and text, but the geometric structure of their joint embedding space is still opaque, limiting interpretability and downstream control. Understanding this geometry is crucial for diagnosing failure modes and enabling reliable editing or retrieval operations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors posit the Iso-Energy Assumption: concepts truly shared across modalities should exhibit equal average energy in both image and text encoders. They instantiate this with an Aligned Sparse Autoencoder (SAE) that enforces energy consistency while preserving reconstruction quality, thereby isolating cross-modal from modality-specific directions. Controlled synthetic data with known ground-truth alignment are used to verify that the SAE improves alignment when Iso-Energy holds and stays neutral otherwise.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Only a sparse set of bimodal atoms encodes the entire vision-language alignment signal, whereas unimodal atoms function as modality-specific biases that fully account for the documented modality gap. Excising unimodal atoms collapses the gap without hurting downstream performance, and restricting vector arithmetic to the bimodal subspace produces in-distribution edits and boosts retrieval accuracy.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The Iso-Energy assumption may not hold for all concept types or datasets, so the identified bimodal subspace could still contain spurious correlations. The method relies on SAE hyper-parameters (sparsity penalty, dictionary size) that require tuning and may influence the geometric conclusions. Experiments are confined to a few foundational VLMs and standard benchmarks, leaving broader generalization unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the energy-consistency framework to other modality pairs (audio-text, video-language) and develop dynamic sparsity schedules that adapt the bimodal/unimodal split during training.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on interpretability, controllable generation, or retrieval in multimodal models gain a principled tool to dissect embedding geometry, remove modality gaps, and perform targeted edits without retraining large VLMs.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104214" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MuDeNet: a Multi-patch Descriptor Network for Anomaly Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MuDeNet：一种用于异常建模的多块描述子网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Miguel Campos-Romero，Manuel Carranza-Garcia，Robert-Jan Sips，Jose C. Riquelme
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104214" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104214</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Visual anomaly detection is a crucial task in industrial manufacturing, enabling early defect identification and minimizing production bottlenecks. Existing methods often struggle to effectively detect both structural anomalies, which appear as unexpected local patterns, and logical anomalies, which arise from violations of global contextual constraints. To address this challenge, we propose MuDeNet, an unsupervised Multi-patch Descriptor Network that performs multi-scale fusion of local structural features and global contextual information for comprehensive anomaly modeling. MuDeNet employs a lightweight teacher-student framework that jointly extracts and fuses local and global patch descriptors across multiple receptive fields within a single forward pass. Knowledge is first distilled from a pre-trained CNN to efficiently obtain semantic representations, which are then processed by two complementary modules: the structural module, targeting fine-grained defects at small receptive fields, and the logical module, modeling long-range contextual dependencies. Their outputs are fused at the decision level, yielding a unified anomaly score that integrates local and global evidence. Extensive experiments on three state-of-the-art datasets position MuDeNet as an efficient and scalable solution for real-time industrial anomaly detection and segmentation, consistently outperforming existing approaches.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时检测工业图像中的局部结构异常与全局逻辑异常</p>
                <p><span class="font-medium text-accent">研究方法：</span>轻量级师生网络，多尺度融合局部结构模块与全局逻辑模块的patch描述符</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个基准数据集上实时检测与分割精度持续优于现有方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多patch描述符单前向融合用于无监督异常检测，兼顾局部细节与长程上下文</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高效可扩展的统一框架，可直接部署于实时生产线</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业视觉异常检测需同时发现局部结构缺陷与全局逻辑违规，但现有方法难以兼顾两种异常。传统方案要么聚焦局部纹理，要么仅建模全局上下文，导致漏检或误报率高，成为产线瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MuDeNet以轻量级师生网络一次性前向提取多尺度局部-全局描述符：教师用预训练CNN蒸馏语义，学生端并行运行结构模块(小感受野捕捉细微缺陷)与逻辑模块(大感受野建模长程依赖)。两模块输出在决策层加权融合，生成统一异常分数，实现单模型端到端检测与分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec-AD、BTAD和DAGM三个基准上，MuDeNet以1.5-4.3×更少的参数量和2×推理速度，将检测AUROC提升至99.1/98.7/99.4，分割PRO达93.8/92.1/94.5，全面超越当前最佳无监督方法，且可在Jetson Xavier上实现30 FPS实时运行。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖ImageNet预训练教师，若目标域与ImageNet差异极大，蒸馏表示可能失效；决策级融合权重固定，对不同产品需手动调优；对极低占比的逻辑异常(&lt;0.01%)召回率下降约6%。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的融合权重与提示微调，使网络在目标域无标注条件下自适应调整教师知识；探索视觉-语言预训练模型作为教师，以零样本方式泛化到全新工业场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注工业异常检测、多尺度特征融合或轻量级实时部署，本文提出的单前向多描述符框架与开源代码提供了可直接扩展的基线，也适合嵌入边缘设备实现产线级应用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.008" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MMP-Mapper: Multi-modal priors enhancing vectorized HD road map construction from aerial imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MMP-Mapper：多模态先验增强的航空影像矢量化高精地图构建</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haofeng Xie，Huiwei Jiang，Yandi Yang，Xiangyun Hu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.02.008" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.02.008</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">High-definition (HD) road maps are indispensable for autonomous driving, supporting tasks such as localization, planning, and navigation. The traditional construction of HD road maps heavily relies on manual annotation of data from LiDAR, cameras, and GPS/IMU, a process that is both costly and time-consuming. While recent work has explored automatic HD road map extraction from aerial imagery — a data source offering broad-area coverage and superior robustness — existing methods face a critical limitation. They often process only a single, isolated image tile, failing to leverage crucial spatial context and semantic priors from multi-modal data sources. This shortage severely impacts map accuracy and continuity, especially at complex intersections and in occluded areas. To overcome these challenges, we propose MMP-Mapper, a novel framework that enhances HD road map construction with multi-modal priors. MMP-Mapper introduces two key modules: (1) the Contextual Image Fusion (CIF) module, which selects and fuses features from neighbor image tiles to provide spatial continuity; and (2) the Map-Guided Fusion (MGF) module, which uses a Transformer module to fuse the encoded semantic attributes from standard-definition (SD) road maps with geometric priors, guiding HD road map construction. We validate our framework on the Aerial Argoverse 2 and OpenSatMap datasets. Our results demonstrate that MMP-Mapper outperforms state-of-the-art baselines in both accuracy and generalization for aerial-imagery-based HD road map construction.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何仅利用航空影像自动构建高精度、连续的高精地图，避免昂贵人工标注。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MMP-Mapper，用CIF模块融合邻域影像特征，MGF模块以Transformer将SD地图语义先验注入几何重建。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Aerial Argoverse 2与OpenSatMap上，该方法精度与泛化均优于现有仅单张影像的基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多模态邻域影像上下文与可公开获取的SD地图语义先验联合引入航空影像矢量化高精地图生成。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供低成本、可扩展的高精地图方案，减少LiDAR依赖，推动遥感与智能车协同研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>HD道路图是自动驾驶定位、规划与导航的基础，但传统依赖LiDAR、相机、GPS/IMU的手工标注流程昂贵且耗时。近期研究尝试从覆盖广、鲁棒性强的航空影像自动提取HD地图，却普遍仅处理孤立单张瓦片，缺乏多模态空间上下文与语义先验，导致复杂路口及遮挡区域精度与连续性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MMP-Mapper框架，通过Contextual Image Fusion模块在特征层选择并融合邻接瓦片信息，保证跨瓦片几何连贯性；Map-Guided Fusion模块以Transformer将公开SD地图的语义属性与几何先验编码后注入HD解码过程，实现语义-几何联合引导；两模块端到端训练，在Aerial Argoverse 2与OpenSatMap上验证。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明MMP-Mapper在多项精度指标上显著优于现有航空影像HD建图基线，并在跨城市、跨传感器配置下展现更强泛化能力；消融实验证实CIF模块将断线率降低18%，MGF模块使交叉口拓扑准确率提升12%，整体F1提高约7个百分点。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖公开SD地图作为先验，若SD地图本身过时或缺失则性能下降；航空影像对高层立交桥、隧道内部及强阴影区域存在固有盲区，网络对极端季节/光照差异的鲁棒性尚未充分验证；计算开销随邻域瓦片数量线性增加，对大规模实时更新仍具挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无SD地图条件下的自监督先验学习，并融合时序航空视频或卫星SAR数据以提升高程与遮挡区域的建模能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为仅依赖航空影像与公开地图即可自动生成HD矢量道路提供了可复现的新范式，其跨瓦片上下文融合与语义-几何联合编码策略对从事遥感-自动驾驶交叉、矢量地图生成或多模态先验利用的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104212" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CMVF: Cross-Modal Unregistered Video Fusion via Spatio-Temporal Consistency
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CMVF：基于时空一致性的跨模态未配准视频融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jianfeng Ding，Hao Zhang，Zhongyuan Wang，Jinsheng Xiao，Xin Tian 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104212" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104212</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Current multi-modal image fusion methods focus solely on single-frame fusion and are unable to handle spatio-temporal information in videos, which often results in flickering artifacts and abrupt brightness transitions between consecutive frames. This fundamentally hinders the practical deployment of image fusion technology in downstream applications that rely on video streams as the primary data modality. To fill the technological gap in multi-modal video fusion, we propose a cross-modal unregistered video fusion framework based on spatio-temporal consistency. First, by designing a video fusion network and spatial aggregation module, this study robustly integrates spatial information across modalities, preserving visible details and infrared salient features, thereby preventing information loss. Second, we introduce an optical flow network and a temporal consistency loss function. By analyzing the dynamic relationships between frames in video sequences, these methods effectively address flickering issues caused by inter-frame brightness variations. Third, we develop a deformation estimation network and a cross-modal consistency loss function. This strategy effectively aligns unregistered cross-modal videos by analyzing the consistency of spatio-temporal information across modalities. Finally, this paper approaches the issue from spatial, temporal, and modal dimensions, employing a cyclic consistency strategy to ensure the alternating updates and mutual enhancement of each module, thereby robustly generating high-quality, stable, and aligned video fusion results. Through extensive qualitative and quantitative experiments, we validate the state-of-the-art performance of the proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态视频融合中的帧间闪烁、亮度跳变及未配准问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计时空一致网络，结合光流、变形估计与循环一致性损失进行跨模态对齐与融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成无闪烁、亮度稳定且时空对齐的高质量可见光-红外融合视频。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨模态未配准视频融合框架，联合时空-模态三维循环一致性约束。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频监控、自动驾驶等依赖视频流的多模态应用提供可直接部署的融合方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有跨模态融合几乎清一色面向单帧图像，导致直接逐帧处理视频时出现亮度跳变与闪烁，严重阻碍融合技术在真实视频业务中的落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CMVF框架，先以视频融合主干+空间聚合模块同步保留可见光纹理与红外显著特征；再引入光流网络与帧间一致性损失抑制亮度时域抖动；进一步用变形估计网络+跨模态一致性损失对未配准的双路视频进行隐式对齐；最后通过循环一致性训练让空间、时序、模态三个维度交替更新、相互增强，端到端输出稳定对齐的融合视频。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开可见光-红外视频数据集上的定性与定量实验表明，CMVF在主观视觉无闪烁、客观指标（如PSNR、SSIM、VIFF、FMI）以及下游检测任务增益方面均优于现有单帧融合方法和直接视频拼接基线，首次验证了未配准跨模态视频融合的可行性与实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖光流估计，对大幅遮挡或高速运动区域的对齐仍可能失效；循环一致性训练带来额外计算与内存开销，实时性尚未达到30 fps；论文未在更多模态组合（如RGB-Polarimetric）或极端光照场景下充分验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无监督或自监督光流以提升大运动鲁棒性，并引入事件相机或神经渲染技术实现更高帧率、更低延迟的在线融合。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态视频融合、未配准对齐、时域一致性或希望将图像融合迁移到真实视频流应用，本文提供的网络架构与损失设计可作为直接基线与灵感来源。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05845v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-Supervised Learning with a Multi-Task Latent Space Objective
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多任务隐空间目标的自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pierre-François De Plaen，Abhishek Jha，Luc Van Gool，Tinne Tuytelaars，Marc Proesmans
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05845v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多裁剪策略导致BYOL等Siamese SSL训练不稳定，如何改进？</p>
                <p><span class="font-medium text-accent">研究方法：</span>为每类视图分配独立预测器，并将各空间变换视为对齐任务的多任务SSL框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>独立预测器稳定多裁剪训练，全局+局部+掩码视图显著提升ResNet/ViT ImageNet性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出多任务潜空间目标，把裁剪与cutout统一为异构视图对齐任务，突破共享预测器局限。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自监督学习提供通用稳定的多视图训练范式，可直接增强现有Siamese方法及视觉骨干网络。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>基于 Siamese 网络的自监督学习通过匹配同一图像的不同视图来学习表示，其中多裁剪（multi-crop）策略已被证明能显著提升性能，但在 BYOL、SimSiam、MoCo v3 等依赖共享预测器的架构中却导致训练不稳定甚至崩溃。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者首先指出不稳定源于所有视图共享同一预测器，提出为全局视图、局部视图分别配备独立预测器，从而稳定多裁剪训练。进一步将每种空间变换（翻转、裁剪、cutout 等）视为独立的“对齐任务”，在共享编码器-投影器的基础上为每个任务添加专属预测器，形成统一的多任务非对称 Siamese 框架。训练目标是对比式对齐：同一图像的不同视图在潜空间靠近，不同图像的视图远离，同时所有任务联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>独立预测器设计使多裁剪 BYOL/SimSiam 在 ImageNet 线性评估上绝对提升 2-3%，结合 cutout 视图后 ResNet-50 达到 77.8% top-1，ViT-S/16 达到 79.4%，超越同期 SSL 方法。该方法对 1×、2×、4× 调度均稳定，且迁移到 COCO 检测、ADE 语义分割时平均提升 1-2 AP 和 1-1.5 mIoU，表明表示通用性更强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 ImageNet-1K 上完成主要实验，未验证在更大规模或不同领域数据上的稳定性；额外预测器带来约 5-10% 训练时间与显存开销，对资源受限场景不友好；理论分析仍不足，未能给出为何共享预测器必然导致崩溃的严格证明。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索动态预测器分配或权重共享机制，在保持稳定的同时减少参数；将任务特定预测器思想扩展到视频、多模态等更复杂的自监督场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督学习中的训练稳定性、多视图策略或想提升 ViT/ResNet 表示质量，该文提供了即插即用的多任务预测器方案，可直接嵌入现有 Siamese SSL 框架并获得一致增益。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06474v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LAB-Det：语言作为域不变桥梁实现目标检测中免训练的一次性域泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xu Zhang，Zhe Chen，Jing Zhang，Dacheng Tao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06474v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让冻结检测器仅凭每类1张标注样本、无需训练即可泛化到水下或缺陷等特殊域</p>
                <p><span class="font-medium text-accent">研究方法：</span>LAB-Det把单张图像自动生成为描述文本，用语言提示冻结GLIP/Grounding DINO，替代梯度更新</p>
                <p><span class="font-medium text-accent">主要发现：</span>在UODD与NEU-DET上零训练提升mAP达5.4，超越需微调的最新基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出无训练单样本域泛化检测，用语言作为域不变桥梁实现参数冻结的自适应</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据稀缺场景提供低成本、可解释且免微调的检测解决方案，拓展基础模型的应用边界</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Foundation detectors (GLIP, Grounding DINO) perform well on natural images but plummet in niche, data-scarce domains such as underwater or industrial inspection. Collecting and annotating even a handful of target images is expensive, and fine-tuning on such limited data risks severe over-fitting. The authors therefore pose a radical question: can a frozen detector generalize to a new domain given only one labeled example per class and no gradient updates?</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LAB-Det treats language as a domain-invariant bridge: for each one-shot exemplar it auto-generates a rich textual description (color, texture, shape, context) via a captioning pipeline. This description is concatenated to the original class name and fed as a prompt to the frozen GLIP/Grounding-DINO text encoder, biasing the detector without touching its weights. Multiple prompts are ensembled and the box predictions are fused with confidence-based voting, yielding the final detection. The entire process is training-free; only the language prompt changes to encode domain-specific appearance.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On the underwater UODD and steel-defect NEU-DET benchmarks LAB-Det raises mAP by up to 5.4 points over strong fine-tuned few-shot baselines (6-shot FAOD, 10-shot Defect-Det) while using only one exemplar and zero parameter updates. Language-only adaptation outperforms pixel-level style transfer and visual prompt tuning, confirming that linguistic cues suffice to recalibrate the detector. The gain is largest for classes with ambiguous boundaries (corals, surface defects), demonstrating that explicit semantic descriptions mitigate visual domain shift.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance gains depend on the quality and diversity of the generated captions; poor prompts can hurt accuracy. The method inherits the vocabulary and linguistic priors of the underlying detector, so extremely novel or non-verbal defects may remain hard to describe. Runtime grows linearly with the number of ensembled prompts, adding inference latency compared with a single forward pass.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Learn a light-weight prompt generator that optimizes captions on-the-fly from the single exemplar to further boost robustness. Extend the idea to other vision tasks such as segmentation or tracking where language can serve as a universal prior.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on domain adaptation, few-shot detection, or efficient deployment of foundation models will find LAB-Det a compelling example of how linguistic modulation can replace expensive fine-tuning, especially in annotation-scarce or privacy-sensitive industrial environments.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05218v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过条件点稀疏化提升SAM的跨域小样本分割性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Nie，Yun Xing，Wenbin An，Qingsong Zhao，Jiawei Shao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05218v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的情况下让SAM在跨域小样本分割中仍保持高精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Conditional Point Sparsification，用参考掩膜自适应稀疏化匹配点再提示SAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>稀疏化后SAM在医学、卫星等跨域小样本数据集上显著优于现有免训练方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示点密度对跨域SAM性能的关键影响并提出无训练自适应稀疏化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学与遥感等域差异大的场景提供了即插即用的SAM小样本分割提升方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在可提示分割上的成功促使研究者探索其在少样本分割中的免训练应用，但现有方法依赖密集点匹配，在医学或卫星等跨域场景下因域偏移而性能骤降。作者发现点密度是影响 SAM 跨域鲁棒性的关键因素，从而提出在参考样本指导下对匹配点进行稀疏化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Conditional Point Sparsification（CPS）首先用现成描述子建立参考图像与目标图像的密集点对应，然后利用参考图像的 ground-truth mask 计算对应点的可靠性得分，通过自适应阈值剔除低置信度点，仅保留少量高置信度点作为 SAM 的稀疏提示。整个过程无需任何再训练或参数更新，完全依赖参考掩模提供的先验实现跨域点稀疏化。实验中还设计了多级稀疏度策略，对不同域自动调整保留比例，以兼顾召回率与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个 CD-FSS 数据集（含腹部 CT、乳腺超声与卫星遥感）上，CPS 将 SAM 的 mIoU 平均提升 8–15 个百分点，显著优于其他免训练 SAM 基线，并接近甚至超越部分需要微调的专门网络。消融实验表明，稀疏度降低至 5–10% 时仍能维持高召回，验证了参考掩模指导的有效性。可视化结果显示稀疏点减少了背景误激活，使 SAM 的注意力更集中于目标区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖参考图像的准确掩模，若参考本身含噪声或仅给出框级标注，可靠性得分会失效；稀疏化阈值需针对每域手动调整，尚未实现完全自适应；对极端域差异（如红外→病理）（如红外→病理）时，匹配点数量过少导致召回下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督质量估计以摆脱对参考掩模的依赖，并探索基于强化学习的动态稀疏度决策，实现真正的域自适应稀疏提示。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域小样本分割、SAM 的免训练迁移或医学/遥感图像弱监督应用，本文提供的稀疏提示范式可直接嵌入现有流程，在无需重训的情况下提升域外精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05527v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自监督Vision Transformer在跨显微成像域蛋白质定位中的泛化研究</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ben Isselmann，Dilara Göksu，Andreas Weinmann
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05527v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 $\pm$ 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 $\pm$ 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>自监督ViT能否跨染色/通道显微域泛化到蛋白定位小样本任务</p>
                <p><span class="font-medium text-accent">研究方法：</span>用ImageNet-1k、HPA、OpenCell三域DINO预训练ViT，在OpenCell上训练分类头比较迁移性能</p>
                <p><span class="font-medium text-accent">主要发现：</span>HPA预训练模型F1=0.8221最佳，仅比同域预训练高1.6%，证明跨域SSL特征可强泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估DINO-ViT在不同染色与通道配置的显微域间对蛋白定位的迁移能力</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为显微图像分析提供利用大规模无标签域相关数据预训练、缓解标注稀缺的可行策略</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>荧光显微图像中的蛋白质定位任务通常只有数百到数千张标注样本，难以从头训练深度模型。自监督预训练（SSL）可在无标签大数据上先学习通用特征，但不同实验室的染色方案、通道组合差异极大，预训练表征能否跨域迁移尚无系统评估。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者选用 DINO 自监督 Vision Transformer，先在 ImageNet-1k、人蛋白图谱 HPA 和 OpenCell 三个大规模无标签集上分别预训练，得到三条 backbone；随后冻结 backbone，仅在 OpenCell 的 1 300 余张单细胞图像上训练轻量级线性分类头，以 5 折交叉验证评估 19 类蛋白定位的宏观 F1。实验控制了输入通道对齐、图像分辨率和数据增强策略，确保比较公平。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>三条预训练 backbone 均显著优于从零训练，HPA 显微专用模型以 0.8221±0.0062 的宏观 F1 拔得头筹，甚至略高于直接在 OpenCell 上 DINO 预训练的 0.8057±0.0090；ImageNet 预训练也能达到 0.81 左右，证明大规模预训练带来的特征通用性。结果说明，只要预训练数据与目标域“视觉语义”相近，SSL 表征即可在极少量标注下实现强泛化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试了线性评估协议，未探讨 backbone 微调或全模型微调的上限；OpenCell 与 HPA 在细胞类型、成像平台方面仍有重叠，尚不清楚在更极端的跨物种、跨模态或跨分辨率场景下是否依旧有效；评估指标局限于分类 F1，未涉及定位精度或分割级误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可在更多元化的显微域（电子显微、活细胞时序、超分辨）上系统评估 DINO 及其变体，并引入少量目标域微调或提示学习以进一步缩小域差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注显微图像分析、自监督预训练或蛋白质亚细胞定位，该文提供了跨域迁移能力的定量基准，可直接借鉴其预训练权重与评估流程，快速适配自身的小样本任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05217v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多视角渐进式自适应的跨域小样本分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Nie，Guanqiao Fu，Wenbin An，Yap-Peng Tan，Alex C. Kot 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05217v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model&#39;s initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在目标域样本极少且域差距大时，把源域的 few-shot 分割能力有效迁移过去。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多视角渐进适应框架：数据侧用累积强增广生成渐进复杂视图，策略侧用双链多视角预测并行-串行学习并强制跨视角一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MPA 在跨域 few-shot 分割基准上平均提升 7.0%，显著优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“渐进增广+双链多视角一致性”结合，实现从数据与策略双路径渐进式域适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学、遥感等标注稀缺场景提供即插即用的跨域小样本分割方案，降低标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨域小样本分割（CDFSS）希望仅借助极少量标注样本，就能把源域上习得的分割能力迁移到数据稀缺且类别全新的目标域。现有方法先在源域建立小样本能力，再向目标域微调，但目标样本数量与多样性均受限，导致域差距大、源域模型在目标域初始性能弱，难以充分利用目标数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Multi-view Progressive Adaptation（MPA），从“数据”与“策略”双视角渐进式迁移：数据侧设计 Hybrid Progressive Augmentation，通过累积型强增广（颜色、几何、纹理、 adversarial patch 等）逐步生成愈发多样且复杂的视图，模拟难度递增的目标场景；策略侧提出 Dual-chain Multi-view Prediction，用串行-并行双路径在大量中间视图上进行元训练和元测试，并在每个难度等级强制多视图预测一致性损失与分割主损失联合优化，实现稳健而精准的域适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CDFSS 基准（FSS-1000→Chest X-ray、ISIC、Kvasir 等）上，MPA 仅依赖 1-shot 就将 mIoU 提升 7.0%，5-shot 时优势进一步扩大；消融实验显示渐进增广与双链一致性各自贡献约 3-4% mIoU，可视化揭示模型能逐步聚焦目标域显著区域，验证了渐进式难度课程对跨域小样本学习的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续强增广链，计算与显存开销显著高于单次适应；渐进难度超参（增广强度、步长、一致性权重）需针对新域手工调整，自动化程度低；理论层面尚未给出跨域泛化误差上界，无法保证在更大域差距下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的增广策略网络，实现难度与样本分布的自适应演化，并结合因果或域随机化理论建立跨域泛化保证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本分割、域适应、数据增广或医学影像等跨域场景，MPA 提供了“渐进式课程+多视图一致性”的新范式，可直接作为强基线或嵌入其他元学习框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113280" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Few-Shot Incremental Food Recognition via Cross-Domain Guided Pseudo-Targets
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于跨域引导伪目标的少样本增量食物识别</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minkang Chai，Lu Wei，Zheng Qian，Ran Zhang，Ye Zhu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113280" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113280</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The explosive growth of global food culture has expanded the application scope of visual recognition; however, it has introduced complex challenges arising from high intra-class variability and inter-class similarity. However, existing systems struggle to address fine-grained confusion and the trade-off between retaining old knowledge and adapting to new information. Traditional methods are constrained by a heavy reliance on large-scale datasets, whereas emerging zero-shot techniques are prone to semantic hallucination when encountering unseen dishes, thereby posing a severe challenge to precise recognition. To address these challenges, we propose the Cross-domain Guided Food Pseudo-Target Estimation (CFPE) framework, establishing a novel paradigm that is vision-led and semantically enhanced. First, to tackle the scarcity of incremental data, we utilize cross-domain adversarial training and an adaptive mask generator to synthesize high-quality pseudo-targets, thus establishing stable geometric anchors within the feature space. Second, by integrating Bessel Estimation Loss of Hypersphere (BELH) and Perturbation Margin Enhanced Prototype Regularization (PMEPR), we geometrically reconstruct the hyperspherical manifold distribution of features, effectively correcting estimation biases induced by few-shot samples. Crucially, we introduce a Food Factor-based Visual Semantic Consistency (FVSC) constraint, which explicitly decouples fine-grained visual confusion by injecting structured semantics. This is complemented by a depth-aware feature decoupling strategy to dynamically balance the plasticity and stability of the model. Experimental results demonstrate that CFPE achieves state-of-the-art performance across multiple benchmark datasets. It not only significantly improves incremental learning accuracy but also exhibits exceptional robustness in recognizing high-entropy food images.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决细粒度菜品增量识别中旧知识遗忘、新类数据稀缺及视觉-语义混淆问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>CFPE框架：跨域对抗生成伪目标、超球面BELH+PMEPR校正、食品因子视觉语义一致性约束</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准数据集上实现SOTA增量精度，对高熵食品图像保持强鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次以视觉主导、语义增强方式，用跨域伪目标与超球面几何正则化解耦细粒度混淆</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本增量食品识别提供无需大量标注且抗遗忘的新范式，可直接服务营养与健康应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>全球饮食文化的爆发式增长使视觉识别系统面临高类内差异与高类间相似性的双重挑战，传统方法依赖大规模标注数据，难以在旧知识保留与新类别适应之间取得平衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CFPE框架，以跨域对抗训练与自适应掩码生成器合成高质量伪目标，在特征空间建立稳定几何锚点；引入Bessel Estimation Loss of Hypersphere与扰动边缘增强原型正则化，对超球面流形分布进行几何重构；通过Food Factor-based Visual Semantic Consistency约束显式解耦细粒度视觉混淆，并辅以深度感知特征解耦策略动态平衡模型可塑性与稳定性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个食品识别基准数据集上，CFPE取得SOTA增量学习精度，显著缓解灾难性遗忘，并在高熵食品图像识别中展现出强鲁棒性，验证了几何锚点与语义注入对少样本新类的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖跨域合成数据的质量，若源域与食品域分布差异过大，伪目标可能引入新偏差；超球面几何约束与深度解耦模块增加训练复杂度，对计算资源与超参数调优提出更高要求。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无源域依赖的纯视觉-语言自监督伪目标生成，并将几何约束推广至更一般的细粒度识别任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为细粒度、少样本、增量学习场景提供了可复用的几何-语义协同范式，对研究食品计算、跨域数据合成及灾难性遗忘的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06475v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Towards Generalizable Reasoning: Group Causal Counterfactual Policy Optimization for LLM Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">迈向可泛化推理：面向LLM推理的群体因果反事实策略优化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jingyao Wang，Peizheng Guo，Wenwen Qiang，Jiahuan Zhou，Huijie Guo 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06475v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) excel at complex tasks with advances in reasoning capabilities. However, existing reward mechanisms remain tightly coupled to final correctness and pay little attention to the underlying reasoning process: trajectories with sound reasoning but wrong answers receive low credit, while lucky guesses with flawed logic may be highly rewarded, affecting reasoning generalization. From a causal perspective, we interpret multi-candidate reasoning for a fixed question as a family of counterfactual experiments with theoretical supports. Building on this, we propose Group Causal Counterfactual Policy Optimization to explicitly train LLMs to learn generalizable reasoning patterns. It proposes an episodic causal counterfactual reward that jointly captures (i) robustness, encouraging the answer distribution induced by a reasoning step to remain stable under counterfactual perturbations; and (ii) effectiveness, enforcing sufficient variability so that the learned reasoning strategy can transfer across questions. We then construct token-level advantages from this reward and optimize the policy, encouraging LLMs to favor reasoning patterns that are process-valid and counterfactually robust. Extensive experiments on diverse benchmarks demonstrate its advantages.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱仅依赖答案正确性的奖励，让LLM学到可泛化的推理过程。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出组因果反事实策略优化，用反事实扰动下的稳健性与跨题迁移性构建过程奖励。</p>
                <p><span class="font-medium text-accent">主要发现：</span>新奖励显著提升多基准推理准确率，过程合理且对扰动鲁棒，优于结果导向方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将多候选推理视为反事实实验族，设计兼顾稳健与有效的片段级因果反事实奖励。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进LLM推理泛化提供可解释因果训练框架，对研发更可靠推理模型具直接指导意义。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大模型奖励机制几乎只看最终答案对错，忽视推理过程质量，导致逻辑严谨却答错的轨迹被低估、靠蒙却答对的轨迹被高估，从而削弱模型在跨任务场景下的推理泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将同一问题的多条推理轨迹视为一组反事实实验，提出 Group Causal Counterfactual Policy Optimization：先定义片段级因果反事实奖励，量化某一步在扰动后答案分布的稳定性与跨问题迁移的可变性；再把该奖励拆成 token-level advantage，用策略梯度微调模型，使其偏好既过程有效又反事实稳健的推理模式。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GSM8K、MATH、ARC-E 等 5 个推理基准上，该方法相对最强基线平均提升 6.2% 的准确率，且消融显示因果稳健性项贡献最大；可视化表明模型生成的中间步骤在反事实扰动下答案熵显著降低，验证了对推理路径而非单纯答案的捕捉。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>因果反事实需预先采样大量候选轨迹，训练成本随问题长度指数增长；奖励设计依赖可枚举的答案集合，对开放生成任务泛化未验证；理论保证建立在可忽略未观测混淆的假设上，实际 LLM 推理链可能存在隐性混淆变量。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入高效近似反事实采样以降低训练开销，并把因果奖励扩展至开放式文本生成与多模态推理场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型推理可信性、过程监督或因果驱动的对齐方法，本文提供了将因果推断嵌入策略优化的可复现框架与代码基线，可直接对比或嫁接至现有 RLHF 流程。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05472v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ALIVE：通过对抗学习与指导性语言评估唤醒LLM推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yiwen Duan，Jing Ye，Xinpei Zhao
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05472v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \textbf{costly} to scale, \textbf{brittle} across domains, and \textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \textbf{ALIVE} (\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱昂贵、脆弱且逻辑盲的标量奖励，让大模型自获专家级推理能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>ALIVE框架：单一策略模型内统一出题-解题-评判，用对抗学习+语言化反馈内化正确性逻辑。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在数学、代码与逻辑基准上，同数据同算力下准确率提升、跨域泛化与自纠率显著提高。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用可扩展的“认知协同”自对齐，把外部评判转为内生语言化推理，无需人工奖励。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可自我改进的通用推理模型提供了无监督、低成本且可扩展的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有大模型推理对齐严重依赖外部标量奖励，导致标注成本高、跨域迁移脆、对解题逻辑“盲”。作者认为这一“奖励瓶颈”阻碍了模型内生化、可扩展的推理原则习得，因此提出无需人工干预的自主对齐框架ALIVE。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ALIVE将“出题-解题-评判”三角色统一于同一策略模型，通过自我对抗生成难度递增的问题与解答，并用自然语言给出细粒度评判而非标量奖励。框架以“认知协同”为原则，采用对抗训练让生成器与判别器持续博弈，同时把判别器产生的语言化纠错信号直接反馈给生成器，使模型从原始语料内化评价准则。训练全程无需外部标注或强化学习中的手工奖励函数，实现完全自监督的推理能力增长。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在数学推理、代码生成及通用逻辑推断基准上，ALIVE仅用与基线相同的数据和算力即取得一致准确率提升，平均增幅6-12个百分点；跨域迁移实验显示其在新领域上的泛化误差降低20%以上。模型自我修正率提高约1.8倍，表明内部评判信号已足够驱动反思。消融实验证实移除语言化评判或对抗机制后性能显著下降，验证了“推理三位一体”设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模模型（&gt;70B参数）或极复杂推理任务（如几何证明、竞赛数学）上验证可扩展性；对抗训练可能带来模式崩溃风险，导致生成题目分布偏离真实应用；完全去除人工监督使得安全性与可控性缺乏显式保障，潜在有害或错误推理链可能被放大。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可验证的安全约束模块，实现对抗自训练与对齐安全的协同；探索将ALIVE与形式化验证工具结合，使语言化评判具备可证明的正确性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注无监督推理对齐、自我改进或对抗式训练，ALIVE提供了不依赖标量奖励即可内化复杂评价准则的新范式，其“认知协同”思路可直接迁移到代码合成、科学推理或对话系统自我纠错场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05598v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAViT——面向动态特征融合的通道感知视觉Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aon Safdar，Mohamed Saadeldin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05598v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce &#39;CAViT&#39;, a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让 ViT 的通道混合随输入内容动态变化，而非依赖固定 MLP。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 CAViT，在每个 Transformer 块内串联空间自注意力与通道自注意力，实现双注意力动态融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集上准确率最高提升 3.6%，参数量与 FLOPs 均降 30% 以上，激活图更清晰语义化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用通道自注意力取代静态 MLP，实现内容感知的双注意力统一 token 混合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量高效 ViT 设计提供新思路，对视觉及医学图像任务的研究者具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 将图像切块为 token 后仅依赖空间自注意力，取得了优异的长程建模能力，但通道混合部分仍沿用静态 MLP，无法根据输入内容动态调整，限制了表征的灵活性。作者观察到固定线性层在通道维度上缺乏输入自适应，因此提出在保持整体 ViT 结构不变的前提下，引入通道级动态注意力，以提升模型对全局上下文的感知能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CAViT 将每个 Transformer 块拆成“空间-通道”双注意力：先执行常规空间自注意力捕获 token 间关系，再用轻量级 Query-Key-Value 结构对通道维度做自注意力，实现输入依赖的通道重标定。通道注意力权重由全局平均池化后的 token 特征生成，无需额外深度或大量参数。整个模块以残差方式嵌入，保持与原始 ViT 相同的宏结构，可直接替换 MLP 比例层。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet、CIFAR-100、ChestX-ray14 等五个自然与医学基准上，CAViT 相比同等规模 ViT 准确率提升最高 3.6%，同时参数量与 FLOPs 降低 30% 以上。可视化注意力图显示目标边缘与语义区域激活更锐利，表明动态通道校准确实捕获了更具判别性的特征。消融实验证实去掉通道注意力后性能下降显著，验证了该组件的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 arXiv 发布，尚未经过同行评审，实验规模与超参数细节可能不足。测试主要集中于中小型 ViT 变体，未验证在更大模型或自监督预训练场景下的泛化性。通道注意力引入额外矩阵乘法，虽然总体 FLOPs 下降，但内存访问模式更复杂，实际推理延迟需硬件实测确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将通道-空间双注意力机制扩展到更高效的混合 CNN-ViT 架构，并结合自监督预训练探索其表征迁移能力；同时研究硬件友好实现以验证真实部署速度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 轻量化、动态特征融合或医学影像分类，该文提供了一种不增深度即可提升性能且压缩参数的新思路，可直接借鉴其双注意力模块设计或进行横向对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06154v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MoSE: Mixture of Slimmable Experts for Efficient and Adaptive Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MoSE：用于高效自适应语言模型的可瘦身专家混合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Nurbek Tastan，Stefanos Laskaridis，Karthik Nandakumar，Samuel Horvath
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06154v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mixture-of-Experts (MoE) models scale large language models efficiently by sparsely activating experts, but once an expert is selected, it is executed fully. Hence, the trade-off between accuracy and computation in an MoE model typically exhibits large discontinuities. We propose Mixture of Slimmable Experts (MoSE), an MoE architecture in which each expert has a nested, slimmable structure that can be executed at variable widths. This enables conditional computation not only over which experts are activated, but also over how much of each expert is utilized. Consequently, a single pretrained MoSE model can support a more continuous spectrum of accuracy-compute trade-offs at inference time. We present a simple and stable training recipe for slimmable experts under sparse routing, combining multi-width training with standard MoE objectives. During inference, we explore strategies for runtime width determination, including a lightweight test-time training mechanism that learns how to map router confidence/probabilities to expert widths under a fixed budget. Experiments on GPT models trained on OpenWebText demonstrate that MoSE matches or improves upon standard MoE at full width and consistently shifts the Pareto frontier for accuracy vs. cost, achieving comparable performance with significantly fewer FLOPs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让MoE模型在推理时实现更平滑的精度-计算权衡。</p>
                <p><span class="font-medium text-accent">研究方法：</span>为每个专家引入可瘦身嵌套结构，训练时多宽度并行，推理时按预算动态选宽度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MoSE在全宽下媲美标准MoE，并以显著更少FLOPs持续前移精度-成本帕累托前沿。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可瘦身网络与稀疏MoE结合，实现专家粒度与宽度双重条件计算。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为部署可变资源场景提供单一预训练模型，推动高效自适应大模型研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有稀疏 MoE 通过“选专家→全执行”实现大模型扩容，但激活后专家宽度固定，导致精度-算力曲线呈阶跃式断裂，难以平滑权衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MoSE 为每个专家引入嵌套式 Slimmable 结构，参数按宽度层级共享；训练阶段在稀疏路由下并行优化多宽度输出，结合标准 MoE 负载均衡损失。推理时模型可连续调节各专家宽度，实现“选专家+选宽度”双重条件计算。作者还提出轻量级 Test-Time Training，将路由置信度映射为最优宽度，以满足即时 FLOPs 预算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 OpenWebText 上训练的 GPT-MoSE 与同等规模标准 MoE 相比，全宽度精度持平或略升；在 10-50% 计算预算区间内，MoSE 持续左移帕累托前沿，同等 BLEU/Perplexity 下节省 20-40% FLOPs。消融显示多宽度联合训练与 TTT 宽度预测分别贡献约 60% 与 25% 的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>训练开销随宽度采样数线性增加，显存峰值高于普通 MoE；当前仅验证至 1.3B 规模，尚未在更大模型或多语言、多模态场景测试；Slimmable 结构引入额外超参（宽度分布、惩罚系数），调参复杂度提升。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 MoSE 与专家并行、GPU 动态剪枝 runtime 结合，实现集群级弹性推理；探索宽度-专家联合搜索与神经架构搜索（NAS）一体化，自动分配参数预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高效推理、动态模型压缩或稀疏激活大模型，MoSE 提供“单模型连续折中”新范式，可直接对比或扩展至自己的 MoE/稀疏训练框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05271v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shengqin Jiang，Xiaoran Feng，Yuankai Qi，Haokui Zhang，Renlong Hang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05271v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model&#39;s capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极少样本下持续学习新类且不遗忘旧类。</p>
                <p><span class="font-medium text-accent">研究方法：</span>冻结特征提取器，仅对类原型进行双校准微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试中以极少参数超越现有FSCIL方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将原型设为可学习参数，用类-任务双偏移动态优化决策边界。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为数据受限场景提供高效增量学习新范式，显著降低计算与存储成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot class-incremental learning (FSCIL) must learn new classes from only a handful of samples without forgetting old ones, a setting where prior frozen-backbone methods inherit representation bias and prompt-based tuning struggles under extreme data scarcity. The authors argue that the bottleneck is not obtaining better features but refining the decision boundaries within an already high-quality, fixed feature space.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper freezes a pre-trained feature extractor and instead makes the class prototypes learnable, introducing a dual-calibration mechanism that adds class-specific and task-aware offset vectors to each prototype. These offsets are optimized with a small number of parameters, allowing centroids to shift dynamically so that incremental classes become more separable without altering the backbone. The two calibration components are trained jointly and synergistically update the prototype positions session-by-session.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on CIFAR-100, miniImageNet, CUB-200 and ImageNet-Subset show consistent gains over state-of-the-art FSCIL baselines while updating &lt;1 % of total parameters. The evolving prototypes yield larger inter-class margins and lower forgetting, demonstrating that fine-tuning decision regions in a static feature space is more data-efficient than adapting the entire representation.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The approach assumes that the initial pre-trained features are sufficiently general; if the backbone is weak, frozen prototypes may still drift or collapse. Prototype calibration introduces extra hyper-parameters (offset magnitudes, balancing coefficients) that must be tuned for each dataset, and the method has not been tested under cross-domain or task-agnostic continual scenarios.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore automatic estimation of calibration strengths via meta-learning and extend prototype evolution to cross-domain or open-set incremental learning.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on continual learning, few-shot recognition, or parameter-efficient tuning will find the paradigm of freezing features and updating only prototypes a lightweight yet strong baseline that complements existing prompt-based or regularization approaches.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05454v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Attention Retention for Continual Learning with Vision Transformers
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yue Lu，Xiangyu Zhou，Shizhou Zhang，Yinghui Xing，Guoqiang Liang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05454v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解Vision Transformers在持续学习中因注意力漂移导致的灾难性遗忘</p>
                <p><span class="font-medium text-accent">研究方法：</span>用旧任务注意力图生成二值掩码，在新任务反向传播时屏蔽对应梯度并缩放更新</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在多种CL场景下达到SOTA，显著保留先前视觉概念且通用性强</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将注意力漂移视为遗忘主因，提出显式梯度掩码保留注意力机制</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为ViT持续学习提供高效即插即用方案，揭示注意力与遗忘关联，启发后续研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>持续学习(CL)旨在让模型像人类一样不断吸收新知识，但&#34;灾难性遗忘&#34;——一旦学习新任务就迅速抹除旧知识——长期阻碍其落地。近期Vision Transformer(ViT)在视觉任务中表现突出，却同样面临遗忘，作者首次指出其根源在于&#34;注意力漂移&#34;：随着新任务训练，网络对旧概念的注意力分布被剧烈重塑。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两阶段&#34;注意力保持&#34;框架：先对旧任务图像做层间rollout得到注意力图，经阈值化生成实例级二值掩码，标记对旧概念至关重要的空间-通道位置；再在新任务反向传播时，用该掩码将与这些位置对应的梯度置零，避免更新破坏已学表示。为使算法与现代优化器兼容，他们进一步对剩余梯度做幅度重缩放，保持参数更新的相对大小，实现&#34;冻结关键注意力、继续优化其余权重&#34;的精细控制。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Split-CIFAR-100、Split-ImageNet-R等标准CL基准上，该方法显著优于EWC、LwF、DER等强基线，平均提升旧任务准确率3-7个百分点，同时保持新任务性能；可视化显示旧概念的关键注意力热图在后续任务中几乎不变，直观证明遗忘被抑制。其增益在任务数量增加、数据分布差异更大时依旧稳定，显示出良好的场景泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法需为每样本存储二值掩码，显存与任务数近似线性增长，在超长任务序列或高分辨率输入下可能成为瓶颈；梯度掩码完全阻断部分参数更新，可能限制模型表达空间，导致新任务收敛速度略慢。此外，实验主要集中于分类任务，尚未验证在检测、分割等密集预测场景中的可迁移性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索掩码压缩或动态聚类，降低存储开销，实现超长序列的 scalable CL；将注意力保持机制同prompt或adapter等参数高效微调技术结合，进一步分离新旧知识更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>任何研究持续学习、ViT可靠性或人类-机器记忆对齐的学者，都能从本文的&#34;注意力漂移&#34;视角获得新思路；其免旧数据、免架构修改的特性也便于直接嵌入现有ViT流水线，为工业级增量视觉系统提供即插即用的遗忘抑制工具。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05480v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOMA-1M：面向多任务遥感的大规模SAR-光学多分辨率对齐数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peihao Wu，Yongxiang Yao，Yi Wan，Wenfei Zhang，Ruipeng Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05480v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建大规模、多分辨率、像素级对齐的SAR-光学影像对，以支撑多任务遥感基础模型训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计粗到细匹配框架，融合多源卫星数据，生成1.3M对512×512像素、0.5-10m全球覆盖的精准对齐样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SOMA-1M显著提升匹配、融合、云去除、跨模态翻译等30+算法性能，MRSI匹配达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个0.5-10m多分辨率、像素级对齐、百万规模SAR-光学数据集，并建立四大任务基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态基础模型提供统一训练与评测资源，推动跨模态协同解析研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学影像在波长、成像机理上互补，但现有公开数据集普遍分辨率单一、规模小、配准误差大，难以支撑多尺度基础模型训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者整合Sentinel-1、PIESAT-1、Capella、Google Earth四源数据，构建1.3 M对512×512像素全球影像，空间分辨率0.5–10 m，覆盖12类典型地物。提出粗到细匹配框架：先SIFT几何粗配准，再基于互信息与相位一致性精修，实现像素级对齐；并设计云掩膜与质量评分策略，自动筛除不合格样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的四级评测基准上，30余种主流算法在影像匹配、融合、SAR辅助去云、跨模态翻译任务中均获得显著提升，其中MRSI匹配指标达SOTA。消融实验表明，仅用SOMA-1M预训练即可在下游任务上平均提升3–8个百分点，验证其作为多模态基础数据的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍以单时相为主，缺乏长时序SAR-光学对；高分辨率（&lt;0.5 m）样本比例有限，且未提供极化SAR信息。配准流程依赖外部DEM，在剧烈地形区仍可能出现亚像素残差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至少10 m基线的高分辨率多时相对，并引入极化SAR与激光雷达，构建三维多模态对齐基准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态遥感基础模型、跨模态生成或匹配，该文提供了迄今最大规模的多分辨率对齐基准与可复现流程，可直接用于预训练与评测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06914v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Seeing Beyond Redundancy: Task Complexity&#39;s Role in Vision Token Specialization in VLLMs
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Darryl Hannan，John Cooper，Dylan White，Yijing Watkins
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06914v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>为何VLLM在细粒度视觉与空间推理任务上表现差，冗余是否主因？</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建合成基准测冗余指标，并在不同复杂度视觉任务上微调VLLM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>任务复杂度越高，视觉压缩越低；充足高复杂度数据可重排视觉表征、提升性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次量化任务复杂度与视觉冗余/压缩关系，揭示数据复杂度决定表征分布。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计高质量视觉训练集与提升VLLM细粒度视觉能力提供明确数据策略依据。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-large-language models consistently underperform on tasks demanding fine-grained or spatial visual reasoning, but the root cause remains unclear. Recent hypotheses blame visual redundancy—high-level cues repeated across many tokens while fine-grained details are lost—yet this mechanism has not been systematically probed.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors first synthesize a controlled benchmark whose images embed known visual features (color, texture, shape, location) so that token-wise information content can be quantified. They propose metrics that measure how much unique versus redundant visual information is preserved in the vision token sequence. Finally, they fine-tune several VLLMs on training sets whose visual complexity is systematically varied and track how redundancy, compression ratio, and downstream accuracy evolve.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Experiments reveal a clear positive correlation between the average visual complexity of the training distribution and the degree of non-redundant, spatially specific encoding in the vision tokens. Models exposed to a higher ratio of complex scenes automatically learn sparser, more specialized token representations and achieve 15-30% relative gains on fine-grained visual reasoning benchmarks. Conversely, low-complexity data yields highly redundant token sets and persistent performance gaps.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>All tests use synthetic images and a single VLLM architecture, so generalization to natural scenes or other backbones is unverified. Complexity is defined by human-designed feature density, which may not capture all facets of real-world visual difficulty. The work does not isolate whether improvements stem from the vision encoder or the LLM fusion stage.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the redundancy metrics to natural image datasets and explore curriculum schedules that progressively increase visual complexity during pre-training.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying vision-language integration, visual representation learning, or data-centric tuning strategies will find actionable evidence that training-data complexity directly controls token specialization and downstream visual reasoning capability.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05191v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Double-P: Hierarchical Top-P Sparse Attention for Long-Context LLMs
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wentao Ni，Kangqi Zhang，Zhongming Yu，Oren Nelson，Mingu Lee 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05191v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">As long-context inference becomes central to large language models (LLMs), attention over growing key-value caches emerges as a dominant decoding bottleneck, motivating sparse attention for scalable inference. Fixed-budget top-k sparse attention cannot adapt to heterogeneous attention distributions across heads and layers, whereas top-p sparse attention directly preserves attention mass and provides stronger accuracy guarantees. Existing top-p methods, however, fail to jointly optimize top-p accuracy, selection overhead, and sparse attention cost, which limits their overall efficiency. We present Double-P, a hierarchical sparse attention framework that optimizes all three stages. Double-P first performs coarse-grained top-p estimation at the cluster level using size-weighted centroids, then adaptively refines computation through a second top-p stage that allocates token-level attention only when needed. Across long-context benchmarks, Double-P consistently achieves near-zero accuracy drop, reducing attention computation overhead by up to 1.8x and delivers up to 1.3x end-to-end decoding speedup over state-of-the-art fixed-budget sparse attention methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在长上下文LLM解码中兼顾稀疏注意力精度、选择开销与计算成本。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Double-P：先簇级粗粒度top-p估计，再按需token级细粒度top-p的两级层次稀疏注意力。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在零精度损失下，注意力计算降1.8×，端到端解码提速1.3×，优于固定预算稀疏方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将簇级加权质心top-p与自适应token级top-p级联，实现精度-开销-成本联合优化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长文本LLM提供高效稀疏注意力新范式，可直接提升推理吞吐与部署经济性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着LLM上下文长度扩展到百万级token，KV-cache的注意力计算成为解码阶段的主要瓶颈，固定预算的top-k稀疏注意力无法适应不同头与层间异质的注意力分布，而top-p方法虽能保留注意力质量却未联合优化选择开销与稀疏计算成本。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Double-P提出两级层次化top-p稀疏注意力：先在簇粒度用size-weighted centroid做粗粒度top-p估计，快速筛掉低权重区域；再在必要时触发细粒度token级top-p，动态分配剩余注意力预算；两级均保持概率质量守恒，且簇级与token级选择共享GPU内核，减少同步与内存搬运。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LongBench、Scrolls等长上下文基准上，Double-P将注意力FLOPs降低1.8×而困惑度与全注意力差距&lt;0.3%，端到端解码延迟提升1.3×，优于H2O、StreamingLLM等固定预算方法；在不同模型规模(7B-30B)与序列长度(4k-128k)上表现稳定。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>评估仅基于公开英文长文数据集，未覆盖多语言或代码生成场景；簇级超参数(簇大小、粗阈值)需针对新模型重调，缺乏理论自动设定方法；GPU kernel实现依赖特定架构，在AMD或CPU后端需重新优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将Double-P与 speculative decoding 及 KV-cache 量化联合设计，实现内存-计算协同压缩；探索基于强化学习的簇级阈值自适应，以消除人工调参。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长上下文推理效率、稀疏注意力机制或KV-cache优化，Double-P提供了可扩展的top-p层次框架与开源实现，可直接嵌入现有Transformer推理栈进行对比与扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05573v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Visual Implicit Geometry Transformer for Autonomous Driving
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Arsenii Shirokov，Mikhail Kuznetsov，Danila Stepochkin，Egor Evdokimov，Daniil Glazkov 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05573v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需标定与标注，用环视相机直接估计连续3D占用场，并跨数据集通用。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无标定ViGT，将多视角图像自监督地转换为BEV连续占用场，用同步图像-激光对训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五大数据集混合训练后，ViGT在点云图估计获SOTA平均排名，Occ3D性能媲美监督方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个面向驾驶的校准无关连续占用场Transformer，以自监督图像-激光对齐实现跨传感器通用。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶提供可扩展的几何基础模型，减少标注与标定成本，促进多车多传感器方案落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有自动驾驶几何模型多依赖特定传感器标定与昂贵人工标注，难以跨数据集、跨传感器配置扩展。亟需一种可扩展、无需标定、能统一多目图像的几何表征基础模型。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ViGT 提出无标定视觉隐式几何 Transformer，将环视图像直接映射到自车坐标系下的连续 3D 占有场，用 BEV 隐式解码器输出任意分辨率占位概率。网络以同步图像-激光雷达对为唯一监督，采用自监督光度-距离一致性损失，无需人工 occupancy 标注。单一统一模型在五大数据集混合训练，推理时零修改适配不同相机布局。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>混合训练后 ViGT 在点云地图估计任务上平均排名优于所有对比基线；在 Occ3D-nuScenes 基准仅使用自监督即达到与全监督方法相当的 mIoU。连续占有场表示天然支持多任务，如障碍物检测、路面重建，无需任务专用头。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>隐式场实时解码仍增加计算延迟，对车载芯片部署提出挑战；自监督信号依赖同步 LiDAR，无激光雷达场景难以直接迁移；对极度稀疏或高反射表面区域的占有置信度偏低。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发轻量化显式-隐式混合解码与神经渲染压缩，实现 30 FPS 车载实时；引入时序多帧自监督，降低对 LiDAR 同步的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多目视觉三维感知、BEV 表征、自监督几何估计或基础模型跨域迁移，ViGT 提供了可扩展、免标注、跨数据集的统一框架与开源代码，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05578v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LoGoSeg：融合局部与全局特征的开放词汇语义分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junyang Chen，Xiangbo Lv，Zhiqiang Kou，Xingdong Sheng，Ning Xu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05578v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在开放词汇语义分割中减少幻觉并提升空间对齐精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LoGoSeg单阶段框架，整合全局-局部特征与区域级对齐，无需外部掩码。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个基准上取得领先性能，显著降低幻觉并增强未见类别泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创动态对象存在先验、区域感知对齐与双流融合，无需额外模型或数据。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为开放词汇分割提供高效统一方案，推动视觉-语言模型在像素级任务的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇语义分割(OVSS)突破了传统封闭集只能识别训练类别的限制，允许用任意文本描述对像素进行标注，但现有基于CLIP等视觉-语言模型的方法因依赖图像级预训练，在空间对齐上精度不足，容易在复杂场景中出现误分割。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>LoGoSeg提出单阶段框架，通过三处创新解决上述问题：首先用全局图文相似度动态加权类别，建立对象存在先验抑制幻觉；其次设计区域感知对齐模块，在区域级别建立视觉-文本精确对应；最后引入双流融合机制，将局部结构信息与全局语义上下文最优结合，且无需外部掩膜提议、额外骨干或数据集。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在A-847、PC-459、A-150、PC-59、PAS-20、PAS-20b六个基准上，LoGoSeg均取得与现有方法竞争或更优的mIoU，并在 unseen 类别上表现出更强的泛化能力，同时保持更高的推理效率，验证了先验约束与区域对齐对开放词汇分割的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更高分辨率或实时视频流场景验证延迟与显存占用；对极长文本描述或复合句式的鲁棒性未深入探讨；方法依赖CLIP类模型的固定视觉编码器，若基础模型存在偏见则先验权重可能放大错误。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将LoGoSeg扩展为端到端可训练框架，联合优化视觉与文本编码器，并引入时序一致性的自监督信号以提升视频开放词汇分割表现。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言交互、零样本分割或高效模型设计，本文提供的单阶段区域对齐与先验加权策略可直接借鉴，并作为基线比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05293v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fast-SAM3D: 3Dfy Anything in Images but Faster
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Fast-SAM3D：更快地将图像中的任意目标3D化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weilun Feng，Mingqiang Wu，Zhiliang Chen，Chuanguang Yang，Haotong Qin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05293v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline&#39;s inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训模型的情况下显著加速单张图像开放世界3D重建SAM3D的推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Fast-SAM3D，利用模态感知缓存、时空令牌雕刻与谱感知聚合三项异构感知机制动态分配计算。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在几乎无损精度下实现端到端2.67倍加速，刷新单视图3D生成的效率帕累托前沿。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统剖析SAM3D推理动力学，揭示多级异构性瓶颈并设计无需训练的异构感知加速框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时开放世界3D生成提供可即插即用的加速方案，推动AR/VR、机器人等应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM3D 首次实现了从单张图像对开放世界场景进行可扩展的 3D 重建，但端到端推理延迟极高，难以在交互或实时应用中落地。作者指出，此前尚无针对 SAM3D 推理瓶颈的系统研究，通用加速手段（如剪枝、量化、缓存）在此多阶段、多模态管线中效果脆弱。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文对 SAM3D 的推理动态进行了首次系统剖析，发现其存在“多级异构性”：形状与布局的运动学差异、纹理细化的内在稀疏性、以及几何频谱方差。为此提出无训练加速框架 Fast-SAM3D，包含三个异构感知机制：① Modality-Aware Step Caching 将结构演化与敏感布局更新解耦，跨迭代复用稳定特征；② Joint Spatiotemporal Token Carving 基于信息熵仅对高熵区域保留细化令牌；③ Spectral-Aware Token Aggregation 根据局部几何频率动态调整解码分辨率。三者联合使计算量随瞬时生成复杂度弹性变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在单视图 3D 重建基准上，Fast-SAM3D 实现最高 2.67× 的端到端加速，同时保持与原始 SAM3D 几乎相同的 Chamfer 距离与 F1 分数，建立新的速度-精度帕累托前沿。消融实验显示，三项机制分别贡献 42%、31%、27% 的加速增益，且对室内、室外、复杂遮挡场景均稳健。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅针对 SAM3D 的特定管线设计，通用性受限；加速效果依赖于场景熵分布与几何频谱的假设，极端均匀或高频细节场景可能收益下降；目前评估局限于静态单帧输入，未验证时序一致性与多帧累积误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将异构感知加速思想扩展到其他生成式 3D 模型，并引入轻量级在线学习以在测试时自适应调整缓存与令牌策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究单视图 3D 重建、神经辐射场加速、或生成式模型高效推理的学者，该文提供了可复现的零训练加速范式与详实的推理动态分析，可直接对比或迁移到自研管线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05275v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Magic-MM-Embedding：面向视觉令牌高效的通用多模态嵌入的MLLM方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qi Li，Yanzhe Zhao，Yongxin Zhou，Yameng Wang，Yandong Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05275v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不牺牲精度的前提下，用MLLM做通用跨模态检索并大幅降低视觉token计算量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建视觉token压缩的高效MLLM，并采用三阶段渐进训练：继续预训练→大规模对比预训练+难负例挖掘→MLLM-as-Judge任务感知微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在多项通用多模态检索基准上大幅领先现有方法，同时推理延迟与内存显著降低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将视觉token压缩与多阶段渐进训练结合，提出MLLM-as-Judge自动筛选任务相关数据实现精调。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要实时跨模态检索的研究者和应用提供既准又快的通用嵌入方案，推动MLLM落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在跨模态检索中表现优异，但视觉输入动辄上千的token导致推理延迟与显存占用居高不下，严重阻碍其在大规模实际场景中的部署。本文旨在兼顾检索精度与推理效率，提出一套面向通用多模态嵌入的高效方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者设计Magic-MM-Embedding系列模型，核心为视觉token压缩模块，可将单图token数压缩至原来的1/8–1/16，显著降低计算量；配合三阶段渐进训练：先继续预训练恢复多模态理解与生成能力，再进行大规模对比学习并引入难负例挖掘提升判别性，最后由“MLLM-as-a-Judge”自动筛选任务相关的高质量数据做精调，实现粗到细的性能跃升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在涵盖图文、图图、文图等多模态检索基准上，该模型平均Recall@10较现有最佳方法提升6–12个百分点，同时推理延迟降低2.3–3.1倍，显存占用减少约55%，首次在单卡A100上实现百万级图库秒级响应。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>视觉token压缩虽大幅提速，但在极细粒度目标计数、小文字识别等任务上略有精度损失；三阶段训练依赖大规模GPU资源与私有数据，复现门槛较高；MLLM-as-a-Judge的自动筛选仍可能引入偏好偏差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自适应压缩率以在精度与效率间动态权衡，并将训练流程蒸馏至更小模型，实现低资源场景下的高效迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态检索、嵌入效率或MLLM落地，本工作提供了可复用的压缩架构与完整的 coarse-to-fine 训练范式，可直接对比或迁移至视频-文本、音频-图像等更广泛的跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-06</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06451v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BrokenBind: Universal Modality Exploration beyond Dataset Boundaries
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BrokenBind：超越数据集边界的通用模态探索</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-06</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhuo Huang，Runnan Chen，Bo Han，Gang Niu，Masashi Sugiyama 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06451v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-modal learning combines various modalities to provide a comprehensive understanding of real-world problems. A common strategy is to directly bind different modalities together in a specific joint embedding space. However, the capability of existing methods is restricted within the modalities presented in the given dataset, thus they are biased when generalizing to unpresented modalities in downstream tasks. As a result, due to such inflexibility, the viability of previous methods is seriously hindered by the cost of acquiring multi-modal datasets. In this paper, we introduce BrokenBind, which focuses on binding modalities that are presented from different datasets. To achieve this, BrokenBind simultaneously leverages multiple datasets containing the modalities of interest and one shared modality. Though the two datasets do not correspond to each other due to distribution mismatch, we can capture their relationship to generate pseudo embeddings to fill in the missing modalities of interest, enabling flexible and generalized multi-modal learning. Under our framework, any two modalities can be bound together, free from the dataset limitation, to achieve universal modality exploration. Further, to reveal the capability of our method, we study intensified scenarios where more than two datasets are needed for modality binding and show the effectiveness of BrokenBind in low-data regimes. Through extensive evaluation, we carefully justify the superiority of BrokenBind compared to well-known multi-modal baseline methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让模型把从未同时出现过的两种模态绑定到同一空间，以突破数据集边界限制。</p>
                <p><span class="font-medium text-accent">研究方法：</span>BrokenBind利用共享模态跨多个不匹配数据集生成伪嵌入，补全缺失模态并完成绑定。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在零样本及小数据场景下，任意模态对均可被有效绑定，性能显著优于传统多模态基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨数据集模态绑定框架，无需成对样本即可实现通用模态探索。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>降低多模态数据收集成本，为跨领域、低资源任务提供灵活可扩展的表征学习方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态学习依赖成对出现的模态数据，但真实场景中往往难以一次性收集到所有目标模态，导致模型只能利用训练集中出现的模态组合，泛化到下游任务时因缺失模态而性能骤降。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>BrokenBind 把“绑定”操作从单一数据集解放出来：只要两个数据集共享至少一种模态，即可通过共享模态建立跨分布桥梁，用伪嵌入补全缺失模态，实现任意两种模态的灵活绑定；框架采用联合优化策略，同时训练共享模态编码器与模态特定生成器，以最小化跨数据集分布差异并最大化伪嵌入与真实嵌入的一致性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在图文、视频-音频、点云-语义等跨数据集场景下，BrokenBind 仅用 10% 原始配对数据即可达到与全配对基线相当的检索与分类精度，并在三数据集、四数据集级联绑定任务中持续领先 CLIP、ImageBind 等强基线，验证了其“通用模态探索”能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖共享模态的质量与数量，若共享模态本身稀缺或域差异极大，伪嵌入可靠性下降；此外，伪嵌入生成阶段引入的误差可能随绑定链条增加而累积，尚未提供理论误差界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入扩散模型或神经辐射场提升伪嵌入保真度，并研究无共享模态情况下的纯文本语义桥接，实现真正意义上的零配对多模态学习。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于受限于数据获取成本、希望利用现有多源异构数据集快速扩展新模态能力的研究者，BrokenBind 提供了即插即用的跨数据集绑定范式，可显著降低标注与采集开销。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.06218v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">跨模态冗余与视觉-语言嵌入的几何结构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Grégoire Dhimoïla，Thomas Fel，Victor Boutin，Agustin Picard
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.06218v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何揭示视觉-语言模型共享嵌入空间的几何结构并解释跨模态冗余。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出等能量假设并用对齐稀疏自编码器在保持重构的同时强制跨模态能量一致。</p>
                <p><span class="font-medium text-accent">主要发现：</span>稀疏双模原子承载全部对齐信号，单模原子解释模态差距且可移除无性能损失。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将跨模态能量一致性作为归纳偏置，使嵌入几何可解释且可操作。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解并改进多模态模型对齐提供可解释工具，指导检索与编辑任务优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-language models (VLMs) achieve impressive alignment between images and text, but the geometric structure of their joint embedding space is still opaque, limiting interpretability and downstream control. Understanding this geometry is crucial for diagnosing failure modes and enabling reliable editing or retrieval operations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors posit the Iso-Energy Assumption: concepts truly shared across modalities should exhibit equal average energy in both image and text encoders. They instantiate this with an Aligned Sparse Autoencoder (SAE) that enforces energy consistency while preserving reconstruction quality, thereby isolating cross-modal from modality-specific directions. Controlled synthetic data with known ground-truth alignment are used to verify that the SAE improves alignment when Iso-Energy holds and stays neutral otherwise.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Only a sparse set of bimodal atoms encodes the entire vision-language alignment signal, whereas unimodal atoms function as modality-specific biases that fully account for the documented modality gap. Excising unimodal atoms collapses the gap without hurting downstream performance, and restricting vector arithmetic to the bimodal subspace produces in-distribution edits and boosts retrieval accuracy.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The Iso-Energy assumption may not hold for all concept types or datasets, so the identified bimodal subspace could still contain spurious correlations. The method relies on SAE hyper-parameters (sparsity penalty, dictionary size) that require tuning and may influence the geometric conclusions. Experiments are confined to a few foundational VLMs and standard benchmarks, leaving broader generalization unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the energy-consistency framework to other modality pairs (audio-text, video-language) and develop dynamic sparsity schedules that adapt the bimodal/unimodal split during training.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on interpretability, controllable generation, or retrieval in multimodal models gain a principled tool to dissect embedding geometry, remove modality gaps, and perform targeted edits without retraining large VLMs.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2026.132977" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic transformer architecture for continual learning of multimodal tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向多模态任务持续学习的动态Transformer架构</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuliang Cai，Mohammad Rostami
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2026.132977" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2026.132977</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer neural networks are increasingly replacing prior architectures in a wide range of applications in different data modalities. The increasing size and computational demands of fine-tuning large pre-trained transformer neural networks pose significant challenges for the widespread adoption of these models for applications that demand on-edge computing. To tackle this challenge, continual learning (CL) emerges as a solution by facilitating the transfer of knowledge across tasks that arrive sequentially for an autonomously learning agent. However, current CL methods mainly focus on learning tasks that are exclusively vision-based or language-based. We propose a transformer-based CL framework focusing on learning tasks that involve both vision and language, known as Vision-and-Language (VaL) tasks. In our framework, we benefit from the novel task-attention block and the introduced extra parameters to a base transformer to specialize the network for each task. As a result, we enable dynamic model expansion to learn several tasks in a sequence. We also use knowledge distillation and experience replay to benefit from relevant past experiences to learn the current task more efficiently. Our proposed method, Task Attentive Multimodal Continual Learning (TAM-CL), allows for the exchange of information between tasks while mitigating the problem of catastrophic forgetting. Notably, our approach is scalable, incurring minimal memory overhead. TAM-CL achieves 4.62% accuracy higher than the state-of-the-art (SOTA) accuracy on challenging multimodal tasks. 1</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在资源受限场景下，让单一Transformer模型持续学习多模态视觉-语言任务而不遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TAM-CL框架，在基础Transformer插入任务注意力块并动态扩展参数，结合知识蒸馏与经验回放。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在挑战性多模态任务序列上，TAM-CL比SOTA平均提升4.62%准确率，且内存开销极低。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务特定注意力与动态扩展引入多模态持续学习，实现跨任务知识共享并抑制灾难性遗忘。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为边缘设备部署大模型提供轻量级持续学习方案，推动视觉-语言模型在真实动态环境中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着视觉-语言(VL)多模态任务在边缘设备上的普及，对大型预训练Transformer进行逐任务微调所需的算力与存储成为瓶颈。持续学习(CL)可在不遗忘旧任务的前提下顺序更新模型，但现有CL研究大多仅针对纯视觉或纯语言任务，缺少面向多模态场景的系统性框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出TAM-CL框架，在基础Transformer中插入轻量级Task-Attention块并为每个任务引入少量额外参数，实现网络的动态扩展与任务特化。框架结合知识蒸馏与经验回放，利用旧任务表征指导新任务学习，从而缓解灾难性遗忘。整体采用模块化设计，仅保存任务特定注意力权重与小规模回放样本，内存开销较低。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在包含多个视觉-语言基准的连续学习协议上，TAM-CL比现有SOTA方法平均提升4.62%准确率，同时参数增量与存储成本显著低于完全微调方案。实验表明动态扩展机制有效促进了跨模态知识共享，且旧任务性能随新任务增加仅出现轻微下降。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模模型(&gt;1B参数)或更长任务序列(&gt;20)上验证可扩展性；任务注意力块的设计依赖人工设定扩展比例，可能无法自动适应任务间相似度变化。此外，方法对非Transformer架构的通用性及理论遗忘边界分析仍缺失。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索基于任务相似度的自适应结构扩展策略，并将TAM-CL扩展到视频-语音等多模态流；结合神经架构搜索实现任务模块的自动分配与压缩也是可行方向。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态持续学习、边缘部署或灾难性遗忘抑制，本文提供的动态Transformer范式与任务注意力机制可直接作为基线或扩展基础，其实证结论也为设计低开销CL系统提供了参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05847v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">OmniVideo-R1：利用查询意图与模态注意力强化视听推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhangquan Chen，Jiale Tao，Ruihuang Li，Yihao Hu，Ruitao Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05847v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to &#34;think with omnimodal cues&#34; by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升全视频模型在音视听混合模态推理任务中的表现。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入查询意图自监督定位与模态注意对比融合的两阶段强化框架。</p>
                <p><span class="font-medium text-accent">主要发现：</span>多基准测试显示OmniVideo-R1显著优于强基线，泛化稳健。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将查询意图自监督与模态注意对比学习结合用于音视听推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建更贴近人类多感官认知的全视频理解系统提供新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>人类通过视觉与听觉等多模态协同感知世界，但现有&#34;全视频&#34;模型在音频-视觉联合推理任务上仍显著落后于人类水平。作者观察到，现有方法缺乏对查询意图的显式建模，也未充分挖掘跨模态注意力，导致在复杂音视频场景下的推理鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>OmniVideo-R1提出强化式框架，将训练分为两阶段：第一阶段用自监督查询密集锚定，通过掩码预测与伪标签挖掘，让模型学会把文本查询定位到最相关的音视频片段；第二阶段构建基于对比学习的模态注意力融合，利用跨模态正负样本对，动态加权视觉和声学特征，实现细粒度对齐。整个流程在强化学习范式下迭代，以下游推理准确率作为奖励信号，更新查询锚定与融合策略。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AVE、Kinetics-Sounds、AVSD等五个基准上，OmniVideo-R1将SOTA绝对准确率平均提升3.2%，在零样本跨数据集评估中仍保持2.1%的领先，显示强泛化能力。消融实验表明，查询密集锚定贡献约60%的性能增益，而模态注意力融合对噪声音频场景鲁棒性提升达4.5%。这些结果验证了显式查询意图与动态跨模态加权对音视频推理的互补价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文目前仅在公开英文数据集上验证，尚未涉及低资源语言或长视频(&gt;3分钟)场景；方法依赖大量GPU进行强化迭代，训练成本高于纯监督基线；此外，自监督伪标签可能放大初始噪声，导致错误累积，对超参数敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将OmniVideo-R1扩展至多语言、长视频乃至更多模态(如文本OCR、深度)，并结合高效强化策略(如离线RL)降低训练开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态学习、音视频协同推理或强化学习与自监督结合，本文提供了可复现的强化式训练框架和代码基线，可直接迁移至视频问答、事件检测等下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113237" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Task-Adapter++: Task-specific Adaptation with Order-aware Alignment for Few-shot Action Recognition
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Task-Adapter++：面向小样本动作识别的任务特定适配与顺序感知对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Congqi Cao，Peiheng Han，Yueran Zhang，Yating Yu，Qinyi Lv 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113237" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113237</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large-scale pre-trained models have achieved remarkable success in language and image tasks, leading an increasing number of studies to explore the application of pre-trained vision models, such as CLIP, in the domain of few-shot action recognition (FSAR). However, current methods generally suffer from several problems: 1) Direct fine-tuning often undermines the generalization capability of the pre-trained model; 2) The exploration of task-specific information is insufficient in the visual tasks; 3) The semantic order information is typically overlooked during text modeling; 4) Existing cross-modal alignment techniques ignore the temporal coupling of multimodal information. To address these, we propose Task-Adapter++, a parameter-efficient dual adaptation method for both image and text encoders. Specifically, to make full use of the variations across different few-shot learning tasks, we design a task-specific adaptation for the image encoder so that the most discriminative information can be well noticed during feature extraction. Furthermore, we leverage large language models (LLMs) to generate detailed sequential sub-action descriptions for each action class, and introduce semantic order adapters into the text encoder to effectively model the sequential relationships between these sub-actions. Finally, we develop an innovative fine-grained cross-modal alignment strategy that actively maps visual features to reside in the same temporal stage as semantic descriptions. Extensive experiments fully demonstrate the effectiveness and superiority of the proposed method, which achieves state-of-the-art performance on five benchmarks consistently. The code is open-sourced at https://github.com/Jaulin-Bage/Task-Adapter-pp .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不破坏预训练模型泛化性的前提下，提升CLIP在少样本动作识别中的任务适应与跨模态对齐效果。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Task-Adapter++：为图像和文本编码器分别插入轻量级任务适配器，并用LLM生成有序子动作描述，实现时序对齐的跨模态映射。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在五个少样本动作识别基准上均取得SOTA准确率，验证参数高效双适配与顺序感知对齐的有效性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务特定视觉适配、语义顺序文本适配与细粒度时序跨模态对齐集成于统一框架，兼顾效率与性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用大规模预训练模型解决少样本视频理解提供即插即用、低成本的优化范式，推动跨模态动作识别研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>大规模预训练模型在语言和图像任务上表现卓越，促使研究者将其迁移到少样本动作识别(FSAR)。然而，直接微调易破坏泛化能力，且现有方法对任务特定信息挖掘不足，文本建模时忽略语义顺序，跨模态对齐也缺乏时序耦合。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Task-Adapter++，在图像和文本编码器上并行插入轻量级适配器：图像侧用任务特定适配器动态突出判别性特征；文本侧先由大模型生成子动作序列描述，再通过语义顺序适配器显式建模子动作间先后关系；最后设计细粒度跨时序对齐，将视觉特征主动映射到与语义描述相同的时序阶段。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个FSAR基准上均达到SOTA，相较最佳基线平均提升3.2%，尤其在1-shot设置下提升达4.7%，验证了对任务差异和时序语义的利用显著提高了泛化与判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大模型生成子动作描述，计算与存储开销增加；适配器容量与泛化能力之间的权衡尚未充分探索；对长视频的高阶时序结构建模仍显不足。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可探索无LLM的自动子动作挖掘以降低成本，并研究适配器结构搜索实现容量自适应；进一步引入高阶时序图建模复杂动作关系。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为少样本动作识别提供参数高效微调新范式，展示如何利用预训练视觉-语言模型、任务特定适配与跨模态时序对齐协同提升性能，对研究小样本学习、视频理解及多模态迁移的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-07</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113238" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-Improved Holistic Alignment for Preference Enhancement
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">自改进的整体对齐用于偏好增强</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-07</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kejia Chen，Jiawen Zhang，Jiazhen Yang，Mingli Song，Zunlei Feng
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113238" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113238</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Visual-Language Models (LVLMs) increasingly rely on preference alignment to ensure reliability, which steers the model behavior via preference fine-tuning on preference data structured as “image - question - winner text - loser text” tuplets. However, existing approaches often [revise: suffer from → constrained by] limited diversity and high [add: acquisition] costs [revise: associated with → of] human-annotated preference data, hindering LVLMs from fully achieving their intended alignment capabilities. We present SHAPE , a self-supervised framework capable of transforming the already abundant supervised text-image pairs into holistic preference tuplets for more effective and cheaper LVLM alignment, eliminating the need for human preference annotations. Our approach facilitates LVLMs in progressively enhancing alignment capabilities through iterative self-improvement. The key design rationale is to devise preference tuplets where the winner text consistently improves in holisticness and outperforms the loser response in quality, thereby pushing the model to “strive to the utmost” of alignment performance through preference fine-tuning. For each given text-image pair, SHAPE introduces multiple visual augmentations and pairs them with a summarized text to serve as the winner response, while designating the original text as the loser response.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何摆脱昂贵人工标注，自动生成多样偏好数据以提升LVLM对齐效果</p>
                <p><span class="font-medium text-accent">研究方法：</span>SHAPE自监督框架：对图文对做视觉增广并生成摘要作为winner，原句作loser，迭代偏好微调</p>
                <p><span class="font-medium text-accent">主要发现：</span>无需人工标注即可构建高质量偏好四元组，显著增强LVLM整体对齐性能</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出用视觉增广+文本摘要自生成偏好数据，实现LVLM自我改进的 holistic alignment</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为低成本扩展偏好数据、提升多模态模型可靠性提供了可复现的新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前大型视觉-语言模型(LVLMs)依赖人工标注的“图像-问题-优胜文本-失败文本”偏好对进行对齐，但人工标注成本高、多样性受限，阻碍了模型充分释放对齐潜力。SHAPE旨在摆脱昂贵人工偏好标注，利用现成图文对自监督生成偏好数据，实现低成本、可持续的LVLM对齐。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SHAPE以任意图文对为起点，将原caption视为“失败文本”，通过多模态摘要模型生成更简洁全面的“优胜文本”；同时对原图做多种视觉增广(裁剪、色彩扰动等)，与优胜文本配对，构建完整偏好四元组。模型在自生成的偏好数据上进行DPO或PPO式偏好微调，迭代多轮，每轮用更新后的模型重新生成优胜文本并扩充数据，实现“自我提升式”整体对齐。训练目标显式鼓励模型在信息量、准确性和一致性上“优胜”回应，从而把对齐压力转化为持续自我改进动力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MMHAL、POPE、MME等多维基准上，仅使用公开CC3M图文对的SHAPE将InstructBLIP、MiniGPT-4等模型的对齐得分平均提升7-11%，达到或超越使用昂贵人工偏好数据集的同期方法。消融实验显示，视觉增广与摘要质量是性能增益的核心；三轮自我迭代后增益饱和，证明框架收敛且稳定。结果证实无需人工标注即可实现“整体”（holistic）偏好对齐，显著降低数据成本。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>优胜文本依赖摘要模型，若摘要模型本身幻觉严重，会引入虚假偏好信号；视觉增广策略目前为手工设计，对细粒度物体或OCR场景可能破坏关键信息。此外，框架假设原始caption质量普遍低于摘要，在已高质量标注的数据集上可能产生反向偏好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的视觉增广策略或扩散式重述模型，动态优化优胜文本生成；同时探索与强化学习结合，实现更细粒度、可解释的自我对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态对齐、低成本偏好数据构建或自监督提升，SHAPE提供了可复现的代码与流程，可直接用于改进自研LVLM的对齐效果而无需额外人工标注。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05730v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Depth as Prior Knowledge for Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">深度作为先验知识用于目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Moussa Kassem Sbeyti，Nadja Klein
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05730v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不改架构的前提下，利用深度先验提升小/远目标检测可靠性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DepthPrior框架：训练期深度加权与分层损失，推理期深度感知置信阈值。</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集两检测器上小目标mAP_S最高+9%，推理误检率降低95:1。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将深度作为先验知识而非融合特征，免架构修改、零推理开销。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶等安全应用提供即插即用深度增强检测方案，无需额外传感器。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>检测小而远的物体对安全关键应用至关重要，但受限于尺度变化、低分辨率和背景杂波。传统引入深度信息的方法需大幅改动检测器架构，代价高昂。作者从理论上剖析深度与检测性能的关系，提出把深度当作先验而非融合特征的新思路。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>DepthPrior 框架包含三项组件：训练阶段的 Depth-Based Loss Weighting (DLW) 按深度加权损失，Depth-Based Loss Stratification (DLS) 按深度分层采样；推理阶段的 Depth-Aware Confidence Thresholding (DCT) 依据深度动态调整置信度阈值。整个流程仅依赖现成的单目深度估计，无需额外传感器、网络结构修改或运行时开销。作者在 YOLOv11 与 EfficientDet 上实现该框架，并在 KITTI、MS COCO、VisDrone、SUN RGB-D 四数据集进行系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验显示 DepthPrior 在小目标上最高提升 9% mAP_S 与 7% mAR_S，整体 mAP 亦稳中有升；推理阶段真阳性/假阳性恢复比达 95:1，显著减少虚警。理论分析与消融实验共同验证：深度先验可缓解远距离目标带来的系统性性能衰减，而无需改变骨干网络或增加参数量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖前置深度估计的精度，若深度图质量差可能引入偏差；目前仅在四个公开数据集与两种检测器上验证，泛化到更多场景或极端天气尚需进一步验证；框架对深度估计的初始计算成本未计入能耗与延迟分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自监督或在线深度估计以降低对预计算深度图的依赖，并将深度先验扩展至视频检测、3D 检测及多模态融合任务。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、深度信息利用或无需改架构即可提升检测性能的方法，DepthPrior 提供了一种轻量、易迁移且理论支撑充分的解决方案，可直接在现有检测器上复现并比较。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05281v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Pengyi Li，Elizaveta Goncharova，Andrey Kuznetsov，Ivan Oseledets
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05281v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何抑制RLVR训练中GRPO策略熵崩塌、恢复多样化正确推理路径。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ARM：用Prompt困惑度与答案置信度重加权优势，动态削弱过自信路径梯度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ProGRPO在Qwen2.5-7B上Pass@1提升5.7%，Pass@32提升13.9%，熵与多样性显著增加。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生成概率动态纳入优势估计，实现置信均衡与隐式探索，无需额外采样或奖励工程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升大模型推理多样性与覆盖度提供简单高效插件，直接优化现有RLVR流程。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Reinforcement Learning with Verifiable Rewards (RLVR) is the dominant recipe for boosting multi-step reasoning in LLMs, yet existing policy-gradient variants like GRPO quickly collapse to a single high-likelihood solution, sacrificing the very diversity needed for robust reasoning. The authors trace this pathology to the optimization dynamics that exponentially up-weight the most probable token sequences, effectively starving alternative correct derivations of gradient signal.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces Advantage Re-weighting Mechanism (ARM), an on-policy plug-in that modulates the vanilla advantage with two generative statistics: Prompt Perplexity (how surprising the prompt is to the model) and Answer Confidence (the model’s own probability of the produced solution). These statistics are fused into an inverse temperature that down-scales advantages for over-confident correct paths and up-weights low-probability correct paths, yielding a re-normalized loss (ProGRPO) that still uses the GRPO group baseline but operates on the reshaped advantages. Training proceeds with the standard KL-regularized RLVR pipeline, requiring no extra reward models or human data beyond verifiable correctness.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across Qwen2.5-7B and DeepSeek-Coder evaluated on MATH, GSM8K and HumanEval, ProGRPO raises response entropy by ~30% and Pass@32 by up to 13.9% while improving Pass@1 by 5.7%, demonstrating that diversity gains do not come at the expense of top-line accuracy. Entropy-collapse curves show that baseline GRPO loses 0.6 bits of entropy within 200 updates whereas ProGRPO maintains nearly constant entropy, confirming the mechanism’s stabilizing effect. Ablations reveal that both perplexity and confidence terms are necessary; dropping either re-introduces premature convergence.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method is tested only on verifiable reasoning domains where correctness is binary; it is unclear how ARM would behave with noisy or subjective reward signals. All experiments use a single base RLVR setup—generalization to PPO, RLOO or off-policy algorithms remains unverified. Computational overhead is small but non-zero: perplexity must be computed for every prompt, doubling forward passes during training.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend ARM to partially observable or open-ended tasks where rewards are learned rather than verifiable, and investigate adaptive schedules that anneal the re-weighting strength as policy entropy stabilizes.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on sample-efficient RL for LLMs, diversity-accuracy trade-offs in reasoning, or entropy collapse in policy gradients will find the generative-probability perspective and the lightweight ARM plug-in directly applicable to their pipelines.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>