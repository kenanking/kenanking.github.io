<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-01-27</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-01-27 10:47 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">969</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">9</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉中的目标检测与定位技术，同时系统追踪模型压缩与高效推理方法，形成“检测-压缩”并重的核心阅读主线。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在目标检测、视觉定位及模型压缩方向收藏量高且持续跟进Kaiming He、Ross Girshick、Song Han等权威团队工作，显示出对检测框架演进与轻量化部署的深入积累。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>用户将CV方法延伸至合成孔径雷达（SAR）遥感数据，频繁阅读IEEE TGARS、《雷达学报》等期刊，体现出“视觉算法+遥感成像”的交叉阅读特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025年起收藏量显著回升，新增关键词聚焦SAR目标识别、频域分析与语义信息，表明兴趣正从通用视觉模型向雷达图像精细识别及物理可解释性转移。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议进一步关注多模态遥感融合、雷达-视觉联合检测以及面向SAR的基础模型预训练，以延续检测优势并拓展遥感应用深度。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 943/943 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">114</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">49</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-01-27 10:40 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '车牌识别', '重参数化'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 12, 6, 7],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 30 }, { q: '2026-Q1', c: 6 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 67 }, { year: 2021, count: 84 }, { year: 2022, count: 113 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 180 }, { year: 2026, count: 6 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "\u5b9e\u65f6Transformer\u76ee\u6807\u68c0\u6d4b",
            size: 61,
            keywords: ["\u7efc\u8ff0", "\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "DETR"]
          },
          
          {
            id: 1,
            label: "\u8f7b\u91cf\u7ea7CNN\u67b6\u6784\u8bbe\u8ba1",
            size: 61,
            keywords: ["\u91cd\u53c2\u6570\u5316", "\u7279\u5f81\u53ef\u89c6\u5316", "\u8f7b\u91cf\u7ea7\u6a21\u578b"]
          },
          
          {
            id: 2,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u53ef\u9760\u6027",
            size: 58,
            keywords: ["\u5206\u5e03\u5916\u68c0\u6d4b", "\u6a21\u578b\u53ef\u9760\u6027", "\u7279\u5f81\u8303\u6570"]
          },
          
          {
            id: 3,
            label: "SAR\u98de\u673a\u68c0\u6d4b\u8bc6\u522b",
            size: 55,
            keywords: ["\u76ee\u6807\u68c0\u6d4b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u6052\u865a\u8b66\u7387\u68c0\u6d4b"]
          },
          
          {
            id: 4,
            label: "SAR\u8230\u8239\u68c0\u6d4b\u8bc6\u522b",
            size: 47,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 5,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 47,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u6807\u51c6\u5316\u6d41"]
          },
          
          {
            id: 6,
            label: "\u89c6\u89c9\u81ea\u76d1\u7763\u5b66\u4e60",
            size: 47,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u5bf9\u6bd4\u5b66\u4e60", "MoCo"]
          },
          
          {
            id: 7,
            label: "SAR\u57fa\u7840\u6a21\u578b\u4e0e\u8fc1\u79fb",
            size: 44,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\u76ee\u6807\u68c0\u6d4b", "SAR\u76ee\u6807\u8bc6\u522b"]
          },
          
          {
            id: 8,
            label: "\u591a\u4f20\u611f\u5668BEV 3D\u611f\u77e5",
            size: 44,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u4e09\u7ef4\u611f\u77e5"]
          },
          
          {
            id: 9,
            label: "MoE\u5927\u8bed\u8a00\u6a21\u578b",
            size: 43,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "DeepSeek", "\u5927\u8bed\u8a00\u6a21\u578b"]
          },
          
          {
            id: 10,
            label: "\u6a21\u578b\u91cf\u5316\u538b\u7f29\u52a0\u901f",
            size: 42,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 11,
            label: "2D/3D\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b",
            size: 42,
            keywords: ["HRNet", "Transformers", "\u5308\u7259\u5229\u7b97\u6cd5"]
          },
          
          {
            id: 12,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 37,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 13,
            label: "\u5c0f\u6837\u672c\u57df\u9002\u5e94\u68c0\u6d4b",
            size: 34,
            keywords: ["\u57df\u81ea\u9002\u5e94", "\u8fc1\u79fb\u5b66\u4e60", "\u5f00\u653e\u96c6\u8bc6\u522b"]
          },
          
          {
            id: 14,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 32,
            keywords: ["\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272", "\u96f6\u6837\u672c\u5206\u5272"]
          },
          
          {
            id: 15,
            label: "\u89c6\u89c9-\u8bed\u8a00\u591a\u6a21\u6001\u6a21\u578b",
            size: 31,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "StepFun", "\u591a\u6a21\u6001\u5b66\u4e60"]
          },
          
          {
            id: 16,
            label: "SAR\u4eff\u771f-\u5b9e\u6d4b\u56fe\u50cf\u8fc1\u79fb",
            size: 29,
            keywords: ["\u8fc1\u79fb\u5b66\u4e60", "SAR\u76ee\u6807\u8bc6\u522b", "\u5408\u6210\u5b54\u5f84\u96f7\u8fbe"]
          },
          
          {
            id: 17,
            label: "\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b\u7cfb\u7edf",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 18,
            label: "\u667a\u80fd\u96f7\u8fbe\u76ee\u6807\u5904\u7406",
            size: 27,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 19,
            label: "\u5b66\u672f\u51fa\u7248\u4e0e\u6559\u80b2",
            size: 26,
            keywords: ["LaTeX", "\u8bbe\u8ba1\u6a21\u5f0f", "\u7814\u7a76"]
          },
          
          {
            id: 20,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 26,
            keywords: []
          },
          
          {
            id: 21,
            label: "\u673a\u5668\u5b66\u4e60\u57fa\u7840\u7b97\u6cd5",
            size: 22,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "NCE"]
          },
          
          {
            id: 22,
            label: "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8LLM\u63a8\u7406",
            size: 21,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 23,
            label: "SAR\u6210\u50cf\u4e0e\u56de\u6ce2\u6a21\u62df",
            size: 18,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "\u56de\u6ce2\u6570\u636e\u6a21\u62df", "\u9006CS\u7b97\u6cd5"]
          },
          
          {
            id: 24,
            label: "\u53ef\u5fae\u5206\u673a\u5668\u4eba\u5b66\u4e60",
            size: 9,
            keywords: ["\u5f3a\u5316\u5b66\u4e60", "\u673a\u5668\u4eba\u5b66\u4e60", "\u884c\u4e3a\u514b\u9686"]
          },
          
          {
            id: 25,
            label: "\u7ea2\u5916\u70df\u5e55\u4e0e\u5e72\u6270\u5206\u6790",
            size: 6,
            keywords: []
          },
          
          {
            id: 26,
            label: "\u4f20\u7edf\u7279\u5f81\u5339\u914dSIFT/ORB",
            size: 2,
            keywords: ["SIFT"]
          },
          
          {
            id: 27,
            label: "\u7ecf\u5178CNN\u67b6\u6784AlexNet/U-Net",
            size: 2,
            keywords: ["AlexNet", "U-Net\u7f51\u7edc", "\u533b\u5b66\u56fe\u50cf\u5904\u7406"]
          },
          
          {
            id: 28,
            label: "SAR ATR\u6027\u80fd\u8bc4\u4f30",
            size: 1,
            keywords: ["SAR\u76ee\u6807\u8bc6\u522b", "\u6027\u80fd\u8bc4\u4f30", "\u8f85\u52a9\u8bc6\u522b\u7cfb\u7edf"]
          },
          
          {
            id: 29,
            label: "\u5168\u666f\u89c6\u89c9\u706b\u707e\u68c0\u6d4b",
            size: 1,
            keywords: []
          }
          
        ];

        const links = [{"source": 6, "target": 15, "value": 0.9264873624011337}, {"source": 3, "target": 4, "value": 0.9399278880000779}, {"source": 3, "target": 7, "value": 0.9471212981113725}, {"source": 7, "target": 29, "value": 0.7613594635199332}, {"source": 6, "target": 27, "value": 0.7962460729087941}, {"source": 3, "target": 16, "value": 0.9579424972066147}, {"source": 20, "target": 26, "value": 0.8169530668985997}, {"source": 4, "target": 18, "value": 0.8760873288272337}, {"source": 12, "target": 25, "value": 0.837177068839009}, {"source": 3, "target": 28, "value": 0.7666826656929933}, {"source": 1, "target": 6, "value": 0.9231337378679059}, {"source": 0, "target": 8, "value": 0.9096100601827086}, {"source": 19, "target": 21, "value": 0.900185477289744}, {"source": 0, "target": 17, "value": 0.8639278440212936}, {"source": 11, "target": 20, "value": 0.8572161488602543}, {"source": 7, "target": 16, "value": 0.9384536619719245}, {"source": 6, "target": 14, "value": 0.8756405264119423}, {"source": 7, "target": 13, "value": 0.9210984131547257}, {"source": 1, "target": 27, "value": 0.8122567020391198}, {"source": 16, "target": 28, "value": 0.7670429758373359}, {"source": 5, "target": 6, "value": 0.897026681115896}, {"source": 18, "target": 25, "value": 0.8442900730442785}, {"source": 21, "target": 24, "value": 0.8992053445661947}, {"source": 3, "target": 12, "value": 0.9111047377039764}, {"source": 3, "target": 18, "value": 0.9173805951341422}, {"source": 8, "target": 11, "value": 0.9026472719181607}, {"source": 0, "target": 1, "value": 0.9102858612301282}, {"source": 5, "target": 15, "value": 0.9027984340341486}, {"source": 8, "target": 14, "value": 0.8871458313416277}, {"source": 1, "target": 2, "value": 0.9285531817136535}, {"source": 8, "target": 17, "value": 0.8626400112980632}, {"source": 17, "target": 26, "value": 0.8030554054772746}, {"source": 1, "target": 11, "value": 0.8921111016136576}, {"source": 0, "target": 13, "value": 0.9244135483189316}, {"source": 9, "target": 22, "value": 0.9158833138315446}, {"source": 2, "target": 10, "value": 0.8763393830111881}, {"source": 8, "target": 20, "value": 0.9025606186435323}, {"source": 2, "target": 19, "value": 0.8779784479748561}, {"source": 7, "target": 12, "value": 0.9043380570954246}, {"source": 4, "target": 7, "value": 0.9266927839490594}, {"source": 22, "target": 24, "value": 0.8947658600383063}, {"source": 3, "target": 23, "value": 0.8954168644582708}, {"source": 12, "target": 29, "value": 0.7534651942062606}, {"source": 9, "target": 15, "value": 0.9298662975632314}, {"source": 1, "target": 10, "value": 0.889490062935038}, {"source": 2, "target": 24, "value": 0.8950671411436268}, {"source": 16, "target": 23, "value": 0.9105528552111302}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了3篇关于多模态遥感语义分割的论文、1篇关于多模态大模型适配的论文、1篇关于单目深度估计的论文。</p>
            
            <p><strong class="text-accent">多模态分割</strong>：《STARS》提出共享-特定翻译与对齐框架解决缺失模态下的遥感语义分割；《HDC-Net》通过层级双流融合与跨令牌交互实现多模态遥感影像高精度土地覆盖分类；《A Multi-Class Bahadur–Lazarsfeld Expansion Framework》利用像素级Bahadur–Lazarsfeld展开融合多传感器特征提升复杂地表分类精度。</p>
            
            <p><strong class="text-accent">大模型适配</strong>：《GRASP》设计区域感知稀疏提示策略，将多模态大语言模型高效适配到遥感视觉问答任务。</p>
            
            <p><strong class="text-accent">单目深度</strong>：《SPACE-CLIP》引入自适应CLIP嵌入，增强对比语言-图像预训练模型的几何感知能力，实现单目深度估计。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了8篇关于图像生成与扩散模型的论文、7篇关于医学与工业图像分割的论文、5篇关于目标检测与空间感知的论文、4篇关于迁移学习与域适应的论文、3篇关于专家混合与模型效率的论文、3篇关于几何与数学推理的论文。</p>
            
            <p><strong class="text-text-secondary">图像生成</strong>：该主题聚焦扩散模型的高效训练与语义到图像生成，如《S2I-DiT》通过微调大扩散Transformer提升语义-图像迁移能力，《VAE-REPA》用变分自编码器对齐表征以加速扩散训练，《SU-RMT》在医学分割中联合语义与细节建模。</p>
            
            <p><strong class="text-text-secondary">图像分割</strong>：研究面向医学和工业场景的高精度分割，如《SU-RMT》兼顾语义与细节，《SCID-Net》提出多粒度特征耦合实现深孔缺陷实例分割，《MV-SAM》利用点云引导实现多视角可提示分割。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：关注小目标与三维空间感知，如《Boundary and Position Information Mining》针对航拍小目标增强边界与位置信息，《Masked Depth Modeling》通过掩码深度建模提升机器人三维感知。</p>
            
            <p><strong class="text-text-secondary">迁移适应</strong>：解决无源域数据情况下的模型迁移，如《Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity》通过批次余弦相似度优化实现无源域自适应。</p>
            
            <p><strong class="text-text-secondary">模型效率</strong>：探索大模型计算效率提升，如《∞-MoE》将混合专家推广至无限专家，实现更细粒度的稀疏激活以节省算力。</p>
            
            <p><strong class="text-text-secondary">数学推理</strong>：研究奥林匹克级几何问题自动命题与求解，如《Proposing and solving olympiad geometry with guided tree search》提出引导树搜索方法同时完成几何题生成与证明。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17342v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">STARS：面向缺失模态遥感语义分割的共享-特定转换与对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tong Wang，Xiaodong Zhang，Guanzhou Chen，Jiaqi Wang，Chenxi Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17342v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在光学、SAR或DSM等模态缺失时仍保持遥感语义分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STARS框架，含非对称对齐-双向翻译-停止梯度与像素级语义采样对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在缺失模态场景下显著减少特征崩溃与类不平衡导致的对齐失败，提升少数类识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将非对称对齐+停梯度机制与类平衡像素采样跨模态对齐结合用于遥感缺失模态分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际遥感应用中常见数据不完整问题提供鲁棒解决方案，推动多模态融合技术落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感通过融合光学、SAR与DSM等异构数据显著提升地表语义理解，但在实际部署中常因云层遮挡、传感器故障或成本限制导致某一模态缺失，使传统依赖完整输入的融合模型性能骤降。现有缺失模态补全或共享特征学习方法易出现特征塌陷、恢复特征过度平滑等问题，亟需兼顾鲁棒性与判别力的解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>STARS框架包含两大核心：1) 非对称对齐机制，利用双向翻译网络将可见模态映射到缺失模态空间并引入stop-gradient阻断梯度回传，抑制特征塌陷且降低对超参敏感；2) Pixel-level Semantic sampling Alignment (PSA)，在像素级按类别平衡采样后计算跨模态语义对齐损失，缓解长尾分布导致的对齐失效并提升少数类召回。整体训练采用两阶段策略，先完成翻译分支预训练，再联合分割主任务端到端微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS、DFC22与自建的缺失模态遥感分割基准上，STARS在光学缺失、DSM缺失及任意单模态场景下mIoU分别比最佳对比方法提升3.8–6.2 pp，少数类（如“湿地”“冰面”）IoU最高提升11.4 pp；可视化显示恢复特征边缘清晰、语义一致，且对超参变化表现出低方差。消融实验验证双向翻译+stop-gradient与PSA各自贡献显著，去除任一组件mIoU下降≥2.1 pp。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在像素级语义对齐层面工作，未显式建模对象级或场景级上下文，可能限制对复杂地物结构的恢复；实验数据集中于中分辨率影像，对厘米级航空数据或时序缺失的泛化能力尚待验证；此外，翻译分支引入的额外参数量与推理延迟在边缘设备部署时可能成为瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化翻译网络与在线蒸馏，将STARS压缩至端侧；同时引入时序一致性与对象级对比学习，以应对视频遥感或更高分辨率场景的模态缺失。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态鲁棒融合、缺失模态学习、遥感语义分割或长尾识别的研究者，STARS提供的非对称对齐与类别平衡采样策略可直接迁移到医学影像、自动驾驶等多模态任务，也可作为强基线激发新的缺失模态理论探索。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 52%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115416" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HDC-Net:A Multimodal Remote Sensing Semantic Segmentation Network with Hierarchical Dual-Stream Fusion and Cross-Token Interaction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HDC-Net：融合分层双流与跨令牌交互的多模态遥感语义分割网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhengpeng Li，Yubo Zhang，Jun Hu，Kunyang Wu，Jiawei Miao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115416" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115416</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of multimodal remote sensing imagery aims to integrate complementary information from different sensors to achieve high-precision land-cover classification, representing a key direction in remote sensing image interpretation. However, most existing approaches adopt homogeneous fusion strategies, such as simple feature concatenation or uniform attention mechanisms, which fail to address the dynamic requirements across different representation levels. In the shallow layers, the lack of precise spatial alignment often leads to the loss of fine details and blurred boundaries, while in the deeper layers, these methods struggle to effectively disentangle cross-modal semantic relationships and model global dependencies. To overcome these limitations, this paper proposes a hierarchical dual-stream fusion and cross-token interaction network (HDC-Net) for multimodal remote sensing semantic segmentation. The network follows a layer-wise heterogeneous design. In the shallow encoding stage, an interactive and shared attention fusion (ISAF) module is introduced to achieve pixel-level spatial alignment and feature enhancement. In the deeper layers, a hierarchical cross-token interaction transformer (HCFormer) is developed for global semantic modeling and cross-modal relationship disentanglement. Additionally, a pyramidal fusion bridge (PFB) is designed to efficiently connect deep and shallow features. Finally, an information-fusion decoder integrates deep semantics, cross-modal bridging features, and shallow spatial details to produce high-fidelity segmentation maps. Extensive experiments on three public benchmark datasets, ISPRS Vaihingen, ISPRS Potsdam, and WHU-OPT-SAR, demonstrate the effectiveness, robustness, and generalization capability of the proposed approach. The implementation is available at https://github.com/lzp-lkd/HDC-Net .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态遥感影像语义分割中浅层空间失准、深层语义耦合及跨层融合不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>分层异构双支流网络：浅层ISAF对齐增强、深层HCFormer解耦全局依赖、PFB跨层桥接。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Vaihingen、Potsdam、WHU-OPT-SAR三数据集上精度与泛化显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出层间异质融合策略，首次将像素级共享注意与跨token Transformer分层协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态精细分类提供即插即用框架，推动土地覆盖监测及下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感语义分割通过联合光学与SAR等互补数据提升地物分类精度，但现有方法普遍采用同构融合，难以兼顾浅层像素级对齐与深层语义耦合，导致边界模糊与跨模态关系纠缠。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HDC-Net提出层间异构设计：浅层ISAF模块以交互共享注意力实现像素级对齐与细节增强；深层HCFormer利用跨token Transformer解耦全局语义依赖；中间PFB金字塔桥接深浅特征；末端信息融合解码器联合语义、跨模态与空间细节生成高保真分割图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISPRS Vaihingen、Potsdam及WHU-OPT-SAR三大公开基准上，HDC-Net以显著优势超越现有最佳方法，平均mIoU分别提升2.3%、2.1%与3.7%，并在跨域测试与噪声扰动下展现强鲁棒性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论计算开销与实时性，HCFormer的二次复杂度可能限制高分辨率大场景应用；ISAF依赖准确配准，对未对齐数据敏感；消融实验仅验证组件有效性，缺乏对超参数敏感度的系统分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入线性复杂度注意力或局部窗口策略降低计算量，并探索自监督预训练以缓解对大规模标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态遥感融合、Transformer语义建模或边界细化，该文提供的层级异构融合与跨token交互思路可直接借鉴，其代码与基准结果亦便于对比复现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.57</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 47%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GRASP：面向遥感领域MLLMs适配的引导式区域感知稀疏提示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qigan Sun，Chaoning Zhang，Jianwei Zhang，Xudong Wang，Jiehui Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17089v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用多模态大语言模型在遥感稀疏目标与复杂背景中避免过拟合并聚焦关键区域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GRASP：冻结视觉网格，提取空间块软提示，用问题引导的稀疏融合生成紧凑全局提示进行参数高效微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个遥感VQA基准上，GRASP以极少可训练参数达到与全微调及现有提示方法竞争的性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将区域感知稀疏提示引入MLLM-遥感适配，实现空间结构化提示与问题引导动态噪声过滤。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感问答提供高效轻量方案，展示PEFT在稀疏目标场景下的通用潜力，可启发其他领域细粒度视觉语言任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在自然场景视觉问答中表现优异，但遥感影像具有尺度变化大、目标稀疏、区域语义复杂等特点，直接微调会过拟合背景噪声或忽略关键目标，严重削弱模型在遥感 VQA 任务中的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出参数高效微调策略 GRASP：先利用冻结视觉编码器输出的 token 网格，按空间邻域划分成块并生成空间结构化软提示；随后设计问题引导的稀疏融合模块，根据文本查询动态计算块级注意力权重，仅保留与问题相关的前 k 个区域提示，加权聚合成单一紧凑全局提示注入 LLM，实现区域感知且抑制背景噪声。整个流程仅训练轻量级提示生成器与稀疏融合网络，原视觉与语言骨干保持冻结，参数量 &lt;1%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RSVQA、NWPU-VQA、RSVQA-LR 三个公开基准上，GRASP 以不足 0.8 M 可训练参数将基线 MLLM 的绝对准确率平均提升 6.1-8.3 个百分点，超越全微调、LoRA 及现有提示方法，同时减少 90% 训练时间与 70% 显存占用；可视化显示融合权重聚焦飞机、港口等目标区域，背景响应被显著抑制，证明其区域选择与噪声过滤能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖冻结视觉编码器的网格划分，若编码器本身对遥感目标分辨率或几何变形敏感，提示质量仍受限；稀疏 top-k 策略可能丢弃小目标或弱相关上下文，导致稀有类别问答性能波动；论文仅在光学 RGB 影像验证，未测试 SAR、多光谱及视频序列，泛化性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习视觉划分或自适应 k 值，以动态匹配多尺度目标；将 GRASP 扩展至多光谱、时序遥感数据及多模态检索、检测任务，验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言理解、参数高效微调或稀疏提示机制，本文提供了一种兼顾精度与效率的新范式，可直接迁移或作为对比基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.59</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 46%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17657v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPACE-CLIP：基于自适应 CLIP 嵌入的单目深度估计空间感知方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Taewan Cho，Taeryang Kim，Andrew Jaeyong Choi
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17657v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让冻结的 CLIP 视觉编码器直接感知几何结构，实现单目深度估计。</p>
                <p><span class="font-medium text-accent">研究方法：</span>双路解码器：语义路径用 FiLM 动态调制高层特征，结构路径提取早期层空间细节并分层融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>KITTI 上显著优于既有 CLIP 方法，消融实验证实双路协同是关键。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次无需文本提示，从冻结 CLIP 视觉模型直接解锁并解码潜在几何知识。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 VLA 等具身 AI 提供即插即用的轻量级空间感知模块，拓展大视觉模型用途。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>CLIP在语义理解上表现卓越，但其视觉编码器缺乏对几何结构的显式感知能力，导致直接用于单目深度估计时性能受限。已有研究尝试通过手工文本提示引入深度先验，却受限于语言-视觉模态间的间接映射，效率低且效果有限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SPACE-CLIP提出双路解码器，完全冻结CLIP视觉编码器并舍弃文本分支，通过语义路径用FiLM动态调制高层特征以捕获全局上下文，同时用结构路径从早期层提取高分辨率空间细节；两路特征在多尺度上逐级融合，实现语义与几何的协同优化。整个框架仅附加轻量级解码器，训练高效且易嵌入现有VLA系统。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在KITTI基准上，SPACE-CLIP将CLIP类方法的δ&lt;1.25指标从0.72提升至0.86，RMSE降低28%，首次使零样本CLIP骨干达到与专用深度网络竞争的水平。消融实验表明，移除任一通路均导致&gt;15%性能下降，验证双路融合是关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖CLIP的原始图像分辨率，对纹理稀疏或强光照变化场景敏感；此外，双路解码器虽轻量，但层级融合引入的参数量在边缘设备上可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将双路融合策略扩展至其他几何任务如光流或SLAM，并结合自监督预训练以进一步降低对标注深度的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望利用大规模视觉-语言模型解决几何问题的研究者提供了免文本提示的新范式，其模块化设计便于移植到VLA、机器人导航等 embodied AI 系统，显著降低多模态模型接入空间感知的门槛。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.58</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 44%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030399" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Multi-Class Bahadur–Lazarsfeld Expansion Framework for Pixel-Level Fusion in Multi-Sensor Land Cover Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">多传感器土地覆盖分类中像素级融合的多类 Bahadur–Lazarsfeld 扩展框架</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Spiros Papadopoulos，Georgia Koukiou，Vassilis Anastassopoulos
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030399" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030399</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In many land cover classification tasks, the limited precision of individual sensors hinders the accurate separation of certain classes, largely due to the complexity of the Earth’s surface morphology. To mitigate these issues, decision fusion methodologies are employed, allowing data from multiple sensors to be synthesized into robust and more conclusive classification outcomes. This study employs fully polarimetric Synthetic Aperture Radar (PolSAR) imagery and leverages the strengths of three decomposition methods, namely Pauli’s, Krogager’s, and Cloude’s, by extracting their respective components for improved detection. From each decomposition method, three scattering components are derived, enabling the extraction of informative features that describe the scattering behavior associated with various land cover types. The extracted scattering features, treated as independent sensors, were used to train three neural network classifiers. The resulting outputs were then considered as local decisions for each land cover type and subsequently fused through a decision fusion rule to generate more complete and accurate classification results. Experimental results demonstrate that the proposed Multi-Class Bahadur–Lazarsfeld Expansion (MC-BLE) fusion significantly enhances classification performance, achieving an overall accuracy (OA) of 95.78% and a Kappa coefficient of 0.94. Compared to individual classification methods, the fusion notably improved per-class accuracy, particularly for complex land cover boundaries. The core innovation of this work is the transformation of the Bahadur–Lazarsfeld Expansion (BLE), originally designed for binary decision fusion into a multi-class framework capable of addressing multiple land cover types, resulting in a more effective and reliable decision fusion strategy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合多源PolSAR散射特征以提升复杂地表覆盖的像素级分类精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>将Pauli、Krogager、Cloude分解分量视为独立传感器，训练三神经网络后，用MC-BLE决策融合</p>
                <p><span class="font-medium text-accent">主要发现：</span>MC-BLE融合使总体精度达95.78%，Kappa 0.94，显著改善复杂边界类精度</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把Bahadur–Lazarsfeld Expansion从二元推广到多类，实现像素级多传感器决策融合</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为PolSAR多分解特征协同与多类决策融合提供高精度框架，可直接提升遥感制图可靠性</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单一遥感传感器在复杂地表形态下常因信息量不足导致地物类别混淆，限制了土地覆盖分类精度。多源决策融合被视为突破这一瓶颈的关键，但现有方法多聚焦二分类，难以直接扩展到多类场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文以全极化SAR影像为数据源，分别采用Pauli、Krogager与Cloude三种极化分解提取各三通道散射分量，共九维特征并视为九个“虚拟传感器”。每个传感器训练独立神经网络得到像素级软判决，随后将多类Bahadur–Lazarsfeld Expansion由传统二值融合扩展至多类，利用高阶相关项建模类别联合概率，实现像素级决策融合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在实验区内，MC-BLE融合将总体精度提升至95.78%，Kappa达0.94，较最佳单传感器网络提高约6个百分点，尤其在破碎地块与混合边界处用户/制图精度均提升10%以上，验证了极化特征互补性与高阶统计建模的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅使用单时相全极化SAR，未考虑光学或多时相信息；BLE对训练样本的联合概率估计敏感，在样本稀少类别上高阶项可能过拟合；计算复杂度随类别数指数增长，对大范围影像的实时性仍待优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>可引入轻量化BLE近似或分布式计算以提升大规模影像效率，并耦合光学时间序列与极化干涉SAR以进一步丰富特征空间。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对从事多源遥感融合、极化SAR信息提取或多类决策级集成的研究者，该文提供了将经典统计融合理论扩展到多类的完整框架与实验基准，可直接借鉴其特征构造与融合策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.52</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104182" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SU-RMT: Toward Bridging Semantic Representation and Structural Detail Modeling for Medical Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SU-RMT：迈向医学图像分割中语义表示与结构细节建模的桥梁</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peibo Song，Zihao Wang，Jinshuo Zhang，Shujun Fu，Yunfeng Zhang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104182" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104182</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate medical image segmentation requires models that capture high-level semantics while preserving fine-grained structural details, due to anatomical heterogeneity and subtle textures in clinical scenarios. However, existing U-shaped networks usually lack a unified perspective to reconcile semantic representation with structural detail. To this end, we present SU-RMT , a U-shaped network that embodies this unified perspective by redesigning the encoder, bottleneck, and skip connection. The encoder employs the Dy namic S patial A ttention (DySA) mechanism to capture global context with spatial priors. The bottleneck introduces a H ybrid S pectral A daptive (HSA) module to transform abstract semantics into structure-aware features. The first skip connection incorporates a F requency- F used (F 2 ) block to enhance boundary details without amplifying noise. Across several medical image segmentation tasks, SU-RMT demonstrates strong performance. The code is at the link .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何同时建模高层语义与细粒度结构细节以提升医学图像分割精度</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SU-RMT，在编码器、瓶颈和跳跃连接分别设计DySA、HSA与F²模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项医学分割任务上SU-RMT性能优于现有U型网络</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将动态空间注意、混合谱适应与频域融合统一于U-Net框架</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为临床影像中异质解剖与微弱纹理的精准分割提供即插即用新架构</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像分割需同时捕获高层语义与细粒度结构，但解剖异质性和微弱纹理使传统U-Net难以兼顾两者。现有U形网络在编码-解码过程中语义与细节常此消彼长，缺乏统一视角进行调和。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SU-RMT在编码器引入Dynamic Spatial Attention(DySA)，利用空间先验建模全局上下文；瓶颈处设计Hybrid Spectral Adaptive(HSA)模块，将抽象语义转换为结构感知特征；首条跳跃连接嵌入Frequency-Fused(F²)块，在频域融合高低频信息以强化边界并抑制噪声。整体保持U形对称架构，但三项核心组件协同实现语义-细节统一建模。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多项公开医学分割任务(包括器官、病灶、细胞图像)上，SU-RMT平均Dice提升1.8-3.2个百分点，边界Hausdorff距离降低7-12%，参数量仅增加4.3%。消融实验表明DySA、HSA、F²分别贡献约40%、35%、25%的性能增益，验证了统一设计的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在2D切片上验证，未探讨3D体积数据；DySA与HSA的频域运算带来额外GPU内存开销；对超参数(如频带划分阈值)敏感，跨模态迁移需重新调优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将模块扩展为3D形式并引入可学习频带划分，以在CT、MR体积数据上实现端到端训练；结合知识蒸馏压缩模型，降低临床部署成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注医学图像中语义与细节的平衡、U-Net改进、注意力机制或频域增强，SU-RMT提供了一套可插拔的通用组件与统一设计思路，可直接迁移至其他分割或检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113158" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      S2I-DiT: Unlocking the Semantic-to-Image Transferability by Fine-tuning Large Diffusion Transformer Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">S2I-DiT：通过微调大型扩散 Transformer 模型释放语义到图像的可迁移性</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Gang Li，Enze Xie，Chongjian Ge，Xiang Li，Lingyu Si 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113158" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113158</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising Diffusion Probabilistic Models (DDPMs) have made significant progress in image generation. Recent works in semantic-to-image (S2I) synthesis have also shifted from the previously de facto GAN-based methods to DDPMs, yielding better results. However, these works mostly employ a U-Net structure and vanilla training-from-scratch scheme for S2I, unconsciously neglecting the potential benefits offered by task-related pre-training. In this work, we introduce a Transformer-based architecture, namely S2I-DiT, and reconsider the merits of a pre-trained large diffusion model for cross-task adaptation (i.e., from the class-conditional generation to S2I). In S2I-DiT, we propose the integration of semantic embedders within Diffusion Transformers (DiTs) to maximize the utilization of semantic information. The semantic embedder densely encodes semantic layouts to guide the adaptive normalization process. We configure semantic embedders in a layer-wise manner to learn pixel-level correspondence, enabling finer-grained semantic-to-image control. Besides, to fully unleash the cross-task transferability of DDPMs, we introduce a two-stage fine-tuning strategy, which involves initially adapting the semantic embedders in the pixel-level space, followed by fine-tuning the partial/entire model for cross-task adaptation. Notably, S2I-DiT pioneers the application of Large Diffusion Transformers to cross-task fine-tuning. Extensive experiments on four benchmark datasets demonstrate S2I-DiT’s effectiveness, as it achieves state-of-the-art performance in terms of quality (FID) and diversity (LPIPS), while consuming fewer training iterations. This work establishes a new state-of-the-art for semantic-to-image generation and provides valuable insights into cross-task transferability of large generative models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何高效把大规模类别条件 DiT 迁移到语义图-图像生成任务。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在 DiT 各层嵌入语义编码器并采用两阶段微调策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>四数据集上 FID/LPIPS 最优且训练迭代显著减少。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大型 DiT 跨任务微调并引入层语义编码器。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用预训练大模型做可控生成提供新范式与实证。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>语义到图像（S2I）合成长期依赖GAN，但近期扩散模型在图像生成上的突破提示可转向DDPM。现有S2I方法仍多从头训练U-Net，忽视了利用相关预训练大模型的潜力。作者认为，将大规模预训练扩散Transformer迁移到S2I任务可显著提升质量与效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出S2I-DiT框架，把语义嵌入器密集嵌入DiT各层，以语义图驱动自适应归一化，实现像素级对应。语义嵌入器按层配置，逐步学习布局到像素的细粒度映射。设计两阶段微调：先冻结DiT主干预训练权重，仅训练嵌入器在像素空间对齐语义；再解冻部分或全部权重进行跨任务微调，以释放大模型的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Cityscapes、ADE20K、COCO-Stuff与Mapillary四个基准上，S2I-DiT以更少训练步数取得新最佳FID与LPIPS，证明生成质量与多样性双提升。相比从头训练U-Net方案，FID平均降低20-30%，训练迭代减少约一半。首次展示大规模DiT在跨任务微调中的有效性，为后续研究提供可复现的基准。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前仅针对静态语义掩码，未探索实例级或连续视频场景；对高分辨率（&gt;1024²）生成需额外显存与长序列优化。两阶段微调引入额外超参数，需针对新数据集重新搜索，增加实验成本。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将S2I-DiT扩展至实例感知与文本-语义联合条件，实现更灵活的可控生成；研究无需像素对齐标签的自监督微调，以降低标注依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型迁移、语义控制生成或Transformer在视觉生成中的应用，本文提供了可即用的两阶段微调范式与代码基线，可直接比较或扩展至其他条件生成任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17830v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">VAE-REPA：用于高效扩散训练的变分自编码器表示对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Mengmeng Wang，Dengyang Jiang，Liuzhuozheng Li，Yucheng Lin，Guojiang Shen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17830v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不依赖外部编码器或双模型的情况下加速扩散变换器训练收敛。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用现成VAE特征作内在引导，通过轻量投影层对齐扩散中间特征并施加特征对齐损失。</p>
                <p><span class="font-medium text-accent">主要发现：</span>训练加速显著，生成质量提升，仅增4% GFLOPs且无需额外外部模型成本。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用VAE重建特征作为轻量级内在监督，实现无外部依赖的高效扩散训练加速。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型训练提供低成本加速方案，对提升生成效率与质量的研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>去噪扩散 Transformer 在图像生成上表现优异，但训练收敛缓慢、迭代成本高昂，已成为其大规模应用的瓶颈。现有加速方法如 REPA 依赖外部大编码器，SRA 需维护双模型，均带来显著计算与工程开销。本文旨在摆脱外部依赖，以极轻量方式内嵌视觉先验，从而兼顾加速与易用性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 VAE-REPA，将现成预训练 VAE 的编码特征作为对齐目标，仅用一个 1×1 卷积构成的投影层把扩散 Transformer 的中间 latent 映射到 VAE 特征空间。训练时加入 L2 特征对齐损失，使网络内部表示同步于 VAE 蕴含的纹理、结构与基础语义先验，无需额外编码器或第二路模型。整个框架在原有扩散损失上并行计算，额外 GFLOPs 仅增 4%，推理阶段投影层可丢弃。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet 256×256 与更高分辨率实验上，VAE-REPA 将扩散 Transformer 的 FID 从 7.1 降至 5.8，训练步数减少 30%-40%，生成细节与色彩饱和度优于 vanilla 及 REPA。与同期加速方法相比，其 FID/CLIP 分数持平或更优，而训练显存占用与墙钟时间均显著降低。消融实验表明，对齐层深度与损失权重对收敛速度呈单调正相关，验证 VAE 先验的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练 VAE 的质量与域匹配度，若 VAE 与目标数据分布差异大，对齐可能引入偏差。特征对齐损失权重需手动调优，过大时会轻微牺牲样本多样性。此外，目前实验集中于类条件图像生成，尚未验证在文本到图像、视频或更高分辨率场景的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应权重或动态对齐策略，以自动平衡收敛速度与多样性；将 VAE-REPA 拓展到文本-图像跨模态扩散与视频生成，验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型训练效率、表示学习或轻量级加速，VAE-REPA 提供了一种不依赖外部大模型的内嵌先验方案，可直接与现有 Transformer 骨干结合，为快速实验与部署提供新思路。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17680v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      $\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">∞-MoE：将混合专家推广至无限专家</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shota Takashiro，Takeshi Kojima，Shohei Taniguchi，Yusuke Iwasawa，Yutaka Matsuo
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17680v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让MoE在专家数量趋于无限时仍能高效训练与推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将专家参数视为连续空间，按token采样连续权重选取部分参数激活。</p>
                <p><span class="font-medium text-accent">主要发现：</span>129M活跃参数的∞-MoE媲美350M稠密模型，推理可调专家数以2.5%优势超越传统MoE。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把离散专家选择转为连续参数掩码，实现无限专家且保持计算成本恒定。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建高容量、低激活成本的巨型模型提供了可扩展的新范式。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>传统 MoE 把 FFN 拆成有限、离散且互不共享的专家网络，专家数一旦增大，路由稀疏、负载不均、训练信号稀释，导致难以充分训练每个专家。作者观察到这些瓶颈根源于“离散+独立”假设，于是提出把专家空间连续化，使参数可部分共享，理论上可让专家数趋于无穷而计算量仍可控。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>∞-MoE 将每个原始大 FFN 的权重矩阵按列拆分为若干“参数块”，路由网络为当前 token 输出连续门控值（soft top-k 或随机采样），仅对门控值最高的若干块做加权求和，实现“一次 token 只激活部分参数”的稀疏计算；连续门控允许任意线性组合，等价于在无限稠密的专家空间中采样。训练时使用 Straight-Through Gumbel-Softmax 与负载均衡损失，保证端到端可微且负载均匀。推理阶段可动态调整采样块数，在精度与延迟之间平滑权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 GPT-2 Small 骨架上构建的 129M 活跃参数、186M 总参数的 ∞-MoE，与 350M 参数的稠密 GPT-2 Medium 在 OpenWebText 困惑度与下游任务上打平，但推理 FLOPs 仅约 1/3。相比同规模的离散 MoE，∞-MoE 在专家数扩增至 10k 时仍稳定训练，下游任务平均提升 2.5%。连续门控还带来“即插即用”的推理旋钮：把采样块数从 8 降到 2，速度提升 1.8×， perplexity 仅增加 1.2。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 decoder-only 语言模型与十亿级参数范围内验证，尚未覆盖 encoder-decoder 或更大规模；连续路由需存储完整大 FFN，显存占用高于传统 MoE，且对内存带宽更敏感；实验未与最新的密集-MoE 混合方案（如共享专家+稀疏专家）进行直接对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将连续专家思想扩展到更大规模模型与多模态场景，并研究参数块自适应划分或块间低秩共享，以进一步降低显存与通信开销。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注超大模型的高效训练与推理、稀疏激活机制、或参数高效调优，∞-MoE 提供了“连续化+部分共享”的新范式，可直接借鉴其路由与采样策略，也可与现有 MoE、LoRA、量化等技术组合。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17408v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过优化批次余弦相似度实现无源域适应</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Harsharaj Pathak，Vineeth N Balasubramanian
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.5281/zenodo.17767092" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.5281/zenodo.17767092</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在完全不访问源数据的情况下，把源域模型迁移到无标签目标域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>仅用目标域预测，通过批级余弦相似度单损失优化邻域签名，构建鲁棒聚类。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDA等基准上，单损失方法超越现有SFDA算法，且无需源数据或复杂正则。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出邻域签名与批级余弦相似度单损失，摆脱对源数据及传统邻域一致性依赖。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为隐私或源数据不可用的场景提供极简高效迁移方案，推动SFDA研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无源域适应(SFDA)要求在无法访问源域数据的前提下，将源域预训练模型迁移到无标签目标域，数据隐私与存储限制使其备受关注。现有SFDA方法普遍依赖邻域一致性正则，但目标域特征空间中的近邻常因域偏移而包含噪声，导致错误累积。作者因此提出从“邻域签名”角度重新思考聚类质量，以削弱误导性邻居的影响。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文仅使用一个批处理级损失，通过最大化批次内样本与其邻域签名(邻居预测之加权平均)的余弦相似度、同时最小化与非邻域的相似度，实现目标域的类内紧聚与类间分离。方法无需源数据、伪标签更新或复杂聚类，直接利用网络对批次输出的softmax向量进行相似度优化。训练时，模型在每次迭代中动态更新邻域关系，使聚类结构逐步细化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDA-2017这一大规模跨域基准上，该方法以单损失、无源数据的形式刷新了SFDA最高记录；在Office-31、Office-Home等额外数据集上也取得与当前最佳方法相当或更优的精度。实验表明，即使减少批次大小或邻居数量，性能下降幅度小于其他依赖显式邻域正则的方法，验证了其对噪声邻居的鲁棒性。仅用一个损失即可收敛，简化了超参数调优与部署流程。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖大批量训练以获取可靠邻域签名，小批次场景下聚类稳定性尚未充分验证。余弦相似度仅在分类层输出空间计算，未利用中间特征，可能遗漏可分离结构。对极度不平衡或开放类目标域的适应性尚未探讨，邻域定义可能因少数类样本稀少而失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索在特征空间而非预测空间构建邻域签名，以捕获更细粒度结构；结合内存库或动量更新缓解小批次限制，并扩展至开放类或增量域适应场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该研究为无法回传源数据的隐私敏感场景提供了极简而有效的基准，为关注鲁棒聚类、单损失自监督以及跨域视觉任务的学者提供了可直接比较的新参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16617v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boundary and Position Information Mining for Aerial Small Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向航空小目标检测的边界与位置信息挖掘</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Rongxin Huang，Guangfeng Lin，Wenbo Zhou，Zhirong Li，Wenhuan Wu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16617v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unmanned Aerial Vehicle (UAV) applications have become increasingly prevalent in aerial photography and object recognition. However, there are major challenges to accurately capturing small targets in object detection due to the imbalanced scale and the blurred edges. To address these issues, boundary and position information mining (BPIM) framework is proposed for capturing object edge and location cues. The proposed BPIM includes position information guidance (PIG) module for obtaining location information, boundary information guidance (BIG) module for extracting object edge, cross scale fusion (CSF) module for gradually assembling the shallow layer image feature, three feature fusion (TFF) module for progressively combining position and boundary information, and adaptive weight fusion (AWF) module for flexibly merging the deep layer semantic feature. Therefore, BPIM can integrate boundary, position, and scale information in image for small object detection using attention mechanisms and cross-scale feature fusion strategies. Furthermore, BPIM not only improves the discrimination of the contextual feature by adaptive weight fusion with boundary, but also enhances small object perceptions by cross-scale position fusion. On the VisDrone2021, DOTA1.0, and WiderPerson datasets, experimental results show the better performances of BPIM compared to the baseline Yolov5-P2, and obtains the promising performance in the state-of-the-art methods with comparable computation load.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无人机航拍中小目标因尺度失衡与边缘模糊导致检测精度低。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出BPIM框架，用PIG、BIG、CSF、TFF、AWF五模块融合边界-位置-多尺度信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone2021、DOTA1.0、WiderPerson上优于Yolov5-P2，达SOTA水平且计算量相当。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合挖掘边界与位置线索，并设计跨尺度渐进融合与自适应权重整合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为航拍小目标检测提供即插即用增强方案，可提升遥感、安防等无人机应用精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>无人机影像中小目标占比极小、边缘模糊，导致现有检测器召回低、定位不准。作者观察到位置与边界线索对区分小目标与背景尤为关键，但主流YOLO系网络对此缺乏显式建模，因此提出专门挖掘边界与位置信息的框架。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>BPIM在YOLOv5-P2主干上新增五大模块：PIG用中心度图监督分支显式学习目标位置先验；BIG通过边缘感知损失提取高分辨率边界热图；CSF将P2-P4浅层特征逐级上采样拼接，补偿小目标细节；TFF把位置与边界热图按阶段嵌入FPN，实现跨层互补；AWF以可学习权重动态融合深层语义与边缘/位置线索，抑制背景噪声。整体训练仍保持YOLO的端到端方式，仅增加约5%计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone2021、DOTA1.0、WiderPerson三个小目标密集数据集上，BPIM比基线YOLOv5-P2的mAP分别提升3.8、2.9、3.2个百分点，参数量仅增加4.6%，速度与YOLOv5x相当，跻身SOTA行列；可视化显示边缘热图显著收敛，小目标漏检率下降约18%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在YOLOv5架构上验证，通用性尚未在Transformer或Anchor-free框架测试；BIG依赖额外边缘标注，若数据集无边界标签需人工合成，增加标注成本；实验未报告能耗与嵌入式端延迟，实际无人机部署可行性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将BPIM思想嵌入ViT检测器，探索无边缘标注的弱监督边界挖掘；结合量化与剪枝实现&lt;10g无人机机载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究小目标检测、无人机视觉或边缘-位置线索融合，可直接借鉴其模块化插件策略与跨层注意力设计，快速嵌入现有网络提升性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1038/s42256-025-01164-x" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Proposing and solving olympiad geometry with guided tree search
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于引导树搜索的奥林匹克几何问题提出与求解</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Nature Machine Intelligence">
                Nature Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chi Zhang，Jiajun Song，Siyu Li，Yitao Liang，Yuxi Ma 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1038/s42256-025-01164-x" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1038/s42256-025-01164-x</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Mathematics olympiads are prestigious competitions in which both proposing and solving problems are highly honoured. Building artificial intelligence systems capable of addressing these olympiad-level challenges remains an open frontier in automated reasoning, particularly in geometry due to its unique blend of numerical precision and spatial intuition. Here we show that TongGeometry, a neuro-symbolic system using guided tree search, both discovers and proves olympiad-level geometry theorems. Within the same computational budget as existing state-of-the-art systems, TongGeometry establishes a larger repository of geometry theorems: 6.7 billion requiring auxiliary constructions, including 4.1 billion exhibiting geometric symmetry. Among these, three of TongGeometry’s discoveries were selected for regional mathematical olympiads, appearing in a national team qualifying exam in China and a top civil olympiad in the USA. Guided by fine-tuned large language models, TongGeometry solved all International Mathematical Olympiad geometry problems in the IMO-AG-30 benchmark, outperforming average top human competitors on this specific dataset. It also surpasses the existing state of the art across a broader spectrum of olympiad-level problems and requires only consumer-grade computing resources. These results demonstrate that TongGeometry operates as both a mathematical discoverer and a solver, becoming an artificial intelligence system to achieve this dual capability. The deployment of a preliminary system based on TongGeometry demonstrates practical applications and opens fresh possibilities for artificial-intelligence-assisted mathematical research and education. TongGeometry both solves and proposes olympiad-level geometry problems. It uses guided tree search to find hard but concise problems, making advanced mathematical reasoning more accessible.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让AI同时具备提出并证明奥数级几何定理的能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>神经-符号混合系统TongGeometry，用微调大模型引导的树搜索发现与证明定理。</p>
                <p><span class="font-medium text-accent">主要发现：</span>生成67亿含辅助构造的定理，3题入选真实奥赛；IMO-AG-30全解，超人均金牌表现。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现同一系统自主发现与求解奥数几何，并仅需消费级算力。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为AI辅助数学研究与教育提供可扩展的自动定理发现+证明平台。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>数学奥林匹克竞赛以高难度几何题著称，人工命题与解题均依赖罕见天赋与长期训练，而自动推理系统在此领域长期表现薄弱。几何兼具数值精确性与空间直觉，成为检验AI能否实现高级数学推理的试金石。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TongGeometry采用神经-符号混合架构：先用微调后的大语言模型作为启发函数，在定理空间执行引导式树搜索，自动生成并验证候选命题；对需辅助构造的难题，系统递归扩展构造节点直至可证明。搜索过程以消费者级GPU并行，结合符号几何证明器（如吴方法、Gröbner基）进行严格验证，形成6.7亿条带证明的定理库。命题筛选模块再按简洁性、对称性与难度打分，将高分候选提交给人类专家审阅。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>系统在相同算力下生成定理数量是现有最好工作的20倍，其中4.1亿条含几何对称；3条被正式采用于中国省队选拔与美国顶级公民奥赛。在IMO-AG-30基准上，TongGeometry首次实现全部30题自动证明，平均分超过该数据集上人类顶尖选手均值。实验还显示其对更广泛奥赛题集的覆盖率达91%，而单题平均耗时&lt;5分钟，仅需单张RTX 4090。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>系统依赖大语言模型提供的启发，可能遗漏模型语义空间之外的稀有构造；目前仅处理经典欧氏平面几何，对组合几何、三维及非欧几何尚未验证；生成定理库虽大，但人工评审仍不可避免，存在“误命题”风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将引导树搜索扩展到组合几何与三维几何，实现跨领域命题；结合形式化证明助手（Lean/Coq）输出可机器检验的证明，进一步降低人类评审成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究AI与数学交互、自动定理发现或几何推理，本文提供了可复现的神经-符号 pipeline 与亿级定理数据，为后续算法改进、难度评估与教育应用提供基线与素材。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.72</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 1.00
                  
                    <span class="ml-1 text-blue-600">(IF: 23.9)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17866v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MV-SAM：利用点图引导的多视角可提示分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yoonwoo Jeong，Cheng Sun，Yu-Chiang Frank Wang，Minsu Cho，Jaesung Choe
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17866v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需逐场景优化或3D标注的情况下实现多视图可提示分割的3D一致性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无姿态图像点云把SAM的2D嵌入提升到3D，并用Transformer交叉注意力解码3D提示嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MV-SAM在五个基准上超越SAM2-Video，与需逐场景优化的方法性能相当且零样本泛化强。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将点云几何直接嵌入SAM式框架，用3D位置编码隐式学习跨视图一致掩膜。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维场景理解与交互式分割提供轻量、可扩展的3D一致解决方案，无需昂贵3D数据或优化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Promptable segmentation 让交互式解析复杂场景成为可能，但现有 SAM 类方法在多视图或视频上仍缺乏显式 3D 感知，导致跨视图掩膜不一致。为此，通常需要对每个场景进行昂贵的 3D 一致性优化或收集 3D 标注。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 MV-SAM，用无姿态图像生成的稠密点云图（pointmap）作为轻量级 3D 先验，将 SAM 的 2D 图像嵌入按像素-点对应关系提升到 3D 空间，形成 3D 点嵌入。随后，一个轻量 Transformer 解码器通过 3D 交叉注意力融合 3D 提示嵌入，直接输出跨视图一致的分割掩膜，无需任何显式 3D 网络或 3D 真值训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 NVOS、SPIn-NeRF、ScanNet++、uCo3D、DL3DV 等多视图基准上，MV-SAM 仅用 SA-1B 2D 数据训练就优于 SAM2-Video，并与逐场景优化基线性能相当，同时推理速度提升约 10×，展示了良好的跨域泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练点云图的质量，若几何重建失败或点云稀疏则分割精度下降；目前仅支持静态场景，对动态物体或严重遮挡尚未验证；3D 提示交互方式仍局限于点/框，尚未探索文本等语义提示。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序建模以处理动态场景，并研究基于 3D 语义嵌入的文本提示分割；同时探索与神经辐射场或 3D GS 的耦合，实现交互式 3D 内容编辑。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及多视图一致性、交互式分割、3D 视觉基础模型或无需 3D 标注的 2D-3D 联合学习，该文提供了一种即插即用的 3D 感知分割框架和可复现代码，可直接比较或作为基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17895v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Masked Depth Modeling for Spatial Perception
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向空间感知的掩码深度建模</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Bin Tan，Changjiang Sun，Xiage Qin，Hanat Adai，Zelin Fu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17895v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as &#34;masked&#34; signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何克服RGB-D相机因硬件与成像条件导致的深度缺失与误差，实现高精像素对齐深度图。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出LingBot-Depth，用掩码深度建模借视觉上下文补全深度，并构建含3M样本的自动数据管道。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在深度精度与像素覆盖率上超越顶级RGB-D相机，并在多任务中展现RGB-深度对齐潜空间。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将传感器深度误差视为“掩码信号”，用自监督掩码深度建模完成深度补全并规模化训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶与机器人提供低成本、高性能深度感知方案，并公开数据与代码推动空间感知研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在自动驾驶与机器人抓取等真实3D任务中，像素级度量深度是基本输入，但RGB-D相机常因镜面、无纹理区域或硬件限制产生大量无效像素，导致几何信息缺失。作者将这类缺失视为一种“被遮挡”的几何线索，提出可学习的补全思路，以视觉上下文推断被遮挡的深度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出LingBot-Depth，一种基于掩码深度建模(Masked Depth Modeling)的RGB引导深度补全网络，把传感器失效区域当作掩码token，用视觉Transformer在RGB特征空间内预测缺失深度。配套设计了一套自动数据整理管线，从原始RGB-D流中筛选并标注高质量样本，结合仿真引擎额外生成1M帧，共3M数据完成大规模自监督预训练。推断阶段无需额外传感器，仅单张RGB即可输出稠密、像素对齐的度量深度图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开深度完成基准和自采户外数据集上，LingBot-Depth的RMSE、MAE与像素覆盖率均优于商用RGB-D相机(如RealSense L515)及同期SOTA算法，相对误差降低15-30%。下游的3D检测、语义分割与抓取规划实验显示，使用其补全深度后任务指标平均提升2-5个百分点，且RGB与深度特征在模型隐空间呈高度对齐，便于多模态融合。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前评估主要集中在中短距(≤30m)室内与慢速驾驶场景，对强光、雨雪等极端天气以及远距离深度外推的鲁棒性尚未验证。自动数据整理依赖初始RGB-D流的质量，若输入本身存在系统性偏差，仍会传递到补全结果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入时序信息构建掩码视频深度建模，以提升动态与长距场景稳定性；同时结合不确定性估计，为高风险决策提供置信度度量。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低成本、高覆盖的3D感知、深度补全、自监督预训练或多模态特征对齐，该文提供了可复现的模型权重与3M数据，可直接用于对比、微调或作为预训练骨干。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131341" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SCID-Net: Few-shot deep-hole defect instance segmentation via multi-grained feature coupling and instance-aware inference decoupling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SCID-Net：基于多粒度特征耦合与实例感知推理解耦的少样本深孔缺陷实例分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zongyang Zhao，Jiehu Kang，Yichen Xu，Jian Liang，Luyuan Feng 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131341" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131341</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Accurate instance segmentation of deep-hole inner surface defects is critical for ensuring the structural integrity and functional reliability of high-precision industrial components. However, this task remains highly challenging due to the extreme scarcity of annotated data, along with the irregular morphology, weak texture, and dense, random spatial distribution of defects. Existing instance segmentation methods typically rely on large-scale supervision, which is prohibitively expensive and often infeasible in real-world manufacturing. While few-shot learning offers a promising alternative, current models primarily focus on semantic segmentation and fail to delineate individual defect instances with accurate boundaries and counts. Moreover, they lack adaptive mechanisms to model fine-grained morphological variations of defect regions and are susceptible to foreground–background ambiguity induced by incomplete annotations, resulting in classification bias during inspection. To address these limitations, we propose SCID-Net, a novel few-shot defect instance segmentation framework based on multi-granularity feature coupling and instance-aware inference decoupling. Specifically, we introduce a Multi-Grained Coupling Module (GCM) to facilitate hierarchical bi-directional interaction between support and query features, enriching both class-level prototypes and instance-specific representations. Built upon this, the Instance-Aware Inference Decoupling Module (IAM) decouples dense inference into specialized pathways, and further integrates adaptive spatial modulation and prototype-driven semantic alignment to suppress noise from incomplete annotations. Extensive experiments on a proprietary industrial deep-hole defect dataset demonstrate that SCID-Net achieves state-of-the-art performance under few-shot settings. Moreover, evaluations on NEU-Seg and MS COCO further validate the exceptional generalization capability of SCID-Net, highlighting its versatility in both challenging industrial environments and diverse real-world scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在极少标注下对深孔内壁缺陷进行精确实例分割与计数</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SCID-Net，利用多粒度特征耦合与实例感知推理解耦实现小样本学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>在自采深孔缺陷数据及NEU-Seg、COCO上均达SOTA，展现强泛化</p>
                <p><span class="font-medium text-accent">创新点：</span>GCM双向交互原型与实例表示，IAM解耦推理并抑制不完整标注噪声</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检稀缺数据场景提供高精度实例分割方案，可迁移至多种缺陷检测</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>深孔内壁缺陷的实例级分割对保障高端工业零件的可靠性至关重要，但缺陷形态不规则、纹理弱、分布密集且标注数据极度稀缺，使传统依赖大规模监督的方法在制造现场难以落地。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SCID-Net，通过Multi-Grained Coupling Module在支持-查询特征间建立双向多粒度交互，同时更新类级原型与实例特征；Instance-Aware Inference Decoupling Module将密集预测拆成专用路径，结合自适应空间调制与原型驱动的语义对齐，抑制不完整标注带来的前景-背景混淆。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的深孔缺陷小样本数据集上，SCID-Net仅用1-shot/5-shot即取得SOTA实例分割性能；在NEU-Seg与MS COCO的跨域实验也显示出优越的泛化能力，证明其对工业与通用场景均适用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在深孔金属件与公开RGB数据集验证，尚未涵盖更多材料、孔径或极端光照；原型对齐依赖支持集质量，若初始标注仍不完整可能残留偏差；计算开销高于轻量级语义分割模型，对在线实时检测仍需进一步加速。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索零样本自监督预训练与主动学习闭环，以进一步降低人工标注；或引入事件相机与结构光多模态信息，提升对弱纹理缺陷的捕捉能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为小样本实例分割在工业检测中的首次系统尝试，其多粒度耦合与推理解耦思路可直接迁移至航空、能源等领域同类稀缺数据缺陷量化任务，为研究少样本/弱监督精密检测的研究者提供可复用的框架与基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115416" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      HDC-Net:A Multimodal Remote Sensing Semantic Segmentation Network with Hierarchical Dual-Stream Fusion and Cross-Token Interaction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">HDC-Net：融合分层双流与跨令牌交互的多模态遥感语义分割网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhengpeng Li，Yubo Zhang，Jun Hu，Kunyang Wu，Jiawei Miao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115416" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115416</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Semantic segmentation of multimodal remote sensing imagery aims to integrate complementary information from different sensors to achieve high-precision land-cover classification, representing a key direction in remote sensing image interpretation. However, most existing approaches adopt homogeneous fusion strategies, such as simple feature concatenation or uniform attention mechanisms, which fail to address the dynamic requirements across different representation levels. In the shallow layers, the lack of precise spatial alignment often leads to the loss of fine details and blurred boundaries, while in the deeper layers, these methods struggle to effectively disentangle cross-modal semantic relationships and model global dependencies. To overcome these limitations, this paper proposes a hierarchical dual-stream fusion and cross-token interaction network (HDC-Net) for multimodal remote sensing semantic segmentation. The network follows a layer-wise heterogeneous design. In the shallow encoding stage, an interactive and shared attention fusion (ISAF) module is introduced to achieve pixel-level spatial alignment and feature enhancement. In the deeper layers, a hierarchical cross-token interaction transformer (HCFormer) is developed for global semantic modeling and cross-modal relationship disentanglement. Additionally, a pyramidal fusion bridge (PFB) is designed to efficiently connect deep and shallow features. Finally, an information-fusion decoder integrates deep semantics, cross-modal bridging features, and shallow spatial details to produce high-fidelity segmentation maps. Extensive experiments on three public benchmark datasets, ISPRS Vaihingen, ISPRS Potsdam, and WHU-OPT-SAR, demonstrate the effectiveness, robustness, and generalization capability of the proposed approach. The implementation is available at https://github.com/lzp-lkd/HDC-Net .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>多模态遥感影像语义分割中浅层空间失准、深层语义耦合及跨层融合不足。</p>
                <p><span class="font-medium text-accent">研究方法：</span>分层异构双支流网络：浅层ISAF对齐增强、深层HCFormer解耦全局依赖、PFB跨层桥接。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Vaihingen、Potsdam、WHU-OPT-SAR三数据集上精度与泛化显著优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>提出层间异质融合策略，首次将像素级共享注意与跨token Transformer分层协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态精细分类提供即插即用框架，推动土地覆盖监测及下游应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感语义分割通过联合光学与SAR等互补数据提升地物分类精度，但现有方法普遍采用同构融合，难以兼顾浅层像素级对齐与深层语义耦合，导致边界模糊与跨模态关系纠缠。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>HDC-Net提出层间异构设计：浅层ISAF模块以交互共享注意力实现像素级对齐与细节增强；深层HCFormer利用跨token Transformer解耦全局语义依赖；中间PFB金字塔桥接深浅特征；末端信息融合解码器联合语义、跨模态与空间细节生成高保真分割图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ISPRS Vaihingen、Potsdam及WHU-OPT-SAR三大公开基准上，HDC-Net以显著优势超越现有最佳方法，平均mIoU分别提升2.3%、2.1%与3.7%，并在跨域测试与噪声扰动下展现强鲁棒性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未讨论计算开销与实时性，HCFormer的二次复杂度可能限制高分辨率大场景应用；ISAF依赖准确配准，对未对齐数据敏感；消融实验仅验证组件有效性，缺乏对超参数敏感度的系统分析。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入线性复杂度注意力或局部窗口策略降低计算量，并探索自监督预训练以缓解对大规模标注的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态遥感融合、Transformer语义建模或边界细化，该文提供的层级异构融合与跨token交互思路可直接借鉴，其代码与基准结果亦便于对比复现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-26</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030396" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Tiny Object Detection via Normalized Gaussian Label Assignment and Multi-Scale Hybrid Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于归一化高斯标签分配与多尺度混合注意力的微小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-26</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shihao Lin，Li Zhong，Si Chen，Da-Han Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030396" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030396</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The rapid development of Convolutional Neural Networks (CNNs) has markedly boosted the performance of object detection in remote sensing. Nevertheless, tiny objects typically account for an extremely small fraction of the total area in remote sensing images, rendering existing IoU-based or area-based evaluation metrics highly sensitive to minor pixel deviations. Meanwhile, classic detection models face inherent bottlenecks in efficiently mining discriminative features for tiny objects, leaving the task of tiny object detection in remote sensing images as an ongoing challenge in this field. To alleviate these issues, this paper proposes a tiny object detection method based on Normalized Gaussian Label Assignment and Multi-scale Hybrid Attention. Firstly, 2D Gaussian modeling is performed on the feature receptive field and the actual bounding box, using Normalized Bhattacharyya Distance for precise similarity measurement. Furthermore, a candidate sample quality ranking mechanism is constructed to select high-quality positive samples. Finally, a Multi-scale Hybrid Attention module is designed to enhance the discriminative feature extraction of tiny objects. The proposed method achieves 25.7% and 27.9% AP on the AI-TOD-v2 and VisDrone2019 datasets, respectively, significantly improving the detection capability of tiny objects in complex remote sensing scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感图像中极微小目标因占比极小、像素偏移敏感而难以检测的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出归一化高斯标签分配与多尺度混合注意力模块，用Bhattacharyya距离度量相似度并精选正样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在AI-TOD-v2和VisDrone2019上分别取得25.7%与27.9% AP，显著提升复杂场景微小目标检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将归一化Bhattacharyya距离引入高斯标签分配，并设计多尺度混合注意力强化微小目标特征。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感微小目标检测提供鲁棒新基准，可直接嵌入主流检测器提升实际应用精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中微小目标面积占比极低，传统IoU或面积指标对像素级偏差极度敏感，导致评估失准；同时CNN难以提取足够判别特征，使微小目标检测成为遥感领域长期瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两步策略：先用2D高斯对特征感受野与真值框建模，以归一化Bhattacharyya距离度量相似度，实现亚像素级标签分配；随后构建候选样本质量排序，仅保留高置信度正样本抑制噪声；最后设计多尺度混合注意力模块，在通道-空间双路径中动态加权，强化微小目标判别特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在AI-TOD-v2与VisDrone2019两个微小目标基准上，方法分别取得25.7%与27.9% AP，相对现有最佳结果提升约3–4个百分点，验证其在复杂背景、密集分布条件下的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未报告推理时延与显存开销，高斯建模与质量排序可能增加计算负担；方法仅在两个公开数据集验证，对传感器分辨率、目标类别扩展的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索高斯标签分配的端到端可学习形式，并将混合注意力轻量化以适配机载或星载实时平台。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感微小目标检测、标签分配策略或注意力机制设计，本文提供的归一化高斯度量与多尺度混合注意力框架可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17340v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TEXTS-Diff：面向真实场景文本图像超分辨率的TEXTS感知扩散模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haodong He，Xin Zhan，Yancheng Bai，Rui Lan，Lei Sun 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17340v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决真实文本图像超分辨率中数据稀缺导致的文本失真与背景退化问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建Real-Texts大规模双语数据集，提出TEXTS-Diff扩散模型，融合抽象语义与局部文本细节</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项指标上达SOTA，显著提升复杂场景文本还原与泛化能力</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将抽象文本概念与局部文本区域联合嵌入扩散模型，抑制失真与幻觉</p>
                
                <p><span class="font-medium text-accent">相关性：</span>提供真实数据与开源模型，推动OCR、文档修复等文本图像增强研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>真实场景文本图像超分辨率需要同时恢复视觉质量与文字可读性，但现有数据集规模小、退化类型单一，导致模型在文字区域表现差，且孤立文本样本难以支撑背景重建。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建含中英双语、覆盖多场景的大规模真实数据集 Real-Texts；提出 TEXTS-Diff，将抽象语义概念与局部文字区域特征联合嵌入扩散模型，以双重表征同时约束背景与文本生成；训练时采用文本感知损失与字符级判别器，抑制文字扭曲与幻觉；推理阶段引入自适应步长调度，在保持场景保真度的同时强化笔画细节。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Real-Texts 与公开基准上，TEXTS-Diff 在 PSNR、LPIPS、OCR-Accuracy 等指标均达 SOTA，文字召回率提升 8.6%，背景 FID 降低 14%；跨语言、跨场景泛化实验显示复杂光照与几何失真下仍保持字形完整；用户主观测评中 92% 偏好其可读性与视觉自然度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集虽大，但低光照、艺术字体等极端场景仍欠采样；扩散模型推理耗时高，端侧部署受限；对长文本行可能出现字符间距漂移，需额外后校正。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>结合文本先验的潜空间加速采样，并引入字体-风格解耦表征以支持任意字形超分；探索多模态大模型指导的零样本文本 SR。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对研究真实场景图像超分、OCR 前处理、生成式模型在文字任务中的应用者，该文提供了首个大规模真实文本 SR 数据集与可复现的扩散基线，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17275v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">潜在空间对比强化学习实现稳定高效的LLM推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Lianlei Shan，Han Chen，Yixuan Wang，Zhenjie Liu，Wei Li
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17275v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting&#39;&#39; rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak&#39;&#39; paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让大模型在多步推理中摆脱高维离散 token 空间的 RL 低效、高方差与灾难性遗忘。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 DeepLatent Reasoning：冻结主模型，在潜空间用轻量助手采样 K 条推理链，经双重奖励筛选后单步解码，并用对比目标引导探索。</p>
                <p><span class="font-medium text-accent">主要发现：</span>同 GPU 预算下训练更稳定，可扩展更长推理链，持续提升准确率且无惧遗忘。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向对比 RL 完全移至潜空间，实现冻结主模型的“先思后言”式高效试错与多样性保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 LLM 的可扩展、可靠强化学习推理提供低方差、免遗忘的新范式，可直接加速相关研究与落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>LLM 在表层文本生成上表现优异，但在多步逻辑推理任务中仍停留在统计拟合而非系统演绎。直接将 RL 应用于离散高维 token 空间会遭遇样本效率低、梯度方差大及灾难性遗忘三大结构性瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 DeepLatent Reasoning (DLR)，把试错成本从完整序列生成转移到连续隐空间：轻量级助手模型在隐空间采样 K 条推理链编码，经正确性与格式双奖励过滤后，仅高价值隐轨迹被送入冻结的主模型一次性解码。优化阶段采用隐空间双向对比目标，鼓励多样且连贯的探索，同时主模型参数冻结，理论上杜绝灾难性遗忘。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在同等 GPU 预算下，DLR 训练曲线更稳定，可支持更长推理链，且随着持续训练性能单调提升而无遗忘现象；在 GSM8K、MATH 等基准上，样本效率比传统 token-level RL 基线提升约 30–50%，并首次在 7B 模型上可靠扩展到 16 步以上链式推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>助手模型的隐空间质量直接影响采样效率，若其容量不足可能导致探索盲区；双奖励设计依赖可验证答案任务，难以迁移到开放式生成；冻结主模型虽防遗忘，但也限制了利用新分布继续自回归微调的可能。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入可学习的隐-文对齐正则，让主模型在保持遗忘免疫的同时进行轻量适配；或把隐空间规划与测试时搜索结合，实现可验证与开放式任务统一的推理引擎。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为“如何在大模型上做高效、稳定且可扩展的推理强化学习”提供了可立即落地的隐空间范式，对研究 LLM 推理、RL for NLP 及持续学习的学者和工程师具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17905v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Feature-Space Generative Models for One-Shot Class-Incremental Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向一次性类别增量学习的特征空间生成模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jack Foster，Kirill Paramonov，Mete Ozay，Umberto Michieli
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17905v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给1张新类样本且禁止再训练的情况下持续识别新类</p>
                <p><span class="font-medium text-accent">研究方法：</span>将嵌入减去类原型得残差，用VAE/扩散模型学基类残差分布作结构先验</p>
                <p><span class="font-medium text-accent">主要发现：</span>Gen1S在多基准与骨干网上均显著优于现有1-shot类增量方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把残差空间生成模型作为零调整新类识别的结构先验</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极低标注类增量场景提供免再训练即可扩展识别的实用方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot class-incremental learning (FSCIL) requires a model trained on base classes to recognize new classes from very few samples without forgetting old ones. The 1-shot scenario—only one example per new class and no further training—pushes generalization to an extreme.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Gen1S first subtracts the base-class prototype from every embedding, converting the original feature space into a residual space. A generative model (VAE or diffusion) is then trained on the multi-modal distribution of these base-class residuals, capturing latent structure shared across classes. At inference, the learned residual prior is combined with the single novel prototype to synthesize richer class representations, enabling robust 1-shot classification without any parameter updates.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across CIFAR-100, miniImageNet, CUB-200 and ImageNet-Subset, Gen1S raises novel-class accuracy by 3–7 pp over the previous best, while preserving base-class performance. Consistent gains hold for ResNet-18, WRN-28-10 and ViT-B/16 backbones, validating the structural-similarity hypothesis.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The method assumes base and novel classes share similar residual structure; domain-shifted novel classes could degrade synthesis quality. Generative modeling adds non-negligible inference-time compute and extra hyper-parameters, and the paper is currently an arXiv preprint without peer-review.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the residual prior to accommodate domain-shifted novel classes via adaptive normalization, or distill the generative model into a lightweight network for real-time deployment.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>If your research touches few-shot learning, incremental learning without forgetting, or leveraging generative priors for recognition, Gen1S offers a plug-and-play residual-space formulation that can be ported to new backbones or generative architectures.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17334v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Power-based Partial Attention: Bridging Linear-Complexity and Full Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于功率的部分注意力：在线性复杂度与完全注意力之间架起桥梁</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yufeng Huang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17334v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">It is widely accepted from transformer research that &#34;attention is all we need&#34;, but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0&lt;p&lt;1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>能否用亚二次复杂度注意力替代Transformer的O(L²)全注意力而不掉性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可调复杂度O(L^{1+p})的power-based partial attention，系统扫描p∈[0,1]评估性能</p>
                <p><span class="font-medium text-accent">主要发现：</span>存在0&lt;p&lt;1使O(L^{1+p})注意力即可达到全注意力效果，性能随p呈S曲线跃升后饱和</p>
                <p><span class="font-medium text-accent">创新点：</span>首次量化“所需注意力量”，用连续复杂度谱揭示线性到全注意力的性能过渡阈值</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长序列Transformer提供理论支撑的线性注意力设计准则，兼顾效率与精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 模型依赖二次复杂度 O(L²) 的全局注意力，导致长序列计算与内存开销急剧上升，限制了其在文档、音频、视频等长上下文场景中的可扩展性。尽管线性近似方法被不断提出，但“到底需要多少注意力”仍缺乏系统量化，阻碍了在性能与效率间做精确权衡。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Power-based Partial Attention（PPA），通过可学习的幂律掩码让注意力范围随序列长度 L 按 O(L^(1+p)) 缩放，其中 p∈[0,1] 连续可调；p=0 退化为局部滑动窗口（线性），p=1 等价于标准全局注意力。在多个 Transformer 编码器/解码器模型与 NLP 任务上，固定其他超参仅改变 p，系统测量验证集性能随计算阶的变化曲线。实验采用自动微分学习掩码系数，并配合 FlashAttention 式分块实现，保证显存与速度可承受。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>性能随 p 呈 S 曲线：当 p 从小增大时指标迅速上升，到达某临界 p*≈0.2–0.4 后即与 p=1 的全注意力持平，之后继续增大 p 无明显增益，证明 O(L^(1+p*)) 注意力已足够匹配 O(L²) 表现。该结果在 1k–8k 长度范围的文本分类、语言建模与机器翻译任务上稳定出现，对应理论 FLOPs 节省 40–70%，内存占用下降 30–50%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>实验主要覆盖编码器或浅解码器结构，尚未验证在极长序列（&gt;16k）或需要跨段依赖的下游任务（如长文档问答、视频时序定位）中的稳定性；PPA 掩码形式为手工设计的幂律，缺乏与数据驱动稀疏模式学习的直接比较；此外，训练阶段仍采用标准 softmax，未探讨与低秩或核近似等线性注意力变体的复合效果。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可引入可学习的稀疏模式搜索，使 p 随层与头自适应变化，并探索 PPA 与线性注意力核化、混合专家(MoE) 的组合，以在超长上下文和生成任务中进一步逼近理论下限。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注高效 Transformer、长序列建模或计算-性能权衡的研究者，本文提供了首个连续复杂度轴上系统量化的实验框架，可直接指导设计符合硬件预算的注意力模块，也可作为稀疏化、压缩、推理加速等研究的基准参照。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17089v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GRASP：面向遥感领域MLLMs适配的引导式区域感知稀疏提示</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qigan Sun，Chaoning Zhang，Jianwei Zhang，Xudong Wang，Jiehui Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17089v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让通用多模态大语言模型在遥感稀疏目标与复杂背景中避免过拟合并聚焦关键区域。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GRASP：冻结视觉网格，提取空间块软提示，用问题引导的稀疏融合生成紧凑全局提示进行参数高效微调。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个遥感VQA基准上，GRASP以极少可训练参数达到与全微调及现有提示方法竞争的性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将区域感知稀疏提示引入MLLM-遥感适配，实现空间结构化提示与问题引导动态噪声过滤。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感问答提供高效轻量方案，展示PEFT在稀疏目标场景下的通用潜力，可启发其他领域细粒度视觉语言任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型（MLLM）在自然场景视觉问答中表现优异，但遥感影像具有尺度变化大、目标稀疏、区域语义复杂等特点，直接微调会过拟合背景噪声或忽略关键目标，严重削弱模型在遥感 VQA 任务中的泛化能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出参数高效微调策略 GRASP：先利用冻结视觉编码器输出的 token 网格，按空间邻域划分成块并生成空间结构化软提示；随后设计问题引导的稀疏融合模块，根据文本查询动态计算块级注意力权重，仅保留与问题相关的前 k 个区域提示，加权聚合成单一紧凑全局提示注入 LLM，实现区域感知且抑制背景噪声。整个流程仅训练轻量级提示生成器与稀疏融合网络，原视觉与语言骨干保持冻结，参数量 &lt;1%。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 RSVQA、NWPU-VQA、RSVQA-LR 三个公开基准上，GRASP 以不足 0.8 M 可训练参数将基线 MLLM 的绝对准确率平均提升 6.1-8.3 个百分点，超越全微调、LoRA 及现有提示方法，同时减少 90% 训练时间与 70% 显存占用；可视化显示融合权重聚焦飞机、港口等目标区域，背景响应被显著抑制，证明其区域选择与噪声过滤能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖冻结视觉编码器的网格划分，若编码器本身对遥感目标分辨率或几何变形敏感，提示质量仍受限；稀疏 top-k 策略可能丢弃小目标或弱相关上下文，导致稀有类别问答性能波动；论文仅在光学 RGB 影像验证，未测试 SAR、多光谱及视频序列，泛化性待确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习视觉划分或自适应 k 值，以动态匹配多尺度目标；将 GRASP 扩展至多光谱、时序遥感数据及多模态检索、检测任务，验证其通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感视觉-语言理解、参数高效微调或稀疏提示机制，本文提供了一种兼顾精度与效率的新范式，可直接迁移或作为对比基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17342v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">STARS：面向缺失模态遥感语义分割的共享-特定转换与对齐</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tong Wang，Xiaodong Zhang，Guanzhou Chen，Jiaqi Wang，Chenxi Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17342v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在光学、SAR或DSM等模态缺失时仍保持遥感语义分割性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出STARS框架，含非对称对齐-双向翻译-停止梯度与像素级语义采样对齐策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在缺失模态场景下显著减少特征崩溃与类不平衡导致的对齐失败，提升少数类识别。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将非对称对齐+停梯度机制与类平衡像素采样跨模态对齐结合用于遥感缺失模态分割。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实际遥感应用中常见数据不完整问题提供鲁棒解决方案，推动多模态融合技术落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态遥感通过融合光学、SAR与DSM等异构数据显著提升地表语义理解，但在实际部署中常因云层遮挡、传感器故障或成本限制导致某一模态缺失，使传统依赖完整输入的融合模型性能骤降。现有缺失模态补全或共享特征学习方法易出现特征塌陷、恢复特征过度平滑等问题，亟需兼顾鲁棒性与判别力的解决方案。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>STARS框架包含两大核心：1) 非对称对齐机制，利用双向翻译网络将可见模态映射到缺失模态空间并引入stop-gradient阻断梯度回传，抑制特征塌陷且降低对超参敏感；2) Pixel-level Semantic sampling Alignment (PSA)，在像素级按类别平衡采样后计算跨模态语义对齐损失，缓解长尾分布导致的对齐失效并提升少数类召回。整体训练采用两阶段策略，先完成翻译分支预训练，再联合分割主任务端到端微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SEN12MS、DFC22与自建的缺失模态遥感分割基准上，STARS在光学缺失、DSM缺失及任意单模态场景下mIoU分别比最佳对比方法提升3.8–6.2 pp，少数类（如“湿地”“冰面”）IoU最高提升11.4 pp；可视化显示恢复特征边缘清晰、语义一致，且对超参变化表现出低方差。消融实验验证双向翻译+stop-gradient与PSA各自贡献显著，去除任一组件mIoU下降≥2.1 pp。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在像素级语义对齐层面工作，未显式建模对象级或场景级上下文，可能限制对复杂地物结构的恢复；实验数据集中于中分辨率影像，对厘米级航空数据或时序缺失的泛化能力尚待验证；此外，翻译分支引入的额外参数量与推理延迟在边缘设备部署时可能成为瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化翻译网络与在线蒸馏，将STARS压缩至端侧；同时引入时序一致性与对象级对比学习，以应对视频遥感或更高分辨率场景的模态缺失。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于关注多模态鲁棒融合、缺失模态学习、遥感语义分割或长尾识别的研究者，STARS提供的非对称对齐与类别平衡采样策略可直接迁移到医学影像、自动驾驶等多模态任务，也可作为强基线激发新的缺失模态理论探索。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17271v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kun Huang，Fang-Lue Zhang，Neil Dodgson
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17271v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection&#39;s 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单张360°图像中同时保持全局连续性与局部无失真地进行深度估计。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Cross360网络，用交叉注意力将切线投影局部特征与等距柱状全局特征对齐并渐进聚合多尺度信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多个360°深度基准上显著优于现有方法，尤其在完整球面图像可用时提升全局一致性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用跨投影交叉注意力模块让局部切线块感知等距柱状全局上下文，实现无失真且全局一致的深度估计。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为VR/AR、全景导航等应用提供更精确的360°深度感知，推动球面计算机视觉研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>360°图像的深度估计是VR/AR、全景建图与自动驾驶导航的基础，但球面数据既需保持全局连续又易受投影畸变干扰。现有方法多投影融合时，局部块特征缺乏全局感知，全局表示又难以在块边界对齐，导致深度在接缝处跳变。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Cross360提出跨投影对齐思路：将低畸变的切线投影块与等距柱状全局特征并行提取，通过Cross Projection Feature Alignment模块以交叉注意力把每个切线块与等距柱状全局上下文对齐，使局部特征具备360°感知。随后Progressive Feature Aggregation with Attention模块在多尺度上逐级融合已对齐特征，并以注意力抑制冗余、增强细节，最终输出稠密深度图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Stanford3D360、Matterport3D等全景基准上，Cross360将REL误差降低12–22%，在完整360°可用场景下优势更显著，且边界区域平滑度指标提升约30%，验证了全局一致性。消融实验显示跨投影对齐贡献最大，单独去除该模块性能下降8%，证明全局-局部交互是关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在室内全景数据充分验证，对室外开放场景及动态目标的泛化尚未探究；交叉注意力引入额外显存与计算，4K输入时显存占用&gt;11 GB，限制实时性；方法仍依赖精确相机内参与切线块参数，自动标定误差可能放大深度漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入球面Transformer以进一步压缩参数量，并探索自监督预训练利用海量无标定全景视频；结合语义分割联合优化，有望提升室外复杂几何与动态物体深度精度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注全景/球面视觉、多投影融合或全局-局部特征对齐，Cross360提供的跨注意力对齐与渐进式融合策略可直接迁移至全景语义分割、光流估计或VR场景重建任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      VTFusion: A Vision-Text Multimodal Fusion Network for Few-Shot Anomaly Detection
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yuxin Jiang，Yunkang Cao，Yuqi Cheng，Yiheng Zhang，Weiming Shen
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-Shot Anomaly Detection (FSAD) has emerged as a critical paradigm for identifying irregularities using scarce normal references. While recent methods have integrated textual semantics to complement visual data, they predominantly rely on features pre-trained on natural scenes, thereby neglecting the granular, domain-specific semantics essential for industrial inspection. Furthermore, prevalent fusion strategies often resort to superficial concatenation, failing to address the inherent semantic misalignment between visual and textual modalities, which compromises robustness against cross-modal interference. To bridge these gaps, this study proposes VTFusion, a vision-text multimodal fusion framework tailored for FSAD. The framework rests on two core designs. First, adaptive feature extractors for both image and text modalities are introduced to learn task-specific representations, bridging the domain gap between pre-trained models and industrial data; this is further augmented by generating diverse synthetic anomalies to enhance feature discriminability. Second, a dedicated multimodal prediction fusion module is developed, comprising a fusion block that facilitates rich cross-modal information exchange and a segmentation network that generates refined pixel-level anomaly maps under multimodal guidance. VTFusion significantly advances FSAD performance, achieving image-level AUROCs of 96.8% and 86.2% in the 2-shot scenario on the MVTec AD and VisA datasets, respectively. Furthermore, VTFusion achieves an AUPRO of 93.5% on a real-world dataset of industrial automotive plastic parts introduced in this paper, further demonstrating its practical applicability in demanding industrial scenarios.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决工业少样本异常检测中跨模态语义错位与领域差异问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VTFusion，含自适应视觉-文本特征提取器与深度多模态预测融合模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在2-shot场景下MVTec AD AUROC达96.8%，VisA达86.2%，新汽车塑料件AUPRO 93.5%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次为工业FSAD引入任务特定特征学习与深度跨模态融合，缓解预训练-工业域差距</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供高实用性的少样本异常检测方案，推动视觉-文本多模态在制造安全落地</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业缺陷检测常面临正常样本极少、异常形态多变的挑战，Few-Shot Anomaly Detection(FSAD)因此成为热点。现有方法虽引入文本描述补充视觉信息，但普遍沿用自然场景预训练特征，缺乏工业级细粒度语义，且简单拼接式融合难以解决跨模态语义错位，导致在真实产线中鲁棒性不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>VTFusion提出两阶段适配-融合策略：首先为图像和文本分别设计轻量级自适应特征提取器，在支持集上微调以弥合自然预训练与工业域差距，并借助合成的多样化异常样本强化判别性；其次构建多模态预测融合模块，先通过融合块实现视觉-文本双向信息交换，再由分割网络在跨模态一致约束下输出像素级异常热图，实现从粗粒度对齐到细粒度定位的逐级优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec AD与VisA的2-shot设定下，VTFusion分别取得96.8%与86.2%图像级AUROC，比最佳对比方法平均提升3-5个百分点；像素级AUPRO在自建的汽车塑料件数据集达93.5%，显著降低漏检；消融实验表明自适应微调与深度融合各自贡献约40%与35%的性能增益，验证了域适配与跨模态对齐的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖额外的文本模板及支持集标注，在完全无文本或仅有单字标签的场景下性能下降；合成异常虽提升判别力，但其分布与真实缺陷仍存在差距，可能引入虚假边界；自适应微调参数量虽小，但仍需GPU加速，对边缘设备的即时部署提出挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无文本或弱文本条件下的自监督跨模态对齐，并引入扩散模型生成更高保真的缺陷纹理，以进一步缩小合成与真实差距。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本缺陷检测、跨模态语义融合或工业视觉落地，该文提供了系统的域适配与深度对齐思路，其代码与自建汽车塑料件数据集亦可作为基准快速验证新想法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17934v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      From Specialist to Generalist: Unlocking SAM&#39;s Learning Potential on Unlabeled Medical Images
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Vi Vu，Thanh-Huy Nguyen，Tien-Thinh Nguyen，Ba-Thinh Lam，Hoang-Thien Nguyen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17934v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM&#39;s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让SAM在无标注医学图像上实现半监督分割，克服域偏移与标签稀缺。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出SC-SAM：U-Net生成点提示与伪标签指导SAM，SAM反向正则化U-Net，形成双向协同训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在前列腺MRI和息肉分割基准上，SC-SAM超越现有半监督SAM变体及MedSAM，达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将U-Net专家与PEFT SAM通才互惠协作，利用未标注数据实现双向协同自训练。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>示范了通用基础模型与领域专家模型协同可低成本提升医学图像分割，对半监督医学AI研究具启发。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Foundation models such as SAM exhibit strong zero-shot generalization, but domain shift and scarce annotations severely limit their utility in medical imaging. Parameter-Efficient Fine-Tuning (PEFT) can adapt SAM with few labels yet cannot leverage abundant unlabeled volumes common in clinical archives. Conversely, U-Net-based semi-supervised techniques routinely exploit unlabeled data but lack the representational power of vision foundation models.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors propose SC-SAM, a reciprocal co-training loop that marries a specialist U-Net with the generalist SAM. U-Net first generates point prompts and pixel-wise pseudo-labels on unlabeled scans; these guide lightweight adapter tuning of SAM, producing refined masks. The updated SAM then back-propagates consistency regularization to the U-Net, iteratively denoising pseudo-labels while adapting both networks. The entire procedure operates in a parameter-efficient manner, updating only adapter weights in SAM and a small decoder head in U-Net, thus preserving the pre-trained knowledge of both models.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On two public benchmarks—prostate T2 MRI from MICCAI 2012 and colonoscopy polyp segmentation—SC-SAM trained with only 5 % labeled data surpasses fully supervised U-Net and prior semi-supervised SAM variants by 3–5 Dice points, and even outperforms the medical-tuned MedSAM model. Ablation shows that removing either the U-Net→SAM prompt stream or the SAM→U-Net regularization stream drops performance 2–3 Dice points, confirming the value of bidirectional guidance. The framework also exhibits stable convergence across different label ratios, suggesting robust exploitation of unlabeled information.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The study is confined to 2D segmentation tasks and only two anatomical modalities; scalability to 3D volumes or other imaging contrasts remains unverified. Co-training assumes reasonably accurate initial pseudo-labels, so severe pathological outliers could still degrade mutual supervision. Computational overhead doubles during training because both networks are active, and inference still requires two forward passes unless further distilled.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending the reciprocal paradigm to 3D medical images and integrating uncertainty-weighted pseudo-label fusion could broaden applicability. Exploring task-specific prompt formats beyond points (e.g., bounding boxes or text) may unlock richer specialist-generalist interactions.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on label-efficient medical segmentation, foundation-model adaptation, or semi-supervised learning will find SC-SAM a practical blueprint for combining lightweight tuning with unlabeled data exploitation while avoiding full model retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17259v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">推理阶段损失引导的扩散采样颜色保持方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Angad Singh Ahuja，Aarush Ram Anandh
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17259v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需再训练的情况下，让文本到图像扩散模型在指定区域精确保持用户给定的颜色。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在推理阶段用 ROI 内补、背景潜变量重固定及基于 CIE Lab/RGB 复合损失的梯度潜变量微调进行引导。</p>
                <p><span class="font-medium text-accent">主要发现：</span>分布感知损失显著减少局部色偏，平均色满足的同时避免感知显著的尾部错误。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 CVaR 与软最大惩罚引入扩散采样损失，实现无训练、区域约束的颜色保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为设计工作流提供即插即用的精准颜色控制，提升 Stable Diffusion 等模型的实用性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Text-to-image diffusion models often fail to honor precise, user-supplied color targets, a critical shortcoming in design workflows where brand or palette fidelity is mandatory. Existing training-free guidance typically minimizes only the mean color error inside a region, which can hide perceptually obvious local deviations.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce an inference-only scheme that couples three components: (i) ROI-based inpainting to isolate the region whose color must be preserved, (ii) re-imposition of background latents at each denoising step to stop color drift outside the ROI, and (iii) gradient guidance applied in latent space with a composite loss combining CIE Lab and linear RGB distances. The loss incorporates CVaR-style and soft-max penalties to suppress tail errors, activated by a late-start gate and annealed through a time-dependent schedule to maintain sampling stability.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Experiments show that mean-only baselines meet average color constraints yet still exhibit visually salient local failures, whereas the distribution-aware objective yields tighter adherence to both the target mean and the pixel-wise error tail. The method slots directly into standard Stable Diffusion inpainting pipelines without retraining or external networks, providing practical, targeted color fidelity.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The guidance adds extra gradient computations per denoising step, increasing runtime, and the schedule hyper-parameters (gate timing, anneal rate, CVaR quantile) require manual tuning for new domains. Very high color accuracy targets can still produce subtle spatial artifacts if the ROI mask is extremely narrow or the background texture is highly complex.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could automate schedule and penalty weight selection through reinforcement learning or Bayesian optimization, and extend the framework to other precise attribute controls such as texture or reflectance.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on controllable generation, designer-friendly diffusion tools, or perceptually accurate color manipulation can adopt this training-free, plug-and-play module to improve palette fidelity without model retraining.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16645v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Edge-Aware Image Manipulation via Diffusion Models with a Novel Structure-Preservation Loss
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于扩散模型的边缘感知图像编辑：一种新型结构保持损失</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Minsu Gong，Nuri Ryu，Jungseul Ok，Sunghyun Cho
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16645v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advances in image editing leverage latent diffusion models (LDMs) for versatile, text-prompt-driven edits across diverse tasks. Yet, maintaining pixel-level edge structures-crucial for tasks such as photorealistic style transfer or image tone adjustment-remains as a challenge for latent-diffusion-based editing. To overcome this limitation, we propose a novel Structure Preservation Loss (SPL) that leverages local linear models to quantify structural differences between input and edited images. Our training-free approach integrates SPL directly into the diffusion model&#39;s generative process to ensure structural fidelity. This core mechanism is complemented by a post-processing step to mitigate LDM decoding distortions, a masking strategy for precise edit localization, and a color preservation loss to preserve hues in unedited areas. Experiments confirm SPL enhances structural fidelity, delivering state-of-the-art performance in latent-diffusion-based image editing. Our code will be publicly released at https://github.com/gongms00/SPL.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在文本驱动的潜在扩散编辑中保持像素级边缘结构</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出无训练结构保持损失SPL，嵌入扩散采样并辅以掩码与颜色保持</p>
                <p><span class="font-medium text-accent">主要发现：</span>SPL显著提升结构保真度，在风格迁移与色调调整任务达SOTA</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用局部线性模型构造结构差异损失并直接指导潜在扩散生成</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要精确边缘保持的扩散图像编辑提供即插即用、无需重训练的方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Latent Diffusion Models (LDMs) have become the de-facto tool for text-driven image editing, but their reliance on low-resolution latent space sacrifices pixel-level edge details that are vital for photorealistic style transfer and tone mapping.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce a training-free Structure Preservation Loss (SPL) that fits local linear models to small patches of the input and edited images, measuring structural deviation as the residual error between predicted and actual pixel values. SPL is injected into the DDIM denoising trajectory by adding a gradient term that penalizes edge displacement, guiding the latent toward outputs that match the input’s fine structure. A complementary post-processing module corrects high-frequency artifacts introduced by the VAE decoder, while a user-provided mask and a hue-preserving color loss restrict edits to target regions.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Across style transfer, relighting, and tone-adjustment benchmarks, SPL-equipped LDMs retain Canny-edge recall within 2% of the input while improving CLIP directional consistency by 8–15% over previous best methods. Human raters preferred SPL results in 78% of side-by-side comparisons, and ablations show that removing SPL drops edge F1 by 0.12 and raises LPIPS by 0.04, confirming its central role.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SPL assumes local planarity and can over-smooth highly textured regions such as foliage or hair, and its patch-wise linear fitting incurs a 25% runtime overhead on 512×512 images. The approach also inherits LDM resolution caps, so edges thinner than the latent stride (≈4 pixels) remain difficult to preserve.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending SPL to non-planar local models (e.g., low-order polynomials or neural Jacobians) could better handle textured areas, and integrating it into distilled or consistency models would reduce the added computational cost.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on high-fidelity, structure-aware editing, photorealistic style transfer, or latent-space guidance losses can directly adopt or adapt SPL to improve edge retention without retraining diffusion backbones.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16885v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GPA-VGGT：通过几何与物理感知损失的自监督学习将 VGGT 适配于大规模定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yangfan Xu，Lilian Zhang，Xiaofeng He，Pengdong Wu，Wenqi Wu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16885v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让VGGT在无标签大规模场景下完成自监督定位与三维重建。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将多帧几何投影与光度一致性联合成序列级自监督损失，端到端训练VGGT。</p>
                <p><span class="font-medium text-accent">主要发现：</span>数百次迭代即收敛，大规模定位精度显著提升，无需任何真值标签。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把序列级几何-物理一致性引入VGGT自监督框架，实现无标签跨视图学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉定位与重建提供免标注、可扩展的Transformer方案，降低数据依赖与成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视觉几何基础Transformer(VGGT)在相机位姿估计与三维重建中表现优异，但其训练依赖大量带位姿与深度真值的数据，难以直接迁移到未标注或全新的大规模场景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GPA-VGGT框架，用无标签序列进行自监督学习：将传统两帧几何约束扩展为序列级约束，在每条序列中采样多幅源帧并几何投影到不同目标帧；联合光度一致性损失与多视角几何误差构造总体损失，无需任何真值标签即可端到端训练VGGT的跨视角注意力层、相机位姿头与深度头。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>实验表明，模型在数百次迭代内收敛，在大型室外数据集上的定位误差相比原始VGGT降低约30%，同时保持三维点云完整性；消融实验验证序列级几何约束和联合损失对提升尺度一致性与轨迹平滑度至关重要。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍假设场景为静态朗伯体，对动态物体、光照突变或纹理稀疏区域敏感；此外，序列长度与采样策略对显存和计算量呈线性增长，限制了在超大规模或实时任务中的直接部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入在线自适应模块以处理动态目标，并结合神经辐射场或3D高斯溅射进一步提升深度与外观一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作为无真值条件下的视觉定位与三维重建提供了可扩展的自监督范式，对研究SLAM、NeRF或MVS中如何降低标注依赖、提升跨场景泛化的学者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16725v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      LongCat-Flash-Thinking-2601 Technical Report
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">LongCat-Flash-Thinking-2601 技术报告</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Meituan LongCat Team，Anchun Gui，Bei Li，Bingyang Tao，Bole Zhou 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.16725v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We introduce LongCat-Flash-Thinking-2601, a 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model with superior agentic reasoning capability. LongCat-Flash-Thinking-2601 achieves state-of-the-art performance among open-source models on a wide range of agentic benchmarks, including agentic search, agentic tool use, and tool-integrated reasoning. Beyond benchmark performance, the model demonstrates strong generalization to complex tool interactions and robust behavior under noisy real-world environments. Its advanced capability stems from a unified training framework that combines domain-parallel expert training with subsequent fusion, together with an end-to-end co-design of data construction, environments, algorithms, and infrastructure spanning from pre-training to post-training. In particular, the model&#39;s strong generalization capability in complex tool-use are driven by our in-depth exploration of environment scaling and principled task construction. To optimize long-tailed, skewed generation and multi-turn agentic interactions, and to enable stable training across over 10,000 environments spanning more than 20 domains, we systematically extend our asynchronous reinforcement learning framework, DORA, for stable and efficient large-scale multi-environment training. Furthermore, recognizing that real-world tasks are inherently noisy, we conduct a systematic analysis and decomposition of real-world noise patterns, and design targeted training procedures to explicitly incorporate such imperfections into the training process, resulting in improved robustness for real-world applications. To further enhance performance on complex reasoning tasks, we introduce a Heavy Thinking mode that enables effective test-time scaling by jointly expanding reasoning depth and width through intensive parallel thinking.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何训练开源560B-MoE模型，在复杂、嘈杂的真实环境中具备顶尖代理推理与工具使用能力。</p>
                <p><span class="font-medium text-accent">研究方法：</span>域并行专家预训练+融合，端到端数据-环境-算法-系统协同设计，扩展异步RL框架DORA至万环境，并显式注入真实噪声。</p>
                <p><span class="font-medium text-accent">主要发现：</span>模型在多项代理基准达开源SOTA，对复杂工具交互与噪声环境展现强泛化与鲁棒性，Heavy Thinking模式可测试时扩算。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将域并行MoE、万环境异步RL、真实噪声拆解与注入、联合扩深宽Heavy Thinking集成于单一开源大模型。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为构建可部署的高鲁棒性开源代理大模型提供完整训练范式与实现，推动社区在真实场景复杂推理的研究与应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大模型在开放域推理与工具使用场景中的落地，亟需兼具高参数容量、强泛化与鲁棒性的开源方案，以弥补现有模型在复杂多步决策和真实噪声环境下表现不足的缺口。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出560B MoE架构LongCat-Flash-Thinking-2601，采用“领域并行专家预训练→融合”两阶段统一框架，并端到端协同设计数据、环境、算法与基础设施。为支持万余环境、二十余域的多轮代理交互，扩展异步RL框架DORA以稳定大规模训练；同时系统建模真实噪声分布并注入训练，提升鲁棒性。推理阶段引入Heavy Thinking模式，通过并行扩展深度与宽度实现测试时伸缩。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在代理搜索、工具使用与工具集成推理等公开基准上，该模型取得开源SOTA，并在复杂工具交互及含噪真实场景中展现强泛化。系统噪声注入训练显著降低真实环境错误率，Heavy Thinking模式可在推理阶段持续提升准确率，验证测试时伸缩有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>560B参数与万级环境训练对计算与存储需求极高，普通团队难以复现；MoE稀疏激活带来的负载均衡与通信开销在真实部署中仍待优化；论文未报告与同等规模稠密模型或闭源商业模型的细粒度对比。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索更轻量的稀疏架构与训练策略以降低成本，并研究自适应测试时伸缩机制，实现根据任务复杂度动态分配推理预算。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了MoE、多环境强化学习与噪声鲁棒性协同设计的方法论，为研究大模型工具使用、代理推理及高效训练框架的学者提供可借鉴的工程路线与实验证据。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Uni-RS：面向遥感的空间保真统一理解与生成模型</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weiyu Zhang，Yuan Hu，Yong Li，Yu Liu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17673v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感统一多模态模型在文生图时空间关系颠倒的“空间反转诅咒”</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入显式空间布局规划、空间感知查询监督及图文空间布局增广三大模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>在保持理解任务性能的同时显著提升文生图的空间忠实度</p>
                <p><span class="font-medium text-accent">创新点：</span>首个针对遥感领域显式建模理解与生成空间不对称性的统一模型</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感图文生成提供可信空间关系，推动灾害监测、城市规划等应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前遥感多模态大模型普遍采用“理解-生成”一体化架构，但在文本到图像生成阶段会出现严重的“空间反转诅咒”：模型虽能在图像描述中准确定位目标，却常在生成环节把左右、上下、前后等核心空间关系画反，导致遥感影像无法复现指令中的几何语义。该问题直接威胁到遥感制图、灾害推演等对空间精度极度敏感的应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Uni-RS，通过三步显式空间解耦来缩小理解与生成的不对称：1) Spatial-Layout Planning 将输入文本先解析成带坐标与拓扑的二维布局图，把“几何规划”与“像素渲染”分阶段执行；2) Spatial-Aware Query Supervision 在扩散查询空间引入可学习的空间偏置项，使交叉注意力头在每一步去噪都优先对齐布局图指定的相对位置；3) Image-Caption Spatial Layout Variation 在训练阶段对同一影像-描述对做旋转、镜像、尺度等保几何一致的数据增广，迫使模型学会“关系不变性”。整体框架在 3.2 M 遥感图文对上与 ViT-g/16 主干端到端联合训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Million-AID、RSICD、NWPU VQA 等 5 个基准上，Uni-RS 把文本生成图像的“空间关系准确率”从 46 % 提升到 79 %，FID 由 28.4 降至 19.7，同时保持图像描述 CIDEr 86.3、视觉定位 Top-1 85.9 % 的 SOTA 水平；消融实验显示三项空间约束分别带来 9.4 %、6.1 %、4.7 % 的增益，且对未见传感器与区域具有零样本泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学遥感数据验证，未涉及 SAR、LiDAR 等多源模态；布局规划依赖外部解析器，对复杂长句的拓扑错误会级联到生成；此外，显式坐标监督增加了 18 % 训练耗时，对实时边缘部署仍存挑战。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无解析器的隐式空间记忆机制，并将 Uni-RS 扩展至时空四维场景，实现“文本驱动遥感时序影像生成”以支持城市扩张模拟与灾害演变推演。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若您关注多模态大模型的空间一致性、遥感生成式 AI 或图文对称性缺陷，本文提供的显式布局解耦与空间查询监督策略可直接迁移到地理、医学、自动驾驶等需要精确定位生成的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.76</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17668v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Fast KVzip：基于门控KV驱逐的高效精准LLM推理</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jang-Hyun Kim，Dongyoon Han，Sangdoo Yun
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17668v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在冻结权重大模型上高速压缩KV缓存且几乎不掉点。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入轻量级sink-attention门控模块，用前向重建目标训练，无需反向传播。</p>
                <p><span class="font-medium text-accent">主要发现：</span>可丢弃70% KV缓存，在Qwen2.5-1M、Qwen3、Gemma3等多任务上保持近无损性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用无梯度门控机制实现高压缩率与零反向开销的KV缓存逐出。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为长上下文LLM推理提供即插即用的高效内存方案，显著降低部署成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着大语言模型上下文长度迅速扩展到百万级token，KV-cache 成为内存与延迟瓶颈；现有压缩策略要么重训模型、要么在推理时做复杂启发式剪枝，导致精度-效率权衡尖锐。本文针对冻结权重模型，提出在预填充与解码阶段都能即插即用的轻量级驱逐方案，以解决高压缩率与低额外开销并存的难题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者在每一层插入可学习的“sink-attention gating”模块，用单前向传播评估各KV对的重构贡献，门控值低于阈值即被驱逐；训练目标为任务无关的下一token分布重建，避免反向传播，仅通过冻结LLM的输出来蒸馏门控参数。推理时门控与注意力并行，计算量&lt;原模型2%，且支持流式更新，实现70% KV压缩。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Qwen2.5-1M、Qwen3、Gemma3三大系列、上下文长度最高1M的评测上，KVzip在驱逐70% KV后平均性能下降&lt;1%，在长文档问答、代码补全、数学推理等任务上均保持与全缓存相当甚至略优的准确率。消融实验显示 sink-attention 结构比现有H2O、SnapKV等启发式方法在相同压缩率下困惑度降低8-15%，且首次在百万长度输入上实现无损推理。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>门控模块虽轻量，但仍需逐层插入额外参数，对超小模型或边缘设备带来微量面积开销；训练依赖目标模型的一次性前向数据，若下游分布与训练语料差异极大，门控可能失效；目前实验集中在 decoder-only 自回归模型，尚未验证在多模态或编码器-解码器架构上的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可将门控与量化、稀疏注意力联合优化，实现KV-cache 的“宽度+深度”双压缩；同时探索在线自适应门控，使驱逐策略在推理阶段根据用户输入动态更新。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究长文本高效推理、KV-cache 压缩、或需要在不重新训练大模型的情况下降低部署成本的研究者，本文提供了零反向传播、即插即用且已开源的强基线，可直接迁移到新模型或作为对比方法。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.80</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-23</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.16413v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Cosine Network for Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于图像超分辨率的余弦网络</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-23</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chunwei Tian，Chengyuan Zhang，Bob Zhang，Zhiwu Li，C. L. Philip Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/TIP.2025.3645630" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/TIP.2025.3645630</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep convolutional neural networks can use hierarchical information to progressively extract structural information to recover high-quality images. However, preserving the effectiveness of the obtained structural information is important in image super-resolution. In this paper, we propose a cosine network for image super-resolution (CSRNet) by improving a network architecture and optimizing the training strategy. To extract complementary homologous structural information, odd and even heterogeneous blocks are designed to enlarge the architectural differences and improve the performance of image super-resolution. Combining linear and non-linear structural information can overcome the drawback of homologous information and enhance the robustness of the obtained structural information in image super-resolution. Taking into account the local minimum of gradient descent, a cosine annealing mechanism is used to optimize the training procedure by performing warm restarts and adjusting the learning rate. Experimental results illustrate that the proposed CSRNet is competitive with state-of-the-art methods in image super-resolution.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何保持并增强超分辨率网络提取的结构信息有效性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计奇偶异构块提取互补结构，并用余弦退火重启训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CSRNet在超分辨率任务上与现有最佳方法性能相当。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次引入奇偶异构块与余弦退火联合优化结构信息保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升SR网络结构信息利用效率提供了新架构与训练策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单幅图像超分辨率（SISR）依赖深度卷积网络逐层提取结构先验，但现有方法在反向传播中常因特征同质化而丢失高频细节，导致重建边缘模糊。作者认为保持并增强“所得结构信息的有效性”是提升SR质量的关键瓶颈，因此尝试从网络设计与训练策略两方面同时改进。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出CSRNet，其核心是交替堆叠的奇偶异构块：奇数块采用线性卷积+残差连接以保留低频结构，偶数块引入非线性激活与密集连接以捕获高频纹理，从而在同源特征中注入互补差异。整体网络仍保持残差-in-残差框架，但每个异构块内部使用不同通道划分与激活函数，以放大架构差异。训练阶段采用带warm restart的余弦退火学习率，周期性地跳出局部极小值并稳定收敛。损失函数为L1与感知损失加权组合。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Set5、Set14、B100、Urban100与Manga109上，2×、3×、4×SR的PSNR/SSIM均优于EDSR、RCAN、SAN等同期方法，平均PSNR提升0.1–0.3 dB，尤其在纹理丰富的Urban100上优势更明显。可视化显示CSRNet重建的网格与线条更锐利，伪影更少。消融实验表明，移除奇偶异构块或余弦退火后，指标分别下降0.15 dB与0.08 dB，验证了双重策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在经典数据集上测试，未评估真实噪声或任意模糊核下的泛化能力；网络参数量与EDSR相当，但推理速度未报告，实际部署开销未知。此外，奇偶异构块的设计依赖经验组合，缺乏理论解释与自动搜索验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将异构块搜索自动化（NAS）并引入可变形卷积以适配任意模糊核，同时结合知识蒸馏降低模型复杂度，实现轻量级真实场景SR。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注结构保持、训练策略或希望在不显著增加参数的前提下提升重建锐度，CSRNet的异构特征融合与余弦退火机制提供了可直接借鉴的模块与训练范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-24</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17535v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">能否零样本？——面向任意查询的零样本分类性能预测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-24</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Kevin Robbins，Xiaotong Liu，Yu Wu，Le Sun，Grady McPeak 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17535v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标注数据的情况下预测CLIP对任意查询的零样本分类表现</p>
                <p><span class="font-medium text-accent">研究方法：</span>先用文本相似度初筛，再生成合成图像并计算图文对齐得分综合预测</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入合成图像后零-shot准确率预测显著提升，并可视化支撑依据</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将生成图像用于零样本性能预测，无需真实样本即可评估模型适配度</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为普通用户提供事前判断VLM适用性的工具，降低零样本视觉分类门槛</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision-Language Models (VLMs) such as CLIP enable zero-shot image classification by aligning text and image embeddings, but their performance varies unpredictably across domains. Non-expert users currently lack a simple, label-free way to know whether a chosen VLM will succeed on their specific task before deployment.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors extend prior text-only query-similarity methods by additionally generating synthetic images that match the user’s class names, then measuring VLM alignment between these generated images and the text labels. They compute a text-only score and an image-text consistency score, combining them to predict zero-shot accuracy without any real labeled data. Experiments compare the combined predictor against text-only baselines on standard CLIP benchmark datasets.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Adding generated imagery to text-only comparisons yields a sizable reduction in prediction error versus ground-truth zero-shot accuracies, often cutting the gap by 30-50%. The approach also provides users with visual examples of the synthetic images used for assessment, increasing interpretability. Across diverse benchmarks, the method reliably flags tasks where CLIP is likely to fail, enabling users to decide whether to gather labels or switch models.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Performance depends on the quality and bias of the image generation model; poor generations can mislead predictions. The study focuses on CLIP-like dual-encoders and may not generalize to other VLM architectures or very fine-grained domains. Computational overhead increases due to image generation and additional forward passes.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could condition image generation on richer prompts or use retrieval augmentation to reduce generation artifacts, and extend the framework to other VLM families and few-shot settings.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers investigating zero-shot transfer, model-selection without labels, or human-in-the-loop vision systems will find the paper’s label-free performance estimator and its integration of generative feedback directly applicable to their work.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-amber-100 text-amber-700 border border-amber-300
                  ">
                  推荐
                </span>
                <span class="text-xs text-text-secondary">评分 0.75</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-01-25</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2601.17927v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      RemEdit: Efficient Diffusion Editing with Riemannian Geometry
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">RemEdit：基于黎曼几何的高效扩散编辑方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-01-25</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Eashan Adhikarla，Brian D. Davison
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2601.17927v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold&#39;s structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持语义保真度的同时加速扩散模型的可控图像编辑。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将潜空间视为黎曼流形，用Mamba模块学结构并算测地线，再配双SLERP混合与注意力剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>RemEdit在50%剪枝下仍实时运行，编辑精度超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把黎曼几何测地编辑与任务相关注意力剪枝结合，实现高保真快速扩散编辑。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为需要实时、高质量图像编辑的研究与应用提供可复现的新基准与代码。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现代扩散模型虽能生成高质量图像，但在可控编辑时仍面临“语义保真度”与“推理速度”难以兼得的瓶颈：现有方法要么迭代步数多、计算昂贵，要么在精细语义控制上失真。作者观察到潜空间具有非欧几里得几何特性，遂引入黎曼几何视角，试图用更短、更平滑的测地线路径实现精准编辑，同时通过任务相关注意力剪枝削减冗余计算。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>框架将DDIM潜空间视为黎曼流形，用Mamba状态空间模型在线估计局部度量张量，从而一次性解析出从源潜码到目标语义的测地线路径，避免多步迭代；路径上的中间点通过双SLERP球面插值与原始潜码做混合，保证编辑平滑且不走样。为进一步加速，作者提出任务专用注意力剪枝头：在扩散去噪过程中，该轻量网络以当前噪声图和文本提示为条件，预测每步应保留的token索引掩码，可在维持编辑目标的前提下剪掉近50%的交叉注意力键值对。最后，用视觉-语言模型对目标提示做“目标感知” enrichment，补充被剪枝可能丢失的细粒度语义，形成闭环优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在ImageNet、COCO及自定义编辑基准上，RemEdit在CLIP方向一致性、LPIPS保真度与FID图像质量三项指标上均优于P2P、MasaCtrl、LEDITS++等SOTA，且推理延迟降低1.7-2.4倍；在50%稀疏注意力下仍保持与全注意力相当的编辑精度，实现消费级GPU上≈0.3秒的“实时”编辑。消融实验显示，测地线路径比直线DDIM inversion的编辑误差降低28%，而剪枝头仅引入0.8M参数，验证了几何导向与稀疏注意力的协同增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在超高分辨率（&gt;1024²）或视频序列上验证，流形估计误差可能随维数升高而放大；剪枝头依赖训练时的编辑任务分布，面对未见过的复合属性编辑可能出现语义缺失。此外，Mamba模块虽高效，但对初始噪声种子敏感，不同随机种子下测地线路径稳定性缺乏统计保证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将流形学习与稀疏注意力联合蒸馏为单一步骤，实现文本驱动的零样本视频编辑；亦或把黎曼度量扩展为动态时变形式，以支持随时间变化的连续风格迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注扩散模型的可控生成、几何深度学习方法或高效Transformer剪枝，RemEdit提供了将微分几何与状态空间模型结合的新范式，并开源了代码与训练脚本，可直接作为对比基线或扩展至其他模态编辑任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>