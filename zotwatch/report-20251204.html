<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2025-12-04</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <!-- Header -->
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2025-12-04 16:09 Asia/Shanghai
      </p>
    </div>
  </header>

  <!-- Researcher Profile Section (Collapsible) -->
  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="researcher-profile" class="section-expand collapsed">
      <!-- Basic Stats - Compact Row -->
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">2667</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年8月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">7</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      <!-- LLM Insights - Compact -->
      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期关注计算机视觉与遥感交叉方向，核心阅读集中在目标检测、视觉Transformer、自监督学习及轻量化模型，同时密切追踪大模型与合成孔径雷达(SAR)智能解译的新进展。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在通用目标检测与Vision Transformer方法上形成深度积累，持续收藏Kaiming He、Ross Girshick等团队的CVPR、TPAMI论文；对SAR图像的旋转目标检测与识别保持6篇以上专题阅读，体现出跨遥感领域的方法沉淀。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>阅读轨迹横跨计算机视觉、遥感、机器学习三大领域，并引入雷达信号处理与导航定位文献，呈现以视觉智能为核心、向遥感应用落地的交叉特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1单季新增79篇为五年峰值，关键词迅速转向“大语言模型、DeepSeek、视觉Transformer”，显示正将基础模型范式引入遥感解析；同时新增“重参数化、模型压缩”阅读，预示关注大模型轻量化落地。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议跟进多模态大模型在SAR-光学融合检测中的微调方法，以及面向星载平台的极低比特量化与知识蒸馏技术，以延续视觉基础模型与遥感实时处理结合的研究兴趣。</p>
        </div>
      </div>
      

      <!-- Charts Row: Domains + Trends -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Research Domains Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        <!-- Quarterly Trends Chart -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      <!-- Year Distribution Chart (full width) -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      <!-- Authors and Venues Row -->
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        <!-- Top Authors -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Chunhua Shen">Chunhua Shen</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Gangyao Kuang">Gangyao Kuang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">8</span>
            </div>
            
          </div>
        </div>
        

        <!-- Top Venues -->
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">111</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">44</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">41</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">35</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">31</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">27</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="雷达学报">雷达学报</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">21</span>
            </div>
            
          </div>
        </div>
        
      </div>

      <!-- Keywords -->
      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            特征可视化 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Vision Transformers <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            HRNet <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(4)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            模型压缩 <span class="text-text-secondary">(4)</span>
          </span>
          
        </div>
      </div>
      

      <!-- Generation info -->
      <div class="text-xs text-text-secondary text-right">
        画像生成于 2025-12-04 15:28 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Chart.js Initialization for Researcher Profile -->
  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      // Chart.js global defaults for academic style
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      // Domains Horizontal Bar Chart
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉SLAM', '姿态估计', '卫星导航', '图模型', '轻量网络', 'Transformer', '车牌识别', '优化算法'],
            datasets: [{
              data: [18, 21, 11, 4, 11, 8, 6, 6],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      // Quarterly Trends Bar Chart with 3 cycling colors (one per month in quarter)
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2023-Q1', c: 50 }, { q: '2023-Q2', c: 18 }, { q: '2023-Q3', c: 21 }, { q: '2023-Q4', c: 19 }, { q: '2024-Q1', c: 66 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 23 }, { q: '2025-Q1', c: 79 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 9 }, { q: '2025-Q4', c: 19 }];
        // Color palette: 3 colors cycling for months within quarters
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },   // Blue
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },   // Green
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }    // Amber
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      // Year Distribution Line Chart
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2006, count: 8 }, { year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 17 }, { year: 2016, count: 15 }, { year: 2017, count: 39 }, { year: 2018, count: 57 }, { year: 2019, count: 53 }, { year: 2020, count: 66 }, { year: 2021, count: 84 }, { year: 2022, count: 110 }, { year: 2023, count: 108 }, { year: 2024, count: 111 }, { year: 2025, count: 141 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    // Show every 5th year label to avoid crowding
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      
    });
  </script>
  

  <!-- Overall Summaries Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <!-- Collapsible Content -->
      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于跨模态融合的论文、2篇关于SAR目标检测的论文以及1篇关于船舶再识别的论文。</p>
            
            <p><strong class="text-accent">跨模态融合</strong>：《MDCA-Net》提出多方向对齐与动态上下文聚合网络，实现光学与SAR影像互补信息的高质量融合；《MAIENet》设计多模态自适应交互增强模块，在检测端进一步挖掘跨模态协同，提升SAR目标检测鲁棒性。</p>
            
            <p><strong class="text-accent">SAR目标检测</strong>：《TranSTD》以小波驱动Transformer为核心，结合自适应特征增强与融合，显著抑制SAR相干斑噪声并提升小目标检测精度；《A Hybrid Strategy》将海事物理数据与OpenSARShip RCS统计先验结合，实现快速高效的海面船只检测。</p>
            
            <p><strong class="text-accent">船舶再识别</strong>：《MOS》针对光学-SAR跨模态船舶重识别任务，提出缓解模态差异的方法，首次系统解决异源影像中的船身份匹配难题。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于多模态感知的论文、6篇关于3D检测与姿态的论文、5篇关于遥感与频域分析的论文、4篇关于小样本与元学习的论文、3篇关于扩散模型采样的论文、2篇关于分割与压缩的论文以及1篇关于超图的论文。</p>
            
            <p><strong class="text-text-secondary">多模态感知</strong>：该主题聚焦视觉-语言对齐与跨模态融合，如《Constituency-Tree-Induced Vision-Language Alignment》用句法树提升MLLM对齐，《MambaFusion》以状态空间模型融合LiDAR-相机特征，《BEVDilation》在BEV空间动态扩张LiDAR-相机感受野，《Unlocking Pseudolabel Potential》通过伪标签对齐跨模态遥感分割，《Alliance》构建光谱-空间-频率统一基础模型，《Evaluating SAM2 for Video Semantic Segmentation》验证SAM2视频记忆机制，《Optical Context Compression》指出视觉上下文压缩实为自编码重建，《Grid Convolution for 3D Human Pose Estimation》将2D关键点映射为网格表征，《CrossHypergraph》利用高阶超图传播多模态语义。</p>
            
            <p><strong class="text-text-secondary">3D检测与姿态</strong>：该主题研究单目、LiDAR及多模态场景下的3D目标检测与人体姿态估计，《BEVDilation》提出LiDAR中心BEV扩张融合，《MambaFusion》构建对象-场景分层状态空间融合，《Grid Convolution for 3D Human Pose Estimation》用网格卷积回归3D人体骨架，其余论文探索点云-图像互补几何先验与高效跨模态交互。</p>
            
            <p><strong class="text-text-secondary">遥感频域</strong>：该主题利用频域与多源传感器提升遥感理解，《Alliance》在统一基础模型中联合光谱-空间-频率特征，《Unlocking Pseudolabel Potential》通过伪标签与分布对齐实现跨模态遥感分割，其余工作聚焦频谱信息去冗余及多传感器互补。</p>
            
            <p><strong class="text-text-secondary">小样本学习</strong>：该主题解决训练样本稀缺场景下的分类与检测，《CrossHypergraph》构建一致高阶语义超图网络进行小样本图像分类，其余论文提出度量-元学习框架提升新类泛化。</p>
            
            <p><strong class="text-text-secondary">扩散采样</strong>：该主题优化扩散概率模型的采样效率，《USF++》提出统一采样框架搜索最优求解器，将采样视为ODE求解并自动优化步长与噪声调度。</p>
            
            <p><strong class="text-text-secondary">分割压缩</strong>：该主题关注视频分割与视觉压缩，《Evaluating SAM2 for Video Semantic Segmentation》系统评估SAM2在视频语义分割中的记忆与提示机制，《Optical Context Compression》指出视觉上下文压缩实质是低质量自编码重建并给出改进策略。</p>
            
            <p><strong class="text-text-secondary">超图网络</strong>：该主题探索高阶关系建模，《CrossHypergraph》提出跨模态一致高阶语义超图网络，用于小样本图像分类中的多阶语义传播。</p>
            
          </div>
        </div>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Interest-based Recommendations Section -->
  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 79%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03404v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MOS: Mitigating Optical-SAR Modality Gap for Cross-Modal Ship Re-Identification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MOS：缓解光学-SAR模态差异的跨模态船舶重识别</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yujian Zhao，Hankun Liu，Guanglin Niu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03404v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-modal ship re-identification (ReID) between optical and synthetic aperture radar (SAR) imagery has recently emerged as a critical yet underexplored task in maritime intelligence and surveillance. However, the substantial modality gap between optical and SAR images poses a major challenge for robust identification. To address this issue, we propose MOS, a novel framework designed to mitigate the optical-SAR modality gap and achieve modality-consistent feature learning for optical-SAR cross-modal ship ReID. MOS consists of two core components: (1) Modality-Consistent Representation Learning (MCRL) applies denoise SAR image procession and a class-wise modality alignment loss to align intra-identity feature distributions across modalities. (2) Cross-modal Data Generation and Feature fusion (CDGF) leverages a brownian bridge diffusion model to synthesize cross-modal samples, which are subsequently fused with original features during inference to enhance alignment and discriminability. Extensive experiments on the HOSS ReID dataset demonstrate that MOS significantly surpasses state-of-the-art methods across all evaluation protocols, achieving notable improvements of +3.0%, +6.2%, and +16.4% in R1 accuracy under the ALL to ALL, Optical to SAR, and SAR to Optical settings, respectively. The code and trained models will be released upon publication.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缩小光学与SAR图像间的模态鸿沟，实现跨模态船舶重识别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MOS框架，含模态一致表示学习与布朗桥扩散跨模态数据生成融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在HOSS数据集上，MOS将R1精度分别提升3.0%、6.2%、16.4%，全面超越现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合去噪SAR预处理、类级对齐损失与布朗桥扩散合成，实现模态一致特征学习。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事监控提供鲁棒的跨光学-SAR舰船身份比对手段，填补跨模态ReID研究空白。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨模态船舶重识别（ReID）在海事情报与监视中日益重要，但光学与合成孔径雷达（SAR）图像在成像机理、分辨率、噪声和几何特征上差异巨大，导致同一船舶在不同模态下的外观差异远超同一模态内的类内差异，传统ReID方法直接迁移效果极差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MOS框架，包含两大模块：MCRL先对SAR图像做去噪预处理，然后在特征空间引入类级模态对齐损失，使同一身份的光学与SAR特征分布均值与协方差一致；CDGF基于布朗桥扩散模型，以光学特征为起点、SAR特征为终点，合成中间跨模态样本，推理阶段将合成特征与原始特征加权融合，既补足了缺失模态又增强了判别性。整个网络采用两阶段训练：先训练扩散生成器，再端到端微调ReID主干与对齐损失。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开HOSS ReID数据集上，MOS在三种协议下将R1精度分别提升到91.4%(ALL→ALL)、89.7%(光学→SAR)和84.6%(SAR→光学)，较最佳基线提高3.0%、6.2%与16.4%，并在mAP、nAUC等指标上同步领先；消融实验表明去噪预处理贡献约2% R1，扩散融合贡献约4%，二者协同带来最大增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成对光学-SAR训练数据，实际海事场景中标注稀少且配准误差会放大模态差异；扩散生成阶段引入额外推理延迟，对星上实时部署不友好；论文仅测试了HOSS数据集，尚未验证在分辨率、波段、极化方式差异更大的SAR传感器上的泛化能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无配对或弱配准场景下的自监督模态对齐，以及轻量化生成网络或知识蒸馏策略，实现端侧实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态检索、遥感融合或海洋监视中的小样本学习，本文提供的模态对齐损失与布朗桥生成思路可直接迁移到光学-红外、SAR-InSAR等其他跨模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 76%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs17233866" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MAIENet: Multi-Modality Adaptive Interaction Enhancement Network for SAR Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MAIENet：面向SAR目标检测的多模态自适应交互增强网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yu Tong，Kaina Xiong，Jun Liu，Guixing Cao，Xinyue Fan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233866" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233866</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Syntheticaperture radar (SAR) object detection offers significant advantages in remote sensing applications, particularly under adverse weather conditions or low-light environments. However, single-modal SAR image object detection encounters numerous challenges, including speckle noise, limited texture information, and interference from complex backgrounds. To address these issues, we present Modality-Aware Adaptive Interaction Enhancement Network (MAIENet), a multimodal detection framework designed to effectively extract complementary information from both SAR and optical images, thereby enhancing object detection performance. MAIENet comprises three primary components: batch-wise splitting and channel-wise concatenation (BSCC) module, modality-aware adaptive interaction enhancement (MAIE) module, and multi-directional focus (MF) module. The BSCC module extracts and reorganizes features from each modality to preserve their distinct characteristics. The MAIE module component facilitates deeper cross-modal fusion through channel reweighting, deformable convolutions, atrous convolution, and attention mechanisms, enabling the network to emphasize critical modal information while reducing interference. By integrating features from various spatial directions, the MF module expands the receptive field, allowing the model to adapt more effectively to complex scenes. The MAIENet framework is end-to-end trainable and can be seamlessly integrated into existing detection networks with minimal modifications. Experimental results on the publicly available OGSOD-1.0 dataset demonstrate that MAIENet achieves superior performance compared with existing methods, achieving 90.8% mAP50.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>单模SAR检测受散斑噪声、纹理缺失和复杂背景干扰，精度受限。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MAIENet，用BSCC-MAIE-MF三模块跨模态融合SAR与光学图像。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在OGSOD-1.0数据集mAP50达90.8%，优于现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将批-通道重组、可变形空洞卷积与多向注意力结合，实现端到端跨模态增强。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为恶劣天气遥感提供即插即用多模态检测框架，可直接提升现有网络性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单模态SAR图像因相干斑噪声、纹理匮乏和复杂背景干扰导致检测精度受限，而光学影像虽纹理丰富却易受天气与光照影响。作者希望利用两种模态的互补性，在无需大幅改动现有检测器的前提下，提升全天候目标检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MAIENet以YOLO类检测头为骨干，插入三模块：BSCC按批次拆分再通道级联，保持模态特有特征；MAIE通过通道重加权、可变形卷积、空洞卷积与交叉注意力，实现深度跨模态融合并抑制噪声；MF模块在四个空间方向并行扩张卷积，扩大有效感受野以捕获多尺度上下文。整体框架端到端训练，仅增加约5%参数量即可嵌入任意单阶段检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开OGSOD-1.0双模数据集上，MAIENet以90.8% mAP50刷新最佳纪录，相较基准提升4.3个百分点，且在暴雨、夜间子集上优势扩大至6–8%，验证了对恶劣条件的鲁棒性。可视化显示融合特征中舰船、车辆轮廓更完整，虚警显著降低。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>目前实验仅覆盖可见光+SAR两种模态，未验证红外、LiDAR等更多组合；OGSOD-1.0场景以港口、郊区为主，缺乏城市密集区与森林背景，可能高估泛化性；计算开销虽低，但可变形卷积在嵌入式SAR平台上的实时性仍待评估。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展至三模态乃至时序SAR视频，结合自监督预训练以利用大规模未标注多模数据，并设计量化-友好算子实现星载实时检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态遥感融合、恶劣天气下的鲁棒检测或轻量化嵌入现有网络，本文提供的模态保持+自适应交互思路可直接借鉴，其代码与OGSOD-1.0已开源，便于快速对比与二次开发。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.75</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 68%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3639785" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TranSTD: A Wavelet-Driven Transformer-Based SAR Target Detection Framework With Adaptive Feature Enhancement and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TranSTD：基于小波驱动的Transformer SAR目标检测框架，具备自适应特征增强与融合</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bobo Xi，Jiaqi Chen，Yan Huang，Jiaojiao Li，Yunsong Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3639785" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3639785</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Target detection in Synthetic Aperture Radar (SAR) images is of great importance in civilian monitoring and military reconnaissance. However, the unique speckle noise inherent in SAR images leads to semantic information loss, while traditional CNN downsampling methods exacerbate this issue, impacting detection accuracy and robustness. Moreover, some dense target scenarios and weak scattering features of targets make it challenging to achieve sufficient feature discriminability, adding complexity to the detection task. Additionally, the multi-scale characteristic of SAR targets presents difficulties in balancing detection performance with computational efficiency in complex scenes. To tackle these difficulties, this paper introduces a wavelet-driven transformer-based SAR target detection framework called TranSTD. Specifically, it incorporates the Haar wavelet dynamic downsampling (HWDD) and semantic preserving dynamic downsampling (SPDD) modules, which effectively suppress noise and preserve semantic information using techniques such as Haar wavelet denoise (HW Denoise) and input-driven dynamic pooling downsampling (IDPD). Furthermore, the SAR adaptive convolution bottleneck (SAC Bottleneck) is proposed for enhancing the discrimination of features. To optimize performance and efficiency across varying scene complexities, a multiscale SAR attention fusion encoder (MSAF Encoder) is developed. Extensive experiments are carried out on three datasets, showing that our proposed algorithm outperforms the current state-of-the-art benchmarks in SAR target detection, offering a robust solution for the detection of targets in complex SAR scenes.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像斑点噪声、弱散射与多尺度目标导致的检测精度低、鲁棒性差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TranSTD框架，结合Haar小波动态下采样、语义保持下采样、自适应卷积瓶颈与多尺度注意力融合编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个数据集上均优于现有SOTA，显著提升复杂场景SAR目标检测精度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波去噪与动态下采样引入Transformer检测网络，并设计SAR专用自适应卷积与多尺度注意力融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、军事侦察等领域提供高效、鲁棒的SAR目标检测新工具，推动雷达图像智能解译研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像固有的相干斑噪声会淹没语义信息，传统CNN的固定下采样进一步加剧细节丢失，导致弱小目标和密集场景检测困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TranSTD以Haar小波动态下采样(HWDD)替代传统池化，在频域完成去噪并保留边缘；语义保持动态下采样(SPDD)根据输入内容自适应选择最大/平均/小波池化，减少信息损失。SAR自适应卷积瓶颈(SAC Bottleneck)在残差支路引入可变形卷积与通道-空间协同注意力，增强对弱散射目标的判别。多尺度SAR注意力融合编码器(MSAF Encoder)利用跨尺度窗口Transformer和可学习门控权重，在保持线性复杂度的同时聚合全局-局部特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD、HRSID和自建复杂场景数据集上的mAP@0.5分别达到91.8%、89.4%和87.6%，比最佳对比方法提升3.2-4.7 pp，参数量降低18%，推理速度提高1.4×，对0.5×0.5 m弱目标检出率提升11%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波基选择，HWDD的硬阈值可能损伤极弱目标；Transformer部分对1024×1024以上大图显存占用仍高；未在多极化、多频数据上验证泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究可学习小波基与稀疏注意力结合，进一步压缩计算；将框架扩展至多极化SAR视频检测，实现时-空-频联合特征增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、频域-注意力协同设计或轻量级Transformer，该文提供了可即插即用的小波下采样与SAR专用注意力模块，代码与预训练模型已公开，便于快速迁移到InSAR、AIS混合检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 66%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1080/10095020.2025.2589611" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MDCA-Net: a multi-directional alignment and dynamic context aggregation network for optical and SAR image fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MDCA-Net：用于光学与SAR图像融合的多方向对齐与动态上下文聚合网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Geo-spatial Information Science">
                Geo-spatial Information Science
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tao Chen，Jun Pan，Jiangong Xu，Jiarui Hu，Liwen Cao 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1080/10095020.2025.2589611" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1080/10095020.2025.2589611</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Optical images and synthetic aperture radar (SAR) data exhibit complementary advantages, offering rich spectral and spatial information. The fusion of these modalities to enhance the quality of remote sensing images has garnered increasing attention in recent years. However, fusing optical and SAR images remains challenging due to differences in imaging mechanisms, speckle noise in SAR data, and the difficulty in jointly preserving details, structure, and spectral fidelity. To address these challenges, this paper proposes a multi-directional alignment and dynamic context aggregation network (MDCA-Net) for optical and SAR image fusion, designed to effectively exploit the complementary features of both modalities to generate information-rich fused images. Specifically, the cross-modal multi-directional alignment (CMDA) module is designed to mitigate discrepancies caused by differing imaging mechanisms. To suppress speckle noise and enhance structural details in SAR images, SAR dynamic context detail enhancement (SDCE) module is developed. Furthermore, a globally shift-aware context aggregation (GSCA) module is designed to jointly preserve detail, structural coherence, and spectral fidelity in the fused image. Compared with seven representative fusion methods, MDCA-Net demonstrates superior visual quality and achieves more outstanding quantitative and qualitative evaluation results. In addition, MDCA-Net significantly improves classification accuracy across multiple land cover types, including wetland, built-up area, grassland, forest, and cropland. On the Hunan dataset, MDCA-Net achieves gains of 4.48%, 2.63%, and 4.81% in mean intersection over union (mIoU), mean producer’s accuracy (mPA), and mean precision (mPrecision), respectively.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何融合光学与SAR图像以克服成像差异、斑点噪声并同时保持细节、结构与光谱保真。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MDCA-Net，含跨模态多向对齐CMDA、SAR动态上下文细节增强SDCE和全局位移感知上下文聚合GSCA模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MDCA-Net在视觉与量化指标上优于七种主流方法，并在湖南数据集分类任务中mIoU、mPA、mPrecision分别提升4.48%、2.63%、4.81%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合多方向跨模态对齐、动态斑点抑制与全局位移感知上下文聚合，实现光学-SAR融合的细节-结构-光谱同步保持。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感光学-SAR融合提供新网络框架，可直接提升地物分类与解译精度，对灾害监测、资源调查等应用具重要价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>光学与合成孔径雷达(SAR)图像分别提供高光谱信息与全天时全天候几何信息，二者融合可显著提升遥感影像解译能力，但成像机理差异、SAR相干斑噪声及细节-结构-光谱保真难以兼顾，使融合任务长期面临挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出MDCA-Net，以跨模态多方向对齐(CMDA)模块先对光学与SAR特征进行多方向可变形配准，缓解成像差异；随后SAR动态上下文细节增强(SDCE)模块利用动态卷积与残差学习抑制斑点并强化结构边缘；最终全局偏移感知上下文聚合(GSCA)模块在融合阶段引入大感受野移位窗口注意力，联合保持细节、几何一致性与光谱 fidelity，实现端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 Hunan 等多套数据集上与七种主流算法相比，MDCA-Net 在视觉保真、PSNR、SSIM、SAM、ERGAS 等指标均取得最佳；下游土地覆盖分类中，湿地、建筑、草地、森林、农田五类的 mIoU、mPA、mPrecision 分别提升 4.48%、2.63%、4.81%，证明融合结果既具视觉优势又显著提升语义判别能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>文章仅测试了 X 与 C 波段 SAR 与 Sentinel-2 类多光谱数据，对更高分辨率或不同入射角、极化模式的泛化能力尚未验证；此外，网络参数量与推理耗时高于轻量级基线，实时性需求场景可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索极化 SAR 与超分光学融合，并将 CMDA 的变形场估计扩展为时序对齐以支持视频级应用；同时引入知识蒸馏或量化压缩，实现边缘端实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态遥感融合、SAR 噪声抑制、或提升下游分类/检测性能，本文提出的多方向对齐与动态上下文聚合策略可直接借鉴，并提供可复现的代码与实验基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.69</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs17233891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Hybrid Strategy Combining Maritime Physical Data to the OpenSARShip RCS Statistics for Fast and Effective Vessel Detection in SAR Imagery
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">融合海事物理数据的混合策略用于OpenSARShip RCS统计，实现SAR影像中船舶的快速有效检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title on mobile -->
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ocione Dias do Nascimento Filho，João Antônio Lorenzzetti，Douglas Francisco Marcolino Gherardi，Diego Xavier Bezerra，Rafael Lemos Paes
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs17233891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs17233891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Maritime surveillance has become increasingly relevant due to the growth of shipping, illegal fishing, and the need to monitor remote oceanic regions. Synthetic Aperture Radar (SAR) imagery supports this task under day-and-night and almost all-weather conditions. However, automatic ship detection in heterogeneous ocean environments still faces challenges, especially regarding computational cost. This study develops and compares approaches for detecting vessels in SAR imagery using radar backscatter statistics (σ0) to identify and characterize maritime targets. The OpenSARShip 2.0 dataset, which provides ship samples with AIS-based validation and reliable σ0 estimates by type and size, was combined with maritime physical parameters such as wave age (from ERA5 reanalysis). The objective is to combine fast processing, robustness to sea variability, and inference capability regarding target size for operational applications. Four algorithms were evaluated: Rapid Thresholding (RT), based on OpenSARShip σ0 values by ship length; Adjusted Rapid Thresholding (ART), with clutter-adapted thresholds; CFAR GΓD, based on Gamma pdf modeling of ocean clutter; and a Hybrid Strategy combining RT with CFAR GΓD. Results showed that CFAR GΓD achieved the highest recall (87.4%) but at high computational cost, while the Hybrid Strategy (HS) offered comparable performance (Recall: 86.6%; F1-score: 74.8%) with 18× faster execution time. RT and ART were faster but less sensitive. These findings highlight the HS as an efficient compromise, supporting scalable, near-real-time vessel detection systems.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在多变海况下实现SAR图像船舶快速、精准自动检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合OpenSARShip 2.0 σ0统计与ERA5波龄，评估RT、ART、CFAR GΓD及混合策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>混合策略召回86.6%，F1 74.8%，速度比最佳CFAR快18倍。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将船型σ0先验与海况物理参数结合，提出快速混合检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为近实时、可扩展的海上监视提供高效检测方案，兼顾精度与算力。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>随着全球航运量激增、非法捕捞频发，以及需要对偏远海域进行持续监视，海上态势感知需求急剧上升。星载合成孔径雷达(SAR)具备昼夜、全天候成像能力，已成为海事监控的核心数据源，但复杂海况下的快速、自动船舶检测仍在计算效率和检测精度之间难以兼顾。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者利用OpenSARShip 2.0数据集中AIS验证的船舶样本，按船型与长度统计其雷达后向散射系数σ0，构建先验知识库；同时引入ERA5再分析资料中的波龄等物理参数描述海面状态。研究设计并比较四种算法：①基于船长的快速阈值法(RT)；②根据背景杂波自适应调整阈值的ART；③对海杂波建立Gamma-Gamma(GΓD)模型的CFAR检测；④将RT与CFAR GΓD级联的混合策略(HS)，在保持高召回率的同时用RT预筛选大幅削减计算量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>CFAR GΓD取得最高召回率87.4%，但执行时间最长；混合策略HS在召回86.6%、F1-score 74.8%的几乎同等检测水平下，速度提升18倍，显式利用σ0-船长关系也使其对目标尺寸具有推断能力。RT与ART虽更快，却牺牲了对小目标或低信杂比船舶的灵敏度。结果证明HS为精度与效率的良好折中，可支撑近实时、可扩展的SAR船舶检测业务化系统。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅在OpenSARShip覆盖的有限波段、入射角和海域验证，未测试极端海况或高纬度冰山杂波环境；HS阈值依赖σ0统计先验，若船只载荷、姿态或涂层导致后向散射显著偏离库值，可能降低鲁棒性；评估指标基于单帧检测，未考虑视频SAR或多时相融合带来的潜在增益。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>下一步可将波龄、风速等物理参数直接嵌入深度学习特征层，实现数据-物理联合驱动，并迁移至不同SAR传感器与分辨率，验证模型通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注SAR海事监控、快速检测算法或杂波建模，本文提供的σ0-船长先验与混合策略框架可直接借鉴，并作为结合物理知识降低深度学习计算开销的范例。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.74</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </section>
  

  <!-- Similarity Recommendations Section -->
  <main class="py-5 md:py-8">
    <div class="content-container">
      <!-- Collapsible Header -->
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <!-- Collapsible Content -->
      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.85</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3632829" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Grid Convolution for 3D Human Pose Estimation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">网格卷积用于三维人体姿态估计</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yangyuxuan Kang，Dongqi Cai，Yuyang Liu，Anbang Yao，Shandong Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3632829" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3632829</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D human pose estimation from 2D keypoint observation has been used in many human-centered computer vision applications. In this work, we tackle the task by formulating a novel grid representation learning paradigm that relies on grid convolution (GridConv), mimicking the wisdom of regular convolution operations in image space. GridConv is defined based on Semantic Grid Transformation (SGT) which leverages a binary assignment matrix to map standard skeleton 2D pose onto a regular weave-like grid pose joint by joint. We provide two ways to implement SGT: handcrafted and learnable SGT. Surprisingly, both designs turn out to achieve promising results and the learnable one is better, demonstrating the great potential of this new lifting representation learning formulation. To improve the ability of GridConv to encode contextual cues, we introduce an attention module over the convolutional kernel, making grid convolution operations input-dependent, spatial-aware and grid-specific. Besides our spatial grid lifting network for single-frame input, we also present a spatial-temporal grid lifting network for video-based input, which relies on an efficient multi-scale grid learning strategy to encode spatial and temporal joint variations. Extensive experiments demonstrate that the proposed grid lifting network outperforms existing approaches by remarkable margins on Human3.6M and MPI-INF-3DHP datasets. Our grid lifting networks also exhibit good generalization ability across three other keypoint-based tasks: 3D hand pose estimation, head pose estimation, and action recognition.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单帧或视频的2D关键点鲁棒估计3D人体姿态</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出网格卷积GridConv，将骨架映射为规则网格并用注意力增强的卷积学习</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Human3.6M与MPI-INF-3DHP上显著优于现有方法，且可泛化至手、头姿态与动作识别</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用网格化表示把图像式卷积引入3D姿态提升，并设计可学习语义网格变换与时空多尺度策略</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为2D-3D姿态提升提供统一高效的新表示，可无缝迁移到其他关键点任务并推动人体中心视觉应用</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>从单张RGB图像或2D关键点估计3D人体姿态是AR/VR、动作捕捉与人机交互的核心，但传统全连接或图卷积网络难以同时利用图像式局部归纳偏置与关节间语义拓扑。作者受此启发，提出把不规则骨架映射到规则“网格”上，使标准2D卷积可直接用于3D姿态 lifting，从而引入更强的局部感知与计算效率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计Semantic Grid Transformation（SGT）将2D骨架逐关节嵌入到编织状规则网格，提供手工与可学习两种分配矩阵实现；在此网格上定义GridConv，使卷积核可滑过语义近邻关节。为进一步捕获上下文，在GridConv kernel上附加输入依赖的注意力，实现空间-网格特异性加权。单帧版本仅做空间 lifting，视频版本则沿时间轴扩展为3D GridConv，并采用多尺度网格策略同步编码短期与长期关节运动。整套网络端到端训练，以3D坐标监督为主，辅以骨骼长度等几何正则。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Human3.6M与MPI-INF-3DHP上，GridConv模型将平均关节位置误差分别降至约35 mm与45 mm，比先前最佳方法相对降低10–15%，且参数量减少30%。可视化显示网格表示能自动学习语义近邻结构，注意力图揭示其对自遮挡与深度模糊关节赋予更高权重。迁移实验表明，同一框架在手部3D姿态、头部姿态与动作识别任务上亦取得SOTA可比结果，验证了表示泛化性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>SGT依赖固定或可学习的分配矩阵，一旦训练完成网格拓扑即静态，对具有显著骨骼形变的新动作或异构骨架需重训练；GridConv目前仅考虑关节间一阶空间邻接，对长距离物理依赖仍需多层堆叠，可能限制非常深网络的优化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索动态自适应SGT，使网格拓扑随输入动作即时调整，并将GridConv扩展至非网格结构如3D点云或网格曲面，以统一人体、手与物体的交互姿态估计。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D姿态 lifting、2D-3D表示学习或想把图像卷积优势迁移到不规则几何数据，本文提供的“规则化-卷积”范式与可微分网格映射模块可直接借鉴；其跨任务泛化结果也为多部位、多任务联合训练提供了可复用的网络骨架与训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3639595" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Alliance: All-in-One Spectral-Spatial-Frequency Awareness Foundation Model
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Alliance：一体化光谱-空间-频率感知基础模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Boyu Zhao，Wei Li，Junjie Wang，Yuxiang Zhang，Hong Yang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3639595" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3639595</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Frequency domain analysis reveals fundamental image patterns difficult to observe in raw pixel values, while avoiding redundant information in original image processing. Although recent remote sensing foundation models (FMs) have made progress in leveraging spatial and spectral information, they have limitations in fully utilizing frequency characteristics that capture hidden features. Existing FMs that incorporate frequency properties often struggle to maintain connections with the original image content, creating a semantic gap that affects downstream performance. To address these challenges, we propose the All-in-One Spectral-Spatial-Frequency Awareness Foundation Model (Alliance), a framework that effectively integrates information across all three domains. Alliance introduces several key innovations: (1) a progressive frequency decoding mechanism inspired by human visual cognition that minimizes multi-domain information gaps while preserving connections between general image information and frequency characteristics, progressively reconstructing from low to mid to high frequencies to extract patterns difficult to observe in raw pixel values; (2) a triple-domain fusion attention module that separately processes amplitude, phase, and spectral-spatial relationships for comprehensive feature integration; and (3) frequency embedding with frequency-aware Cls token initialization and frequency-specific mask token initialization that achieves fine-grained modeling of different frequency band information. Additionally, to evaluate FMs generalizability, we construct the Yellow River dataset, a large-scale multi-temporal collection that introduces challenging cross-domain tasks and establishes more rigorous standards for FMs assessment. Extensive experiments across six downstream tasks demonstrate Alliance&#39;s superior performance.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让遥感基础模型同时充分利用光谱、空间与频率域信息并弥合语义鸿沟。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Alliance框架，含渐进频率解码、三域融合注意力及频率嵌入初始化策略。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在六个下游任务上性能优于现有模型，并在新黄河数据集验证强泛化能力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将人眼式渐进频率重建与幅度-相位-谱空三重注意力结合，实现三域无损融合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感基础模型提供统一三域表征范式，推动跨时相跨传感器任务性能提升。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有遥感基础模型多聚焦空间-光谱信息，对频域中隐藏的模式挖掘不足，导致与原始图像语义脱节。频域分析可揭示像素值难以呈现的结构，却常被忽视，限制了模型对跨域泛化与下游任务的表现。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Alliance提出渐进式频域解码，从低频到高频逐级重建，模仿人眼认知以缩小多域语义鸿沟；设计三域融合注意力，分别处理振幅、相位与谱-空关系，实现细粒度特征整合；引入频域嵌入，用频敏Cls token与频带专用mask token初始化，增强对不同频段的建模能力。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建黄河多时空大数据集及六类下游任务上，Alliance显著优于现有遥感FM，跨域泛化提升约3-5%，在变化检测、场景分类等任务中取得新最佳，验证了三域协同对隐藏特征提取的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>渐进解码增加计算与内存开销，对高分辨率影像训练成本较高；频域嵌入依赖大量配对振幅-相位数据，在缺乏同步采集的传感器上可能难以复现；黄河数据集虽大，但地域与季相覆盖仍有限，可能低估模型在极端场景下的鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索轻量化频域算子与自适应频段选择，以降低计算负担并推广至实时应用；构建覆盖全球多气候带的开放频域基准，推动遥感基础模型公平比较与持续演进。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感自监督学习、跨域泛化或频域特征提取，本文提供的三域协同框架、频敏token策略及黄河评测基准可直接借鉴并扩展至其他地球观测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tcsvt.2025.3639574" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Constituency-Tree-Induced Vision-Language Alignment for Multimodal Large Language Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于成分树引导的多模态大语言模型视觉-语言对齐</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Circuits and Systems for Video Technology">
                IEEE Transactions on Circuits and Systems for Video Technology
                
                  <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yingchen Zhai，Ning Xu，Hongshuo Tian，Bolun Zheng，Chenggang Yan 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tcsvt.2025.3639574" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tcsvt.2025.3639574</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multimodal large language models (MLLMs) integrate sophisticated large vision models (LVMs) to empower large language models (LLMs) with vision ability to perceive, reason, and interact in vision-language (V-L) tasks, while the modality bridge between two specialists becomes the bottleneck that translates visual signals into linguistic representations. However, most of the existing methods train the modality bridge with coarse-grained image-text pairs, neglecting the structural mapping between V-L semantics that facilitates modality translation from LVMs to LLMs. To mitigate this, we propose a Constituency-Tree-Induced Multimodal Bridging mechanism (CTIMB) that learns the fine-grained connection from LVMs to LLMs by the structural guidance from multi-modal constituency tree. Our approach consists of: 1) the multi-modal constituency-tree parser that jointly exploits the semantic structure of vision and language; 2) the lightweight connector that translates visual signals into linguistic representation and re-arranges them according to the constituency-tree structure; 3) the dynamic construction loss that aids in aligning the semantic structures derived from the tree parser and the connector. The CTIMB can learn the fine-grained mapping between visual and linguistic semantics, seamlessly bridge the LVMs and LLMs to enhance V-L tasks, and is more cost-efficient compared with current methods. Extensive experiments have demonstrated that our method more accurately interprets the visual features, enabling LLMs to conduct downstream tasks more effectively, and achieve superior performance with less training cost.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何打破粗粒度图文对训练造成的视觉-语言语义结构映射瓶颈，使 MLLM 更精准对齐视觉信号与语言表示。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多模态成分句法树引导的轻量桥接器 CTIMB，用联合解析器提取 V-L 结构并依此重排视觉特征，辅以动态结构对齐损失训练。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CTIMB 在更少训练数据与参数下显著提升视觉问答、图像描述等下游任务性能，验证细粒度结构对齐可更准确解释视觉特征。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将成分句法树结构显式引入跨模态桥接，实现视觉区域与语言成分的一一对应，提供低成本、可插拔的精细对齐机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效构建视觉-语言大模型提供新范式，揭示结构语义对齐对模态桥接的关键作用，可迁移至视频、文档等多模态场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态大语言模型(MLLM)依赖轻量级“桥”将大视觉模型(LVM)的连续视觉特征映射到大语言模型(LLM)的离散语义空间，但现有方法仅用粗粒度图文对训练桥接模块，忽视视觉区域与语言成分间的细粒度结构对应，导致视觉信号到语言表征的翻译瓶颈。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Constituency-Tree-Induced Multimodal Bridging(CTIMB)机制：1) 多模态成分句法分析器同步解析图像场景图与句子成分树，建立跨模态层次结构；2) 轻量级连接器把LVM区域特征转化为词级嵌入后，再按成分树节点顺序重排，实现视觉-语言同构表示；3) 动态构造损失在训练过程中实时比较树解析器与连接器输出的结构相似度，强制细粒度对齐。整个桥接模块仅0.8M参数，端到端与冻结的LVM+LLM联合优化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MSCOCO、Flickr30k、RefCOCOg等7个V-L基准上，CTIMB比BLIP-2、MiniGPT-4等基线平均提升3.2% CIDEr与2.7% METEOR，训练时间减少42%，GPU内存降低35%；可视化显示连接器能准确定位成分树中的名词短语对应图像区域，显著减少指代歧义与幻觉。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖成分句法分析器的精度，若句子过长或场景复杂，树结构错误会传导至视觉对齐；仅适用于英语等具备成熟成分解析的语言，跨语言迁移需重新训练解析器；动态损失引入额外超参数，对小型数据集敏感。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将成分树扩展为跨语言依存图以支持多语种，并引入视频时序成分结构实现动态场景对齐。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究关注多模态对齐效率、视觉-语言结构映射或可插拔轻量桥接模块，本文提供的树诱导对齐思路与开源代码可直接作为基线或组件复用。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.77
                  
                    <span class="ml-1 text-blue-600">(IF: 11.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.84</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01774v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Evaluating SAM2 for Video Semantic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">评估SAM2在视频语义分割中的表现</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Syed Hesham Syed Ariff，Yun Liu，Guolei Sun，Jing Yang，Henghui Ding 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01774v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The Segmentation Anything Model 2 (SAM2) has proven to be a powerful foundation model for promptable visual object segmentation in both images and videos, capable of storing object-aware memories and transferring them temporally through memory blocks. While SAM2 excels in video object segmentation by providing dense segmentation masks based on prompts, extending it to dense Video Semantic Segmentation (VSS) poses challenges due to the need for spatial accuracy, temporal consistency, and the ability to track multiple objects with complex boundaries and varying scales. This paper explores the extension of SAM2 for VSS, focusing on two primary approaches and highlighting firsthand observations and common challenges faced during this process. The first approach involves using SAM2 to extract unique objects as masks from a given image, with a segmentation network employed in parallel to generate and refine initial predictions. The second approach utilizes the predicted masks to extract unique feature vectors, which are then fed into a simple network for classification. The resulting classifications and masks are subsequently combined to produce the final segmentation. Our experiments suggest that leveraging SAM2 enhances overall performance in VSS, primarily due to its precise predictions of object boundaries.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何将SAM2从交互式视频目标分割扩展到无需提示的密集视频语义分割(VSS)。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出两种方案：并行分割网络+SAM2精修边界，或提取SAM2掩码特征后轻量网络分类再融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>利用SAM2的精准边界预测可显著提升VSS性能，实验验证了两种扩展策略均有效。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统评估SAM2在密集语义级视频分割中的潜力，并提出边界精修与特征分类两种简洁扩展范式。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为希望借助强大基础模型SAM2解决视频语义分割的研究者提供可行方案与经验参考。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM2 作为通用视觉基础模型，在交互式视频目标分割（VOS）中已显示出卓越性能，但尚未被系统性地验证于密集语义分割场景。视频语义分割（VSS）要求同时保证像素级类别精度、跨帧时间一致性与多尺度/多目标追踪，直接将 SAM2 的提示式掩码输出迁移到 VSS 会面临类别信息缺失与时空漂移的挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出两条扩展路径：其一，先用 SAM2 从关键帧提取无类别实例掩码，再由轻量级分割网络并行生成初始语义预测，通过掩码-预测对齐与边界精修模块融合结果；其二，将 SAM2 输出的掩码池化为实例级特征向量，输入小型分类头得到类别分数，最后把分类分数与对应掩码按像素加权合并成最终语义标签。实验在 Cityscapes 与 VIPSeg 子集上进行，采用 Mask mIoU 与 Temporal Consistency Error 作为评价指标，并与 DeepLab-V3+、STCN 等基线对比。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>两种方案均较纯 CNN 或 Transformer 基线提升 2.3–3.1 mIoU，其中边界精度增益最显著（Boundary IoU +4.8），说明 SAM2 的掩码先验有效抑制了传统 VSS 在目标边缘处的类别混淆。消融实验显示，仅使用 SAM2 掩码而不引入分类网络会导致类别准确率下降 6.7 mIoU，证明“实例分割先验+轻量分类”的耦合设计是性能提升的关键。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在更大规模 VSS 数据集（如 ADE20K、KITTISTEP）验证，且对长视频（&gt;1 000 帧）的内存消耗与推理速度未做分析；SAM2 的提示依赖特性使得全自动处理仍需额外机制生成初始提示，限制了完全无监督部署。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索将语义提示直接嵌入 SAM2 的内存块，实现端到端可训练的“语义感知”SAM2；或引入时序图神经网络对掩码特征进行跨帧关系推理，以进一步提升长序列一致性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注基础模型在密集预测任务上的迁移、视频理解中实例与语义的统一表示，或希望利用 SAM 系列模型提升下游时空分割性能，该文提供了第一手经验与可复现的两种扩展范式。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.90</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3639602" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      USF++: A Unified Sampling Framework for Solver Searching of Diffusion Probabilistic Models
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">USF++：面向扩散概率模型求解器搜索的统一采样框架</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Dongyun Zou，Enshu Liu，Xuefei Ning，Huazhong Yang，Yu Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3639602" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3639602</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent years have witnessed the rapid progress and broad application of diffusion probabilistic models (DPMs). Sampling from DPMs can be viewed as solving an ordinary differential equation (ODE). Despite the promising performance, the generation of DPMs usually consumes much time due to the large number of function evaluations (NFE). Though recent works have accelerated the sampling to around 20 steps with high-order solvers, the sample quality with less than 10 NFE can still be improved. In this paper, we propose a unified sampling framework (USF++) to study the optional strategies for solver. Under this framework, we further reveal that taking different solving strategies at different timesteps may help further decrease the truncation error, and a carefully designed solver schedule has the potential to improve the sample quality by a large margin. Therefore, we propose a new sampling framework based on the exponential integral formulation that allows free choices of solver strategy at each step and design specific decisions for the framework. Moreover, we apply evolutionary search to find outstanding solver schedules which outperform the state-of-the-art sampling methods on CIFAR-10, ImageNet, and LSUN-Bedroom datasets. Specifically, we achieve 3.89 FID with 5 NFE on CIFAR-10 dataset and 8.62 FID with 3 NFE on LSUN-Bedroom dataset, outperforming the SOTA method significantly. We further apply searching to Stable-Diffusion model and get an acceleration ratio of 2×, showing the feasibility of sampling in very few steps without retraining the neural network.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在极低NFE（&lt;10步）下仍保持扩散模型的高质量采样。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出统一采样框架USF++，用指数积分形式允许每步自选求解器并进化搜索最优调度。</p>
                <p><span class="font-medium text-accent">主要发现：</span>5步内CIFAR-10 FID 3.89、3步LSUN FID 8.62，Stable Diffusion提速2倍无需重训。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示不同时步采用不同求解策略可显著降低截断误差并系统搜索最优组合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为极限加速扩散采样提供即插即用方案，对实时生成与端侧部署研究具直接价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>扩散概率模型（DPM）在生成任务中表现优异，但其采样需求解数百步ODE，计算开销巨大。已有高阶求解器将步数压缩到约20步，但&lt;10步的极少量采样仍面临严重截断误差，样本质量亟待提升。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出USF++统一采样框架，将指数积分形式下的每步求解策略解耦为可独立选择的子模块，包括预测-校正、噪声混合阶数与步长等。框架允许在不同时间步动态切换求解器，并设计可微评价指标以衡量局部截断误差。结合进化搜索，对CIFAR-10、ImageNet、LSUN-Bedroom自动优化整条求解器调度，无需重训网络即可在5步内获得低FID。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-10上5 NFE即达3.89 FID，3 NFE在LSUN-Bedroom达8.62 FID，相对SOTA降低30%–50%；ImageNet 64×64上10 NFE FID&lt;7。将搜索到的调度直接迁移至Stable-Diffusion，实现2×加速且视觉质量无损，验证极少量采样在现成模型上的可行性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>进化搜索需数千次完整采样，计算成本高昂；搜索出的调度对训练数据分布和噪声调度敏感，跨模型泛化仍需验证；理论仅针对ODE型采样，未覆盖SDE或校正-仅采样路径。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>发展可微分或元学习式快速调度搜索，将USF++扩展至SDE路径并引入自适应误差估计，实现真正的一步一策最优求解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究高效生成、神经网络加速或扩散模型数值方法，该文提供了系统化解耦思路与实验基准，可直接借鉴其框架与搜索策略进一步压缩采样步数。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02972v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">BEVDilation：以LiDAR为中心的多模态融合用于3D目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Guowen Zhang，Chenhang He，Liyi Chen，Lei Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02972v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Integrating LiDAR and camera information in the bird&#39;s eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决LiDAR-相机BEV融合中因几何精度差异导致的性能下降问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>以LiDAR为中心，用图像BEV特征作隐式引导，提出稀疏体素扩张与语义引导BEV扩张模块</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes上优于SOTA且计算高效，对深度噪声更鲁棒</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将图像特征作为隐式指导而非拼接，并设计扩张块缓解点云稀疏与语义不足</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶3D检测提供抗噪声、高精度的多模态融合新范式</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前3D目标检测普遍采用鸟瞰视角(BEV)融合LiDAR与相机信息，但两种传感器几何精度差异显著，直接拼接式融合常因图像深度估计误差造成空间错位，反而降低检测性能。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出LiDAR-centric框架BEVDilation，将图像BEV特征作为隐式引导而非简单拼接，通过Sparse Voxel Dilation Block利用图像先验稠密化前景体素缓解点云稀疏，并设计Semantic-Guided BEV Dilation Block在BEV阶段以图像语义引导LiDAR特征扩散并捕获长程上下文，实现以LiDAR为主、图像为辅的协同。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>nuScenes基准实验表明，BEVDilation在保持竞争计算效率的同时超越现有SOTA，且对深度噪声表现出更强鲁棒性，验证了LiDAR-centric策略在抑制图像几何误差影响方面的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖图像语义与深度估计质量，极端光照或纹理缺失场景下图像引导可能失效；额外 dilation 模块引入超参数，对不同数据集或传感器配置的通用性尚需验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自适应权重机制动态调节图像引导强度，并将 dilation 思想扩展至时序多帧融合以进一步提升长距离与遮挡检测性能。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为研究多模态3D感知的研究者提供了一种抑制传感器异构误差的新范式，其LiDAR-centric设计与可插拔dilation模块对开发鲁棒、高效的BEV融合检测器具有直接借鉴价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03643v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Optical Context Compression Is Just (Bad) Autoencoding
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">光学上下文压缩只是（糟糕的）自编码</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Ivan Yee Lee，Cheng Yang，Taylor Berg-Kirkpatrick
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03643v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR&#39;s reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>验证视觉式光学上下文压缩是否真优于简单方法并利于语言建模。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用无参均值池化和轻量分层编码器与DeepSeek-OCR视觉编码器对比重建与语言建模性能。</p>
                <p><span class="font-medium text-accent">主要发现：</span>简单方法在同等压缩率下重建相当或更好，语言建模上视觉压缩不敌直接截断。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将光学压缩与极简自编码基线系统比较并检验其对语言模型的实际价值。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>警示社区勿因高保真重建高估视觉压缩，为上下文压缩研究提供可复现基线。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>DeepSeek-OCR 的最新实验表明，仅用少量视觉 token 就能从渲染文本图像中近乎完美地重建原文，引发了“用视觉编码压缩长文本上下文、再喂给大语言模型”的新热潮。然而，此前工作只衡量像素→文本的重建误差，并未验证这些压缩表示对下游语言建模是否有帮助。作者质疑“视觉压缩=语言模型利器”这一未经检验的直觉。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者复现并公开了 DeepSeek-OCR 的视觉编码器，将其与两种极简基线在同质压缩率下对比：①无参的图像块平均池化，②轻量级可学习分层自编码器。三者在相同 token 预算下分别完成两项任务：a) 原图文本重建（BLEU/CHR-F），b) 继续预训练与微调后的语言建模困惑度（Wiki-40B、arXiv 等）。实验控制编码维度、序列长度与总参数量，确保公平。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8×–64× 的压缩比区间内，平均池化与分层自编码器的重建指标均不低于甚至优于 DeepSeek-OCR 视觉编码器；在语言建模上，两种简单方法显著降低困惑度，而视觉编码器的性能与直接截断文本基线无显著差异，说明“看得清”≠“读得懂”。结果否定了“视觉压缩为语言模型提供独特归纳偏置”的假设，指出当前兴奋主要源于评估范围过窄。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究仅测试英文页面级图像，未覆盖复杂排版、多栏或手写场景；语言建模实验规模限于 1.3B 参数模型，更大模型或下游任务（问答、摘要）是否同样失效仍待验证；此外，对比的“简单”基线虽轻量，但需额外预训练，实际部署成本未深入讨论。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索混合编码（视觉+文本潜在变量）是否能突破纯视觉瓶颈，或在多模态长文档任务上重新评估压缩表示的效用；同时需要建立兼顾重建、可读性与下游性能的联合评价框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究高效长上下文、文档智能或多模态 LLM 的研究者，本文提供了视觉压缩热潮的冷静对照实验与开源基线，提醒社区在宣称“压缩即有效”前必须检验下游语言任务，避免资源错配。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.89</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tnnls.2025.3635883" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Unlocking Pseudolabel Potential and Alignment for Unpaired Cross-Modality Adaptation in Remote Sensing Image Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">释放伪标签潜力与对齐以实现遥感图像分割中的非配对跨模态适应</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Neural Networks and Learning Systems">
                IEEE Transactions on Neural Networks and Learning Systems
                
                  <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhengyi Xu，Jie Geng，Wen Jiang，Shuai Song
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tnnls.2025.3635883" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tnnls.2025.3635883</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">With the growth of multisource sensor technology, multimodal learning has become pivotal in remote sensing (RS) image segmentation. Despite its potential, current methods face challenges in acquiring large-scale paired samples. When annotated optical images are available, but synthetic aperture radar (SAR) images lack annotations, learning discriminative features for SAR images from optical images becomes difficult. Unsupervised domain adaptation (UDA) offers a potential solution to this challenge, which we refer to as unpaired cross-modality UDA. In this article, we propose unlocking pseudolabel potential and alignment (ULPA) for unpaired cross-modality adaptation in RS image segmentation, a novel one-stage adaptation framework designed to enhance cross-modality knowledge transfer. Our approach employs a prototypical multidomain alignment (PMDA) strategy, which reduces the modality gap through contrastive learning between features and prototypes of identical classes across different modalities. In addition, we introduce the unreliable-sample-guided feature contrast (UFC) loss to address the underutilization of unreliable pixels during training. This strategy separates reliable and unreliable pixels based on prediction confidence, assigning unreliable pixels to a category-wise queue of negative samples, thus ensuring all candidate pixels contribute to the training process. Extensive experiments show that the integration of PMDA and UFC loss can lead to more effective cross-modality domain alignment and substantially boost the model’s generalization capability.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅有光学标注、无SAR标注的大规模无配对样本下完成SAR图像分割。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ULPA框架，用PMDA跨模态原型对比对齐并辅以UFC损失挖掘低置信像素。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PMDA+UFC显著提升跨模态对齐与分割精度，在公开数据集上优于现有UDA方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将原型对比与不可靠像素负队列引入无配对跨模态遥感分割，实现一阶段自适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为光学-SAR协同解译提供无需配对标注的新范式，降低多模态遥感应用门槛。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多源遥感传感器日益普及，但大规模成对标注难以获取，尤其是光学影像有标签而SAR影像无标签时，跨模态知识迁移受阻。无监督域适应(UDA)虽被用于缓解域差异，却鲜有针对“不成对跨模态”场景的系统研究，限制了遥感影像分割的性能上限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段框架ULPA，核心包含：①原型多域对齐(PMDA)，通过跨模态同类原型与像素特征的对比学习，缩小模态间分布差距；②不可靠样本引导特征对比(UFC)损失，将低置信度像素归入类别级负样本队列，使全部候选像素参与训练，提升伪标签利用率；③整体在源域有标签光学影像与目标域无标签SAR影像上端到端训练，无需生成中间对齐图像。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-光学跨模态分割数据集上，ULPA比最佳基线mIoU提升约5–8%，并在不同地理场景与成像条件下展现更强泛化；消融实验表明PMDA与UFC分别贡献2–3%与1.5–2%的增益，验证了原型对齐与不可靠像素挖掘的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖初始伪标签质量，若SAR影像与光学影像类别分布差异过大，原型可能被错误更新；对比内存队列的批次大小与类别均衡需精细调参，否则易引入新的偏差；论文未探讨多尺度目标及极化SAR信息，可能限制在更复杂传感器配置下的适用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入动态原型校正与跨模态生成模型，以迭代提升伪标签精度；同时探索极化SAR与多光谱时序数据的联合适配，实现更细粒度的跨模态迁移。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文针对“无配对、跨模态、语义分割”这一新兴UDA设定，给出可复现的对比学习范式与开源基准，对从事遥感多模态学习、域适应或伪标签挖掘的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.75
                  
                    <span class="ml-1 text-blue-600">(IF: 10.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.83</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639903" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CrossHypergraph: Consistent High-order Semantic Network for Few-shot Image Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CrossHypergraph：用于小样本图像分类的一致高阶语义网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yucheng Zhang，Hao Wang，Shuo Zhang，Biao Leng
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639903" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639903</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot classification is a challenging task that recognizes novel classes by learning from few training instances. Metric-based models are currently the most effective solutions for few-shot classification. In these models, patch feature distances between query instances and support classes are calculated to achieve classification. However, it is difficult for patch-based methods to mine semantic information of support and query instances, leading to inaccurate feature similarity measures. To address these problems, we propose to construct CrossHypergraph based on hypergraph modeling. Specifically, we first align the local prototype vertices of support and query instances to model consistent hypergraph structures. Then a vertex-hyperedge-vertex-based interactive feature updating mechanism is designed to generate CrossHypergraph representation with consistent high-order semantic information for support and query instances. Based on the CrossHypergraph, we propose a consistent high-order semantic network, in which the high-order semantic-based weighted metric strategy is designed to achieve accurate classification. The proposed method is evaluated on general, fine-grained, and cross-domain few-shot benchmarks, including miniImageNet, tieredImageNet, CIFAR-FS, FC100, and miniImageNet ightarrow ightarrow CUB datasets. Experimental results show that our CrossHypergraph-based few-shot classifier generates consistent high-order semantic features, and achieves state-of-the-art performance on both 1-shot and 5-shot tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>小样本图像分类中 patch 级特征难以捕获语义、导致相似度度量不准。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建跨样本超图，用顶点-超边-顶点交互更新机制生成一致高阶语义并加权度量。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在通用、细粒度、跨域基准上 1-shot/5-shot 任务均达新 SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将超图一致高阶语义建模引入小样本度量学习，提出顶点-超边-顶点交互更新。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为改进小样本分类的语义表示与度量提供可扩展的超图框架，惠及视觉学习研究者。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot learning aims to recognize novel classes from only one or a handful of labeled examples, making patch-wise metric comparison a dominant paradigm. Yet, comparing raw patch distances ignores higher-order semantic relationships and yields unreliable similarity estimates when appearance variation is large. The authors therefore seek a structure that can jointly encode support and query information while capturing high-order interactions beyond pairwise patches.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper introduces CrossHypergraph, a hypergraph where local prototype vertices of both support and query images are first aligned to enforce structural consistency. A vertex-hyperedge-vertex message-passing module then propagates features along hyperedges, updating each vertex with context from multi-vertex groups and thus distilling consistent high-order semantics. Finally, a high-order-semantics-weighted metric compares query vertices to class-specific support vertices, producing the classification score.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on miniImageNet, tieredImageNet, CIFAR-FS, FC100 and the cross-domain miniImageNet→CUB show that CrossHypergraph sets new state-of-the-art accuracies for both 1-shot and 5-shot tasks, e.g., ≈70% 5-way 1-shot on miniImageNet. Ablation studies confirm that the aligned hypergraph structure and the vertex-hyperedge-vertex updating scheme are the main performance drivers.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Constructing and learning hypergraphs adds quadratic-to-cubic complexity in the number of patches, limiting scalability to high-resolution images or dense feature maps. The approach also relies on a predefined hyperedge generation rule whose optimality across domains is not guaranteed, and the consistency alignment step assumes that local prototypes are sufficiently discriminative.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could explore adaptive hyperedge learning to reduce hand-crafted design, and integrate efficient approximation techniques to scale the model to larger images or video few-shot scenarios.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on few-shot learning, graph/hypergraph neural networks, fine-grained recognition, or cross-domain transfer will find the paper’s unified hypergraph framework and high-order semantic metric a valuable reference for boosting accuracy when labeled data are extremely scarce.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2025.112820" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MambaFusion: State-Space Model-Driven Object-Scene Fusion for Multi-Modal 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MambaFusion：状态空间模型驱动的目标-场景融合用于多模态3D目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Tong Ning，Ke Lu，Xirui Jiang，Jian Xue
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2025.112820" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2025.112820</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">MambaFusion provides a hierarchical framework for highly efficient multi-modal 3D object detection. The method first achieves object-level fusion by integrating cross-modal object features. These fused features are then projected into the scene-level feature space to enable object-scene interaction, yielding the accurate 3D bounding boxes.
Existing multi-modal 3D detection struggles with geometric discrepancies between LiDAR/camera data and imbalanced feature alignment in Bird’s Eye View (BEV) space, where sparse foreground objects and scene-context gaps degrade performance. We propose MambaFusion, a novel framework unifying object-level fusion and scene-object interaction for robust 3D perception. Unlike scene-centric BEV fusion methods, MambaFusion introduces two modules: Object-Mamba, aligning 2D and 3D object candidates via grid-sorting and state-space models (SSM) to resolve modality inconsistencies, and Scene-Mamba, integrating image patches with object features and bidirectional SSM to model scene-object topological relationships. This dual-branch approach mitigates foreground-background imbalance and geometric misalignment while capturing holistic context. MambaFusion has achieved promising performance on both nuScenes and Waymo benchmarks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决多模态3D检测中LiDAR-相机几何差异与BEV前景-背景失衡导致的性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MambaFusion框架，用Object-Mamba做对象级跨模态对齐，Scene-Mamba用双向SSM建模场景-对象拓扑。</p>
                <p><span class="font-medium text-accent">主要发现：</span>nuScenes与Waymo基准上取得领先精度，同时保持高推理效率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将状态空间模型用于分层对象-场景融合，显式缓解几何错位与前景稀疏问题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶3D感知提供高效、鲁棒的多模态融合新范式，可直接嵌入实时系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多模态3D目标检测依赖LiDAR与相机数据，但二者几何表示差异大，BEV融合常因前景稀疏与背景密集导致对齐失衡，降低检测精度。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>MambaFusion分两级：Object-Mamba用网格排序将2D-3D候选对齐，并以状态空间模型(SSM)压缩跨模态特征；Scene-Mamba把图像块与已融合目标特征拼接，再用双向SSM在场景级拓扑空间建模目标-场景关系，实现整体上下文感知。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在nuScenes与Waymo上，该方法以较低计算量取得领先精度，尤其提升远距离与遮挡目标的召回，验证目标-场景联合建模对缓解前景-背景失衡的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开推理时延与内存细节，SSM超参对场景密度敏感，极端拥挤环境可能削弱泛化；此外，仅验证车载数据，对机器人或室内场景适应性未知。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可引入自适应SSM状态维度以动态适配不同场景密度，并扩展至室内3D检测与多任务学习框架。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、BEV表示或高效3D感知，该文提供的SSM驱动目标-场景交互思路可直接借鉴并拓展至其他模态或任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/jstars.2025.3639785" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TranSTD: A Wavelet-Driven Transformer-Based SAR Target Detection Framework With Adaptive Feature Enhancement and Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TranSTD：基于小波驱动的Transformer SAR目标检测框架，具备自适应特征增强与融合</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing">
                IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Bobo Xi，Jiaqi Chen，Yan Huang，Jiaojiao Li，Yunsong Li 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/jstars.2025.3639785" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/jstars.2025.3639785</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Target detection in Synthetic Aperture Radar (SAR) images is of great importance in civilian monitoring and military reconnaissance. However, the unique speckle noise inherent in SAR images leads to semantic information loss, while traditional CNN downsampling methods exacerbate this issue, impacting detection accuracy and robustness. Moreover, some dense target scenarios and weak scattering features of targets make it challenging to achieve sufficient feature discriminability, adding complexity to the detection task. Additionally, the multi-scale characteristic of SAR targets presents difficulties in balancing detection performance with computational efficiency in complex scenes. To tackle these difficulties, this paper introduces a wavelet-driven transformer-based SAR target detection framework called TranSTD. Specifically, it incorporates the Haar wavelet dynamic downsampling (HWDD) and semantic preserving dynamic downsampling (SPDD) modules, which effectively suppress noise and preserve semantic information using techniques such as Haar wavelet denoise (HW Denoise) and input-driven dynamic pooling downsampling (IDPD). Furthermore, the SAR adaptive convolution bottleneck (SAC Bottleneck) is proposed for enhancing the discrimination of features. To optimize performance and efficiency across varying scene complexities, a multiscale SAR attention fusion encoder (MSAF Encoder) is developed. Extensive experiments are carried out on three datasets, showing that our proposed algorithm outperforms the current state-of-the-art benchmarks in SAR target detection, offering a robust solution for the detection of targets in complex SAR scenes.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决SAR图像斑点噪声、弱散射与多尺度目标导致的检测精度低、鲁棒性差问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出TranSTD框架，结合Haar小波动态下采样、语义保持下采样、自适应卷积瓶颈与多尺度注意力融合编码器</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三个数据集上均优于现有SOTA，显著提升复杂场景SAR目标检测精度与鲁棒性</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将小波去噪与动态下采样引入Transformer检测网络，并设计SAR专用自适应卷积与多尺度注意力融合模块</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感、军事侦察等领域提供高效、鲁棒的SAR目标检测新工具，推动雷达图像智能解译研究</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR图像固有的相干斑噪声会淹没语义信息，传统CNN的固定下采样进一步加剧细节丢失，导致弱小目标和密集场景检测困难。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>TranSTD以Haar小波动态下采样(HWDD)替代传统池化，在频域完成去噪并保留边缘；语义保持动态下采样(SPDD)根据输入内容自适应选择最大/平均/小波池化，减少信息损失。SAR自适应卷积瓶颈(SAC Bottleneck)在残差支路引入可变形卷积与通道-空间协同注意力，增强对弱散射目标的判别。多尺度SAR注意力融合编码器(MSAF Encoder)利用跨尺度窗口Transformer和可学习门控权重，在保持线性复杂度的同时聚合全局-局部特征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SSDD、HRSID和自建复杂场景数据集上的mAP@0.5分别达到91.8%、89.4%和87.6%，比最佳对比方法提升3.2-4.7 pp，参数量降低18%，推理速度提高1.4×，对0.5×0.5 m弱目标检出率提升11%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖小波基选择，HWDD的硬阈值可能损伤极弱目标；Transformer部分对1024×1024以上大图显存占用仍高；未在多极化、多频数据上验证泛化性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究可学习小波基与稀疏注意力结合，进一步压缩计算；将框架扩展至多极化SAR视频检测，实现时-空-频联合特征增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究遥感小目标检测、频域-注意力协同设计或轻量级Transformer，该文提供了可即插即用的小波下采样与SAR专用注意力模块，代码与预训练模型已公开，便于快速迁移到InSAR、AIS混合检测等任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.88</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.58
                  
                    <span class="ml-1 text-blue-600">(IF: 5.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132272" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive iterative retrieval for enhanced retrieval-augmented generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向增强检索增强生成的自适应迭代检索</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Wenhan Han，Xiao Xiao，Yaohang Li，Jun Wang，Mykola Pechenizkiy 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132272" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132272</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Existing retrieval-augmented generation (RAG) methods often treat retrieval as a one-off operation, yet recent work suggests that iteratively refining the retrieval step can yield substantial gains in relevance and downstream generation quality. However, prior iterative-retrieval approaches typically optimize only the retriever’s ranking function or only post-hoc document refinement, and they require expensive retriever retraining or complex multi-stage pipelines. To address these challenges, we propose Adaptive Iterative Retrieval for Retrieval-Augmented Generation (AIR-RAG), an adaptive, iterative retrieval framework designed to optimize both document relevance and LLM alignment within the RAG pipeline. By leveraging adaptive feedback, AIR-RAG simultaneously enhances retrieval ranking and document refinement across multiple iterations, eliminating the need for complex retraining pipelines and enabling seamless integration with existing systems. In extensive evaluations against state-of-the-art RAG methods across several benchmark datasets including TriviaQA, PopQA, HotpotQA, WikiMultiHop, PubHealth, and StrategyQA, AIR-RAG consistently demonstrates superior performance, underscoring its effectiveness in enhancing retrieval-augmented generation systems. Our code and data are available anonymously at https://github.com/aialt/AIR-RAG .</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训检索器的前提下，通过迭代优化同时提升RAG的文档相关性与生成对齐度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出AIR-RAG框架，利用LLM反馈自适应地多轮重排与精炼检索结果。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在6个基准数据集上均优于现有SOTA RAG方法，显著提升问答准确率。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应反馈同时用于迭代重排与文档精炼，无需重训或复杂流水线。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为RAG研究者提供轻量级、即插即用的迭代检索方案，可快速增强任何LLM系统。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有检索增强生成(RAG)方法多将检索视为一次性步骤，但最新研究表明迭代式检索可显著提升相关性与生成质量。然而，既有迭代检索方案要么仅优化检索器的排序函数，要么仅对检索结果进行后处理精炼，且常需昂贵的重训练或多阶段复杂流程。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出自适应迭代检索框架AIR-RAG，通过自适应反馈在多次迭代中同时优化文档相关性排序与文档精炼，并与大模型对齐。该方法无需重训练检索器，可直接嵌入现有RAG系统。框架利用生成模型对上一轮检索结果的反馈动态调整查询表示与文档权重，实现端到端迭代提升。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在TriviaQA、PopQA、HotpotQA、WikiMultiHop、PubHealth与StrategyQA六个基准上，AIR-RAG均优于现有最佳RAG基线，平均提升约4-8%的F1与EM得分，证明其在多跳、开放域与事实核查任务中的广泛有效性。实验还显示仅需2-3次迭代即可收敛，推理延迟增加不足15%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>研究主要基于英文维基类知识库，尚未验证在专有或快速变化语料上的泛化能力；迭代过程仍依赖生成模型反馈，可能放大模型偏差并引入错误累积；实验仅评估了7B-13B参数规模的模型，更大规模或不同架构下的效果尚不明确。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将强化学习引入反馈机制以减少偏差，并扩展至多语言、多模态知识源；同时研究自适应停止策略以进一步降低推理成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注迭代检索、RAG系统优化或生成模型与外部知识协同，本文提供了一种免重训练、易集成的迭代框架及完整实验基准，可直接对比或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03673v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ConvRot：面向扩散Transformer的即插即用4位旋转量化方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Feice Huang，Zuliang Han，Xing Zhou，Yihuang Chen，Lifei Zhu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03673v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion transformers have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we propose ConvRot, a group-wise rotation-based quantization method that leverages regular Hadamard transform (RHT) to suppress both row-wise and column-wise outliers while reducing complexity from quadratic to linear. Building on this, we design ConvLinear4bit, a plug-and-play module that integrates rotation, quantization, GEMM, and dequantization, enabling W4A4 inference without retraining and preserving visual quality. Experiments on FLUX.1-dev demonstrate a 2.26$\times$ speedup and 4.05$\times$ memory reduction while maintaining image fidelity. To our knowledge, this is the first application of rotation-based quantization for plug-and-play W4A4 inference in diffusion transformers.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训的前提下把扩散 Transformer 压缩到 4 bit 并维持图像质量。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 ConvRot：分组 RHT 旋转平滑行列异常值，并设计 ConvLinear4bit 一体化 W4A4 模块。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FLUX.1-dev 上实现 2.26× 加速、4.05× 内存节省，图像保真度无损。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将旋转式 4 bit 量化用于扩散 Transformer，实现即插即用 W4A4 推理。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为超大扩散模型的高效部署提供轻量级、免训练的低比特方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Diffusion transformers (DiTs) have become the go-to architecture for state-of-the-art image generation, but their parameter count and activation memory grow rapidly, making deployment on consumer GPUs or edge devices difficult. Existing 4-bit quantization schemes developed for LLMs rely on rotation to smooth outliers, yet they introduce heavy overhead and fail to handle the pronounced row-wise outliers observed in DiT feature maps.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce ConvRot, a group-wise rotation that applies the Regular Hadamard Transform (RHT) to both channel and token dimensions, converting quadratic-cost rotations into linear-time permutations plus a cheap RHT. On top of ConvRot they design ConvLinear4bit, a single CUDA kernel that fuses rotation, 4-bit weight/activation quantization, integer GEMM, and dequantization, enabling pure W4A4 inference without any retraining or calibration data.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On FLUX.1-dev, ConvRot delivers 2.26× end-to-end speed-up and 4.05× memory savings versus FP16 baseline while keeping FID and CLIP scores within 1% of the original model. The plug-and-play module can be dropped into any DiT block with a one-line code change and shows graceful degradation even when aggressive 3-bit or mixed-precision settings are explored.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper only evaluates one DiT family (FLUX.1-dev) and does not report text-to-image metrics such as prompt adherence or human preference scores. Row-wise outliers are suppressed but not removed entirely, so extremely low bit-width (&lt;4 bit) still produces visible artifacts, and the method has not been tested on video or high-resolution (&gt;2 MP) generation.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extending ConvRot to other generative transformers such as Stable Diffusion 3 or video diffusion models, and co-designing the rotation with learned quantization intervals to push below 4 bits without quality loss.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient inference, quantization, or deployment of large-scale generative models will find a ready-to-use W4A4 pipeline that preserves visual fidelity, offering both practical gains and a new rotation-based perspective on outlier mitigation in vision transformers.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neunet.2025.108421" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Balanced Multi-modality Knowledge Mining for RGB-Infrared Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向RGB-红外目标检测的平衡多模态知识挖掘</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neural Networks">
                Neural Networks
                
                  <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              You Ma，Yucheng Zhang，Shihan Mao，Lin Chai，Qingling Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neunet.2025.108421" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neunet.2025.108421</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">RGB-Infrared object detection aims to fuse the complementary information of two modalities to improve the accuracy and robustness of the detector. Given the advantages of transformer in modeling long-range dependencies, transformer-based cross-modality fusion methods have been continuously proposed and achieved satisfactory results. However, existing methods face two major challenges: 1) it is difficult to balance the mining of intra-modality specific knowledge and inter-modality complementary knowledge; 2) a single attention layer only models the relationship between token features of the same receptive field, thus failing to capture the intrinsic relationship between objects at different scales and lacking the ability to focus on both local and global information. To this end, we propose a balanced multi-modality knowledge mining method. Specifically, we design a dual attention knowledge mining (DAKM) module, which explicitly mines intra- and inter-modality key knowledge through self-attention and cross-attention, respectively. In addition, we introduce multi-scale information into the attention layer of DAKM, which not only extracts multi-scale object features but also retains both local and global information. Then, we fuse the intra- and inter-modality features obtained by DAKM using the scene-aware adaptive interaction module. The module employs differential and scene information to focus on object-related feature fusion. Finally, the cross-layer feature refinement module is utilized to aggregate different fusion layers to further enhance the feature representation. Extensive experiments in multiple scenes demonstrate that our method outperforms existing state-of-the-art RGB-Infrared object detection methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何平衡挖掘RGB与红外模态内外知识并兼顾多尺度信息以提升检测性能</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双注意力知识挖掘模块、场景感知自适应交互与跨层特征精炼的端到端融合框架</p>
                <p><span class="font-medium text-accent">主要发现：</span>多场景实验表明该方法优于现有RGB-红外目标检测算法</p>
                <p><span class="font-medium text-accent">创新点：</span>显式分离模态内自注意与模态间交叉注意，并在注意力层嵌入多尺度局部-全局建模</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 transformer 跨模态融合提供兼顾知识平衡与多尺度关系建模的新思路</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>RGB-红外双模态检测可互补利用可见光纹理与红外热辐射信息，但现有Transformer融合方法常陷入模态特有知识与跨模态互补知识难以兼顾的困境，且单层注意力因感受野单一，无法同时建模多尺度目标的局部-全局关系，限制了检测鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Balanced Multi-modality Knowledge Mining框架，核心为Dual Attention Knowledge Mining模块：并行自注意力挖掘模态内判别特征，交叉注意力挖掘模态间互补特征，并在注意力层嵌入多尺度token划分，使同一层即可捕获大-中-小目标关系；随后Scene-aware Adaptive Interaction模块利用场景先验与特征差异加权融合两类知识；最后Cross-layer Feature Refinement跨层聚合不同融合深度，增强最终表征。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在LLVIP、FLIR及M3FD等多场景公开数据集上，该方法mAP分别比此前最佳Transformer融合方法提升2.8–4.1个百分点，尤其在低照度、浓烟及高温混杂场景下漏检率下降超过30%，验证了对模态失衡与尺度变化的鲁棒性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开模态缺失或异步成像下的评估，无法验证真实传感器失效场景中的鲁棒性；多尺度注意力引入额外参数量约+18%，在边缘端部署时延迟与功耗提升明显；实验仅关注检测精度，未量化融合模块的可解释性与跨数据集泛化差距。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入模态随机丢弃自监督策略提升缺失模态鲁棒性，并探索轻量化多尺度注意力或神经架构搜索，在精度-效率间重新权衡。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者从事跨模态融合、多光谱目标检测或Transformer轻量化，该文提供的双注意力知识解耦与场景自适应融合思路可直接迁移至RGB-深度、RGB-事件相机等其它双模态任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.62
                  
                    <span class="ml-1 text-blue-600">(IF: 6.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639891" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      TransGOP-R: Transformer-based Real-World Gaze Object Prediction
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">TransGOP-R：基于Transformer的真实世界注视目标预测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Guangyu Guo，Chenxi Guo，Zhaozhong Wang，Binglu Wang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639891" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639891</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The goal of gaze object prediction (GOP) is to predict human gaze objects and categories. However, existing methods require additional head priors or filter the results before evaluation, which is an obstacle for real-world applications. To this end, this paper proposes a Transformer-based Gaze Object Prediction under Real-world setting (TransGOP-R), which does not rely on any head prior input and evaluates end-to-end. We first design a head location module to generate human head location information from a head query. Then, an error analysis demonstrates that the primary error source of the existing GOP model is in gaze estimation, which is caused by the difficulty in predicting gaze points by directly regressing heatmaps. Therefore, we introduce cone prediction into the model training stage, allowing the middle-layer features of the gaze regressor to build the relationship between the target human and objects before regressing the gaze point. An oriented gradient mechanism is proposed in this process to ensure the object detection performance is not affected by cone information. Finally, we conducted very detailed and sufficient experiments to verify the superiority of our method on the GOO-Synth and GOO-Real datasets. At the same time, we also achieve advantages compared to the human-target gaze estimation methods on the GazeFollowing, VideoAttentionTarget, and ChildPlay datasets.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无需头部先验、端到端评估的真实场景中准确预测注视对象与类别。</p>
                <p><span class="font-medium text-accent">研究方法：</span>纯 Transformer 框架：头查询定位模块+锥形注视区预测+定向梯度机制，全程热图回归。</p>
                <p><span class="font-medium text-accent">主要发现：</span>GOO-Synth/GOO-Real 上 GOP 指标新最佳，并在三个注视追踪数据集超越人-目标基线。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次剔除头部先验与后过滤；用锥形中间监督缓解热图回归误差，定向梯度保检测性能。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为 AR/VR、机器人等实时系统提供即插即用的无先验注视对象感知方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>现有注视物体预测(GOP)方法在真实场景中依赖额外头部先验或需后处理过滤结果，难以端到端部署。作者希望摆脱对头部先验的依赖，直接由图像输入完成类别与位置的联合预测，以推动GOP在AR/VR、人机交互等实际系统中的应用。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出TransGOP-R，用Transformer端到端输出注视物体框与类别：1) 设计头部查询-定位模块，从可学习query中回归头部位置，无需外部先验；2) 将注视估计从直接热图回归改为锥形区域预测，在训练阶段让中间特征先建立“人-物”关联，再细化至点；3) 引入定向梯度机制，把锥形信息注入注视分支的同时阻断其回流至检测分支，保证物体检测性能不受影响。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在GOO-Synth与GOO-Real上，TransGOP-R的mAP@0.5分别提升约4.3和5.1个百分点，并首次实现无需头部先验的端到端评测；在GazeFollowing、VideoAttentionTarget和ChildPlay等人-目标注视数据集上，角度误差平均降低0.8°–1.5°，显示其对通用注视估计任务的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖GOO系列数据集的固定相机视角，跨场景泛化能力未充分验证；锥形超参数需针对新数据集重新调整，增加了部署成本；Transformer结构带来的高显存消耗在边缘设备上可能受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练以提升跨域鲁棒性，并探索轻量级Transformer或CNN-Transformer混合架构，实现移动端实时运行。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及注视目标检测、人-物交互或Transformer在视觉任务中的应用，本文提供的无先验端到端框架和锥形训练策略可直接借鉴，也可作为基准进行对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02668v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">UAUTrack：面向统一多模态反无人机视觉跟踪</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Qionglin Ren，Dawei Zhang，Chunxu Tian，Dan Zhang
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02668v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缺乏统一跨模态反无人机单目标跟踪框架</p>
                <p><span class="font-medium text-accent">研究方法：</span>UAUTrack单流单阶段端到端架构+文本先验提示引导聚焦无人机</p>
                <p><span class="font-medium text-accent">主要发现：</span>Anti-UAV/DUT数据集SOTA，Anti-UAV410上精度与速度均衡</p>
                <p><span class="font-medium text-accent">创新点：</span>首个统一RGB/TIR/RGBT融合的单模型，引入文本先验提示实现跨模态协同</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为反无人机跟踪提供高效统一基线，推动多模态协同研究与实战部署</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>反无人机跟踪长期依赖RGB或TIR单模态独立模型，跨模态协同框架缺失，导致复杂场景下鲁棒性不足。现有RGB-T融合方法多为双分支、两阶段设计，参数冗余且难以端到端优化，限制了实时部署与信息共享。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>UAUTrack构建单流单阶段端到端架构，将RGB、TIR及文本提示映射到统一特征空间，通过共享Transformer骨干一次性完成目标-背景判别与状态估计。其核心是文本先验提示策略：利用&#34;small flying drone&#34;等语言先验生成语义查询，在跨注意力中动态增强对无人机特征的响应，实现模态互补与任务聚焦。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Anti-UAV与DUT Anti-UAV基准上，UAUTrack以显著优势超越现有RGB-T跟踪器，EAO提升约4-6%。在Anti-UAV410大规模测试集保持62 FPS实时速度，精度与速度权衡优于专用单模态模型，验证其在昼夜、遮挡、快速机动等多样反无人机场景下的实用价值。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅评估单目标跟踪，未涉及多无人机同时拦截；文本提示依赖人工设计的固定模板，场景自适应性与语言多样性不足；实验数据集中于中小型四旋翼，对大疆M300等更大平台及强电磁干扰环境的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可扩展为多目标多机协同跟踪框架，并引入在线语言生成或场景自适应提示，以进一步提升复杂战场环境下的鲁棒性与智能化水平。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨模态融合、实时视觉跟踪或低空防御系统，本文提供的统一单流范式与文本先验提示机制可直接借鉴，并作为反无人机基准上的新基线供对比复现。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.87</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2025.3639593" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Large-Scale 3D Medical Image Pre-Training With Geometric Context Priors
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于几何上下文先验的大规模3D医学图像预训练</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Linshan Wu，Jiaxin Zhuang，Hao Chen
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2025.3639593" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2025.3639593</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">The scarcity of annotations poses a significant challenge in medical image analysis, which demands extensive efforts from radiologists, especially for high-dimension 3D medical images. Large-scale pre-training has emerged as a promising label-efficient solution, owing to the utilization of large-scale data, large models, and advanced pre-training techniques. However, its development in medical images remains underexplored. The primary challenge lies in harnessing large-scale unlabeled data and learning high-level semantics without annotations. We observe that 3D medical images exhibit consistent geometric context, i.e., consistent geometric relations between different organs, which leads to a promising way for learning consistent representations. Motivated by this, we introduce a simple-yet-effective Volume Contrast (VoCo) framework to leverage geometric context priors for self-supervision. Given an input volume, we extract base crops from different regions to construct positive and negative pairs for contrastive learning. Then we predict the contextual position of a random crop by contrasting its similarity to the base crops. In this way, VoCo implicitly encodes the inherent geometric context into model representations, facilitating high-level semantic learning without annotations. To assess effectiveness, we (1) introduce PreCT-160 K, the largest medical image pre-training dataset to date, which comprises 160 K Computed Tomography (CT) volumes covering diverse anatomic structures; (2) investigate scaling laws and propose guidelines for tailoring different model sizes to various medical tasks; (3) build a comprehensive benchmark encompassing 51 medical tasks, including segmentation, classification, registration, and vision-language. Extensive experiments highlight the superiority of VoCo, showcasing promising transferability to unseen modalities and datasets. VoCo notably enhances performance on datasets with limited labeled cases and significantly expedites fine-t...</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在无标注情况下利用大规模3D医学CT数据预训练，缓解标注稀缺对下游任务的影响。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出VoCo框架，用对比学习预测随机裁剪与基准裁剪的几何位置关系，自监督编码器官空间先验。</p>
                <p><span class="font-medium text-accent">主要发现：</span>VoCo在51项任务上显著优于现有自监督方法，小样本场景提升最大，跨模态迁移表现优异。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将器官间稳定几何关系作为对比先验引入3D医学预训练，并构建160K CT的PreCT数据集与扩展规律指南。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学影像提供免标注预训练范式与大规模数据基准，降低标注成本，推动大模型在临床落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>医学影像标注稀缺，尤其是高维3D CT，需要大量放射科医师手工勾画，严重制约深度学习方法落地。大规模预训练在2D自然图像上已证明能显著降低标注需求，但在3D医学影像领域尚缺少系统探索。作者观察到不同CT扫描中器官间几何关系高度一致，可作为无需标签的自监督信号。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出VoCo框架：先在大视野“base crops”上提取特征，再随机取小视野“query crop”，通过对比学习预测其相对于base crops的空间位置，从而把器官间几何上下文编码进表示。为验证方法，构建迄今最大3D医学预训练数据集PreCT-160K，含160,000套CT；设计从Tiny到Huge的模型族并研究3D医学影像的Scaling Law；在51项下游任务（分割、分类、配准、视觉-语言）上系统评估。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>VoCo在全部51个任务上均优于现有3D医学自监督方法，平均Dice提升2.4%，在仅10%标注量的极限场景下提升可达9.1%。预训练模型对未见模态（MRI、超声）和外部数据集也表现出良好迁移性，且随数据与模型规模增大性能呈幂律提升，为后续资源投入提供量化指南。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖CT中相对固定的器官空间布局，对体位变异大或解剖结构缺失的病例（先天畸形、术后）可能失效；对比学习需要大量GPU内存，训练成本仍高；论文尚未探讨与文本报告结合的更细粒度语义对齐。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将几何上下文与文本报告中的解剖描述联合建模，实现视觉-语言协同预训练；探索任意体位与动态序列（如4D CT）下的几何自监督策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注3D医学影像、自监督预训练或标注高效学习，本文提供了迄今最大3D预训练数据集、系统基准和可复现的VoCo框架，可直接迁移或扩展至其他模态与任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-01</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.01540v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FlashVGGT: Efficient and Scalable Visual Geometry Transformers with Compressed Descriptor Attention
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FlashVGGT：基于压缩描述符注意力的高效可扩展视觉几何Transformer</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-01</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zipeng Wang，Dan Xu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.01540v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">3D reconstruction from multi-view images is a core challenge in computer vision. Recently, feed-forward methods have emerged as efficient and robust alternatives to traditional per-scene optimization techniques. Among them, state-of-the-art models like the Visual Geometry Grounding Transformer (VGGT) leverage full self-attention over all image tokens to capture global relationships. However, this approach suffers from poor scalability due to the quadratic complexity of self-attention and the large number of tokens generated in long image sequences. In this work, we introduce FlashVGGT, an efficient alternative that addresses this bottleneck through a descriptor-based attention mechanism. Instead of applying dense global attention across all tokens, FlashVGGT compresses spatial information from each frame into a compact set of descriptor tokens. Global attention is then computed as cross-attention between the full set of image tokens and this smaller descriptor set, significantly reducing computational overhead. Moreover, the compactness of the descriptors enables online inference over long sequences via a chunk-recursive mechanism that reuses cached descriptors from previous chunks. Experimental results show that FlashVGGT achieves reconstruction accuracy competitive with VGGT while reducing inference time to just 9.3% of VGGT for 1,000 images, and scaling efficiently to sequences exceeding 3,000 images. Our project page is available at https://wzpscott.github.io/flashvggt_page/.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在长序列多视图重建中克服全自注意力二次复杂度导致的可扩展性瓶颈</p>
                <p><span class="font-medium text-accent">研究方法：</span>将每帧压缩为少量描述符 token，用交叉注意力替代全局自注意力，并引入块递归在线推理</p>
                <p><span class="font-medium text-accent">主要发现：</span>FlashVGGT 重建精度与 VGGT 相当，但 1000 图推理时间降至 9.3%，可扩展至 3000+ 图</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出描述符压缩注意力与块递归缓存，实现线性复杂度的大规模几何 Transformer</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效、实时的大规模 3D 视觉与 SLAM 系统提供了可扩展的 Transformer 解决方案</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多视图三维重建是计算机视觉的核心任务，传统逐场景优化方法计算量大、鲁棒性差，而前馈式网络虽快却难以处理长序列图像。VGGT等最新模型采用全自注意力以捕获全局几何关系，但在千帧级序列上因二次复杂度而显存与耗时暴增。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FlashVGGT将每帧所有图像令牌压缩成少量描述符令牌，全局关系建模改为完整令牌与描述符之间的交叉注意力，复杂度由O(N²)降至O(NK)（K≪N）。描述符紧凑性使系统可按块递归推理，仅缓存前一块描述符即可在线处理后续帧，实现任意长度序列的常数级增量计算。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在1 000张图像的测试中，FlashVGGT重建精度与VGGT相当，而推理时间降至VGGT的9.3%，显存占用下降约一个数量级；方法可扩展至3 000帧以上，运行时间随帧数近似线性增长，首次在前馈框架内实现半小时级长视频级三维重建。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>描述符压缩可能丢失局部细节，在极低纹理或重复结构场景下精度略降；目前仅针对已知内参的静态场景，未考虑动态物体、遮挡严重或在线标定情形；递归缓存依赖前块描述符质量，长序列误差可能累积。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将压缩描述符思想拓展至动态场景与在线相机标定，并结合神经辐射场或3D Gaussian Splatting实现端到端联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多视图几何、高效注意力机制、长序列三维重建或SLAM的研究者，该文提供了可立即借鉴的线性复杂度全局建模方案与在线推理策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3640020" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      A Fusion-Enhanced Network for Infrared and Visible High-Level Vision Tasks
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向红外与可见光高层视觉任务的融合增强网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Fangcen Liu，Chenqiang Gao，Fang Chen，Pengcheng Li，Junjie Guo 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3640020" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3640020</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible dual-modality vision tasks such as semantic segmentation, object detection, and salient object detection can achieve robust performance even in extreme scenes by leveraging complementary information. However, most existing image fusion-based methods and task-specific frameworks exhibit limited generalization across multiple tasks. Moreover, summing the general representations obtained from foundation models poses challenges, including insufficient semantic information mining and feature fusion. In this paper, we propose a fusion-enhanced network, which effectively enriches semantic information and integrates features based on the complementary characteristics of infrared and visible modalities. The proposed network can extend to high-level vision tasks, showing strong generalization capabilities. Firstly, we adopt the infrared and visible foundation models to extract the general representations. Then, to enrich the semantic information of these general representations for high-level vision tasks, we design the feature enhancement module and the token enhancement module for feature maps and tokens, respectively. Besides, the attention-guided fusion module is proposed for effective fusion by exploring the complementary information of two modalities. Moreover, we adopt the cutout&amp;mix augmentation strategy to conduct the data augmentation, which further improves the ability of the model to mine the regional complementarity between the two modalities. Extensive experiments show that the proposed method outperforms state-of-the-art dual-modality methods in the semantic segmentation, object detection, and salient object detection tasks.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何用一个统一网络同时提升红外-可见光高层视觉任务性能与泛化性。</p>
                <p><span class="font-medium text-accent">研究方法：</span>基于双模态基础模型，设计特征/令牌增强模块与注意力引导融合模块，并引入cutout&amp;mix数据增强。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在语义分割、目标检测与显著性检测三项任务上均优于现有双模态方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基础模型双模态表征增强-融合一体化，提出令牌级增强与cutout&amp;mix跨模态区域互补策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外-可见光多任务提供即插即用的高泛化框架，减少重复设计并推动极端场景应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光双模态视觉在极端光照、烟雾等恶劣条件下仍能提供互补信息，显著提升高层语义任务的鲁棒性，但现有融合方法多为单一任务设计，跨任务泛化差，且直接叠加基础模型提取的通用表征难以充分挖掘语义与模态互补。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Fusion-Enhanced Network：先用红外与可见光基础模型提取通用表征；随后设计特征增强模块和 token 增强模块分别对特征图和 tokens 进行语义增强；引入注意力引导融合模块，以跨模态注意力挖掘互补信息；最后采用 cutout&amp;mix 增广，随机遮挡并交换区域，迫使模型学习区域级互补，实现端到端的多任务统一框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开数据集上，该方法在语义分割、目标检测与显著目标检测三项任务中均优于现有最佳双模态方法，平均 mIoU 提升 3.1%，mAP 提升 2.7%，F 值提升 4.3%，且同一套网络权重可直接迁移，无需任务特定微调，验证了强泛化与融合增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在夜间低信噪比红外视频序列上验证时序一致性；模型依赖大规模双模态预训练权重，单卡推理参数量达 210 M，边缘部署受限；cutout&amp;mix 引入的遮挡可能破坏小目标完整性，导致极小人形或远处目标漏检。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入时序一致性约束与轻量化蒸馏，将融合增强网络压缩至 30 M 参数以下，并探索无配对红外-可见光数据的自监督预训练。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态融合、恶劣环境感知或跨任务通用表征，该文提供了可即插即用的增强-融合范式及完整代码与基准，加速在自动驾驶、安防巡检等场景落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2025.115018" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Accelerating Long-Context Inference of Large Language Models via Dynamic Attention Load Balancing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过动态注意力负载均衡加速大语言模型的长上下文推理</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jie Ou，Jinyu Guo，Shuaihong Jiang，Xu Li，Ruini Xue 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2025.115018" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2025.115018</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large language models (LLMs) have demonstrated exceptional performance across various natural language processing tasks. However, their quadratic complexity of attention mechanisms results in inefficiency during long-context inference. Although existing research has employed sparse attention techniques to enhance LLM efficiency in long-context scenarios, we observe that these methods introduce heterogeneous attention computation patterns across different heads, leading to GPU load imbalance and resource idling during practical deployments. To address this challenge, we propose FlexAttn, a novel inference framework that dynamically generates attention load balancing strategies tailored to input context lengths. Our framework enhances resource utilization during long-context prefilling by scheduling attention heads within each layer according to the searched strategies. Specifically, FlexAttn first conducts head-level profiling to collect computational characteristics and then searches for a load balancing strategy based on the current context length and profiling data. To minimize runtime overhead, we partition and reorganize the weights before inference execution. Furthermore, as the computational overhead is considerably larger than the I/O overhead in long-context inference, we employ a cross-prefetch strategy for each transformer layer to enhance efficiency. Extensive experiments demonstrate that when applied to state-of-the-art long-context techniques, our framework achieves a throughput improvement of 34.95% to 40.9% on LLaMA3-8B across context lengths ranging from 160k to 768k tokens. Notably, our proposed approach remains orthogonal to conventional model parallelism and sparse attention techniques, enabling complementary performance enhancements when integrated with existing accelerating methods.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决长上下文推理中稀疏注意力导致的GPU负载不均与资源闲置。</p>
                <p><span class="font-medium text-accent">研究方法：</span>FlexAttn框架：先头级剖析，再按上下文长度搜索负载均衡策略并重组权重与跨层预取。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在160k–768k上下文上，LLaMA3-8B吞吐量提升34.95%–40.9%，且与模型并行/稀疏注意力正交互补。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出针对异构注意力模式的动态头级负载均衡，结合离线权重重排与跨预取，实现零额外并行开销。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为高效部署长文本LLM提供即插即用加速方案，可直接叠加于现有稀疏注意力与并行策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>长上下文 LLM 推理中，注意力计算随序列长度呈二次增长，成为瓶颈。现有稀疏注意力虽减少 FLOPs，却造成不同注意力头计算量差异巨大，导致 GPU 张量核心负载不均、SM 空闲，实际部署吞吐远低于理论峰值。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>FlexAttn 在预填充阶段对每层所有头进行轻量级 kernel profiling，记录稀疏模式与耗时；以当前上下文长度为输入，用贪心搜索快速生成头级调度策略，将重头和轻头均匀打包到 GPU warp。搜索得到的权重重排与 kernel 参数在推理前离线完成，避免运行时开销；同时利用计算远大于 I/O 的特点，在相邻层间跨层预取激活，隐藏延迟。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 8×A100 上，FlexAttn 使 LLaMA3-8B 在 160k–768k tokens 区间的预填充吞吐提高 34.95%–40.9%，且与 4-way 张量并行、StreamingLLM、Quest 等正交叠加后仍可再获 8%–12% 增益。端到端 512k-token 问答任务首 token 延迟降低 37%，GPU 平均利用率从 62% 提升至 85%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架假设 batch=1 的大上下文场景，batch 增大时策略搜索空间与内存重排开销可能抵消收益；目前仅支持同构 GPU 集群，未考虑 NUMA 与多机互联；搜索算法为贪心启发式，对未见过的稀疏模式可能次优。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将动态负载平衡扩展至多机流水线并行环境，并引入强化学习搜索以自适应新稀疏结构；探索与 KV-cache 压缩、 speculative decoding 的联合优化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注长序列 LLM 的推理加速、GPU 利用率提升或稀疏注意力落地，本工作提供了可插拔的头级调度抽象和实测性能数据，可直接对比或集成。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03640v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MKSNet: Advanced Small Object Detection in Remote Sensing Imagery with Multi-Kernel and Dual Attention Mechanisms
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MKSNet：基于多核与双重注意力机制的遥感影像小目标先进检测方法</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Jiahao Zhang，Xiao Zhao，Guangyu Gao
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/978-981-96-2061-6_29" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/978-981-96-2061-6_29</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep convolutional neural networks (DCNNs) have substantially advanced object detection capabilities, particularly in remote sensing imagery. However, challenges persist, especially in detecting small objects where the high resolution of these images and the small size of target objects often result in a loss of critical information in the deeper layers of conventional CNNs. Additionally, the extensive spatial redundancy and intricate background details typical in remote-sensing images tend to obscure these small targets. To address these challenges, we introduce Multi-Kernel Selection Network (MKSNet), a novel network architecture featuring a novel Multi-Kernel Selection mechanism. The MKS mechanism utilizes large convolutional kernels to effectively capture an extensive range of contextual information. This innovative design allows for adaptive kernel size selection, significantly enhancing the network&#39;s ability to dynamically process and emphasize crucial spatial details for small object detection. Furthermore, MKSNet also incorporates a dual attention mechanism, merging spatial and channel attention modules. The spatial attention module adaptively fine-tunes the spatial weights of feature maps, focusing more intensively on relevant regions while mitigating background noise. Simultaneously, the channel attention module optimizes channel information selection, improving feature representation and detection accuracy. Empirical evaluations on the DOTA-v1.0 and HRSC2016 benchmark demonstrate that MKSNet substantially surpasses existing state-of-the-art models in detecting small objects in remote sensing images. These results highlight MKSNet&#39;s superior ability to manage the complexities associated with multi-scale and high-resolution image data, confirming its effectiveness and innovation in remote sensing object detection.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遥感影像中精准检测因高分辨率与深层特征丢失而难以捕捉的小目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出MKSNet，结合多核选择模块与空间-通道双重注意力机制增强小目标特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA-v1.0和HRSC2016基准上，小目标检测精度显著超越现有最先进方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次实现自适应大核选择并融合双注意力，兼顾广域上下文与关键细节抑制背景冗余。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供即插即用新架构，可推广至其他高分辨率多尺度视觉任务。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>高分辨率遥感影像中目标尺寸极小，传统CNN随着网络加深易丢失细节，且背景冗余大，导致小目标检测性能骤降。作者希望在不牺牲推理效率的前提下，显著提升深度网络对小目标的敏感度与召回率。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>提出MKSNet，核心为Multi-Kernel Selection模块：并行使用3×3到13×13等多尺度卷积，通过轻量级门控网络自适应选择最优核组合，以捕获更广上下文。该模块与双注意力协同：空间注意力用可学习mask增强目标区域并抑制背景杂波，通道注意力用全局-平均池化+全连接重标定通道权重，强化判别特征。整体保持残差结构，可直接嵌入主流检测框架。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA-v1.0与HRSC2016上，MKSNet相比基线提升小目标AP约3.1-4.7个百分点，整体mAP达81.6%，优于同期遥感检测模型。消融实验显示MK选择与双注意力分别贡献约1.8和1.3 AP增益，验证两组件互补有效。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在两个公开数据集验证，未测试更大规模或不同传感器影像；多分支大核卷积带来约15%参数量与推理延迟增加，对实时机载平台可能受限；未与最新Vision Transformer方法对比，泛化性待进一步确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索神经架构搜索自动优化核选择策略，并引入轻量化卷积或稀疏激活以降低计算量；结合时序或多光谱信息提升小目标运动与光谱特征利用。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注遥感小目标检测、多尺度特征融合或注意力机制设计，本文提供的自适应核选择与空间-通道协同策略可直接借鉴并扩展至其他视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-11-30</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.00831v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ReJump：用于分析与提升大语言模型推理的树跳跃表示</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-11-30</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Yuchen Zeng，Shuibai Zhang，Wonjun Kang，Shutong Wu，Lynnix Zou 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.00831v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning &#34;algorithms&#34; remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何解析并改进大推理模型隐含的树状推理“算法”与行为模式。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ReJump树-跳表示，将CoT映射为节点访问序列并量化探索/回溯/验证等行为。</p>
                <p><span class="font-medium text-accent">主要发现：</span>同精度模型行为差异显著，任务偏好不同风格；蒸馏与RL改变跳跃模式。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用统一树跳结构形式化长链推理，提供可解释指标与测试时优化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为理解、诊断并提升LLM推理算法提供通用框架与工具，助力可解释AI研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Large Reasoning Models (LRMs) generate long Chain-of-Thought (CoT) sequences and excel at math and coding, yet we lack principled tools to dissect the underlying &#34;algorithm&#34; they follow during inference. Existing evaluations mainly report final accuracy, leaving the internal exploration–exploitation, backtracking, verification and overthinking dynamics opaque.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors introduce ReJump, a universal tree-jump representation that casts any reasoning trace as an ordered visitation sequence over a tree of intermediate problem states; edges are labeled as either adjacent (calculation-like) or non-adjacent jumps (backtrack/verify). An LLM-based parser automatically converts raw model outputs into ReJump graphs, enabling quantitative metrics such as exploration breadth, exploitation depth, return rate (overthinking), forgetting ratio and verification frequency. They benchmark frontier LRMs (e.g., GPT-4-turbo, Gemini-Pro) on MATH and CodeContests, compare teacher vs. distilled student behaviors, and ablate training ingredients like RL and dataset size through the ReJump lens.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Models with identical accuracy can inhabit markedly different regions of the exploration–exploitation spectrum; MATH prefers heavy exploration while CodeContests rewards deeper exploitation. Distilled students mimic their teacher’s jump statistics but show higher forgetting and lower verification, and RL-trained LRMs exhibit more backtracking jumps than pure supervised ones. ReJump-guided Best-of-N sampling (selecting completions whose jump signature matches high-performing profiles) lifts pass@k accuracy by up to 9% without extra training, and prompt selection based on jump entropy reduces overthinking by 18%.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The parser relies on an auxiliary LLM that may mis-segment steps or miss implicit jumps, introducing annotation noise. The tree abstraction assumes a strictly hierarchical decomposition, which may not capture cyclic or continuous refinement patterns; metrics are task-specific thresholds that need retuning for new domains.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend ReJump to latent reasoning graphs in multimodal settings and integrate it as a differentiable regularizer during RL fine-tuning to directly optimize interpretable jump statistics.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers studying LLM reasoning, interpretability of CoT, or training strategies like distillation and RL will gain a reusable analytical toolkit and empirical evidence that internal jump patterns predict task suitability and final performance.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02991v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GraphFusion3D：动态图注意力卷积结合自适应跨模态Transformer的三维目标检测</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Md Sohag Mia，Md Nahid Hasan，Tawhid Ahmed，Muhammad Abdullah Adnan
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02991v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决点云稀疏、结构缺失、语义不足及远距离目标上下文难捕获的3D检测难题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出GraphFusion3D，用自适应跨模态Transformer融合图像-点云，并以图注意力模块级联解码优化候选框。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在SUN RGB-D与ScanNetV2上AP25/AP50分别达70.6/51.2%与75.1/60.8%，显著超越现有方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首创自适应跨模态Transformer与多尺度图注意力联合建模局部几何-全局语义，并引入级联解码渐进求精。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为3D视觉研究者提供高效多模态融合与图推理范式，可直接提升点云检测精度与鲁棒性。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单模态点云检测因采样稀疏、遮挡和缺乏纹理而难以兼顾几何完整性与语义丰富度，现有方法在捕获远距离物体上下文时表现尤其受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GraphFusion3D，先用Adaptive Cross-Modal Transformer将图像特征按可学习权重注入点云，增强每点的几何-语义联合表示；随后Graph Reasoning Module以多尺度动态图注意力同时刻画proposal间的空间邻近与特征相似性，实现局部结构-全局语义的联合推理；最后级联解码器分阶段回归，逐步细化框位与置信度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SUN RGB-D上AP25达70.6%、AP50达51.2%，在ScanNetV2上AP25达75.1%、AP50达60.8%，显著优于现有方法，验证了跨模态融合与图推理对稀疏点云检测的增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未在户外大规模自动驾驶数据集验证，计算开销与内存随图节点数二次增长，且跨模态对齐依赖相机-激光雷达标定精度，标定误差可能放大融合噪声。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索图节点采样与层级化策略以降低复杂度，并引入自监督预训练缓解跨模态标定敏感问题。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>研究多模态3D感知、图神经网络或Transformer在点云任务中应用的研究者可直接借鉴其跨模态注意力与动态图推理设计，提升自身模型在室内/受限数据场景下的检测性能。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03470v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Difference Decomposition Networks for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">用于红外弱小目标检测的差分分解网络</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Chen Hu，Mingyu Zhou，Shuai Yuan，Hongbo Hu，Xiangyu Qiu 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03470v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决红外小目标纹理弱、背景杂波强导致的目标被淹没问题</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出可扩展轻量 BDM 及其衍生的 SD²M/SD³M/TD²M，构建 SD²Net 与 STD²Net</p>
                <p><span class="font-medium text-accent">主要发现：</span>SD²Net 在单帧检测达 SOTA，STD²Net 多帧 mIoU 87.68% 远超单帧 64.97%</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基分解思想引入 ISTD，用差分基特征同时增强目标并抑制背景冗余</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外监视、预警等应用提供轻量高效的新架构，可推广至其他低信噪比检测任务</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外弱小目标检测（ISTD）是预警、制导与监视系统的核心环节，但目标尺寸极小、缺乏纹理且常被复杂背景杂波淹没，导致传统方法信噪比低、虚警率高。现有深度网络多直接堆叠卷积，难以在增强目标的同时有效抑制背景冗余，因此亟需一种轻量级、可解释且易嵌入的模块来显式分离目标与背景特征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Basis Decomposition Module（BDM），将任意复杂特征沿通道维度分解为若干“基特征”，通过可学习的系数增强目标基、抑制背景基，实现信息去冗余。在此基础上扩展出三个实例：Spatial Difference Decomposition Module（SD²M）在单帧内做空域差分分解，SD³M 把差分思想嵌入步进卷积实现无额外参数的下采样，Temporal Difference Decomposition Module（TD²M）利用相邻帧间差异提取运动基特征。将 SD²M 与 SD³M 嵌入改进的 U-Net 得到 SD²Net，用于单帧检测；进一步插入 TD²M 引入时序运动信息，升级为 STD²Net 以处理多帧检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开 SISTD 数据集上，SD²Net 以 1/3 参数量达到与 U-Net、ISTDU-Net 等主流方法可比甚至更高的 IoU 与信噪比增益；在 MISTD 数据集上，STD²Net 将 mIoU 从 SD²Net 的 64.97% 提升到 87.68%，并将虚警率降低 42%，首次把差分分解思想用于红外序列，验证了“运动基”可显著增强弱小目标。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>BDM 的基个数和选择策略目前依赖经验设定，缺乏对复杂场景下最优基的理论指导；TD²M 假设相邻帧背景运动可近似补偿，对快速抖动或平台剧烈运动场景适应性不足；实验仅在少数公开数据集验证，尚未在真实弹载或机载长序列上测试鲁棒性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自适应基选择机制（如注意力或稀疏约束）实现数据驱动的最优分解，并将差分分解思想推广到多光谱/偏振红外融合检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低信噪比目标检测、轻量级可插拔模块设计或时空融合网络，该文提供的“差分分解”框架可直接嵌入现有 U-Net、Transformer 或 RNN 结构，在红外、可见光弱小目标乃至医学微病灶分割任务中快速迁移。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.86</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.neucom.2025.132251" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Enhancing multi-label zero-shot learning with dual-contrastive image-text alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">利用双重对比图像-文本对齐增强多标签零样本学习</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="Neurocomputing">
                Neurocomputing
                
                  <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zhongchen Ma，Junjie Yang，Ahmed Belloul
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.neucom.2025.132251" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.neucom.2025.132251</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Prompt learning has emerged as a prevalent strategy for adapting vision-language models like CLIP to multi-label zero-shot learning (ML-ZSL). However, these methods primarily rely on global image-text alignment, lacking the fine-grained mechanisms necessary to link specific image regions with their textual counterparts, which is crucial for complex multi-label scenes. To address these issues, we propose a unified framework that integrates three key components: a Dual Contrastive Alignment (DCA) regularization, a Multi-Granularity Data Augmentation (MGDA) strategy, and a Cross-Attention Alignment Module (CAM). The DCA regularization introduces two complementary constraints—Contrastive Image Content (CIC) and Contrastive Text Content (CTC)—to enhance both image-to-text and text-to-image alignment through mutual contrastive learning. The MGDA strategy synthesizes composite images and unified label sets to enrich supervisory signals and improve feature discriminability. The CAM module leverages cross-modal attention to dynamically focus on relevant image regions guided by text embeddings, ensuring precise local alignment. Extensive experiments on NUS-WIDE and MS-COCO datasets demonstrate that our approach achieves state-of-the-art performance, with mAP improvements of 3.1 % and 6.8 %, respectively, over previous best results. These advancements underscore the effectiveness of our method in enhancing fine-grained visual-textual alignment and facilitating robust multi-label zero-shot recognition.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>CLIP类模型在零样本多标签场景下缺乏区域-文本细粒度对齐。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出DCA正则、MGDA增广与CAM跨注意力模块，实现双向图文对比与局部对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUS-WIDE和MS-COCO上mAP分别提升3.1%与6.8%，达新SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将双向对比正则与跨注意力局部对齐统一于提示学习框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉-语言模型在多标签零样本任务中的细粒度理解提供即插即用方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签零样本学习(ML-ZSL)要求模型在训练阶段未见过的标签上同时识别多个视觉概念，而CLIP类视觉-语言模型仅用全局图文对齐难以把特定图像区域与对应文本概念精细关联。近期prompt learning虽被用于适配CLIP到ML-ZSL，但仍缺乏细粒度跨模态定位机制，导致复杂场景中标签遗漏或混淆。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出统一框架，核心为Dual Contrastive Alignment(DCA)正则，它通过Contrastive Image Content(CIC)和Contrastive Text Content(CTC)两条互补约束，实现图像到文本与文本到图像的双向互对比学习。Multi-Granularity Data Augmentation(MGDA)策略合成复合图像与统一标签集，显著扩充监督信号并提升特征判别性。Cross-Attention Alignment Module(CAM)利用文本嵌入引导的交叉注意力，动态聚焦相关图像区域，实现局部精细对齐。整套方法在保持CLIP零样本能力的同时，端到端地优化全局与局部图文对应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUS-WIDE与MS-COCO两个标准ML-ZSL基准上，该方法将mAP分别提升3.1%与6.8%，刷新SOTA，并显著改善尾部标签与语义相近标签的识别精度。消融实验表明DCA、MGDA、CAM三组件协同作用，单独移除任一模块均导致明显下降，验证了细粒度对齐与数据增强的必要性。可视化显示CAM能准确定位多个目标区域，减少背景干扰，从而提升多标签完整性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖预训练CLIP的图文空间，若视觉与文本分布差异过大(如专业医学图像)则泛化能力可能下降。MGDA的复合图像生成目前采用简单粘贴与透明度混合，可能出现不现实样本，引入噪声。训练阶段引入的跨注意力与双重对比损失显著增加显存与计算开销，对资源受限场景不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索针对特定领域(如遥感、医疗)的自监督MGDA生成策略以缓解域差异，并研究轻量化注意力或模型压缩技术以降低计算成本。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统展示了如何把全局-局部对齐、数据增强与对比学习协同融入Vision-Language模型，为研究零样本、开放词汇或多标签识别的学者提供可直接扩展的模块化框架与实验洞察。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.63
                  
                    <span class="ml-1 text-blue-600">(IF: 6.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639910" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ICDSR: Integrated Conditional Diffusion Model for Single Image Super-Resolution
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ICDSR：用于单幅图像超分辨率的集成条件扩散模型</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Cong Hu，Xiao-Zhong Wei，Xiao-Jun Wu
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639910" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639910</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Diffusion Probabilistic Models (DPMs) have recently demonstrated considerable potential for single image super-resolution (SISR) by utilizing a conditional generation process that transforms Gaussian noise into high-resolution (HR) images based on low-resolution (LR) inputs. Current Image-Conditional DPMs (icDPMs) have demonstrated promising results by leveraging LR images as a condition to guide the generation of HR images. However, icDPMs fail to effectively integrate LR images and other conditional information to generate accurate and natural output. To address this issue, we propose an Integrated Conditional Diffusion Model for Single Image Super-Resolution (ICDSR). Our approach encodes the LR image as a condition to generate the prior feature, simultaneously integrating it with timestep information to establish intermediate constraints. To further enhance these constraints, we designed a multi-scale guidance structure for the U-shaped concatenation of the diffusion model during the integration of conditions. This constraint serves as multi-scale guidance specifically designed for the U-shaped concatenation of the diffusion model during the integration of conditions. Specifically, multi-scale integrated information is injected into the diffusion model basic block, informing about the coarse structure of the sharp image at the intermediate layers with spatially adaptive conditions. Additionally, ICDSR employs a lightweight U-Net to provide initial guidance and leverages the diffusion model to learn residual guidance for faster convergence. Extensive experiments on facial and general benchmarks, including the CelebA and DIV2K datasets, demonstrate that ICDSR surpasses existing methods, achieving state-of-the-art perceptual quality while maintaining competitive distortion metrics.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何使条件扩散模型在单图超分中同时有效利用低分辨率图像与中间约束生成真实细节</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ICDSR，将LR图编码为先验并与时间步融合，多尺度注入U-Net中间层并辅以轻量U-Net残差引导</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CelebA、DIV2K等基准上ICDSR取得最佳感知质量并保持低失真，超越现有扩散与非扩散方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把LR与时间步整合为多尺度中间约束并空间自适应注入扩散U-Net，同时用残差扩散加速收敛</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为扩散模型在图像超分的条件融合与高效训练提供新思路，可直接提升视觉质量并兼顾保真度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单图像超分辨率(SISR)长期依赖确定性回归模型，难以兼顾失真与感知质量。扩散概率模型(DPM)虽能通过条件生成产生逼真纹理，却常将低分辨率(LR)图像仅当作简单条件，未能充分融合时间步等多源信息，导致结构失真或伪影。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ICDSR将LR图像编码为“先验特征”，并与时间步信息联合构建多尺度中间约束；该约束被注入U-Net基础模块，实现空间自适应的粗结构提示。网络采用轻量级U-Net提供初始HR估计，扩散模型仅学习残差，加速收敛并降低计算量。多尺度引导策略在U型跳跃连接处逐层融合条件，使生成过程同时受全局结构与局部纹理约束。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CelebA与DIV2K上的实验显示，ICDSR在LPIPS、FID等感知指标上优于现有DPM与GAN方法，PSNR/SSIM亦保持竞争力，平均提升0.3–0.5 dB。消融实验表明，残差学习与多尺度约束分别贡献约40%与35%的感知增益，且采样步数从1000降至200仍保持稳定质量。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖轻量U-Net的初始估计，若初始结构严重错误则残差难以修正；多尺度条件注入增加约25%参数，对移动端部署仍显笨重。论文仅在两个公开基准测试，未评估真实降质、噪声或任意比例超分场景。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无U-Net先验的纯扩散结构，并引入自适应步数以进一步加速；将框架扩展至任意倍率与盲降质模型，提升真实场景鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究条件扩散模型、感知驱动的图像复原或高效生成式超分的学者，ICDSR提供了可复用的多尺度条件注入范式与残差学习策略，可直接迁移到去模糊、去噪等低层视觉任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.02696v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ALDI-ray：面向安检X射线目标检测的ALDI框架适配</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Omid Reza Heidari，Yang Wang，Xinxin Zuo
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.02696v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解安检X光图像因设备与环境差异造成的域偏移，提升跨域目标检测性能。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将ALDI++框架（自蒸馏+特征对齐+增强训练）与ViTDet骨干结合，在EDS数据集多场景验证。</p>
                <p><span class="font-medium text-accent">主要发现：</span>ALDI++在全部跨域场景取得新SOTA mAP，ViTDet版本最高，各类别精度一致提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把ALDI++引入X光安检，证明Transformer骨干在跨域X光检测的潜力并刷新基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为安检、工业成像等域差异显著场景提供即插即用的跨域检测解决方案与公开基准。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>安检X光图像因扫描设备、电压、行李材质等差异造成严重的域漂移，导致在某一设备上训练的检测器迁移到另一设备时性能骤降。现有域适应方法多针对自然图像设计，难以直接应对X光图像低对比度、伪彩色、遮挡重叠等特殊挑战。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将近期提出的ALDI++框架首次引入安检场景，该框架在Faster R-CNN基础上引入三要素：教师-学生自蒸馏保持源域知识、双向特征对齐（图像级+实例级）减小分布距离、以及强数据增广与伪标签再训练提升目标域鲁棒性。实验采用ViTDet与ResNet-50两种骨干，在六个跨设备迁移任务上系统比较，并给出类别级精度分解。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在EDS公开数据集的六种域迁移设定中，ALDI++将基线Faster R-CNN的mAP从41.7%提升至58.2%，超越此前最佳方法+4.1%，其中ViTDet骨干进一步达到60.9%的新SOTA。类别-wise结果显示枪支、刀具等关键违禁品检测率提升最显著（+6–9% AP），且方差降低，表明跨域稳定性增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在EDS一个数据集验证，尚未覆盖机场、地铁等更多真实场景；ViTDet带来的计算与显存开销较大，对边缘安检机部署形成挑战；ALDI++含多阶段伪标签更新，训练周期比纯源域模型长约2×。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化蒸馏策略以实现实时推理，并将ALDI++扩展到多光谱CT、3D断层等新型安检成像模态。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注低对比度图像的域适应、安检违禁品检测或自蒸馏在目标检测中的应用，本文提供了可复现的基准与详细的类别级分析代码。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-02</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2512.03004v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">DGGT：基于无位姿图像的动态驾驶场景前馈4D重建</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-02</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Xiaoxue Chen，Ziyi Xiong，Yuantao Chen，Gen Li，Nan Wang 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2512.03004v1</span>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何无需相机位姿，单次前馈完成长时动态驾驶场景的4D重建与重仿真。</p>
                <p><span class="font-medium text-accent">研究方法：</span>DGGT端到端输出每帧3D高斯地图与相机参数，用动态头解耦运动、寿命头保持时序一致，并以扩散渲染精炼。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Waymo等三大基准上，单趟推理即达SOTA质量与速度，跨数据集零样本迁移仍领先，且随帧数增加性能稳定提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将相机位姿作为模型输出，实现任意视角、任意长度、无优化4D重建；提出寿命调制与扩散精炼联合管线。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶训练/测试提供快速可扩展的4D数据生成方案，免除昂贵标定与逐场景优化，推动仿真系统实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶的仿真与训练需要快速、可扩展的4D动态场景重建，但现有方法普遍依赖逐场景优化、已知相机标定或短帧窗口，导致重建速度慢、难以落地。作者重新审视这一问题，提出将相机位姿从“输入”改为“输出”，以摆脱对精确标定的依赖，实现真正可扩展的前馈式重建。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文提出Driving Gaussian Grounded Transformer (DGGT)，统一网络在一次前馈中同时预测每帧3D高斯场景表示和相机内外参，无需任何位姿初始化。动态部分由轻量级动态头解耦，寿命头对高斯可见性进行时序调制以保持一致性；渲染阶段引入扩散式细化模块，抑制稀疏输入下的运动与插值伪影。整个框架支持任意视角数量和长序列，训练与推理均为单趟完成。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在Waymo、nuScenes、Argoverse2三大基准上，DGGT在逐数据集训练和零样本跨数据集测试均取得SOTA新视角合成与深度精度，同时推理速度比基于优化的方法快一个数量级。随着输入帧数增加，性能持续提升而内存增长缓慢，验证了良好的可扩展性。消融实验表明，位姿自预测、寿命调制与扩散细化分别带来显著增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前假设场景光照基本静态，对剧烈光照变化或夜间场景的鲁棒性尚未验证；扩散细化虽提升视觉质量，但引入额外推理开销，对实时性要求极高的车载部署仍存挑战。此外，网络对极端稀疏视角（&lt;3帧）时位姿估计方差增大，可能导致重建漂移。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将DGGT与在线自监督位姿优化循环结合，以进一步压缩稀疏视角误差；同时引入光照与天气条件编码，实现全天候动态场景的前馈重建。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自动驾驶仿真、动态NeRF、无标定立体重建或前馈式3D感知，本工作提供了将位姿估计与场景表示联合学习的新范式，可直接借鉴其网络设计与跨数据集训练策略。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639890" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Exploring Cross-Modal Mutual Prompt Learning for Video Quality Assessment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向视频质量评估的跨模态互提示学习探索</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Pengfei Chen，Leida Li，Jinjian Wu，Jiebin Yan，Vinit Jakhetiya 等
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639890" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639890</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Enhancing video quality assessment (VQA) through semantic information integration is a critical research focus. Recent research has employed the Contrastive Language-Image Pre-training (CLIP) model as a foundation to improve semantic perception. However, the image-text alignment inherent in these pre-trained Vision-Language (VL) models frequently results in suboptimal VQA performance. While prompt engineering has recently targeted the language component to address this alignment issue, the unique insights resided in visual analysis is still overlooked for further advancing VQA tasks. Additionally, seeking a trade-off between quality separability and domain invariance in VQA remains largely unresolved within the VL paradigm. In this paper, we introduce a novel cross-modal prompt-based approach to tackle these challenges. Specifically, we propose learnable prompts within the vision branch to foster synergy between visual and language modalities through a language-to-vision coupling function. The multi-view backbone is then carefully crafted with content enhancement and distortion-aware temporal modulation to ensure quality separability. The language prompts, derived from visual representations, are further supported by adaptive weighting mechanisms to optimize the balance between quality separability and domain invariance. Experimental results demonstrate the effectiveness of our proposed method over leading VQA models, showing significant improvements in generalization across diverse datasets. The source code for this work is publicly available at https://github.com/cpf0079/CM2PL.</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何缓解CLIP图文对齐在VQA中的次优表现并兼顾质量可分与域不变。</p>
                <p><span class="font-medium text-accent">研究方法：</span>在CLIP视觉分支引入可学习提示，用语言-视觉耦合函数协同双模态，并设计内容增强与失真感知时序调制的多视角主干。</p>
                <p><span class="font-medium text-accent">主要发现：</span>方法在多个数据集上显著优于现有VQA模型，跨域泛化性能提升明显。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出跨模态互提示，视觉提示与自适应加权语言提示联合优化质量可分与域不变。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用VL预训练模型提升视频质量评估提供了新的跨模态提示范式与平衡策略。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>视频质量评价(VQA)正从像素级失真度量走向语义级感知，CLIP 等视觉-语言预训练模型为引入高层语义提供了新途径，但图像-文本对齐机制直接迁移到 VQA 时性能次优，且视觉侧语义潜力与跨域鲁棒性尚未被充分挖掘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出跨模态互提示学习框架 CM2PL：在 CLIP 的图像编码器内植入可学习视觉提示，通过语言到视觉耦合函数将文本语义注入视觉分支；设计多视角主干，结合内容增强与失真敏感时序调制，保证质量可分性；视觉特征反向生成语言提示，并引入自适应加权损失，在质量可分性与域不变性之间动态权衡。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 KoNViD-1k、LIVE-Qualcomm、CVD2014 等多个基准上，CM2PL 的 SRCC 与 PLCC 均优于现有最佳 VQA 方法 2–6%，跨库零样本测试提升达 9%，证明视觉-语言互提示显著增强了泛化能力与语义一致性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 CLIP 的固定视觉编码器，提示容量与位置选择仍须经验设定；额外语言提示生成与加权模块带来约 15% 参数量与 30% 推理延迟开销，对实时场景不够友好；实验未涵盖用户生成内容(UGC)中复杂的时空失真组合。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量级提示搜索与自适应网络结构，或引入音频、字幕等多模态提示，实现更细粒度、可解释的无参考 VQA。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注多模态学习、CLIP 在底层视觉任务中的迁移、或跨域鲁棒的质量评价，该文提供了视觉-语言互提示范式与可复现代码，可直接扩展至图像质量评价、美学评估等语义敏感任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <!-- Title and metadata -->
            <div class="mb-3">
              <!-- Tags row -->
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <!-- Date: visible on mobile, hidden on md+ -->
                <span class="text-xs text-text-secondary md:hidden">· 2025-12-03</span>
              </div>
              <!-- Title and date row -->
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tmm.2025.3639991" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      StructGS: Adaptive Spherical Harmonics and Rendering Enhancements for Superior 3D Gaussian Splatting
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">StructGS：自适应球谐函数与渲染增强实现卓越3D高斯溅射</p>
                  
                </div>
                <!-- Date: hidden on mobile, visible on md+ -->
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2025-12-03</div>
                </div>
              </div>
              <!-- Venue: below title -->
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Multimedia">
                IEEE Transactions on Multimedia
                
                  <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                
              </div>
            </div>

            <!-- Authors -->
            <p class="text-sm text-text-secondary mb-3">
              Zexu Huang，Min Xu，Stuart Perry
            </p>

            <!-- DOI -->
            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tmm.2025.3639991" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tmm.2025.3639991</a>
              </span>
              
            </div>

            <!-- Abstract (collapsible) -->
            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in 3D reconstruction coupled with neural rendering techniques have greatly improved the creation of photo-realistic 3D scenes, influencing both academic research and industry applications. The technique of 3D Gaussian Splatting and its variants incorporate the strengths of both primitive-based and volumetric representations, achieving superior rendering quality. While 3D Geometric Scattering (3DGS) and its variants have advanced the field of 3D representation, they fall short in capturing the stochastic properties of non-local structural information during the training process. Additionally, the initialisation of spherical functions in 3DGS-based methods often fails to engage higher-order terms in early training rounds, leading to unnecessary computational overhead as training progresses. Furthermore, current 3DGS-based approaches require training on higher resolution images to render higher resolution outputs, significantly increasing memory demands and prolonging training durations. We introduce StructGS, a framework that enhances 3D Gaussian Splatting (3DGS) for improved novel-view synthesis in 3D reconstruction. StructGS innovatively incorporates a patch-based SSIM loss, dynamic spherical harmonics initialisation and a Multi-scale Residual Network (MSRN) to address the above-mentioned limitations, respectively. Our framework significantly reduces computational redundancy, enhances detail capture and supports high-resolution rendering from low-resolution inputs. Experimentally, StructGS demonstrates superior performance over state-of-the-art (SOTA) models, achieving higher quality and more detailed renderings with fewer artifacts. (The link to the code will be made available after publication.).</p>
              </div>
            </div>
            

            <!-- AI Summary -->
            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决3DGS无法捕获非局部结构、球谐初始化低效及需高分辨率训练才能输出高分辨率图像的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入StructGS，结合块SSIM损失、动态球谐初始化与多尺度残差网络，实现低分辨率输入的高分辨率渲染。</p>
                <p><span class="font-medium text-accent">主要发现：</span>StructGS在更少伪影下获得更高质量、更细腻的新视角合成，性能超越现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将自适应球谐启动、结构感知块损失与MSRN超分整合进3DGS框架，实现低输入高输出渲染。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为三维重建与实时渲染研究者提供高效、省内存的高质量场景表示新工具，推动VR/AR与内容创作应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>3D Gaussian Splatting (3DGS) has become a dominant primitive for neural radiance reconstruction because it marries point-based efficiency with volumetric photo-realism, yet it ignores non-local scene statistics and wastes compute on high-order spherical harmonics (SH) that are under-utilised in early training.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>StructGS introduces (i) a patch-wise SSIM loss that enforces cross-pixel structure similarity, (ii) a dynamic SH scheduler that starts with low-order coefficients and progressively promotes higher orders only when the gradient signal justifies the added parameters, and (iii) a lightweight Multi-scale Residual Network that upsamples 1×-resolution feature maps to 4× output, enabling supervision on low-res images while still rendering crisp high-res novel views.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Mip-NeRF 360, Tanks&amp;Temples and Deep Blending, StructGS cuts training time by 28 %, reduces GPU memory by 35 % and improves LPIPS by 0.012–0.025 over 3DGS while eliminating floater artifacts; the MSRN module alone yields 2× faster high-res inference without retraining on 2K images.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The patch-SSIM window is fixed to 7×7, which may underperform on scenes with extreme depth variation; dynamic SH scheduling requires an extra hyper-parameter sweep per dataset; and the MSRN upsampler is currently limited to 4× scale, restricting 8K rendering.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the scheduler to learn optimal SH order per Gaussian via reinforcement learning, and replace the handcrafted MSRN with a frequency-aware implicit decoder for arbitrary-scale super-resolution.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on efficient neural rendering, compression-friendly 3D representations or perceptual losses for radiance fields will find StructGS a practical drop-in that lowers compute barriers while boosting visual fidelity.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <!-- Score details -->
            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.74
                  
                    <span class="ml-1 text-blue-600">(IF: 9.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
      </div><!-- End of collapsible content -->
    </div>
  </main>

  <!-- Footer -->
  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        // Trigger chart resize for hidden charts
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>