<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZotWatch 文献推荐 - 2026-02-06</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            'bg-primary': '#f8fafc',
            'bg-card': '#ffffff',
            'bg-hover': '#f1f5f9',
            'text-primary': '#1e293b',
            'text-secondary': '#64748b',
            'border-color': '#e2e8f0',
            'accent': '#2563eb',
            'accent-hover': '#1d4ed8',
          }
        }
      }
    }
  </script>
  <style>
    .section-expand { transition: max-height 0.5s ease-out; overflow: hidden; }
    .section-expand.collapsed { max-height: 0; }
    .section-expand.expanded { max-height: none; }
    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif; }
    .content-container { max-width: 48rem; margin: 0 auto; padding: 0 1rem; }
    @media (min-width: 768px) { .content-container { padding: 0 2rem; } }
    @media (min-width: 1024px) { .content-container { padding: 0 4rem; } }

    .cluster-graph-container { width: 100%; height: 280px; touch-action: pan-y; }
    @media (min-width: 640px) { .cluster-graph-container { height: 320px; } }
    @media (min-width: 768px) { .cluster-graph-container { height: 350px; } }
    #cluster-graph svg { overflow: visible; }
  </style>
</head>
<body class="bg-bg-primary min-h-screen text-text-primary">
  <header class="bg-bg-card border-b border-border-color">
    <div class="content-container py-6 md:py-8">
      <h1 class="text-xl md:text-2xl font-bold text-text-primary">ZotWatch 文献推荐</h1>
      <p class="text-xs md:text-sm text-text-secondary mt-1">
        共 35 篇论文 ·
        生成于 2026-02-06 11:32 Asia/Shanghai
      </p>
    </div>
  </header>

  
  <section class="py-4 md:py-6 border-b border-border-color">
    <div class="content-container">
      <button id="btn-researcher-profile" onclick="toggleSection('researcher-profile')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm0 3c1.66 0 3 1.34 3 3s-1.34 3-3 3-3-1.34-3-3 1.34-3 3-3zm0 14.2c-2.5 0-4.71-1.28-6-3.22.03-1.99 4-3.08 6-3.08 1.99 0 5.97 1.09 6 3.08-1.29 1.94-3.5 3.22-6 3.22z"/>
          </svg>
          <span class="truncate">研究兴趣画像</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-researcher-profile">点击展开</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform" id="icon-researcher-profile" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="researcher-profile" class="section-expand collapsed">
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <div class="grid grid-cols-4 gap-3 text-center">
          <div>
            <div class="text-xl font-bold text-accent">975</div>
            <div class="text-xs text-text-secondary">收藏论文</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">5年9月</div>
            <div class="text-xs text-text-secondary">收藏时长</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">&gt;10</div>
            <div class="text-xs text-text-secondary">兴趣领域</div>
          </div>
          <div>
            <div class="text-xl font-bold text-accent">10</div>
            <div class="text-xs text-text-secondary">高频作者</div>
          </div>
        </div>
      </div>

      
      <div class="bg-accent/10 rounded-lg border border-accent/30 p-4 mb-4">
        <h3 class="text-sm font-semibold text-accent mb-2 flex items-center gap-2">
          <svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          AI 研究兴趣分析
        </h3>
        <div class="space-y-2 text-xs text-text-primary leading-relaxed">
          <p><span class="font-medium text-accent">研究兴趣：</span>用户长期聚焦计算机视觉核心任务（目标检测、视觉定位、姿态估计）与模型高效化（压缩、知识蒸馏、重参数化），同时积极追踪自监督/对比学习等表征学习新范式。</p>
          <p><span class="font-medium text-accent">深度关注：</span>在CVPR、ICCV、TPAMI等顶会顶刊持续收藏115+篇文献，对Kaiming He、Ross Girshick等团队的检测与表征学习工作保持9-24篇的系统跟踪，形成从基础模型到压缩落地的完整阅读链条。</p>
          <p><span class="font-medium text-accent">跨学科倾向：</span>约93篇遥感类文献（TGRS、GRSL、雷达学报）与SAR关键词显示，用户将通用视觉方法迁移至遥感解译，体现“CV+遥感”交叉收藏特征。</p>
          <p><span class="font-medium text-accent">兴趣演变：</span>2025-Q1单季新增102篇达峰值，关键词同步出现DeepSeek、大语言模型、扩散模型，提示兴趣正向多模态大模型与生成式AI快速扩展；2024-Q3后收藏量回落，可能进入主题筛选期。</p>
          <p><span class="font-medium text-accent">阅读建议：</span>建议关注多模态检测（Vision-Language Detection）、轻量化Transformer与SAR图像生成评估基准，以延续检测+压缩+遥感的交叉优势。</p>
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">研究领域分布</h4>
          <div style="height: 180px;">
            <canvas id="domainsChart"></canvas>
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-3">阅读趋势（季度）</h4>
          <div style="height: 180px;">
            <canvas id="trendsChart"></canvas>
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">论文发表年份分布</h4>
        <div style="height: 180px;">
          <canvas id="yearDistChart"></canvas>
        </div>
      </div>
      

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-4">
        <h4 class="text-sm font-medium text-text-primary mb-3">
          研究方向聚类图谱
          <span class="text-xs text-text-secondary font-normal ml-2">(30 个聚类)</span>
        </h4>
        <div id="cluster-graph" class="cluster-graph-container"></div>
        <div class="text-xs text-text-secondary mt-3">
          覆盖 949/949 篇论文（仅含摘要的文献参与聚类）
        </div>
      </div>
      

      <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mb-4">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">高频关注作者</h4>
          <div class="grid grid-cols-2 gap-x-4 gap-y-1 text-xs">
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Kaiming He">Kaiming He</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">24</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Ross Girshick">Ross Girshick</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">18</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Jian Sun">Jian Sun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">15</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Song Han">Song Han</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiangyu Zhang">Xiangyu Zhang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">14</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Piotr Dollár">Piotr Dollár</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">12</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Li Liu">Li Liu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Xiang Li">Xiang Li</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">11</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Yann LeCun">Yann LeCun</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Wenfeng Liang">Wenfeng Liang</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">10</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Zhenda Xie">Zhenda Xie</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
            <div class="flex justify-between truncate">
              <span class="text-text-primary truncate" title="Feng Xu">Feng Xu</span>
              <span class="text-text-secondary ml-1 flex-shrink-0">9</span>
            </div>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4">
          <h4 class="text-sm font-medium text-text-primary mb-2">常关注期刊/会议</h4>
          <div class="space-y-1 text-xs">
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Computer Vision and Pattern Recognition">Computer Vision and Pattern Recognition</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">115</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Geoscience and Remote Sensing">IEEE Transactions on Geoscience and Remote Sensing</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">50</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="Neural Information Processing Systems">Neural Information Processing Systems</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">46</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Computer Vision">International Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">38</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">32</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="International Conference on Learning Representations">International Conference on Learning Representations</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">29</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="European Conference on Computer Vision">European Conference on Computer Vision</span>
              <span class="text-xs px-1 py-0.5 rounded bg-amber-100 text-amber-700 ml-1 flex-shrink-0">
                会议
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">23</span>
            </div>
            
            <div class="flex items-center justify-between">
              <span class="text-text-primary truncate flex-1" title="IEEE Geoscience and Remote Sensing Letters">IEEE Geoscience and Remote Sensing Letters</span>
              <span class="text-xs px-1 py-0.5 rounded bg-blue-100 text-blue-700 ml-1 flex-shrink-0">
                期刊
              </span>
              <span class="text-text-secondary ml-1 flex-shrink-0">22</span>
            </div>
            
          </div>
        </div>
        
      </div>

      
      <div class="bg-bg-card rounded-lg border border-border-color p-4 mb-3">
        <h4 class="text-sm font-medium text-text-primary mb-2">高频关键词</h4>
        <div class="flex flex-wrap gap-1.5">
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            大语言模型 <span class="text-text-secondary">(14)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            合成孔径雷达 <span class="text-text-secondary">(12)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            自监督学习 <span class="text-text-secondary">(11)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            综述 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            域自适应 <span class="text-text-secondary">(9)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            目标检测 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            深度学习 <span class="text-text-secondary">(8)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            迁移学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            扩散模型 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            强化学习 <span class="text-text-secondary">(7)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            注意力机制 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            重参数化 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            DeepSeek <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            生成对抗网络 <span class="text-text-secondary">(6)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            基础模型 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            Transformers <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            卷积神经网络 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            旋转目标检测 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            知识蒸馏 <span class="text-text-secondary">(5)</span>
          </span>
          
          <span class="px-2 py-0.5 bg-bg-hover rounded text-xs text-text-primary">
            SAR目标识别 <span class="text-text-secondary">(5)</span>
          </span>
          
        </div>
      </div>
      

      <div class="text-xs text-text-secondary text-right">
        画像生成于 2026-02-06 11:15 Asia/Shanghai
        · kimi-k2-turbo-preview
      </div>
      </div>
    </div>
  </section>
  

  
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      Chart.defaults.font.family = "-apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif";
      Chart.defaults.font.size = 11;
      Chart.defaults.color = '#64748b';
      Chart.defaults.animation = false;

      
      const domainsCtx = document.getElementById('domainsChart');
      if (domainsCtx) {
        new Chart(domainsCtx, {
          type: 'bar',
          data: {
            labels: ['视觉定位', '目标检测', '模型压缩', '姿态估计', '对比学习', '人脸识别', '卫星导航', 'Transformer'],
            datasets: [{
              data: [22, 35, 18, 15, 10, 11, 6, 9],
              backgroundColor: 'rgba(37, 99, 235, 0.7)',
              borderColor: 'rgba(37, 99, 235, 1)',
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 16
            }]
          },
          options: {
            indexAxis: 'y',
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              },
              y: {
                grid: { display: false },
                ticks: { font: { size: 10 } }
              }
            }
          }
        });
      }
      

      
      const trendsCtx = document.getElementById('trendsChart');
      if (trendsCtx) {
        const trends = [{ q: '2024-Q1', c: 68 }, { q: '2024-Q2', c: 18 }, { q: '2024-Q3', c: 4 }, { q: '2024-Q4', c: 24 }, { q: '2025-Q1', c: 102 }, { q: '2025-Q2', c: 34 }, { q: '2025-Q3', c: 14 }, { q: '2025-Q4', c: 31 }, { q: '2026-Q1', c: 9 }];
        const monthColors = [
          { bg: 'rgba(59, 130, 246, 0.7)', border: 'rgba(59, 130, 246, 1)' },
          { bg: 'rgba(16, 185, 129, 0.7)', border: 'rgba(16, 185, 129, 1)' },
          { bg: 'rgba(245, 158, 11, 0.7)', border: 'rgba(245, 158, 11, 1)' }
        ];
        const bgColors = trends.map((t, i) => monthColors[i % 3].bg);
        const borderColors = trends.map((t, i) => monthColors[i % 3].border);
        new Chart(trendsCtx, {
          type: 'bar',
          data: {
            labels: trends.map(t => t.q.replace('-', '\n')),
            datasets: [{
              data: trends.map(t => t.c),
              backgroundColor: bgColors,
              borderColor: borderColors,
              borderWidth: 1,
              borderRadius: 2,
              barThickness: 20
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: { enabled: false }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: { font: { size: 9 }, maxRotation: 0 }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const yearDistCtx = document.getElementById('yearDistChart');
      if (yearDistCtx) {
        const yearData = [{ year: 2007, count: 3 }, { year: 2008, count: 2 }, { year: 2009, count: 2 }, { year: 2010, count: 9 }, { year: 2011, count: 10 }, { year: 2012, count: 3 }, { year: 2013, count: 12 }, { year: 2014, count: 14 }, { year: 2015, count: 18 }, { year: 2016, count: 15 }, { year: 2017, count: 41 }, { year: 2018, count: 59 }, { year: 2019, count: 56 }, { year: 2020, count: 68 }, { year: 2021, count: 84 }, { year: 2022, count: 114 }, { year: 2023, count: 111 }, { year: 2024, count: 114 }, { year: 2025, count: 181 }, { year: 2026, count: 9 }];
        new Chart(yearDistCtx, {
          type: 'line',
          data: {
            labels: yearData.map(d => d.year),
            datasets: [{
              data: yearData.map(d => d.count),
              borderColor: 'rgba(37, 99, 235, 1)',
              backgroundColor: 'rgba(37, 99, 235, 0.1)',
              borderWidth: 2,
              fill: true,
              tension: 0.3,
              pointRadius: 2,
              pointHoverRadius: 4
            }]
          },
          options: {
            responsive: true,
            maintainAspectRatio: false,
            plugins: {
              legend: { display: false },
              tooltip: {
                callbacks: {
                  label: function(context) {
                    return context.parsed.y + ' 篇';
                  }
                }
              }
            },
            scales: {
              x: {
                grid: { display: false },
                ticks: {
                  font: { size: 9 },
                  maxRotation: 45,
                  callback: function(value, index) {
                    const year = this.getLabelForValue(value);
                    return year % 5 === 0 ? year : '';
                  }
                }
              },
              y: {
                beginAtZero: true,
                grid: { color: 'rgba(226, 232, 240, 0.5)' },
                ticks: { precision: 0 }
              }
            }
          }
        });
      }
      

      
      const clusterContainer = document.getElementById('cluster-graph');
      if (clusterContainer) {
        const clusters = [
          
          {
            id: 0,
            label: "SAR \u76ee\u6807\u8bc6\u522b\u4e0e\u57df\u81ea\u9002\u5e94",
            size: 115,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u76ee\u6807\u8bc6\u522b", "\u8fc1\u79fb\u5b66\u4e60"]
          },
          
          {
            id: 1,
            label: "\u81ea\u76d1\u7763\u4e0e\u5c11\u6837\u672c\u89c6\u89c9\u5b66\u4e60",
            size: 80,
            keywords: ["\u81ea\u76d1\u7763\u5b66\u4e60", "\u57df\u81ea\u9002\u5e94", "\u5bf9\u6bd4\u5b66\u4e60"]
          },
          
          {
            id: 2,
            label: "\u65cb\u8f6c\u76ee\u6807\u5b9e\u65f6\u68c0\u6d4b",
            size: 56,
            keywords: ["\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b", "\u7efc\u8ff0", "DETR"]
          },
          
          {
            id: 3,
            label: "SAR \u8230\u8239\u68c0\u6d4b\u4e0e CFAR",
            size: 53,
            keywords: ["\u5408\u6210\u5b54\u5f84\u96f7\u8fbe", "SAR\u8230\u8239\u68c0\u6d4b", "\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408"]
          },
          
          {
            id: 4,
            label: "\u7ea2\u5916\u5f31\u5c0f\u76ee\u6807\u68c0\u6d4b",
            size: 46,
            keywords: ["\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b", "\u5f31\u5c0f\u76ee\u6807\u8ddf\u8e2a", "\u7ea2\u5916\u56fe\u50cf"]
          },
          
          {
            id: 5,
            label: "\u6269\u6563\u6a21\u578b\u56fe\u50cf\u751f\u6210",
            size: 41,
            keywords: ["\u6269\u6563\u6a21\u578b", "\u56fe\u50cf\u6062\u590d", "\u611f\u77e5-\u5931\u771f\u6743\u8861"]
          },
          
          {
            id: 6,
            label: "\u5927\u6a21\u578b\u63d0\u793a\u4e0e\u63a8\u7406\u4f18\u5316",
            size: 41,
            keywords: ["\u5927\u8bed\u8a00\u6a21\u578b", "\u5f3a\u5316\u5b66\u4e60", "DeepSeek"]
          },
          
          {
            id: 7,
            label: "MoE \u5927\u6a21\u578b\u9ad8\u6548\u63a8\u7406",
            size: 39,
            keywords: ["\u6df7\u5408\u4e13\u5bb6\u6a21\u578b", "\u5927\u8bed\u8a00\u6a21\u578b", "DeepSeek"]
          },
          
          {
            id: 8,
            label: "\u591a\u4f20\u611f\u5668 BEV \u4e09\u7ef4\u611f\u77e5",
            size: 37,
            keywords: ["\u591a\u4efb\u52a1\u5b66\u4e60", "\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5", "\u591a\u6a21\u6001"]
          },
          
          {
            id: 9,
            label: "2D/3D \u4eba\u4f53\u59ff\u6001\u4f30\u8ba1",
            size: 35,
            keywords: ["HRNet", "Transformers"]
          },
          
          {
            id: 10,
            label: "\u901a\u7528\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578b",
            size: 35,
            keywords: ["\u56fe\u50cf\u5206\u5272", "\u57fa\u7840\u6a21\u578b", "\u901a\u7528\u5206\u5272"]
          },
          
          {
            id: 11,
            label: "\u8f7b\u91cf\u7ea7 Vision Transformer",
            size: 30,
            keywords: ["\u8f7b\u91cf\u7ea7\u6a21\u578b", "\u6ce8\u610f\u529b\u673a\u5236", "Vision Transformers"]
          },
          
          {
            id: 12,
            label: "\u65e0\u7ea6\u675f\u8f66\u724c\u68c0\u6d4b\u8bc6\u522b",
            size: 28,
            keywords: ["\u8f66\u724c\u8bc6\u522b", "Internet of Things", "Microcontrollers"]
          },
          
          {
            id: 13,
            label: "\u591a\u4f20\u611f\u5668\u5168\u5c40\u4f4d\u59ff\u4f30\u8ba1",
            size: 27,
            keywords: []
          },
          
          {
            id: 14,
            label: "\u53ef\u5fae\u5206\u7f16\u7a0b\u4e0e\u673a\u5668\u5b66\u4e60\u57fa\u7840",
            size: 26,
            keywords: ["\u5e95\u5c42\u7b97\u6cd5", "\u6027\u80fd\u4f18\u5316", "\u53ef\u5fae\u5206\u7f16\u7a0b"]
          },
          
          {
            id: 15,
            label: "\u667a\u80fd\u96f7\u8fbe\u6297\u5e72\u6270\u4e0e\u8bc6\u522b",
            size: 25,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60", "\u7279\u5f81\u878d\u5408", "\u4eba\u5de5\u667a\u80fd"]
          },
          
          {
            id: 16,
            label: "\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e0e\u68af\u5ea6\u65b9\u6cd5",
            size: 24,
            keywords: ["\u751f\u6210\u5bf9\u6297\u7f51\u7edc", "\u4f18\u5316\u5668", "\u5206\u5e03\u5f0f\u8bad\u7ec3"]
          },
          
          {
            id: 17,
            label: "\u6301\u7eed\u5b66\u4e60\u4e0e\u6b8b\u5dee\u7f51\u7edc",
            size: 24,
            keywords: ["\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc", "\u6b8b\u5dee\u8fde\u63a5", "\u6df1\u5ea6\u6b8b\u5dee\u5b66\u4e60"]
          },
          
          {
            id: 18,
            label: "\u65e0\u8bed\u8a00\u89c6\u89c9\u8868\u5f81\u5b66\u4e60",
            size: 23,
            keywords: ["Computer Science - Computer Vision and Pattern Recognition", "CMC", "CPC"]
          },
          
          {
            id: 19,
            label: "\u795e\u7ecf\u7f51\u7edc\u91cf\u5316\u538b\u7f29",
            size: 21,
            keywords: ["\u6a21\u578b\u538b\u7f29", "\u6a21\u578b\u91cf\u5316", "\u6574\u6570\u63a8\u7406"]
          },
          
          {
            id: 20,
            label: "\u591a\u5c3a\u5ea6\u76ee\u6807\u68c0\u6d4b\u7efc\u8ff0",
            size: 21,
            keywords: ["\u7efc\u8ff0", "\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u673a\u5236", "\u591a\u5c3a\u5ea6\u5377\u79ef"]
          },
          
          {
            id: 21,
            label: "\u8f7b\u91cf CNN \u67b6\u6784\u8bbe\u8ba1",
            size: 20,
            keywords: ["\u6a21\u578b\u538b\u7f29", "VGG", "\u91cd\u53c2\u6570\u5316"]
          },
          
          {
            id: 22,
            label: "\u7a7f\u5899\u96f7\u8fbe\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b",
            size: 20,
            keywords: ["\u591a\u6a21\u5757\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "\u6742\u6ce2\u6291\u5236", "\u7a00\u758f\u6062\u590d"]
          },
          
          {
            id: 23,
            label: "\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4e0e\u6d41\u6a21\u578b",
            size: 20,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b", "NCE"]
          },
          
          {
            id: 24,
            label: "\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\u67b6\u6784\u6f14\u8fdb",
            size: 19,
            keywords: ["\u91cd\u53c2\u6570\u5316", "ResNet", "\u6b8b\u5dee\u7f51\u7edc"]
          },
          
          {
            id: 25,
            label: "\u6df1\u5ea6\u6a21\u578b\u53ef\u89e3\u91ca\u53ef\u89c6\u5316",
            size: 13,
            keywords: ["\u7279\u5f81\u53ef\u89c6\u5316", "Grad-CAM", "\u5206\u5e03\u5916\u68c0\u6d4b"]
          },
          
          {
            id: 26,
            label: "TinyML \u8fb9\u7f18\u667a\u80fd\u7cfb\u7edf",
            size: 11,
            keywords: ["\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6", "\u7cfb\u7edf\u4f18\u5316"]
          },
          
          {
            id: 27,
            label: "\u5728\u7ebf\u76ee\u6807\u8ddf\u8e2a\u4e0e\u5173\u8054",
            size: 10,
            keywords: ["SIFT", "\u5308\u7259\u5229\u7b97\u6cd5", "\u591a\u57df\u6cdb\u5316"]
          },
          
          {
            id: 28,
            label: "\u6269\u6563 Transformer \u91cf\u5316",
            size: 6,
            keywords: ["\u6807\u51c6\u5316\u6d41", "\u6d41\u6a21\u578b"]
          },
          
          {
            id: 29,
            label: "\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e0e\u6570\u636e\u6d41",
            size: 3,
            keywords: ["StepFun"]
          }
          
        ];

        const links = [{"source": 2, "target": 27, "value": 0.8830752333840928}, {"source": 3, "target": 4, "value": 0.9012679685009678}, {"source": 7, "target": 29, "value": 0.8492637553300961}, {"source": 18, "target": 29, "value": 0.8392484878882785}, {"source": 8, "target": 9, "value": 0.8905423651900435}, {"source": 8, "target": 12, "value": 0.8667003709930683}, {"source": 2, "target": 11, "value": 0.9283010175703301}, {"source": 19, "target": 21, "value": 0.9019312645755561}, {"source": 2, "target": 8, "value": 0.9203129981761866}, {"source": 1, "target": 18, "value": 0.9437973154209442}, {"source": 2, "target": 20, "value": 0.9456066259678658}, {"source": 6, "target": 17, "value": 0.9150795688873237}, {"source": 16, "target": 25, "value": 0.8901541417582164}, {"source": 21, "target": 24, "value": 0.9248824864011752}, {"source": 5, "target": 18, "value": 0.8919095843520352}, {"source": 4, "target": 20, "value": 0.9109903153073827}, {"source": 2, "target": 4, "value": 0.919351094523423}, {"source": 0, "target": 4, "value": 0.9134944611653821}, {"source": 9, "target": 13, "value": 0.8611157472759342}, {"source": 1, "target": 5, "value": 0.8918893068252775}, {"source": 1, "target": 11, "value": 0.9432122613352345}, {"source": 19, "target": 26, "value": 0.8808829359996554}, {"source": 0, "target": 22, "value": 0.9012413399077841}, {"source": 11, "target": 28, "value": 0.8886640046377248}, {"source": 6, "target": 7, "value": 0.9207971335461342}, {"source": 24, "target": 25, "value": 0.9277324887715549}, {"source": 15, "target": 22, "value": 0.9066506694958371}, {"source": 21, "target": 26, "value": 0.8950990066341606}, {"source": 14, "target": 17, "value": 0.9042363695871469}, {"source": 14, "target": 23, "value": 0.9034536250717172}, {"source": 0, "target": 3, "value": 0.9405766715298327}, {"source": 8, "target": 10, "value": 0.8927630740474666}, {"source": 8, "target": 13, "value": 0.9018865674905154}, {"source": 11, "target": 18, "value": 0.936032065686293}, {"source": 0, "target": 15, "value": 0.8869737861791469}, {"source": 2, "target": 12, "value": 0.8655168084498254}, {"source": 1, "target": 10, "value": 0.9053558443698786}, {"source": 19, "target": 28, "value": 0.9221071793736463}, {"source": 9, "target": 27, "value": 0.9066356160212629}, {"source": 11, "target": 21, "value": 0.9102451630783859}, {"source": 7, "target": 11, "value": 0.8994511365517345}, {"source": 16, "target": 17, "value": 0.9093642161080732}, {"source": 16, "target": 23, "value": 0.8725030199089625}];

        const academicColors = [
          '#4E79A7', '#F28E2B', '#E15759', '#76B7B2', '#59A14F',
          '#EDC948', '#B07AA1', '#FF9DA7', '#9C755F', '#BAB0AC',
          '#86BCB6', '#D37295', '#8CD17D', '#499894', '#FABFD2',
          '#79706E', '#D4A6C8', '#9D7660', '#B6992D', '#A0CBE8'
        ];
        const colorScale = (i) => academicColors[i % academicColors.length];

        let svg, simulation, node, link, sizeScale;
        let lastWidth = 0;
        let simulationStopped = false;

        function createGraph() {
          const width = clusterContainer.clientWidth;
          const height = clusterContainer.clientHeight || 280;
          const isMobile = width < 500;

          lastWidth = width;
          clusterContainer.innerHTML = '';

          const minRadius = isMobile ? 16 : 20;
          const maxRadius = isMobile ? 38 : 48;
          sizeScale = d3.scaleSqrt()
            .domain([0, d3.max(clusters, d => d.size)])
            .range([minRadius, maxRadius]);

          svg = d3.select('#cluster-graph')
            .append('svg')
            .attr('width', width)
            .attr('height', height)
            .attr('viewBox', [0, 0, width, height]);

          // Create layered groups for z-order control
          // Order (bottom to top): dimmed links -> dimmed nodes -> highlighted links -> highlighted nodes
          const layerDimmedLinks = svg.append('g').attr('class', 'layer-dimmed-links');
          const layerDimmedNodes = svg.append('g').attr('class', 'layer-dimmed-nodes');
          const layerHighlightedLinks = svg.append('g').attr('class', 'layer-highlighted-links');
          const layerHighlightedNodes = svg.append('g').attr('class', 'layer-highlighted-nodes');

          const chargeStrength = isMobile ? -100 : -150;
          const linkDistance = isMobile ? 60 : 90;

          simulationStopped = false;
          simulation = d3.forceSimulation(clusters)
            .force('link', d3.forceLink(links).id(d => d.id).distance(linkDistance).strength(0.2))
            .force('charge', d3.forceManyBody().strength(chargeStrength))
            .force('center', d3.forceCenter(width / 2, height / 2))
            .force('collision', d3.forceCollide().radius(d => sizeScale(d.size) + (isMobile ? 4 : 6)))
            .velocityDecay(0.7)
            .alphaDecay(0.08)
            .alphaMin(0.01);

          // Initially all links in dimmed layer
          link = layerDimmedLinks
            .attr('stroke', '#e2e8f0')
            .attr('stroke-opacity', 0.6)
            .selectAll('line')
            .data(links)
            .join('line')
            .attr('stroke-width', d => Math.max(1, d.value * 1.5));

          // Initially all nodes in dimmed layer
          node = layerDimmedNodes
            .selectAll('g')
            .data(clusters)
            .join('g')
            .style('cursor', 'grab');

          node.append('circle')
            .attr('r', d => sizeScale(d.size))
            .attr('fill', (d, i) => colorScale(i))
            .attr('fill-opacity', 0.85)
            .attr('stroke', '#fff')
            .attr('stroke-width', isMobile ? 1.5 : 2);

          const fontSize = isMobile ? 7 : 8;
          const lineHeight = isMobile ? 8 : 9;
          const charWidth = isMobile ? 5 : 5.5;

          function wrapText(text, radius) {
            text.each(function() {
              const textEl = d3.select(this);
              const label = textEl.text();
              const maxWidth = radius * 1.2;

              textEl.text(null);

              let lines = [];
              let currentLine = '';
              for (let char of label) {
                if ((currentLine.length + 1) * charWidth > maxWidth && currentLine) {
                  lines.push(currentLine);
                  currentLine = char;
                } else {
                  currentLine += char;
                }
              }
              if (currentLine) lines.push(currentLine);

              if (lines.length > 2) {
                lines = lines.slice(0, 2);
                lines[1] = lines[1].substring(0, Math.max(0, lines[1].length - 1)) + '..';
              }

              const startY = -(lines.length - 1) * lineHeight / 2;

              lines.forEach((line, i) => {
                textEl.append('tspan')
                  .attr('x', 0)
                  .attr('dy', i === 0 ? startY : lineHeight)
                  .text(line);
              });
            });
          }

          node.append('text')
            .text(d => d.label)
            .attr('text-anchor', 'middle')
            .attr('dominant-baseline', 'middle')
            .attr('font-size', fontSize + 'px')
            .attr('font-weight', '500')
            .attr('fill', '#fff')
            .attr('pointer-events', 'none')
            .each(function(d) {
              wrapText(d3.select(this), sizeScale(d.size));
            });

          node.append('text')
            .text(d => `(${d.size}篇)`)
            .attr('text-anchor', 'middle')
            .attr('dy', d => sizeScale(d.size) + (isMobile ? 10 : 12))
            .attr('font-size', fontSize + 'px')
            .attr('fill', '#94a3b8');

          node.append('title')
            .text(d => `${d.label}\n论文数: ${d.size}\n关键词: ${d.keywords.join(', ')}`);

          const drag = d3.drag()
            .on('start', dragstarted)
            .on('drag', dragged)
            .on('end', dragended);

          node.call(drag);

          simulation.on('tick', () => {
            node.attr('transform', d => {
              const r = sizeScale(d.size);
              const padding = isMobile ? 5 : 8;
              const maxY = height - r - (isMobile ? 12 : 15);
              d.x = Math.max(r + padding, Math.min(width - r - padding, d.x));
              d.y = Math.max(r + padding, Math.min(maxY, d.y));
              return `translate(${d.x},${d.y})`;
            });

            link
              .attr('x1', d => d.source.x)
              .attr('y1', d => d.source.y)
              .attr('x2', d => d.target.x)
              .attr('y2', d => d.target.y);
          });

          simulation.on('end', () => {
            simulationStopped = true;
          });

          function dragstarted(event, d) {
            if (!event.active) simulation.alphaTarget(0.05).restart();
            d.fx = d.x;
            d.fy = d.y;
            highlightNode(d);
            d3.select(this).style('cursor', 'grabbing');
          }

          function dragged(event, d) {
            const r = sizeScale(d.size);
            const padding = isMobile ? 5 : 8;
            const maxY = height - r - (isMobile ? 12 : 15);
            d.fx = Math.max(r + padding, Math.min(width - r - padding, event.x));
            d.fy = Math.max(r + padding, Math.min(maxY, event.y));
          }

          function dragended(event, d) {
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
            d3.select(this).style('cursor', 'grab');
            resetHighlight();
          }

          function highlightNode(selectedNode) {
            const connectedIds = new Set();

            links.forEach(l => {
              if (l.source.id === selectedNode.id) connectedIds.add(l.target.id);
              if (l.target.id === selectedNode.id) connectedIds.add(l.source.id);
            });

            // Step 1: Style dimmed elements
            link
              .attr('stroke-opacity', 0.1)
              .attr('stroke', '#cbd5e1')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.each(function(d) {
              d3.select(this).select('circle')
                .attr('fill-opacity', 0.25)
                .attr('stroke', '#fff')
                .attr('stroke-width', isMobile ? 1.5 : 2);

              d3.select(this).selectAll('text')
                .attr('opacity', 0.3);
            });

            // Step 2: Move dimmed elements to bottom layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });
            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Step 3: Style and move highlighted links to top
            link.filter(l => l.source.id === selectedNode.id || l.target.id === selectedNode.id)
              .attr('stroke-opacity', 1)
              .attr('stroke', '#2563eb')
              .attr('stroke-width', d => Math.max(2.5, d.value * 3))
              .each(function() {
                layerHighlightedLinks.node().appendChild(this);
              });

            // Step 4: Style and move connected nodes to top
            node.filter(d => connectedIds.has(d.id))
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#60a5fa')
                  .attr('stroke-width', isMobile ? 2.5 : 3);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });

            // Step 5: Style and move selected node to very top
            node.filter(d => d.id === selectedNode.id)
              .each(function(d) {
                d3.select(this).select('circle')
                  .attr('fill-opacity', 1)
                  .attr('stroke', '#2563eb')
                  .attr('stroke-width', isMobile ? 3 : 4);

                d3.select(this).selectAll('text')
                  .attr('opacity', 1);

                layerHighlightedNodes.node().appendChild(this);
              });
          }

          function resetHighlight() {
            // Move all elements back to dimmed layers
            link.each(function() {
              layerDimmedLinks.node().appendChild(this);
            });

            node.each(function() {
              layerDimmedNodes.node().appendChild(this);
            });

            // Reset styles
            node.select('circle')
              .attr('fill-opacity', 0.85)
              .attr('stroke', '#fff')
              .attr('stroke-width', isMobile ? 1.5 : 2)
              .style('filter', 'none');

            link
              .attr('stroke-opacity', 0.6)
              .attr('stroke', '#e2e8f0')
              .attr('stroke-width', d => Math.max(1, d.value * 1.5));

            node.selectAll('text')
              .attr('opacity', 1);
          }
        }

        createGraph();

        let resizeTimeout;
        window.addEventListener('resize', () => {
          clearTimeout(resizeTimeout);
          resizeTimeout = setTimeout(() => {
            const newWidth = clusterContainer.clientWidth;
            if (Math.abs(newWidth - lastWidth) > 10) {
              clusters.forEach(d => { d.x = undefined; d.y = undefined; d.fx = null; d.fy = null; });
              createGraph();
            }
          }, 250);
        });
      }
      
    });
  </script>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <button id="btn-overall-summaries" onclick="toggleSection('overall-summaries')"
              class="w-full text-left text-lg md:text-xl font-bold text-text-primary mb-3 md:mb-4 flex items-center justify-between group hover:text-accent transition-colors">
        <span class="flex items-center gap-2 flex-1 min-w-0">
          <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 24 24">
            <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
          </svg>
          <span class="truncate">本期研究趋势概览</span>
        </span>
        <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
          <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-overall-summaries">点击收起</span>
          <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-overall-summaries" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
          </svg>
        </span>
      </button>

      <div id="overall-summaries" class="section-expand expanded">
      <div class="space-y-4 md:space-y-6">
        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            精选推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了2篇关于SAR智能解译的论文、1篇关于多源信息融合的论文、1篇关于红外小目标检测的论文及1篇关于视频SAR动目标检测的论文。</p>
            
            <p><strong class="text-accent">SAR智能解译</strong>：《SAR-RAG》提出视觉问答式语义检索增强生成框架，实现SAR图像自动目标识别；《SOMA-1M》构建百万级SAR-光学多分辨率对齐数据集，支撑跨模态遥感多任务学习。</p>
            
            <p><strong class="text-accent">多源信息融合</strong>：《MSIF-SSTR》融合雷达、AIS、光电等多源数据，提出“快走私者”快艇轨迹识别方法，提升近岸走私行为发现能力。</p>
            
            <p><strong class="text-accent">红外小目标检测</strong>：《SPIRIT》将视觉基础模型适配于单帧与多帧红外小目标检测，实现统一框架下的监视与预警应用。</p>
            
            <p><strong class="text-accent">视频SAR动目标检测</strong>：《PC-YOLO》在主成分图上运行YOLO，解决视频SAR强杂波、低信杂比下的运动目标检测难题。</p>
            
          </div>
        </div>
        

        
        <div class="bg-bg-card rounded-lg border border-border-color p-4 md:p-5">
          <h3 class="text-base font-semibold text-text-primary mb-3 flex items-center gap-2">
            <svg class="w-4 h-4 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            相似度推荐总结
          </h3>
          <div class="text-sm text-text-primary leading-relaxed space-y-2">
            <p>本期推荐涵盖了9篇关于目标检测的论文、6篇关于知识蒸馏与模型压缩的论文、5篇关于跨模态/多模态学习的论文、4篇关于小目标检测的论文、3篇关于域泛化与迁移学习的论文、2篇关于图像分解与增强的论文以及1篇关于SAR视觉问答的论文。</p>
            
            <p><strong class="text-text-secondary">目标检测</strong>：该主题聚焦无人机、遥感等场景下的旋转/定向目标检测与尺度自适应问题，代表作如《GADet》提出几何感知旋转框检测，《Adaptive image zoom-in》通过自适应放大提升无人机小目标检测精度。</p>
            
            <p><strong class="text-text-secondary">知识蒸馏</strong>：研究如何通过松弛约束、特征对齐等方式提升学生网络性能，《Relaxed Knowledge Distillation》放宽匹配条件以保留更多有益知识，多篇论文探索了跨模态、跨任务蒸馏策略。</p>
            
            <p><strong class="text-text-secondary">跨模态学习</strong>：探索3D点云-文本、视觉-语言等多模态对齐与融合，《Cross-modal contrastive learning》提出隐式语义对齐的对比学习框架，实现点云与文本的联合表征。</p>
            
            <p><strong class="text-text-secondary">小目标检测</strong>：针对红外/可见光图像中极小目标信噪比低、特征弱的问题，《Dynamic High-frequency Convolution》利用高频分量增强，《SPIRIT》将视觉基础模型适配于单帧与多帧红外小目标检测。</p>
            
            <p><strong class="text-text-secondary">域泛化</strong>：致力于在未见目标域上保持性能，《Prompt Disentanglement》通过语言引导与表征对齐解耦提示，实现跨域泛化。</p>
            
            <p><strong class="text-text-secondary">图像分解</strong>：研究从单张混合图像中分离反射与透射成分，《Complementary Mixture-of-Experts》引入互补专家与交叉注意力提升野外反射分离效果。</p>
            
            <p><strong class="text-text-secondary">SAR问答</strong>：《SAR-RAG》提出基于语义检索与多模态大模型的合成孔径雷达图像视觉问答系统，用于自动目标识别。</p>
            
          </div>
        </div>
        
      </div>
      </div>
    </div>
  </section>
  

  
  <section class="py-5 md:py-8 border-b border-border-color">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-interest-works" onclick="toggleSection('interest-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="currentColor" viewBox="0 0 20 20">
              <path d="M9.049 2.927c.3-.921 1.603-.921 1.902 0l1.07 3.292a1 1 0 00.95.69h3.462c.969 0 1.371 1.24.588 1.81l-2.8 2.034a1 1 0 00-.364 1.118l1.07 3.292c.3.921-.755 1.688-1.54 1.118l-2.8-2.034a1 1 0 00-1.175 0l-2.8 2.034c-.784.57-1.838-.197-1.539-1.118l1.07-3.292a1 1 0 00-.364-1.118L2.98 8.72c-.783-.57-.38-1.81.588-1.81h3.461a1 1 0 00.951-.69l1.07-3.292z"/>
            </svg>
            <span class="truncate">精选推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">基于研究兴趣匹配，共 5 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-interest-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-interest-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">基于研究兴趣匹配，共 5 篇</p>
      </div>

      <div id="interest-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 62%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.04712v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-RAG：基于语义搜索、检索与MLLM生成的SAR视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              David F. Ramirez，Tim Overman，Kristen Jaskie，Joe Marvin，Andreas Spanias
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.04712v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-1" onclick="toggleSection('interest-abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升合成孔径雷达自动目标识别在车辆类别与尺寸估计上的准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>将MLLM与语义向量库结合，先检索相似SAR图像示例再生成判别结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入SAR-RAG记忆库后，分类精度与尺寸回归误差均显著优于纯MLLM基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图像检索增强生成框架用于SAR-ATR，实现示例驱动的上下文判别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为国防遥感领域提供可解释、可扩展的示例辅助识别范式，降低标注依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-1" onclick="toggleSection('interest-detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)自动目标识别(ATR)是国防与安全领域的关键任务，但SAR图像中军用车辆外观相似、信噪比低，导致类别与尺寸判别困难。传统仅依赖单幅测试图像的识别方法难以充分利用历史标注样本的丰富语义信息，因此需要引入能检索并复用已标注样本的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-RAG框架，将多模态大语言模型(MLLM)与向量数据库结合，实现“检索增强生成”：先用共享视觉编码器将测试图像与库存图像编码为语义嵌入，通过近似最近邻搜索召回最相似的K例带标注样本；随后把检索到的图像-标签-尺寸三元组作为上下文提示，与测试图像一起输入MLLM，由模型在注意力机制下对比差异并输出目标类别及长、宽、高回归值。整个流程构成一个可插拔的“ATR记忆库”，无需重新训练即可动态更新库存。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-ATR数据集上的实验表明，增加SAR-RAG后，Top-5检索命中率提升18%，分类准确率比纯MLLM基线提高6.7%，车辆长度、宽度、高度回归的RMSE分别降低12%、15%与10%。消融实验显示，检索数量K=5时增益饱和，且语义嵌入空间采用CLIP-style对比预训练比ImageNet迁移更具判别性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体数据集名称与实验代码，结果可复现性受限；检索库仅含同传感器、同分辨率图像，跨传感器、跨视角或不同噪声水平下的泛化性能未验证；MLLM推理延迟与显存占用随库存规模线性增长，实时部署存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨模态检索以兼容可见光/红外辅助库，并研究基于强化学习的动态检索数量决策，以在精度与效率间自适应折衷。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事SAR目标识别、多模态检索或记忆增强生成研究，该文提供了将大模型与专用向量库耦合的新范式，可直接借鉴其提示模板与评估协议，加速自身算法的验证与落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 57%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05480v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOMA-1M：面向多任务遥感的大规模SAR-光学多分辨率对齐数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peihao Wu，Yongxiang Yao，Yi Wan，Wenfei Zhang，Ruipeng Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05480v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-2" onclick="toggleSection('interest-abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建大规模、多分辨率、像素级对齐的SAR-光学影像对，以支撑多任务遥感基础模型训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计粗到细匹配框架，融合多源卫星数据，生成1.3M对512×512像素、0.5-10m全球覆盖的精准对齐样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SOMA-1M显著提升匹配、融合、云去除、跨模态翻译等30+算法性能，MRSI匹配达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个0.5-10m多分辨率、像素级对齐、百万规模SAR-光学数据集，并建立四大任务基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态基础模型提供统一训练与评测资源，推动跨模态协同解析研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-2" onclick="toggleSection('interest-detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学影像在波长、成像机理上互补，但现有公开数据集普遍分辨率单一、规模小、配准误差大，难以支撑多尺度基础模型训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者整合Sentinel-1、PIESAT-1、Capella、Google Earth四源数据，构建1.3 M对512×512像素全球影像，空间分辨率0.5–10 m，覆盖12类典型地物。提出粗到细匹配框架：先SIFT几何粗配准，再基于互信息与相位一致性精修，实现像素级对齐；并设计云掩膜与质量评分策略，自动筛除不合格样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的四级评测基准上，30余种主流算法在影像匹配、融合、SAR辅助去云、跨模态翻译任务中均获得显著提升，其中MRSI匹配指标达SOTA。消融实验表明，仅用SOMA-1M预训练即可在下游任务上平均提升3–8个百分点，验证其作为多模态基础数据的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍以单时相为主，缺乏长时序SAR-光学对；高分辨率（&lt;0.5 m）样本比例有限，且未提供极化SAR信息。配准流程依赖外部DEM，在剧烈地形区仍可能出现亚像素残差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至少10 m基线的高分辨率多时相对，并引入极化SAR与激光雷达，构建三维多模态对齐基准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态遥感基础模型、跨模态生成或匹配，该文提供了迄今最大规模的多分辨率对齐基准与可复现流程，可直接用于预训练与评测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.eswa.2026.131525" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      MSIF-SSTR: A ”Quick Smuggler” Smuggling Speedboat Trajectory Recognition Method Based on Multi-source Information Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">MSIF-SSTR：一种基于多源信息融合的“快艇走私者”走私快艇轨迹识别方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Expert Systems with Applications">
                Expert Systems with Applications
                
                  <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zhuhua Hu，Yifeng Sun，Yaochi Zhao，Wei Wu，Lingkai Kong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.eswa.2026.131525" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.eswa.2026.131525</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-3" onclick="toggleSection('interest-abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cooperating with maritime administrative departments to identify smuggling activities and enhance the control ability of nearshore vessels holds significant practical significance. However, existing research mostly relies on basic AIS data and simple features, making it difficult to deal with complex vessel behaviors. Especially when identifying covert and flexible smuggling activities, it is prone to misjudgment and has limited effectiveness. In real-world enforcement, distinguishing truly suspicious ”Quick Smuggler” smuggling from benign high-speed transit requires modeling subtle, deep-level spatio-temporal cues that couple motion dynamics with external conditions (e.g., wind, wave, visibility) and context. Simple linear mappings and shallow temporal encoders often overfit speed bursts or local detours, causing elevated false alarms. By contrast, dilated-convolutional receptive fields in TCNs capture multi-scale temporal dependencies efficiently, while KAN layers provide adaptive nonlinear function bases to fit complex, locally varying trajectory patterns. This synergy is particularly suited to covert nighttime operations under shifting sea states, where genuine smuggling exhibits trajectory micro-structures and weather-conditioned behaviors that are hard to emulate by normal craft. To address these challenges, this study proposes a Multi-Source Information Fusion-based ”Quick Smuggler” Smuggling Speedboat Trajectory Recognition method (MSIF-SSTR). First, we construct the HN_BF dataset, comprising real-world nighttime radar trajectories from the Qiongzhou Strait and corresponding meteorological data. Next, parallel TCN networks are employed to separately extract motion features, and meteorological features, enabling the model to better capture global temporal dependencies during feature extraction. Finally, the fused features are fed into an LSTM for classification, while a Kolmogorov-arnold networks (KAN) module replaces traditional fully connected layers to improve the representation of complex trajectory patterns. Experimental results demonstrate that MSIF-SSTR achieves F1-scores exceeding 94.2% on the HN_BF dataset, outperforming state-of-the-art methods with higher computational efficiency. Field applications confirm the model’s robustness.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>夜间高速走私快艇与正常高速船难以区分，需降低误报。</p>
                <p><span class="font-medium text-accent">研究方法：</span>并行TCN提取运动/气象特征，LSTM+KAN分类，融合多源信息。</p>
                <p><span class="font-medium text-accent">主要发现：</span>HN_BF数据集F1&gt;94.2%，计算高效且实地鲁棒。</p>
                <p><span class="font-medium text-accent">创新点：</span>首用TCN-KAN协同捕获天气耦合的微观轨迹结构识别走私。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为海事部门提供可部署的精准高速走私识别工具，减少空耗执法。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-3" onclick="toggleSection('interest-detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Unable to extract background</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Unable to extract methodology details</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Unable to extract results</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Unable to extract limitations</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>{&#34;background&#34;:&#34;近岸高速快艇走私活动隐蔽且机动性强，传统仅依赖AIS与简单运动特征的识别方法在夜间、复杂海况下误报率高，难以满足海警实时执法需求。&#34;,&#34;methodology_details&#34;:&#34;作者构建琼州海峡夜间雷达轨迹-气象配对数据集HN_BF，并行TCN分别对运动序列与气象序列进行多尺度时间依赖编码，拼接后送入LSTM做时序建模；分类头用KAN网络替代全连接层，以自适应基函数</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.70</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.66
                  
                    <span class="ml-1 text-blue-600">(IF: 7.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 51%
                </span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030510" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PC-YOLO: Moving Target Detection in Video SAR via YOLO on Principal Components
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PC-YOLO：基于主成分YOLO的视频SAR运动目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Han，Xinrong Wang，Jiaqing Jiang，Chao Xue，Rui Qin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030510" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030510</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-4" onclick="toggleSection('interest-abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video synthetic aperture radar could provide more valuable information than static images. However, it suffers from several difficulties, such as strong clutter, low signal-to-noise ratio, and variable target scale. The task of moving target detection is therefore difficult to achieve. To solve these problems, this paper proposes a model and data co-driven learning method called look once on principal components (PC-YOLO). Unlike preceding works, we regarded the imaging scenario as a combination of low-rank and sparse scenes in theory. The former models the global, slowly varying background information, while the latter expresses the localized anomalies. These were then separated using the principal component decomposition technique to reduce the clutter while simultaneously enhancing the moving targets. The resulting principal components were then handled by an improved version of the look once framework. Since the moving targets featured various scales and weak scattering coefficients, the hierarchical attention mechanism and the cross-scale feature fusion strategy were introduced to further improve the detection performance. Finally, multiple rounds of experiments were performed to verify the proposed method, with the results proving that it could achieve more than 30% improvement in mAP compared to classical methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强杂波、低信噪比和尺度变化的视频SAR中可靠检测运动目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视频SAR分解为低秩背景与稀疏异常，用主成分提取运动目标，再以改进YOLO检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PC-YOLO在mAP上较经典方法提升超30%，显著增强多尺度弱散射运动目标检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把低秩-稀疏分解的主成分输入YOLO，并引入层级注意与跨尺度融合，实现模型-数据协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频SAR运动目标检测提供鲁棒新框架，可直接提升监视、侦察等遥感应用的效率与精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-4" onclick="toggleSection('interest-detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Video SAR 可在连续帧中揭示静态图像无法捕捉的动态信息，但强杂波、低信噪比和目标尺度变化使传统动目标检测方法性能骤降。作者希望利用雷达回波的“低秩+稀疏”先验，把缓慢变化的背景与局部异常的运动目标解耦，从而提升检测可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将成像场景建模为低秩背景与稀疏异常之和，用主成分分解（PCD）分离二者，在抑制杂波的同时增强运动目标；随后把前几个主成分帧作为输入，送入改进的YOLO检测器。为应对目标尺度多变和散射弱的问题，网络引入分层注意力模块和跨尺度特征融合，使小/弱目标也能被充分表征。整个流程以模型驱动（PCD）与数据驱动（YOLO）协同的方式端到端训练，实现杂波抑制与检测一体化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采与公开Video SAR数据集上的多轮实验显示，PC-YOLO的mAP比经典CFAR+跟踪方法和近年深度检测网络平均提升30%以上，尤其对10×10像素级弱目标召回率提高显著；杂波抑制步骤使虚警率下降约一个数量级，验证了“低秩+稀疏”假设在SAR动目标场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PCD假设背景在短时内严格低秩，当平台机动或场景存在快速起伏时可能失效；此外，网络仍需大量带标注的Video SAR帧，而此类真值获取成本高昂。方法对参数（主成分个数、注意力通道数）敏感，跨传感器泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线低秩更新或自监督PCD，以适应平台机动和长时序列；结合无监督域适应减少对标注数据的依赖，并探索在无人机Mini-SAR、车载SAR等小型平台上的实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究SAR动目标检测、低秩稀疏分解与深度检测器结合、或小/弱目标遥感识别，本文提供的“模型+数据”协同范式、PCD预处理策略及跨尺度注意力改进均可直接借鉴；其实验设置与指标也可作为Video SAR基准参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.67</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium bg-accent/10 text-accent border border-accent/30">
                  匹配度 48%
                </span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01843v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPIRIT：面向统一单帧与多帧红外小目标检测的视觉基础模型适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Xu，Xi Li，Fei Gao，Jie Guo，Haojuan Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01843v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-interest-abstract-5" onclick="toggleSection('interest-abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-interest-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="interest-abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一单帧与多帧红外弱小目标检测并克服可见光预训练模型失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SPIRIT框架：轻量物理插件PIFR抑制背景、PGMA用历史先验约束跨帧关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项IRSTD基准上显著超越VFM基线并达SOTA，兼顾视频与单帧场景。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFMs适配至IRSTD，提出秩稀疏分解特征增强与先验引导记忆关联机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外数据稀缺场景提供可迁移的预训练模型利用方案，推动监控预警实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-interest-detail-5" onclick="toggleSection('interest-detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="interest-detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测(IRSTD)在监视与预警中不可或缺，但公开红外数据稀缺，难以训练深度模型；同时，实际系统既需单帧检测也需视频跟踪，现有方法往往只能处理其一。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SPIRIT框架，将视觉基础模型(VFM)轻量级地适配到IRSTD：空间上，PIFR插件用秩-稀疏分解抑制结构化背景、增强稀疏目标峰值；时间上，PGMA插件把历史帧生成的软空间先验注入记忆交叉注意力，约束跨帧关联，无视频时自动退化为单帧推理。整个框架保持VFM骨干不变，仅插入物理引导模块，实现统一单帧/多帧推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个IRSTD基准上，SPIRIT一致优于直接使用VFM的基线，并取得新的SOTA，显著降低虚警；视频模式下利用时序先验后，目标轨迹连续性提升，单帧模式下仍保持高检测率，验证了统一框架的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖红外序列中背景可秩-稀疏分解的假设，对剧烈抖动或复杂云层边缘可能失效；PGMA的软先验需足够历史帧，短序列或极低信噪比场景下增益有限；论文尚未在真实嵌入式红外载荷上验证延迟与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自适应秩-稀疏参数以应对动态背景，并将框架蒸馏为端侧轻量化网络，实现实时机载红外预警。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究红外小目标检测、视觉基础模型迁移或统一单帧-视频推理，该文提供了将VFMs与物理先验结合的范例和可插拔模块，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.68</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </section>
  

  <main class="py-5 md:py-8">
    <div class="content-container">
      <div class="mb-3 md:mb-4">
        <button id="btn-similarity-works" onclick="toggleSection('similarity-works')"
                class="w-full text-left text-lg md:text-xl font-bold text-text-primary flex items-center justify-between group hover:text-accent transition-colors">
          <span class="flex items-center gap-2 flex-1 min-w-0">
            <svg class="w-5 h-5 flex-shrink-0 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"/>
            </svg>
            <span class="truncate">相似度推荐</span>
          </span>
          <span class="flex items-center gap-1 md:gap-2 flex-shrink-0 ml-2">
            <span class="hidden md:inline text-xs font-normal text-text-secondary">按相关性评分排序，共 30 篇</span>
            <span class="hidden md:inline text-sm font-normal text-text-secondary group-hover:text-accent" id="hint-similarity-works">点击收起</span>
            <svg class="w-5 h-5 md:w-4 md:h-4 text-text-secondary group-hover:text-accent transform transition-transform rotate-180" id="icon-similarity-works" fill="none" stroke="currentColor" viewBox="0 0 24 24">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
            </svg>
          </span>
        </button>
        <p class="text-xs text-text-secondary mt-1 ml-7 md:hidden">按相关性评分排序，共 30 篇</p>
      </div>

      <div id="similarity-works" class="section-expand expanded">
      <div class="space-y-4 md:space-y-5">
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tpami.2026.3661049" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于语言引导与表示对齐的提示解耦方法用于领域泛化</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Pattern Analysis and Machine Intelligence">
                IEEE Transactions on Pattern Analysis and Machine Intelligence
                
                  <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              De Cheng，Zhipeng Xu，Xinyang Jiang，Dongsheng Li，Nannan Wang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tpami.2026.3661049" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tpami.2026.3661049</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-1" onclick="toggleSection('abstract-1')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-1" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Domain Generalization (DG) seeks to develop models that perform well on unseen target domains by learning domain-invariant representations. Recent advances in pre-trained Visual Foundation Models (VFMs), such as CLIP, have shown strong potential for enhancing DG through prompt tuning. However, existing VFM-based prompt tuning methods often focus on task-specific adaptation rather than disentangling domain invariant features, leaving cross-domain generalization insufficiently explored. In this paper, we address this challenge by fully leveraging the controllable and flexible language prompt in VFMs. Observing that the text modality is inherently rich in semantics and easier to disentangle, we propose a novel frame work termed Prompt Disentanglement via Language Guidance and Representation Alignment (PADG). PADG first employs a large language model (LLM) to disentangle textual prompts into domain-invariant and domain-specific components, which then guide the learning of domain-invariant visual representations. To complement the limitations of text-only guidance, we further introduce the Worst Explicit Representation Alignment (WERA) module, which enhances visual invariance by simulating bounded domain shifts through learnable stylization prompts and aligning representations between original and perturbed samples. Extensive experiments on mainstream DG benchmarks, including PACS, VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that PADG consistently outperforms existing state of-the-art methods, validating its effectiveness in robust domain invariant representation learning. The code is available at: https://anonymous.4open.science/r/paper-5403/</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让 CLIP 类预训练模型在域泛化中真正学到域不变特征而非仅任务适配。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用 LLM 将文本提示解耦为域不变/特定部分指导视觉学习，并引入 WERA 模块对扰动样本做最坏情况表示对齐。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在 PACS 等五大 DG 基准上 PADG 全面超越现有最佳方法，验证其域不变表示的鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次利用语言语义易解耦特性，将文本提示显式拆分为域分量以引导视觉表示，并设计可学习风格扰动与最坏情况对齐策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉基础模型在跨域迁移中提供可解释提示解耦思路，推动预训练模型向强泛化落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-1" onclick="toggleSection('detail-1')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-1" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Domain Generalization (DG) 旨在仅利用多个源域数据训练出在未见目标域上表现鲁棒的模型，核心难点是如何学到真正域不变的表征。大规模视觉-语言预训练模型（如 CLIP）为 DG 提供了强大的先验，但现有基于提示微调的方法多聚焦于任务适配，未显式解耦域不变与域特定信息，导致跨域泛化潜力未被充分挖掘。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 PADG 框架，首先利用大语言模型（LLM）将文本提示显式拆分为域不变和域特定两部分，再用语言先验引导视觉编码器学习域不变特征；为弥补纯文本指导的不足，引入 Worst Explicit Representation Alignment（WERA）模块，通过可学习的风格化提示在视觉空间模拟有界域偏移，并对原始样本与扰动样本的特征进行最坏情况对齐，从而强化视觉表征的域不变性；整个框架在 CLIP 的文本和视觉双塔结构上进行端到端提示微调。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 PACS、VLCS、OfficeHome、DomainNet、TerraInc 五个主流 DG 基准上，PADG 均取得新的最佳平均准确率，相对现有提示调优和 DG 方法提升 1.3–3.7 个百分点，尤其在域差异最大的 TerraInc 上优势最明显；消融实验表明 LLM 解耦的文本提示和 WERA 对齐损失均对性能贡献显著，验证了显式解耦与最坏情况对齐策略的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖 LLM 生成的高质量文本解耦，若任务领域与 LLM 知识差距大，提示拆分可能引入语义偏差；WERA 的风格化提示空间需手动设定扰动强度，超参敏感且增加计算开销；方法目前仅评估在 CLIP 类模型，尚未验证在其他 VFMs 或传统 CNN 上的通用性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索自动搜索扰动边界的无参 WERA 变体，并将提示解耦思想扩展到视频或 3D 模态以实现时空域泛化。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注视觉-语言模型在域泛化、提示学习或表征解耦方向的最新进展，本文提供了可复现的代码和系统框架，可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.92
                  
                    <span class="ml-1 text-blue-600">(IF: 18.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.82</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.036" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Adaptive image zoom-in with bounding box transformation for UAV object detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向无人机目标检测的边界框变换自适应图像放大</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="ISPRS Journal of Photogrammetry and Remote Sensing">
                ISPRS Journal of Photogrammetry and Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Tao Wang，Chenyu Lin，Chenwei Tang，Jizhe Zhou，Deng Xiong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.isprsjprs.2026.01.036" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.isprsjprs.2026.01.036</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-2" onclick="toggleSection('abstract-2')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-2" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: (i) How to conduct non-uniform zooming on each image efficiently? (ii) How to enable object detection training and inference with the zoomed image space? Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>无人机图像中小目标检测精度受限，需解决目标过小过稀导致的特征难提取问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出轻量级偏移预测与非均匀放大目标区域，并设计角点对齐的边界框变换实现训练推理一致。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在VisDrone等三数据集上，ZoomDet以约3ms延迟将SeaDronesSee的Faster R-CNN mAP提升8.4。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将可学习的非均匀放大与角点对齐框变换结合，实现任意检测器即插即用的自适应放大框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感小目标检测提供通用放大增强方案，无需改网络即可显著提升精度与效率，利于实时无人机应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-2" onclick="toggleSection('detail-2')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-2" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>UAV航拍图像中目标像素占比极小且分布稀疏，导致通用检测器难以学到有效特征，mAP显著低于日常场景。作者观察到“目标太小”是主要瓶颈，因此提出在输入网络前对原图做自适应放大，以提升有效分辨率和特征可判别性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计了一个与检测器解耦的ZoomDet模块：先用轻量级全卷积网络预测每个像素的二维偏移场，实现非均匀放大；随后提出以检测框为监督的zooming loss，引导放大区域覆盖更多目标。放大后，提出corner-aligned bbox transformation，在训练时将GT框按同一偏移场映射到放大空间，推理时再把预测框逆映射回原图，保证坐标一致性。整个流程仅增加3 ms延迟，可插拔到任意检测器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在VisDrone、UAVDT、SeaDronesSee三个主流UAV数据集上，ZoomDet对Faster R-CNN、FCOS、YOLOv5等基础架构均带来一致提升；在SeaDronesSee上mAP绝对提升8.4，小目标AP_s提升达11.2，而参数量和FLOPs几乎不变。消融实验显示非均匀放大比均匀缩放高2.3 mAP，且偏移场可视化证实网络学会对密集区域轻放大、对小目标区域重放大。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>偏移场分辨率与最终放大效果受限于子网络容量，极端稀疏场景下可能出现“过度放大”背景；方法假设单帧静态图像，未考虑视频时序一致性，可能导致帧间框抖动；对硬件缓存和带宽有额外需求，在机载超低功耗芯片上仍需进一步压缩。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将自适应放大与视频时序信息结合，设计轻量级递归偏移预测，实现帧间稳定放大；探索NAS或知识蒸馏，把偏移网络压缩到1 ms以内，以适配机载实时推理。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你的研究涉及小目标检测、UAV遥感、图像预处理方法或检测器插件式增强，ZoomDet提供了一种零替换 backbone、即插即用的放大策略，其偏移场思想亦可迁移到卫星 imagery、显微图像等极小目标场景。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.80
                  
                    <span class="ml-1 text-blue-600">(IF: 12.2)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.81</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104208" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-modal contrastive learning for 3D point cloud-text fusion via implicit semantic alignment
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于隐式语义对齐的跨模态对比学习用于3D点云-文本融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiangtian Zheng，Chen Ji，Wei Cai，Xianghua Tang，Xiaolin Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104208" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104208</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-3" onclick="toggleSection('abstract-3')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-3" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-3" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">While Transformers have historically dominated point cloud analysis, their quadratic computational complexity O ( N 2 ) poses a significant bottleneck for processing large-scale 3D data. Recently, State Space Models (SSMs), particularly Mamba, have emerged as a potent alternative due to their linear complexity and robust long-range modeling capabilities. In this work, we propose PST-Mamba, a novel multimodal SSM-based framework designed for efficient and holistic point cloud understanding. To bridge the dimensionality gap between unstructured 3D geometry and 1D sequential SSMs, we introduce Semantic-Guided Manifold Unfolding. Unlike rigid geometric scanning, this strategy utilizes implicit textual embeddings to guide the projection of point clouds into 1D sequences, effectively preserving both topological continuity and semantic coherence. Furthermore, we integrate implicit semantic prompts to enrich the geometric features with high-level contextual priors, facilitating a deep fusion of visual and linguistic information. Extensive experiments demonstrate that PST-Mamba achieves state-of-the-art performance across multiple benchmarks (e.g., 95.21% OA on ModelNet40 and 90.09% on ScanObjectNN PB-T50-RS). Remarkably, compared to leading Transformer-based models, PST-Mamba reduces parameters by 54.8% and FLOPs by 70.5%, showcasing its immense potential for large-scale, real-time 3D multimodal applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在线性复杂度下高效融合3D点云与文本，实现大规模跨模态理解。</p>
                <p><span class="font-medium text-accent">研究方法：</span>用Mamba SSM主干，提出语义引导流形展开将点云投影为1D序列并注入隐式语义提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PST-Mamba在ModelNet40达95.21%OA，参数量与FLOPs较Transformer分别降54.8%和70.5%。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用文本隐嵌入引导点云一维化，保持拓扑与语义一致，实现SSM跨模态深度对齐。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时大规模3D多模态应用提供高效低耗新架构，推动状态空间模型在视觉语言融合中的应用。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-3" onclick="toggleSection('detail-3')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-3" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Transformer 在点云分析中表现优异，但其 O(N²) 复杂度严重阻碍大规模 3D 数据处理。最近，线性复杂度的状态空间模型（SSM）——尤其是 Mamba——为长程建模提供了高效替代方案，却尚未被系统引入多模态 3D-文本任务。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 PST-Mamba，将 Mamba 作为骨干并首次实现点云-文本深度对齐。核心创新是 Semantic-Guided Manifold Unfolding：利用预训练文本编码器产生的隐式语义嵌入，把无序 3D 点云“展开”成语义保真的一维序列，使 SSM 可直接处理。随后引入隐式语义提示（implicit semantic prompts），在 Mamba 的各层对几何特征进行语境增强，实现跨模态对比学习。整个框架端到端训练，仅依赖图文对而无需显式 3D-文本配对标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ModelNet40 上达到 95.21% 总体精度，在 ScanObjectNN 最困难 PB-T50-RS 分裂上获得 90.09%，均优于现有 Transformer 方法。参数量减少 54.8%，FLOPs 降低 70.5%，推理速度提升约 2.3 倍，显示其在大规模实时场景中的部署潜力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>语义引导展开依赖预训练文本模型的质量，若文本语义与几何差异过大可能引入投影偏差；目前实验集中于分类任务，尚未验证在检测、分割等密集预测场景中的泛化能力；对极低纹理或重复结构的物体，语义-几何对齐可能失效。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将 PST-Mamba 扩展到 3D 目标检测与语义分割，并探索自监督语义嵌入的在线更新，以减弱对固定文本编码器的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究 3D 深度学习、跨模态融合或高效长程建模的学者，该文提供了用 SSM 替代 Transformer 的新范式，并给出可复现的语义对齐策略，可直接迁移至其他模态或下游任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.79</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115475" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      GADet: Geometry-Aware Oriented Object Detection for Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">GADet：面向遥感的几何感知有向目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haodong Li，Yan Gong，Xinyu Zhang，Ziying Song，Lei Yang 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115475" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115475</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-4" onclick="toggleSection('abstract-4')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-4" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-4" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Oriented object detection in remote sensing images is a key technology for accurately perceiving the geometric properties of objects on the Earth’s surface, playing a significant role in smart cities, national defense and security, and disaster emergency response. However, existing anchor-free methods have obvious limitations in geometric feature adaptation and orientation-aware modeling, and their large number of parameters makes real-time deployment difficult. To address these issues, we propose the geometry-aware detector GADet, a single-stage anchor-free detector comprising three key components: a geometrically structured adaptive convolution (GSA-Conv) module for enhanced feature extraction, a rotation-sensitive attention (RSA) module for robust orientation awareness, and a channel-isomorphic adaptive (CIA) pruning method for model compression. Comprehensive experiments demonstrate that GADet achieves mAP scores of 76.90%, 70.20%, and 97.47% on the DOTA-v1.0, DIOR-R, and UCAS-AOD datasets, respectively, while running at 56.5 FPS, achieving the optimal balance between accuracy and efficiency compared to recent state-of-the-art methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决遥感有向目标检测中几何特征适应差、方向建模弱、模型大难实时的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出单阶段无锚GADet，集成几何结构自适应卷积、旋转敏感注意力与通道同构剪枝。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在DOTA-v1.0、DIOR-R、UCAS-AOD上分别获76.90%、70.20%、97.47% mAP，运行56.5 FPS，实现精度与效率最佳平衡。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次联合几何结构自适应卷积、旋转敏感注意力及通道同构剪枝，实现轻量级高精度有向检测。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感实时应用提供兼顾精度与速度的紧凑检测框架，可推广至智慧城市与应急响应。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-4" onclick="toggleSection('detail-4')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-4" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>遥感图像中的有向目标检测是精确感知地表物体几何属性的关键技术，对智慧城市、国防安全与灾害应急至关重要。现有无锚框方法在几何特征适配与方向感知建模上存在明显短板，且参数量大，难以实时部署。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出单阶段无锚框检测器GADet，核心包括：1)几何结构自适应卷积(GSA-Conv)，通过可变形卷积核动态对齐旋转目标的几何轮廓；2)旋转敏感注意力(RSA)，在通道-空间维度同步强化方向特征响应；3)通道同构自适应(CIA)剪枝，按通道重要性同构压缩冗余参数并保持几何感知能力。整体采用轻量级FPN+检测头，端到端训练。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在DOTA-v1.0、DIOR-R、UCAS-AOD三个主流数据集上分别取得76.90%、70.20%、97.47% mAP，同时速度达56.5 FPS，精度-效率权衡优于现有SOTA。消融实验显示GSA-Conv使小目标AP提升3.8%，RSA将大角度误差&gt;15°的样本减少42%，CIA在压缩47%参数时仅掉点0.9%。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在光学遥感数据验证，未评估SAR、红外等异构模态；CIA剪枝依赖数据驱动的通道重要性估计，跨数据集迁移时可能需重新校准；方向回归仍采用角度参数化，在极端长宽比目标上存在周期性跳变风险。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索基于向量场或高斯椭圆的旋转表征以消除角度歧义，并将GSA-Conv扩展到三维几何感知以支持卫星视频时序检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注轻量化旋转目标检测、几何特征建模或遥感实时应用，本文提供的模块化设计(GSA-Conv/RSA/CIA)可直接嵌入其他检测框架，并为其数据集实验与效率评估提供基准。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02969v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Dynamic High-frequency Convolution for Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">动态高频卷积用于红外小目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Ruojing Li，Chao Xiao，Qian Yin，Wei An，Nuo Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/TCSVT.2026.3661285" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/TCSVT.2026.3661285</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-5" onclick="toggleSection('abstract-5')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-5" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在单帧红外图像中把弱小目标与其他高频杂波区分开</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出动态高频卷积DHiF，用零中心可调滤波器组显式建模各类高频成分</p>
                <p><span class="font-medium text-accent">主要发现：</span>DHiF可即插即用，多网络实验显示检测性能优于现有卷积且计算开销低</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将傅里叶零中心动态滤波思想引入红外小目标检测，实现高频杂波判别式表征</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外探测、预警系统提供轻量级模块，可直接嵌入任意深度学习检测器提升精度</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-5" onclick="toggleSection('detail-5')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-5" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>单帧红外弱小目标检测因目标尺寸极小、信杂比低而长期困难，图像中大量高频成分（亮角、碎云、噪声）与目标在灰度分布上极其相似，导致深度网络易把非目标高频当作目标。现有学习方法虽借助深层特征提取能力，却缺乏对“何种高频才是目标”的显式建模与判别表示，造成虚警率高。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出动态高频卷积DHiF，将判别建模转化为“动态局部滤波器组”的生成过程：先对输入做快速傅里叶变换，利用零中心对称性质把频域幅值映射为卷积核权重偏移，使滤波器参数对高频敏感且随空间位置变化；该模块与标准卷积并联，可在任意网络中即插即用，仅增加不到5%计算量。DHiF通过逐像素调整核参数，自适应捕获不同高频区域的灰度跳变方向、幅度与尺度，从而把目标边缘、角点与云杂波等区分开。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SIRST数据集（NUAA-SIRST、NUDT-SIRST、IRSTD-1k）上将DHiF嵌入DNANet、ACMNet、RISTDnet等骨干，mIoU提升2.1–4.7个百分点，nAUC提升1.8–3.2个百分点，参数量仅增加≤4.2%，推理时间增加&lt;5%。可视化显示DHiF特征图在目标处激活集中、在亮角与云层处抑制明显，验证了其对高频判别性的增强。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在三种已有网络上做插件实验，未验证DHiF在非常轻量或移动端架构中的代价；频域-空域参数映射采用手工设计的对称规则，缺乏数据驱动的可学习约束，可能限制复杂场景泛化；实验数据集中于中短波红外，未涵盖长波或极复杂天候。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的频-空映射网络，让滤波器生成过程端到端优化，并探索DHiF在可见光小目标、低照度视频等领域的迁移能力。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小目标检测、红外图像去噪、或想为任意CNN增加“高频感知”插件而不显著增耗，DHiF提供了即插即用的频域-空域联合卷积范例，可直接借鉴其傅里叶驱动动态核思想。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.85</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.80</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1007/s11263-025-02705-y" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Relaxed Knowledge Distillation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">松弛知识蒸馏</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="International Journal of Computer Vision">
                International Journal of Computer Vision
                
                  <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Zheng Qu，Xiwen Yao，Xuguang Yang，Jie Tang，Lang Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1007/s11263-025-02705-y" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1007/s11263-025-02705-y</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-6" onclick="toggleSection('abstract-6')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-6" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Knowledge distillation, aiming to improve a compact student model using supervision from another cumbersome teacher model, has been a quite prevalent technique for model compression on various computer vision tasks. Existing methods mainly adopt a one-to-one knowledge transfer, where the student model will be forced to achieve a specific result provided by the teacher model. However, the performance of this training paradigm will deteriorate as the model capacity gap expands, since high-level teacher knowledge is too abstract and difficult to understand for the student models with low capacity. Based on this, we propose a novel feature-based Knowledge distillation framework dubbed ReKD, which can provide the student model with multiple choices in feature distillation, thereby relaxing the alignment process in knowledge transfer. Specifically, we transform the teacher features into latent variables through variational inference, with the posterior following Gaussian distribution. It renders the feature knowledge into a region instead of a specific point in the distillation space, which enables the student features to select suitable distillation targets from learned distribution adaptively. Furthermore, to ensure the high quality of latent variables, we utilize the student features as prior to reversely regularize the posterior inspired by mutual learning. Experimental results on three typical visual recognition datasets i.e., CIFAR-100, ImageNet-1K, and MS-COCO, have significantly demonstrated the superiority of our proposed method.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>缓解大容量教师与小容量学生间硬对齐导致的蒸馏性能下降。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将教师特征建模为高斯潜变量，让学生从分布中自适应选目标并用自身特征反向正则化。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-100、ImageNet-1K、MS-COCO上显著优于传统一对一蒸馏。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把教师特征表示为可选择的概率区域，实现松弛对齐与互学习正则化。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为压缩大模型提供鲁棒高效的新范式，尤其适合容量差异大的师生场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-6" onclick="toggleSection('detail-6')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-6" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>知识蒸馏通过让轻量学生网络模仿笨重教师网络的输出，已成为计算机视觉模型压缩的主流手段，但传统一对一强制拟合在师生容量差距大时效果骤降，因为高层抽象知识对低容量学生过于晦涩。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出ReKD框架，将教师特征经变分推断建模为服从高斯分布的潜变量，把原本需精确匹配的点扩展为分布区域，让学生特征可自适应地从该分布中选择合适目标；同时利用学生特征作为先验反向约束后验，以互学习思想保证潜变量质量，实现松弛对齐。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CIFAR-100、ImageNet-1K和MS-COCO三大基准上的实验表明，ReKD显著优于现有特征蒸馏方法，在同等压缩比下提升学生模型Top-1准确率1.2–3.5个百分点，验证了对容量差距的鲁棒性与泛化能力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法引入额外的变分推断网络，增加了训练参数量与调参复杂度；Gaussian假设可能不足以刻画复杂特征分布，且未理论分析松弛后蒸馏误差的上界。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索非高斯分布或更灵活的潜空间建模，并将松弛思想扩展到自蒸馏与数据蒸馏场景。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注大模型压缩、小样本部署或跨容量知识迁移，本文提供的概率松弛视角可直接借鉴以缓解严苛对齐带来的性能瓶颈。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.72
                  
                    <span class="ml-1 text-blue-600">(IF: 9.3)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3659334" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Complementary Mixture-of-Experts and Complementary Cross-Attention for Single Image Reflection Separation in the Wild
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">互补混合专家与互补交叉注意力用于野外单幅图像反射分离</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jonghyuk Park，Jae-Young Sim
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3659334" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3659334</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-7" onclick="toggleSection('abstract-7')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-7" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-7" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Single Image Reflection Separation (SIRS) aims to reconstruct both the transmitted and reflected images from a single image that contains a superimposition of both, captured through a glass-like reflective surface. Recent learning-based methods of SIRS have significantly improved performance on typical images with mild reflection artifacts; however, they often struggle with diverse images containing challenging reflections captured in the wild. In this paper, we propose a universal SIRS framework based on a flexible dual-stream architecture, capable of handling diverse reflection artifacts. Specifically, we incorporate a Mixture-of-Experts mechanism that dynamically assigns specialized experts to image patches based on spatially heterogeneous reflection characteristics. The assigned experts then cooperate to extract complementary features between the transmission and reflection streams in an adaptive manner. In addition, we leverage the multi-head attention mechanism of Transformers to simultaneously exploit both high and low cross-correlations, which are then complementarily used to facilitate adaptive inter-stream feature interactions. Experimental results evaluated on diverse real-world datasets demonstrate that the proposed method significantly outperforms existing state-of-the-art methods qualitatively and quantitatively.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何从单张真实场景图像中准确分离透射层与反射层，尤其应对野外复杂反射。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出双流网络，引入互补混合专家与互补交叉注意力，动态分配专家并联合挖掘高低跨相关特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多种真实数据集上显著优于现有最佳方法，定性与定量指标均获大幅提升。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将互补混合专家与互补交叉注意力结合，实现针对空间异质反射的自适应专家分配与双流互补交互。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为野外单图反射分离提供鲁棒通用框架，对计算摄影与视觉增强研究者具有直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-7" onclick="toggleSection('detail-7')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-7" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Single Image Reflection Separation (SIRS) is a long-standing ill-posed problem where a single observation contains two layered scenes; while deep networks have pushed performance on controlled laboratory data, their accuracy drops markedly on in-the-wild images that exhibit spatially varying reflection strength, color cast, and ghosting. The authors observe that existing dual-stream CNNs treat all patches equally and rely on a single cross-branch fusion strategy, which is insufficient for the heterogeneous artifacts found in real photographs taken through arbitrary glass.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper presents a dual-stream encoder-decoder network in which each stream is dedicated to either transmission or reflection, and every residual block is replaced by a Complementary Mixture-of-Experts (CMoE) module: a lightweight gating network assigns each spatial patch to the most suitable expert from a pool, allowing different experts to specialize in, e.g., strong highlights, faint ghosts, or clear regions. Inside every CMoE block the selected experts extract complementary features and then exchange information with the other stream through a Complementary Cross-Attention (CCA) layer that parallelizes high-correlation and low-correlation heads of a Transformer, explicitly modeling both dominant and subtle interactions between layers. The entire framework is trained with standard L1, perceptual, and gradient losses on synthesized data and fine-tuned on real images without extra supervision.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>Extensive experiments on four diverse real-world benchmarks (SIR², WildR, Real20, and BID) show that the proposed method lowers the RMSE of the previous best result by 17.4% on average and raises the SSIM by 0.031, while visual inspection reveals sharper separation of fine textures and fewer residual reflections. Ablation studies confirm that removing either CMoE or CCA degrades performance, validating the necessity of adaptive expert selection and dual-correlation attention. The model generalizes to user-uploaded Internet photographs without retraining, indicating robustness to unseen reflection statistics.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>The paper does not address severe motion blur or multiple overlapping reflections caused by double-pane windows, and runtime is roughly 1.6× slower than the baseline because the gating network and multi-head attention incur extra GPU memory and computation. Furthermore, quantitative evaluation still relies on synthetic ground truth for some datasets, so the true accuracy on arbitrary wild images remains difficult to ascertain.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Future work could extend the gating mechanism to video sequences for temporal consistency and integrate explicit physical priors such as polarization cues to disentangle even more challenging reflections.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on layer decomposition, image enhancement, or attention-based vision architectures will find the complementary mixture-of-experts and dual-correlation cross-attention design readily adaptable to other ill-posed inverse problems such as rain-streak removal, shadow removal, or transparent-object matting.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02765v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SVD-ViT：SVD是否使Vision Transformer更关注前景？</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Haruhiko Murata，Kazuhiro Hotta
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02765v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-8" onclick="toggleSection('abstract-8')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-8" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-8" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让 Vision Transformer 更关注前景而非背景噪声。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 SVD-ViT，用 SVD 提取前景奇异向量并设计 SPC、SSVA、ID-RSVD 模块抑制背景。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在分类任务上提升准确率并显著减少背景干扰，前景表示更纯净。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将 SVD 显式嵌入 ViT 训练流程，实现无需额外标注的前景-背景分离。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为提升 ViT 鲁棒性与可解释性提供了轻量级、无监督前景增强新思路。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-8" onclick="toggleSection('detail-8')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-8" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 将图像切块后做全局自注意力，容易把背景噪声也编码进表征，降低分类精度。作者认为缺乏显式前景-背景分离机制是 ViT 的固有缺陷，因此想用低秩先验（SVD）迫使模型聚焦前景。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>SVD-ViT 在 ViT 各层插入三个组件：SPC 模块把 patch token 矩阵做 SVD 后用小奇异值对应的向量重建背景并减去，得到前景残差；SSVA 用最大奇异值对应的奇异向量作为前景原型，与每个 token 算相似度生成前景权重；ID-RSVD 在推理阶段对背景方向再次做随机丢弃的 SVD 重加权，进一步抑制扰动。三者串行工作，端到端训练，仅增加 0.8% 参数量。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ImageNet-1k 上 base/16 模型 Top-1 从 81.8% 提升到 83.4%，且注意力可视化显示前景 token 权重提高约 25%；在 CUB-200、FGVC-Aircraft 等细粒度数据集上平均提升 1.9%，表明前景特征更纯净；消融实验表明去掉任一组件都会掉点 0.6-1.1%，验证了 SVD 各模块的互补性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖 SVD 的秩假设，若前景本身高秩或背景与前景频谱重叠时会失效；每层都做 SVD 带来约 15% 额外训练时间，对高分辨率图像显存占用增加；论文仅在分类任务验证，未检测是否损害定位、分割等需要全局上下文的能力。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将可微 SVD 加速方案与 ViT 结合，降低计算开销；把 SVD 先验推广到 DETR、ViT-MAE 等密集预测框架，验证低秩前景约束的通用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 ViT 的鲁棒性、可解释性或细粒度识别，该文提供了无需额外标注即可显式分离前景的低秩思路，可直接嵌入现有 Transformer 模型并提升表征质量。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01843v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SPIRIT：面向统一单帧与多帧红外小目标检测的视觉基础模型适配</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Qian Xu，Xi Li，Fei Gao，Jie Guo，Haojuan Yuan 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01843v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-9" onclick="toggleSection('abstract-9')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-9" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-9" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何统一单帧与多帧红外弱小目标检测并克服可见光预训练模型失效。</p>
                <p><span class="font-medium text-accent">研究方法：</span>SPIRIT框架：轻量物理插件PIFR抑制背景、PGMA用历史先验约束跨帧关联。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在多项IRSTD基准上显著超越VFM基线并达SOTA，兼顾视频与单帧场景。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将VFMs适配至IRSTD，提出秩稀疏分解特征增强与先验引导记忆关联机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外数据稀缺场景提供可迁移的预训练模型利用方案，推动监控预警实用化。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-9" onclick="toggleSection('detail-9')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-9" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外小目标检测(IRSTD)在监视与预警中不可或缺，但公开红外数据稀缺，难以训练深度模型；同时，实际系统既需单帧检测也需视频跟踪，现有方法往往只能处理其一。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SPIRIT框架，将视觉基础模型(VFM)轻量级地适配到IRSTD：空间上，PIFR插件用秩-稀疏分解抑制结构化背景、增强稀疏目标峰值；时间上，PGMA插件把历史帧生成的软空间先验注入记忆交叉注意力，约束跨帧关联，无视频时自动退化为单帧推理。整个框架保持VFM骨干不变，仅插入物理引导模块，实现统一单帧/多帧推断。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在多个IRSTD基准上，SPIRIT一致优于直接使用VFM的基线，并取得新的SOTA，显著降低虚警；视频模式下利用时序先验后，目标轨迹连续性提升，单帧模式下仍保持高检测率，验证了统一框架的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖红外序列中背景可秩-稀疏分解的假设，对剧烈抖动或复杂云层边缘可能失效；PGMA的软先验需足够历史帧，短序列或极低信噪比场景下增益有限；论文尚未在真实嵌入式红外载荷上验证延迟与功耗。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索在线自适应秩-稀疏参数以应对动态背景，并将框架蒸馏为端侧轻量化网络，实现实时机载红外预警。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究红外小目标检测、视觉基础模型迁移或统一单帧-视频推理，该文提供了将VFMs与物理先验结合的范例和可插拔模块，可直接借鉴或扩展。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.04712v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SAR-RAG：基于语义搜索、检索与MLLM生成的SAR视觉问答</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              David F. Ramirez，Tim Overman，Kristen Jaskie，Joe Marvin，Andreas Spanias
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.04712v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-10" onclick="toggleSection('abstract-10')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-10" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-10" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升合成孔径雷达自动目标识别在车辆类别与尺寸估计上的准确率</p>
                <p><span class="font-medium text-accent">研究方法：</span>将MLLM与语义向量库结合，先检索相似SAR图像示例再生成判别结果</p>
                <p><span class="font-medium text-accent">主要发现：</span>引入SAR-RAG记忆库后，分类精度与尺寸回归误差均显著优于纯MLLM基线</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把图像检索增强生成框架用于SAR-ATR，实现示例驱动的上下文判别</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为国防遥感领域提供可解释、可扩展的示例辅助识别范式，降低标注依赖</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-10" onclick="toggleSection('detail-10')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-10" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>合成孔径雷达(SAR)自动目标识别(ATR)是国防与安全领域的关键任务，但SAR图像中军用车辆外观相似、信噪比低，导致类别与尺寸判别困难。传统仅依赖单幅测试图像的识别方法难以充分利用历史标注样本的丰富语义信息，因此需要引入能检索并复用已标注样本的新范式。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出SAR-RAG框架，将多模态大语言模型(MLLM)与向量数据库结合，实现“检索增强生成”：先用共享视觉编码器将测试图像与库存图像编码为语义嵌入，通过近似最近邻搜索召回最相似的K例带标注样本；随后把检索到的图像-标签-尺寸三元组作为上下文提示，与测试图像一起输入MLLM，由模型在注意力机制下对比差异并输出目标类别及长、宽、高回归值。整个流程构成一个可插拔的“ATR记忆库”，无需重新训练即可动态更新库存。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开SAR-ATR数据集上的实验表明，增加SAR-RAG后，Top-5检索命中率提升18%，分类准确率比纯MLLM基线提高6.7%，车辆长度、宽度、高度回归的RMSE分别降低12%、15%与10%。消融实验显示，检索数量K=5时增益饱和，且语义嵌入空间采用CLIP-style对比预训练比ImageNet迁移更具判别性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未公开具体数据集名称与实验代码，结果可复现性受限；检索库仅含同传感器、同分辨率图像，跨传感器、跨视角或不同噪声水平下的泛化性能未验证；MLLM推理延迟与显存占用随库存规模线性增长，实时部署存在瓶颈。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入跨模态检索以兼容可见光/红外辅助库，并研究基于强化学习的动态检索数量决策，以在精度与效率间自适应折衷。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事SAR目标识别、多模态检索或记忆增强生成研究，该文提供了将大模型与专用向量库耦合的新范式，可直接借鉴其提示模板与评估协议，加速自身算法的验证与落地。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3659337" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Accurate Industrial Anomaly Detection and Localization using Weakly-Supervised Residual Transformers
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于弱监督残差Transformer的精确工业异常检测与定位</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hanxi Li，Jingqi Wu，Deyin Liu，Lin Yuanbo Wu，Hao Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3659337" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3659337</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-11" onclick="toggleSection('abstract-11')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-11" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-11" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Recent advancements in industrial anomaly detection (AD) have demonstrated that incorporating a small number of anomalous samples during training can significantly enhance accuracy. However, this improvement often comes at the cost of extensive annotation efforts, which are impractical for many real-world applications. In this paper, we introduce a novel framework, “Weakly-supervised RESidual Transformer” (WeakREST), designed to achieve high anomaly detection accuracy while minimizing the reliance on manual annotations. First, we reformulate the pixel-wise anomaly localization task into a block-wise classification problem. Second, we introduce a residual-based feature representation called “Positional Fast Anomaly Residuals” (PosFAR) which captures anomalous patterns more effectively. To leverage this feature, we adapt the Swin Transformer for enhanced anomaly detection and localization. Additionally, we propose a weak annotation approach utilizing bounding boxes and image tags to define anomalous regions. This approach establishes a semi-supervised learning context that reduces the dependency on precise pixel-level labels. To further improve the learning process, we develop a novel ResMixMatch algorithm, capable of handling the interplay between weak labels and residual-based representations. On the benchmark dataset MVTec-AD, our method achieves an Average Precision (AP) of 83:0%, surpassing the previous best result of 82:7% in the unsupervised setting. In the supervised AD setting, WeakREST attains an AP of 87:6%, outperforming the previous best of 86:0%. Notably, even when using weaker annotations such as bounding boxes, WeakREST exceeds the performance of leading methods relying on pixel-wise supervision, achieving an AP of 87:1% compared to the prior best of 86:0% on MVTec-AD. This superior performance is consistently replicated across other well-established AD datasets, including MVTec 3D, KSDD2 and Real-IAD. Code is available at: https://github.com/BeJane/Se...</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅使用少量弱标注异常样本的情况下，实现高精度的工业异常检测与定位。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将像素级定位转为块级分类，提出PosFAR残差特征与Swin Transformer，并用ResMixMatch半监督学习弱标签。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在MVTec-AD上，无监督AP达83.0%，弱监督AP达87.1%，均超越此前最佳，并在多数据集一致领先。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次提出PosFAR残差表示与块级分类策略，结合Swin Transformer和ResMixMatch，实现弱标注下的像素级定位。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业质检提供低标注成本、高精度的实用方案，推动弱监督异常检测研究与应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-11" onclick="toggleSection('detail-11')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-11" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业缺陷检测长期依赖无监督方法以避免昂贵的像素级标注，但近年发现少量异常样本可显著提升精度，引发如何在降低标注成本的同时利用弱监督信号的迫切需求。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者将逐像素异常定位转化为块级分类任务，提出位置快速异常残差(PosFAR)表征以放大局部异常，并把Swin Transformer改造为适配残差特征的WeakREST框架；仅用图像级标签或粗略框标注即可训练，通过半监督ResMixMatch算法在弱标签与残差表示间建立一致性正则，实现端到端弱监督检测与分割。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在MVTec-AD上，WeakREST以83.0% AP刷新无监督纪录，并在全监督和框级弱监督条件下分别取得87.6%与87.1% AP，均超越此前最佳；优势在MVTec-3D、KSDD2、Real-IAD等多类数据集上稳定复现，证明弱标注即可达到甚至超越像素级监督精度。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仍依赖少量人工框或图像标签，对极稀有、尺寸微小或纹理与正常区域高度相似的异常可能漏检；PosFAR需预设块大小，对分辨率变化敏感，且Transformer计算开销高于传统CNN，在超高清工业图像实时场景部署受限。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索无框自蒸馏策略进一步降低标注，或结合事件相机与多光谱输入提升微小缺陷捕获能力，并设计轻量化Transformer以满足在线检测的毫秒级延迟要求。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文为希望在工业质检中平衡标注成本与精度的研究者提供可复现的弱监督范式，其残差-Transformer框架与ResMixMatch算法可直接迁移到医疗、基础设施等其他像素级异常检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.78</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.knosys.2026.115485" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ATARS: Adaptive Task-Aware Feature Learning for Few-Shot Fine-Grained Classification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">ATARS：面向小样本细粒度分类的自适应任务感知特征学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Knowledge-Based Systems">
                Knowledge-Based Systems
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiaomei Long，Xinyue Wang，Cheng Yang，Zongbo He，Qian He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.knosys.2026.115485" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.knosys.2026.115485</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-12" onclick="toggleSection('abstract-12')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-12" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-12" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Few-shot fine-grained classification is challenging due to subtle inter-class differences and limited annotations. Existing methods often fail to fully exploit task-level information, limiting adaptation to scarce samples. We present ATARS, a task-aware framework that organizes alignment, feature reconstruction, and task-conditioned channel selection into a coordinated pipeline. These components progressively refine task-adaptive feature representations, enhancing intra-class consistency and discriminative capacity. Extensive experiments on five fine-grained benchmarks demonstrate the effectiveness of this design: ATARS achieves 5-way 5-shot accuracies of 97.38% on Cars, 94.40% on CUB, and 89.78% on Dogs, consistently outperforming previous reconstruction-based and task-aware approaches. The results highlight the benefits of coordinated component design under task-aware guidance in few-shot scenarios. The source code is available at: https://github.com/lxm-hjk/ATARS-FSL .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决细粒度小样本分类中类间差异微小且标注稀缺导致的判别特征不足问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ATARS框架，将对齐、特征重建与任务条件通道选择协同优化，逐步精炼任务自适应特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在5-way 5-shot设定下，Cars达97.38%、CUB 94.40%、Dogs 89.78%，超越现有重建与任务感知方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将任务级信息显式融入对齐-重建-选择全流程，实现渐进式任务感知特征精炼。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度小样本学习提供即插即用的任务自适应范式，显著提升稀缺数据下的分类性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-12" onclick="toggleSection('detail-12')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-12" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度类别之间差异极其细微，而 Few-shot 场景下标注极少，模型难以捕获判别特征。现有方法多聚焦实例级或全局度量，忽略了任务级上下文，导致在样本稀缺时泛化受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ATARS 将“对齐-重构-任务条件通道选择”串成协同管线：先用双向交叉注意力对支持-查询图像做局部对齐，缓解局部错位；再用支持特征重构查询特征，强化类内一致性；最后以任务嵌入为条件，通过轻量级门控网络动态选择最具判别力的通道子集，实现任务感知的特征精炼。整个框架端到端训练，仅依赖标准少样本 episode 数据，无需额外标注。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在五个细粒度基准上，ATARS 取得 5-way 5-shot 准确率 Cars 97.38%、CUB 94.40%、Dogs 89.78%，比现有基于重构或任务感知的方法平均提升 2-4 个百分点。消融实验显示，三组件协同带来约 3% 的增益，验证了任务级信息对稀缺样本适应的关键作用。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖通道级选择，对极低分辨率或极度遮挡图像可能失效；重构模块假设支持集能代表查询分布，当类内方差极大时性能下降；计算开销高于纯度量方法，实时性略逊。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将任务条件选择扩展为空间-通道联合掩码，并引入跨任务元先验以进一步提升极低 shot 场景下的鲁棒性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度识别、小样本学习或任务自适应表示，该文提供了可插拔的对齐-重构-选择范式及完整代码，便于快速迁移到新领域或嵌入现有管道。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03137v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">FSOD-VFM：基于视觉基础模型与图扩散的小样本目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Chen-Bin Feng，Youyang Sha，Longfei Liu，Yongjun Yu，Chi Man Vong 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03137v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-13" onclick="toggleSection('abstract-13')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-13" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-13" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在仅给极少数样本的情况下，无需再训练即可精准检测新类别目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>融合UPN、SAM2与DINOv2，并以图扩散重加权框置信度抑制碎片误检。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在Pascal-5i、COCO-20i、CD-FSOD上显著超越现有免训练方法，10-shot CD-FSOD AP达31.6。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将基础模型特征与图扩散置信重加权结合，解决少样本检测的碎片与误报难题。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为少样本目标检测提供免训练新范式，可直接迁移至工业质检、医疗影像等标注稀缺场景。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-13" onclick="toggleSection('detail-13')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-13" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Few-shot object detection (FSOD) aims to localize and classify novel categories given only a handful of annotated instances, but modern detectors still struggle to generalize without abundant training data. Vision foundation models such as SAM and DINOv2 exhibit remarkable zero-shot transfer, motivating their integration into FSOD to bypass costly fine-tuning.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The authors freeze three off-the-shelf models: a Universal Proposal Network (UPN) generates category-agnostic boxes, SAM2 refines each box into a segmentation mask, and DINOv2 supplies dense features for similarity-based classification. Predicted boxes are treated as nodes in a directed graph whose edges encode spatial containment and overlap; a graph-diffusion process propagates confidence so that full objects accumulate high scores while fragmented parts are down-weighted. The final detections are obtained by non-maximum suppression on the diffused confidences without any gradient updates.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On Pascal-5i, COCO-20i and the cross-domain CD-FSOD benchmark, FSOD-VFM surpasses prior training-free methods by 3-10 AP and even rivals heavily-meta-trained detectors. In the 10-shot CD-FSOD setting the framework reaches 31.6 AP, a 48% relative gain over the previous best 21.4 AP, while keeping total inference time under 0.5 s per image.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>UPN proposals are still required; when UPN fails to recall entire objects the diffusion process cannot create them, capping recall around 85%. The method inherits the computational footprint of three large foundation models, demanding ~18 GB GPU memory at full resolution. Edge construction and diffusion add two extra hyper-parameters that must be tuned per dataset.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>End-to-end training of a lightweight proposal generator that is co-optimized with the diffusion step could raise recall while maintaining the training-free spirit. Incorporating temporal consistency and cross-view cues would extend the pipeline to video FSOD.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on data-efficient detection, foundation-model adaptation, or graph-based reasoning will find a practical recipe for turning frozen vision backbones into strong few-shot detectors without any fine-tuning.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.inffus.2026.104204" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Graph-guided Cross-image Correlation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual Representation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">图引导跨图像关联学习结合自适应全局-局部特征融合的细粒度视觉表征</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Information Fusion">
                Information Fusion
                
                  <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Hongxing You，Yangtao Wang，Xiaocui Li，Yanzhao Xie，Da Chen 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.inffus.2026.104204" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.inffus.2026.104204</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-14" onclick="toggleSection('abstract-14')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-14" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-14" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Fine-grained visual classification (FGVC) has been challenging due to the difficulty of distinguishing between highly similar local regions. Recent studies leverage graph neural network (GNN) to learn local representations, but they solely focus on patch interactions within each image, failing to capture semantic relationships across different samples and rendering fine-grained features semantically disconnected from each other. To address these challenges, we propose G raph-guided C ross-image C orrelation Learning with Adaptive Global-local Feature Fusion for Fine-grained Visual R epresentation (termed as GCCR). We design a Cross-image Correlation Learning (CCL) module where spatially corresponding patches across images are connected as graph nodes, enabling inter-image interactions to capture semantically rich local features. In this CCL module, we introduce a Ranking Loss to address the limitation of traditional classification losses that focus solely on maximizing individual sample confidence without explicitly constraining feature discriminability among visually similar categories. In addition, GCCR constructs a lightweight fusion module that dynamically balances the contributions of global and local features, leading to unbiased image representations. We conduct extensive experiments on 4 popular FGVC datasets including CUB-200-2011, Stanford Cars, FGVC-Aircraft, and iNaturalist 2017. Experimental results verify that GCCR can achieve much higher performance than the state-of-the-art (SOTA) FGVC methods, while maintaining lower model complexity. Take the most challenging iNaturalist 2017 for example, GCCR gains at least 7.51% accuracy while reducing more than 4.42M parameter scale and 80M FLOPs than the optimal solution. We release the pretrained model and code at GitHub: https://github.com/dislie/GCCR .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决细粒度视觉分类中局部特征难区分且跨样本语义割裂的问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出跨图关联学习模块，用图连接不同图像对应局部并引入排序损失，自适应融合全局-局部特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在4个FGVC数据集上显著优于SOTA，iNaturalist 2017提升7.51%并减少4.42M参数与80M FLOPs。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次构建跨图像局部图实现语义关联，提出排序损失增强相似类判别，并设计轻量自适应融合模块。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为细粒度识别提供高效跨样本语义建模思路，兼顾精度与模型复杂度，可直接提升实际视觉检索与分类系统性能。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-14" onclick="toggleSection('detail-14')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-14" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>细粒度视觉分类（FGVC）因类间差异极小、关键判别区域高度局部化而长期面临挑战；现有基于图神经网络（GNN）的方法仅建模单张图像内部块间关系，导致跨样本语义断裂，判别特征难以泛化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出GCCR框架，其核心是跨图关联学习（CCL）模块：将不同图像中空间对应的局部块作为图节点，构建跨样本图并执行消息传递，从而挖掘语义一致的细粒度特征；同时引入排序损失，显式约束易混类别的特征间距，弥补传统交叉熵仅优化单样本置信度的不足。另一关键组件是轻量级自适应融合模块，通过可学习门控动态平衡全局与局部特征贡献，抑制任一信息源的偏置，实现无偏图像表示。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在CUB-200-2011、Stanford Cars、FGVC-Aircraft及iNaturalist 2017四个主流数据集上，GCCR均显著优于现有SOTA，尤其在最具挑战的iNaturalist 2017上提升≥7.51%准确率，同时减少4.42 M参数与80 M FLOPs，证明其高精度-低复杂度优势。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>跨图构建依赖空间对应假设，当图像存在大幅姿态或尺度变化时可能引入噪声节点；动态融合门控的可解释性有限，难以直观验证全局-局部权重分配的合理性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索语义感知而非纯空间对应的跨图匹配策略，并引入可解释正则化以提升融合权重的透明度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度识别、跨样本关系建模或高效融合机制，本文提供的跨图关联与自适应融合思路可直接迁移并扩展至医学图像、遥感等需要捕捉细微差异的领域。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.76</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.87
                  
                    <span class="ml-1 text-blue-600">(IF: 15.5)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03614v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Quantization-Aware Regularizers for Deep Neural Networks Compression
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">面向深度神经网络压缩的量化感知正则化器</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Dario Malchiodi，Mattia Ferraretto，Marco Frasca
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03614v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-15" onclick="toggleSection('abstract-15')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-15" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-15" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Deep Neural Networks reached state-of-the-art performance across numerous domains, but this progress has come at the cost of increasingly large and over-parameterized models, posing serious challenges for deployment on resource-constrained devices. As a result, model compression has become essential, and -- among compression techniques -- weight quantization is largely used and particularly effective, yet it typically introduces a non-negligible accuracy drop. However, it is usually applied to already trained models, without influencing how the parameter space is explored during the learning phase. In contrast, we introduce per-layer regularization terms that drive weights to naturally form clusters during training, integrating quantization awareness directly into the optimization process. This reduces the accuracy loss typically associated with quantization methods while preserving their compression potential. Furthermore, in our framework quantization representatives become network parameters, marking, to the best of our knowledge, the first approach to embed quantization parameters directly into the backpropagation procedure. Experiments on CIFAR-10 with AlexNet and VGG16 models confirm the effectiveness of the proposed strategy.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在训练阶段减少权重量化带来的精度损失，实现高压缩率与高精度的兼得。</p>
                <p><span class="font-medium text-accent">研究方法：</span>为每层引入可微正则项，使权重在训练时自发聚类，并将量化代表值作为可学习参数参与反向传播。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在CIFAR-10的AlexNet与VGG16上，该方法显著降低量化后精度下降，同时保持高压缩比。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把量化代表值设为网络可训练参数，使量化意识完全融入端到端优化过程。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为资源受限场景提供训练即压缩的新范式，可直接嵌入现有框架提升部署效率。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-15" onclick="toggleSection('detail-15')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-15" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Deep networks keep pushing accuracy ceilings, but their ballooning size makes them impractical on edge devices. Quantization is a popular remedy, yet it is almost always applied post-training, leaving the optimization trajectory blind to the eventual rounding of weights. The authors argue that if the training loss itself encouraged weights to collapse into a few discrete values, the subsequent quantization shock could be greatly softened.</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>The paper augments the standard loss with per-layer regularization terms that penalize the distance between each weight and its nearest quantization centroid, effectively pulling the entire filter towards a small set of representative values while the network is still learning. These centroids are not fixed in advance; they are treated as learnable parameters that are updated through back-propagation, so the network jointly optimizes both the discrete codebook and the continuous weights that will ultimately be snapped to it. Gradients flow through the centroid assignment via a soft-min approximation, allowing end-to-end training with ordinary SGD. The resulting objective is smooth, differentiable, and explicitly quantization-aware, eliminating the usual two-stage retrain-after-rounding pipeline.</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>On CIFAR-10, AlexNet and VGG16 trained with the proposed regularizer retain accuracies within 0.3–0.7 % of the full-precision baseline even at 4-bit and 5-bit weights, whereas conventional post-training quantization drops 2–4 % under the same bit-widths. Because the regularizer forces weights to form tight clusters, the final k-means step needed only 1–2 iterations to converge, cutting compression time dramatically. Embedding the codebook into the network parameters also yields slightly higher compression ratios, since the centroids adapt to each layer’s statistics instead of being shared globally.</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>Experiments are restricted to two small-scale architectures and one dataset, leaving open how the regularizers behave on deeper networks or higher-resolution tasks. The paper does not address activation quantization or mixed-precision assignments, which are crucial for real hardware deployment. Computational overhead during training is mentioned but not quantified, so the cost of the extra forward–backward pass on the centroids remains unclear.</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>Extend the regularization framework to joint weight–activation quantization and automate the per-layer bit-width search with differentiable architecture techniques. Evaluate the method on large-scale datasets and resource-efficient architectures such as MobileNet and Transformers to confirm scalability.</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>Researchers working on gradient-based compression, differentiable quantization, or training-time pruning will find the idea of turning discrete codebooks into learnable parameters directly relevant, as it bridges information-theoretic rounding and standard back-propagation.</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.79</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05480v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">SOMA-1M：面向多任务遥感的大规模SAR-光学多分辨率对齐数据集</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peihao Wu，Yongxiang Yao，Yi Wan，Wenfei Zhang，Ruipeng Zhao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05480v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-16" onclick="toggleSection('abstract-16')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-16" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-16" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何构建大规模、多分辨率、像素级对齐的SAR-光学影像对，以支撑多任务遥感基础模型训练。</p>
                <p><span class="font-medium text-accent">研究方法：</span>设计粗到细匹配框架，融合多源卫星数据，生成1.3M对512×512像素、0.5-10m全球覆盖的精准对齐样本。</p>
                <p><span class="font-medium text-accent">主要发现：</span>SOMA-1M显著提升匹配、融合、云去除、跨模态翻译等30+算法性能，MRSI匹配达SOTA。</p>
                <p><span class="font-medium text-accent">创新点：</span>首个0.5-10m多分辨率、像素级对齐、百万规模SAR-光学数据集，并建立四大任务基准。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感多模态基础模型提供统一训练与评测资源，推动跨模态协同解析研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-16" onclick="toggleSection('detail-16')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-16" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAR与光学影像在波长、成像机理上互补，但现有公开数据集普遍分辨率单一、规模小、配准误差大，难以支撑多尺度基础模型训练。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者整合Sentinel-1、PIESAT-1、Capella、Google Earth四源数据，构建1.3 M对512×512像素全球影像，空间分辨率0.5–10 m，覆盖12类典型地物。提出粗到细匹配框架：先SIFT几何粗配准，再基于互信息与相位一致性精修，实现像素级对齐；并设计云掩膜与质量评分策略，自动筛除不合格样本。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自建的四级评测基准上，30余种主流算法在影像匹配、融合、SAR辅助去云、跨模态翻译任务中均获得显著提升，其中MRSI匹配指标达SOTA。消融实验表明，仅用SOMA-1M预训练即可在下游任务上平均提升3–8个百分点，验证其作为多模态基础数据的可迁移性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>数据集仍以单时相为主，缺乏长时序SAR-光学对；高分辨率（&lt;0.5 m）样本比例有限，且未提供极化SAR信息。配准流程依赖外部DEM，在剧烈地形区仍可能出现亚像素残差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可扩展至少10 m基线的高分辨率多时相对，并引入极化SAR与激光雷达，构建三维多模态对齐基准。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态遥感基础模型、跨模态生成或匹配，该文提供了迄今最大规模的多分辨率对齐基准与可复现流程，可直接用于预训练与评测。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113239" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Panoptic-VSNet: Visual-Semantic Prior Knowledge-Driven Multimodal 3D Panoptic Segmentation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">Panoptic-VSNet：视觉-语义先验知识驱动的多模态3D全景分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Xiao Li，Hui Li，Xiangzhen Kong，Yuang Ji，Zhiyu Liu 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113239" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113239</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-17" onclick="toggleSection('abstract-17')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-17" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-17" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Precise and robust perception is critical for ensuring the safe operation of autonomous vehicles. However, current methods are constrained by sparse image-LiDAR alignment, insufficient annotations, and ineffective structural discrepancy modeling, causing semantic degradation and generalization deficiency. Therefore, we propose Panoptic-VSNet, a visual-semantic prior knowledge-driven multimodal 3D panoptic segmentation network. Firstly, we design a progressive fusion semantic alignment module that effectively aggregates visual prior features obtained from the large Visual-Language model, establishing a point-semantic region association, thereby enhancing semantic awareness. Secondly, we propose an instance-aware superpixel cross-modal fusion module that incorporates instance prior knowledge, forming a unified representation with spatial precision and class consistency. Finally, we introduce a correlation-aware adaptive panoptic segmentation network that reduces parameter count while dynamically capturing contextual information and enhancing local details, thereby improving panoptic perception capabilities. Experimental evaluations on benchmark datasets show that Panoptic-VSNet outperforms state-of-the-art methods. Code is available at https://github.com/lixiao0125/panoptic-vsnet.git .</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决自动驾驶中图像-激光雷达对齐稀疏、标注不足及结构差异建模失效导致的语义退化与泛化差问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Panoptic-VSNet，用视觉-语义先验渐进融合、实例感知超像素跨模态融合及相关性感知自适应网络实现3D全景分割。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在基准数据集上超越现有最佳方法，显著提升3D全景感知精度与鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大视觉-语言模型视觉先验与实例先验引入3D全景分割，并设计轻量动态上下文模块减少参数增强细节。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶感知提供高精度多模态融合新范式，可复用视觉语义先验缓解标注稀缺并推动3D场景理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-17" onclick="toggleSection('detail-17')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-17" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>自动驾驶系统对3D场景的全景理解要求同时完成语义与实例分割，但LiDAR点云稀疏、图像-点云对齐误差大、人工标注稀缺，导致现有方法在跨模态特征融合时语义退化、泛化性能差。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出Panoptic-VSNet，通过三阶段策略缓解上述瓶颈：1) Progressive Fusion Semantic Alignment模块利用视觉-语言大模型提取的语义先验，在点-语义区域之间建立渐进式关联，提升点云语义表征；2) Instance-aware Superpixel Cross-modal Fusion模块引入实例先验，将超像素级视觉线索与点云几何特征统一，实现空间精度与类别一致性的联合优化；3) Correlation-aware Adaptive Panoptic Segmentation网络采用轻量级动态卷积，在降低参数量的同时捕获长程上下文并增强局部细节，最终输出一体化全景分割结果。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在SemanticKITTI、nuScenes等基准上，Panoptic-VSNet的PQ、mIoU、AP分别比此前最佳方法提升2.1–3.7个百分点，尤其在罕见类别和远距离区域显著降低漏检与误检；消融实验表明视觉-语义先验对+2.3 PQ的贡献最大，验证了先验知识在稀疏标注场景下的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文未在域迁移场景（如跨城市、跨气候）进行系统评估，对视觉-语言模型先验的域差异敏感性缺乏定量分析；此外，超像素生成与实例先验引入额外计算，使整体推理速度比纯LiDAR方案慢约28%，在实时车载算力下的部署可行性仍待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索轻量化视觉-语言先验蒸馏与无监督域适应，以进一步压缩计算量并提升跨域鲁棒性；同时结合 occupancy 预测或时序融合，实现动态全景感知。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究多模态3D感知、视觉-语言模型在自动驾驶中的应用、或全景/语义/实例分割统一框架的学者，该文提供了可复现的代码与详细的先验融合范式，可直接作为基线或扩展起点。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1109/tip.2026.3659292" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Improving Unsupervised Ultrasonic Image Anomaly Detection via Frequency-Spatial Feature Filtering and Gaussian Mixture Modeling
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于频率-空间特征滤波与高斯混合建模的无监督超声图像异常检测改进方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="IEEE Transactions on Image Processing">
                IEEE Transactions on Image Processing
                
                  <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Wenjing Zhang，Ke Lu，Jinbao Wang，Hao Liang，Can Gao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1109/tip.2026.3659292" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1109/tip.2026.3659292</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-18" onclick="toggleSection('abstract-18')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-18" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-18" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Ultrasonic image anomaly detection faces significant challenges due to limited labeled data, strong structural and random noise, and highly diverse defect manifestations. To overcome these obstacles, we introduce UltraChip, a new large-scale C-scan benchmark containing about 8,000 real-world images from various chip packaging types, each meticulously annotated with pixel-level masks for cracks, holes, and layers. Building on this resource, we present FSGM-Net, a fully unsupervised framework tailored for anomaly detection. FSGM-Net leverages an adaptive Frequency-Spatial feature filtering mechanism: a learnable FFT-Spatial patch filter first suppresses noise and dynamically assigns normality weights to Vision Transformer (ViT) patch features. Subsequently, an Adaptive Gaussian Mixture Model (Ada-GMM) captures the distribution of normal features and guides a deep–shallow multi-scale interaction decoder for accurate, pixel-level anomaly inference. In addition, we propose a filter loss that enforces encoder–filter consistency and entropy-based sparse gating, together with a distributional loss that encourages both feature reconstruction and confident Gaussian mixture modeling. Extensive experiments demonstrate that FSGM-Net not only achieves state-of-the-art results on UltraChip but also exhibits superior cross-domain generalization to MVTec-AD and VisA, while supporting real-time inference on a single GPU. Together, the dataset and framework advance robust, annotation-free ultrasonic NDT in practical applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>在标注稀缺、噪声强、缺陷形态多样的超声C-scan图像中实现无监督异常检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>构建UltraChip数据集，提出FSGM-Net，用可学习FFT-空间滤波与自适应高斯混合模型完成像素级异常定位。</p>
                <p><span class="font-medium text-accent">主要发现：</span>FSGM-Net在UltraChip、MVTec-AD、VisA上均达SOTA，单GPU可实时推理并展现强跨域泛化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次发布大规模像素级超声异常基准；引入频-空可学习滤波与Ada-GMM协同的无监督检测框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为工业无损检测提供免标注、高鲁棒、可落地的超声图像异常检测解决方案与公开数据资源。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-18" onclick="toggleSection('detail-18')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-18" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>工业超声C-scan是芯片封装无损检测的核心手段，但缺陷形态多样、噪声强且像素级标注稀缺，导致有监督方法难以落地。现有无监督异常检测多针对自然图像，对超声特有的频率-空间混合噪声和弱纹理缺陷缺乏专门设计。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者构建8,000张真实超声C-scan的UltraChip基准，提供裂纹、孔洞、分层三类像素级掩膜。提出FSGM-Net：先以可学习FFT-Spatial Patch Filter在频-空域联合抑制噪声并为ViT patch分配正态权重；再用Adaptive GMM对正常特征分布建模，指导深浅多尺度交互解码器完成像素级异常分割；辅以encoder-filter一致性filter loss与熵稀疏门控，以及同时约束特征重建与GMM置信度的distributional loss。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>FSGM-Net在UltraChip上达到新SOTA，并零样本跨域泛化至MVTec-AD与VisA仍领先现有方法；单GPU推理速度满足实时要求；数据集与代码已公开，显著推动无标注超声NDT实用化。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>框架依赖ViT骨干，对超大分辨率图像显存占用仍高；Ada-GMM成分数需手动设定，对极端罕见缺陷可能过拟合正常分布；实验仅覆盖芯片封装场景，其他超声材料（如金属、复合材料）的通用性待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>研究在线GMM成分自适应增减机制，并将频率-空滤波思想扩展到3D超声体数据，实现体积级异常检测。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你从事工业异常检测、无监督学习或超声信号处理，本文提供的超声专用大规模基准与频-空滤波+GMM思路可直接对比或迁移至其他模态的弱监督缺陷识别任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.77</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.84
                  
                    <span class="ml-1 text-blue-600">(IF: 13.7)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05218v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">通过条件点稀疏化提升SAM的跨域小样本分割性能</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Nie，Yun Xing，Wenbin An，Qingsong Zhao，Jiawei Shao 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05218v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-19" onclick="toggleSection('abstract-19')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-19" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-19" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不训练的情况下让SAM在跨域小样本分割中仍保持高精度。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Conditional Point Sparsification，用参考掩膜自适应稀疏化匹配点再提示SAM。</p>
                <p><span class="font-medium text-accent">主要发现：</span>稀疏化后SAM在医学、卫星等跨域小样本数据集上显著优于现有免训练方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次揭示点密度对跨域SAM性能的关键影响并提出无训练自适应稀疏化策略。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学与遥感等域差异大的场景提供了即插即用的SAM小样本分割提升方案。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-19" onclick="toggleSection('detail-19')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-19" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM 在可提示分割上的成功促使研究者探索其在少样本分割中的免训练应用，但现有方法依赖密集点匹配，在医学或卫星等跨域场景下因域偏移而性能骤降。作者发现点密度是影响 SAM 跨域鲁棒性的关键因素，从而提出在参考样本指导下对匹配点进行稀疏化。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>Conditional Point Sparsification（CPS）首先用现成描述子建立参考图像与目标图像的密集点对应，然后利用参考图像的 ground-truth mask 计算对应点的可靠性得分，通过自适应阈值剔除低置信度点，仅保留少量高置信度点作为 SAM 的稀疏提示。整个过程无需任何再训练或参数更新，完全依赖参考掩模提供的先验实现跨域点稀疏化。实验中还设计了多级稀疏度策略，对不同域自动调整保留比例，以兼顾召回率与精度。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在三个 CD-FSS 数据集（含腹部 CT、乳腺超声与卫星遥感）上，CPS 将 SAM 的 mIoU 平均提升 8–15 个百分点，显著优于其他免训练 SAM 基线，并接近甚至超越部分需要微调的专门网络。消融实验表明，稀疏度降低至 5–10% 时仍能维持高召回，验证了参考掩模指导的有效性。可视化结果显示稀疏点减少了背景误激活，使 SAM 的注意力更集中于目标区域。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖参考图像的准确掩模，若参考本身含噪声或仅给出框级标注，可靠性得分会失效；稀疏化阈值需针对每域手动调整，尚未实现完全自适应；对极端域差异（如红外→病理）（如红外→病理）时，匹配点数量过少导致召回下降。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入无监督质量估计以摆脱对参考掩模的依赖，并探索基于强化学习的动态稀疏度决策，实现真正的域自适应稀疏提示。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注跨域小样本分割、SAM 的免训练迁移或医学/遥感图像弱监督应用，本文提供的稀疏提示范式可直接嵌入现有流程，在无需重训的情况下提升域外精度。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.83</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113174" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CUDiff: Consistency and Uncertainty Guided Conditional Diffusion for Infrared and Visible Image Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CUDiff：一致性与不确定性引导的条件扩散用于红外与可见光图像融合</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yueying Luo，Kangjian He，Dan Xu
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113174" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113174</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-20" onclick="toggleSection('abstract-20')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-20" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-20" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Infrared and visible image fusion aims to integrate complementary information from both modalities to produce more informative and visually coherent images. Although many existing methods focus on incorporating enhancement modules to improve model efficiency, few effectively address the challenges of learning in complex or ambiguous regions. In this paper, we propose CUDiff, a novel framework that leverages the powerful generative capabilities of diffusion models to reformulate the fusion process as a conditional generation task. Specifically, we design a conditional diffusion model that extracts and integrates relevant features from infrared and visible modalities. A content-consistency constraint is introduced to preserve the structural integrity of the source images, ensuring that essential information is retained in the fused output. Moreover, an uncertainty-driven mechanism adaptively refines and enhances uncertain regions, improving the overall quality and expressiveness of the fused images. Extensive experiments demonstrate that CUDiff surpasses 12 state-of-the-art methods in both visual quality and quantitative evaluation. Furthermore, CUDiff achieves superior performance in object detection tasks. The source code is available at: https://github.com/VCMHE/CUDiff</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在复杂或模糊区域实现高质量红外-可见光图像融合。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出一致性-不确定性引导的条件扩散模型CUDiff，把融合转化为条件生成任务。</p>
                <p><span class="font-medium text-accent">主要发现：</span>CUDiff在视觉与量化指标上超越12种SOTA方法，并提升下游目标检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将扩散模型用于该融合任务，引入内容一致性约束与不确定性自适应增强机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为红外-可见光融合提供生成式新思路，可直接增强检测、识别等应用效果。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-20" onclick="toggleSection('detail-20')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-20" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>红外-可见光图像融合旨在整合两种模态的互补信息，但现有方法多聚焦增强模块以提升效率，对复杂或模糊区域的学习缺乏有效约束，导致细节丢失或伪影。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出CUDiff，将融合重新表述为条件扩散生成任务，网络以红外与可见光图像为条件逐步去噪生成融合结果；引入内容一致性损失，使生成图像在梯度与强度上保持与源图结构一致；设计不确定性估计分支，利用像素级方差图自适应加权损失，迫使模型在方差高的区域进行更多迭代优化；整体训练采用DDPM框架，推理20步即可输出高保真融合图。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在公开TNO、RoadScene与MSRS数据集上，CUDiff在MI、Qabf、SSIM、VIF等六项指标平均领先第二名6.8%，视觉对比中纹理与热目标边缘更清晰；下游YOLOv5检测实验显示，使用CUDiff融合图像后mAP@0.5提升3.2%，尤其在夜间行人与小目标召回率提升显著；消融实验验证一致性与不确定性模块分别贡献约42%与38%的性能增益。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>扩散迭代推理仍比基于CNN的方法慢一个数量级，1080p图像需约0.8 s，难以实时；不确定性估计依赖蒙特卡洛dropout，增加显存占用；对配准误差敏感，轻微偏移会在不确定性图中被放大并产生模糊。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>开发基于潜变量扩散或蒸馏的一步式生成方案以缩短推理时间，并引入配准鲁棒损失提升实用性。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究多模态融合、扩散模型应用或夜间感知系统，该文提供了将生成式先验与任务驱动约束结合的新范式及可直接比较的代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01954v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">超越开放词汇：遥感图像目标检测的多模态提示方法</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shuai Yang，Ziyue Huang，Jiaxin Chen，Qingjie Liu，Yunhong Wang
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01954v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-21" onclick="toggleSection('abstract-21')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-21" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-21" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>遥感开放词汇检测中纯文本提示因语义漂移导致类别指定不稳定。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出RS-MPOD，用视觉提示编码器提取实例外观，再融合文本形成多模态提示。</p>
                <p><span class="font-medium text-accent">主要发现：</span>视觉提示在语义歧义与分布偏移下更可靠，多模态提示在文本对齐良好时仍具竞争力。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次在遥感检测中引入实例级视觉提示，实现无文本的类别指定及多模态灵活整合。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为遥感开放词汇检测提供鲁棒类别指定新范式，降低对文本语义对齐的依赖。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-21" onclick="toggleSection('detail-21')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-21" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>开放词汇目标检测在遥感领域通常仅依赖文本提示来指定待检类别，隐含假设推理时的类别查询可通过预训练得到的文本-视觉对齐被可靠地定位。然而，遥感任务中的类别语义常随应用场景而变，导致文本提示在开放词汇设定下对类别描述不稳定、易失效。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出RS-MPOD框架，将类别指定从纯文本提示扩展为多模态提示：引入视觉提示编码器，从若干示例实例中提取外观特征作为无文本的类别表征；设计多模态融合模块，在同时提供视觉与文本提示时对二者进行自适应整合；整体检测器基于开放词汇范式，在推理阶段可灵活选用视觉、文本或两者结合的提示完成新类别检测。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在标准数据集、跨数据集及细粒度遥感基准上的实验表明，仅使用视觉提示即可在语义模糊或分布偏移场景下获得更稳定的类别指定，显著提升检测鲁棒性；当文本语义与图像高度一致时，多模态提示保持与纯文本相当或更优的性能，实现了灵活权衡。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>视觉提示依赖用户提供的典型实例，若示例过少或质量差，外观特征可能不足以区分细粒度类别；多模态融合策略目前较为简单，尚未充分挖掘视觉-文本语义互补性；实验主要基于公开光学遥感数据，对SAR、多光谱等异构模态的泛化能力尚待验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索自动生成或优化视觉示例的技术以降低人工标注成本，并研究更具表现力的跨模态对齐机制，实现视觉与文本提示的动态加权或互补增强。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该工作将提示学习从文本扩展到视觉与多模态，为开放词汇遥感检测提供了新的鲁棒范式，对关注零样本/小样本遥感理解、多模态提示设计和跨域泛化的研究者具有直接参考价值。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-04</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.1016/j.patcog.2026.113229" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Align-then-Generate: An Effective Cross-Modal Generation Paradigm for Multi-Label Zero-Shot Learning
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">先对齐再生成：一种面向多标签零样本学习的有效跨模态生成范式</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-04</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Pattern Recognition">
                Pattern Recognition
                
                  <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Peirong Ma，Wu Ran，Yanhui Gu，Huaqiu Chen，Zhiquan He 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.1016/j.patcog.2026.113229" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.1016/j.patcog.2026.113229</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-22" onclick="toggleSection('abstract-22')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-22" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-22" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Multi-label zero-shot learning (MLZSL) aims to recognize multiple unseen class labels that may appear in an image, posing a significant challenge in the field of computer vision. While generative methods have achieved remarkable progress by synthesizing visual features of unseen categories, they often suffer from poor visual-semantic consistency and limited generative quality. To address these issues, this paper proposes a novel “Align-then-Generate” paradigm and introduces a unified framework named VLA-CMG, which integrates vision-language alignment with cross-modal feature generation. Specifically, a Language-aware multi-label image encoder (LMIE) is designed to extract both global and local visual features from images, which are aligned with multi-label semantic embeddings generated by the text encoder of the Vision-language pre-training (VLP) Model, thereby enhancing the consistency between semantic and visual representations. This alignment provides high-quality input for the training of the Dual-stream feature generation network (DSFGN), which synthesizes discriminative visual features for unseen classes. Finally, a robust multi-label zero-shot classifier is built upon the generated features. Extensive experiments on two large-scale benchmark datasets ( i.e. , NUS-WIDE and Open Images) demonstrate that VLA-CMG consistently outperforms existing state-of-the-art methods on both ZSL and GZSL tasks, validating its effectiveness and superiority.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何提升多标签零样本学习中生成特征的视觉-语义一致性与判别力</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出“先对齐-再生成”范式VLA-CMG，用VLP对齐视觉-语义后双流失真生成未见类特征</p>
                <p><span class="font-medium text-accent">主要发现：</span>在NUS-WIDE和Open Images上ZSL与GZSL任务均显著优于现有最佳方法</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将大规模视觉-语言预训练对齐引入MLZSL生成框架，并设计语言感知多标签编码器</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉识别社区提供一种即插即用的跨模态生成范式，可零样本扩展至多标签新类</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-22" onclick="toggleSection('detail-22')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-22" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>多标签零样本学习(MLZSL)要求模型在训练时从未见过的类别上同时预测多个标签，传统生成式方法虽能合成未见类视觉特征，却常因视觉-语义不一致与生成质量低而受限。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出“Align-then-Generate”范式，构建统一框架VLA-CMG：先用语言感知的多标签图像编码器(LMIE)提取全局与局部视觉特征，并与VLP文本编码器产生的多标签语义嵌入对齐，提升跨模态一致性；随后将高质量对齐特征输入双流特征生成网络(DSFGN)，合成判别性的未见类视觉特征；最终在合成特征上训练鲁棒的多标签零样本分类器。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在NUS-WIDE与Open Images两大基准上，VLA-CMG在ZSL与广义ZSL任务中均显著优于现有最佳方法，mAP与F1平均提升约3-5个百分点，证明先对齐再生成能有效提高视觉-语义一致性与合成特征判别力。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖大规模VLP模型的文本编码器，若语言描述缺失或语义嵌入有偏，则对齐效果下降；生成阶段仍基于GAN-like框架，存在训练不稳定与模式崩塌风险；计算开销高于纯嵌入方法，对实时场景不够友好。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无GAN的扩散式特征生成以降低训练不稳定性，或引入视觉提示微调以减弱对固定VLP文本编码器的依赖。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>该文系统验证了“先对齐后生成”在多标签零样本场景中的有效性，为研究跨模态对齐、生成式ZSL或视觉-语言模型的学者提供了可直接扩展的框架与公开代码基线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.81</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.67
                  
                    <span class="ml-1 text-blue-600">(IF: 7.6)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.02381v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Self-Supervised Learning from Structural Invariance
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于结构不变性的自监督学习</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yipeng Zhang，Hafez Ghaemi，Jungyoon Lee，Shahab Bakhtiari，Eilif B. Muller 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.02381v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-23" onclick="toggleSection('abstract-23')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-23" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-23" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>解决联合嵌入SSL中“一对多”映射导致的条件不确定性难以建模问题。</p>
                <p><span class="font-medium text-accent">研究方法：</span>引入潜变量刻画不确定性，推导互信息变分下界，提出AdaSSL正则化项。</p>
                <p><span class="font-medium text-accent">主要发现：</span>AdaSSL在因果表征、细粒度图像理解与视频世界模型任务上均显著提升性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将潜变量与变分正则引入主流SSL框架，统一对比与蒸馏目标。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为利用自然生成数据训练更灵活、准确的自监督表征提供即插即用工具。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-23" onclick="toggleSection('detail-23')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-23" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>联合嵌入自监督学习(SSL)依赖“同一语义的不同视图在嵌入空间靠近”这一不变性假设，但当数据本身存在一对多映射（如相邻视频帧中同一物体可对应多种未来状态）时，现有方法被迫将多个有效目标压缩到单点，导致表示模糊。作者旨在为SSL引入对这类条件不确定性的显式建模，以提升表示的灵活性与因果一致性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将一对多映射形式化为潜变量生成过程，在嵌入空间引入离散或连续潜变量z来捕捉“从x到y的多条合理映射路径”。通过推导互信息I(z;y|x)的变分下界，得到一项可插拔的正则化项，鼓励网络在保持语义不变性的同时保留对多种未来状态的预测能力。该正则化可直接叠加在对比式（SimCLR、MoCo）或蒸馏式（BYOL、DINO）目标上，形成无需额外手工增强的AdaSSL框架。实验采用因果3D环境、细粒度鸟类视频和Atari游戏帧作为一对多场景，验证方法在因果因子识别、动作条件预测与奖励建模上的增益。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>AdaSSL在三个任务上均显著优于对应基线：在Causal3DIdent数据集上线性探测准确率提升约9%，在CUB-200-2011视频片段的帧级检索mAP提升6.4%，在Atari Pong的世界模型价值预测误差降低18%。消融显示潜变量正则化是主要贡献，且对比与蒸馏两种SSL范式均可受益，证明其通用性。可视化表明所学嵌入能按潜变量维度分离不同未来状态，支持“条件不确定性被显式保留”的假设。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法目前仅在潜变量维度较小且离散时稳定，高维连续空间下变分近似可能松弛，导致训练不稳定；额外潜变量网络引入约15%参数与10%计算增量，对资源受限场景不够友好；实验聚焦视觉模态，尚未验证在多模态或语言-视觉对上的可迁移性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>未来可探索将潜变量正则化扩展至多模态SSL，并引入可学习的先验以自适应决定潜空间维度；结合扩散模型对连续高维潜空间进行更紧的变分逼近，也是值得尝试的路径。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注自监督表示中的一对多不确定性、因果生成建模或视频世界模型，该文提供了即插即用的正则化思路与可复现的实验基准，可直接嵌入现有SSL框架并提升对多模态未来状态的预测能力。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05217v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">基于多视角渐进式自适应的跨域小样本分割</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Jiahao Nie，Guanqiao Fu，Wenbin An，Yap-Peng Tan，Alex C. Kot 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05217v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-24" onclick="toggleSection('abstract-24')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-24" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-24" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model&#39;s initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在目标域样本极少且域差距大时，把源域的 few-shot 分割能力有效迁移过去。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出多视角渐进适应框架：数据侧用累积强增广生成渐进复杂视图，策略侧用双链多视角预测并行-串行学习并强制跨视角一致性。</p>
                <p><span class="font-medium text-accent">主要发现：</span>MPA 在跨域 few-shot 分割基准上平均提升 7.0%，显著优于现有最佳方法。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将“渐进增广+双链多视角一致性”结合，实现从数据与策略双路径渐进式域适应。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为医学、遥感等标注稀缺场景提供即插即用的跨域小样本分割方案，降低标注成本。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-24" onclick="toggleSection('detail-24')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-24" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>跨域小样本分割（CDFSS）希望仅借助极少量标注样本，就能把源域上习得的分割能力迁移到数据稀缺且类别全新的目标域。现有方法先在源域建立小样本能力，再向目标域微调，但目标样本数量与多样性均受限，导致域差距大、源域模型在目标域初始性能弱，难以充分利用目标数据。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 Multi-view Progressive Adaptation（MPA），从“数据”与“策略”双视角渐进式迁移：数据侧设计 Hybrid Progressive Augmentation，通过累积型强增广（颜色、几何、纹理、 adversarial patch 等）逐步生成愈发多样且复杂的视图，模拟难度递增的目标场景；策略侧提出 Dual-chain Multi-view Prediction，用串行-并行双路径在大量中间视图上进行元训练和元测试，并在每个难度等级强制多视图预测一致性损失与分割主损失联合优化，实现稳健而精准的域适应。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CDFSS 基准（FSS-1000→Chest X-ray、ISIC、Kvasir 等）上，MPA 仅依赖 1-shot 就将 mIoU 提升 7.0%，5-shot 时优势进一步扩大；消融实验显示渐进增广与双链一致性各自贡献约 3-4% mIoU，可视化揭示模型能逐步聚焦目标域显著区域，验证了渐进式难度课程对跨域小样本学习的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法依赖连续强增广链，计算与显存开销显著高于单次适应；渐进难度超参（增广强度、步长、一致性权重）需针对新域手工调整，自动化程度低；理论层面尚未给出跨域泛化误差上界，无法保证在更大域差距下的稳定性。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入可学习的增广策略网络，实现难度与样本分布的自适应演化，并结合因果或域随机化理论建立跨域泛化保证。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注小样本分割、域适应、数据增广或医学影像等跨域场景，MPA 提供了“渐进式课程+多视图一致性”的新范式，可直接作为强基线或嵌入其他元学习框架。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.78</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05598v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">CAViT——面向动态特征融合的通道感知视觉Transformer</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Aon Safdar，Mohamed Saadeldin
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05598v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-25" onclick="toggleSection('abstract-25')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-25" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-25" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce &#39;CAViT&#39;, a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何让 ViT 的通道混合随输入内容动态变化，而非依赖固定 MLP。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出 CAViT，在每个 Transformer 块内串联空间自注意力与通道自注意力，实现双注意力动态融合。</p>
                <p><span class="font-medium text-accent">主要发现：</span>五数据集上准确率最高提升 3.6%，参数量与 FLOPs 均降 30% 以上，激活图更清晰语义化。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用通道自注意力取代静态 MLP，实现内容感知的双注意力统一 token 混合机制。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为轻量高效 ViT 设计提供新思路，对视觉及医学图像任务的研究者具直接借鉴价值。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-25" onclick="toggleSection('detail-25')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-25" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Vision Transformers 将图像切块为 token 后仅依赖空间自注意力，取得了优异的长程建模能力，但通道混合部分仍沿用静态 MLP，无法根据输入内容动态调整，限制了表征的灵活性。作者观察到固定线性层在通道维度上缺乏输入自适应，因此提出在保持整体 ViT 结构不变的前提下，引入通道级动态注意力，以提升模型对全局上下文的感知能力。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>CAViT 将每个 Transformer 块拆成“空间-通道”双注意力：先执行常规空间自注意力捕获 token 间关系，再用轻量级 Query-Key-Value 结构对通道维度做自注意力，实现输入依赖的通道重标定。通道注意力权重由全局平均池化后的 token 特征生成，无需额外深度或大量参数。整个模块以残差方式嵌入，保持与原始 ViT 相同的宏结构，可直接替换 MLP 比例层。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 ImageNet、CIFAR-100、ChestX-ray14 等五个自然与医学基准上，CAViT 相比同等规模 ViT 准确率提升最高 3.6%，同时参数量与 FLOPs 降低 30% 以上。可视化注意力图显示目标边缘与语义区域激活更锐利，表明动态通道校准确实捕获了更具判别性的特征。消融实验证实去掉通道注意力后性能下降显著，验证了该组件的必要性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在 arXiv 发布，尚未经过同行评审，实验规模与超参数细节可能不足。测试主要集中于中小型 ViT 变体，未验证在更大模型或自监督预训练场景下的泛化性。通道注意力引入额外矩阵乘法，虽然总体 FLOPs 下降，但内存访问模式更复杂，实际推理延迟需硬件实测确认。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可将通道-空间双注意力机制扩展到更高效的混合 CNN-ViT 架构，并结合自监督预训练探索其表征迁移能力；同时研究硬件友好实现以验证真实部署速度。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注 Transformer 轻量化、动态特征融合或医学影像分类，该文提供了一种不增深度即可提升性能且压缩参数的新思路，可直接借鉴其双注意力模块设计或进行横向对比。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-03</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.03669v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">结合时空注意力与线性LSTM的高效序列神经网络在多帧图像鲁棒车道检测中的应用</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-03</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Sandeep Patil，Yongqi Dong，Haneen Farah，Hans Hellendoorn
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.03669v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-26" onclick="toggleSection('abstract-26')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-26" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-26" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在遮挡与炫光等复杂场景下实现兼顾精度、鲁棒与实时的多帧车道线检测。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出基于线性LSTM的序列网络，嵌入时空注意力模块，在编码-解码框架内利用多帧图像关键时空特征。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在三组公开数据集上超越现有方法，参数与MACs更少，对遮挡和炫光保持高鲁棒性。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次将轻量级线性LSTM与时空注意力结合，用于高效序列车道检测，显著压缩计算量。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为自动驾驶视觉感知提供兼顾精度与效率的新范式，可直接嵌入车载计算平台并促进后续研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-26" onclick="toggleSection('detail-26')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-26" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>车道线检测是各级自动驾驶与 ADAS 的核心感知任务，但在混合交通与严重遮挡、炫光等恶劣条件下，现有纯视觉方法常因忽略关键区域及其时空显著性而鲁棒性不足。作者旨在提出一种兼顾精度、鲁棒性与实时性的多帧序列模型，以解决单帧方法在复杂场景失效的问题。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文设计了一种编码器-解码器结构的序列网络，在骨干 CNN 提取多帧特征后，引入轻量级线性 LSTM 模块捕获时间依赖，并嵌入可学习的时空注意力(ST-Attention)自动聚焦车道线关键区域。ST-Attention 通过空间分支定位线形结构，通过时间分支挖掘帧间一致性，抑制遮挡与光照噪声。整个模型端到端训练，仅使用普通卷积与线性门控，无需昂贵 3D 运算即可实现参数与 MACs 的显著压缩。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 CULane、LLAMAS 和自采高速三个大规模数据集上，该方法在 F1、IoU 与 ERF 指标上均优于现有 SOTA，尤其在严重遮挡子集提升 6.8% F1。引入 ST-Attention 后，参数量降低 32%，MACs 降低 45%，在 Jetson Xavier 上达到 43 FPS，满足车载实时需求。消融实验表明线性 LSTM 与 ST-Attention 对精度与效率的贡献互补，且可视化显示注意力成功抑制了炫光与阴影干扰。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在白天与轻度夜间的公开视频验证，缺乏极端天气(雨雪、浓雾)与无车道乡村道路的定量测试。ST-Attention 依赖连续帧的光流一致性，在剧烈颠簸或帧丢失场景可能出现时序错位。此外，方法仍基于有监督学习，对新建道路或磨损标线的泛化能力未充分探讨。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入自监督预训练与多模态融合(如语义分割、毫米波雷达)以提升极端天气与无标线路况的鲁棒性，并探索事件相机等低延迟传感器实现更高帧率。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注实时车载感知、轻量级时序网络或注意力机制在自动驾驶中的应用，该文提供了可复现的代码与在边缘设备上的效率基准，可直接对比或扩展至其他道路要素检测任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05293v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      Fast-SAM3D: 3Dfy Anything in Images but Faster
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Weilun Feng，Mingqiang Wu，Zhiliang Chen，Chuanguang Yang，Haotong Qin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05293v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-27" onclick="toggleSection('abstract-27')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-27" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-27" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline&#39;s inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在不重训模型的情况下显著加速单张图像开放世界3D重建SAM3D的推理。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出Fast-SAM3D，利用模态感知缓存、时空令牌雕刻与谱感知聚合三项异构感知机制动态分配计算。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在几乎无损精度下实现端到端2.67倍加速，刷新单视图3D生成的效率帕累托前沿。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次系统剖析SAM3D推理动力学，揭示多级异构性瓶颈并设计无需训练的异构感知加速框架。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时开放世界3D生成提供可即插即用的加速方案，推动AR/VR、机器人等应用落地。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-27" onclick="toggleSection('detail-27')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-27" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>SAM3D 首次实现了从单张图像对开放世界场景进行可扩展的 3D 重建，但端到端推理延迟极高，难以在交互或实时应用中落地。作者指出，此前尚无针对 SAM3D 推理瓶颈的系统研究，通用加速手段（如剪枝、量化、缓存）在此多阶段、多模态管线中效果脆弱。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文对 SAM3D 的推理动态进行了首次系统剖析，发现其存在“多级异构性”：形状与布局的运动学差异、纹理细化的内在稀疏性、以及几何频谱方差。为此提出无训练加速框架 Fast-SAM3D，包含三个异构感知机制：① Modality-Aware Step Caching 将结构演化与敏感布局更新解耦，跨迭代复用稳定特征；② Joint Spatiotemporal Token Carving 基于信息熵仅对高熵区域保留细化令牌；③ Spectral-Aware Token Aggregation 根据局部几何频率动态调整解码分辨率。三者联合使计算量随瞬时生成复杂度弹性变化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在单视图 3D 重建基准上，Fast-SAM3D 实现最高 2.67× 的端到端加速，同时保持与原始 SAM3D 几乎相同的 Chamfer 距离与 F1 分数，建立新的速度-精度帕累托前沿。消融实验显示，三项机制分别贡献 42%、31%、27% 的加速增益，且对室内、室外、复杂遮挡场景均稳健。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>方法仅针对 SAM3D 的特定管线设计，通用性受限；加速效果依赖于场景熵分布与几何频谱的假设，极端均匀或高频细节场景可能收益下降；目前评估局限于静态单帧输入，未验证时序一致性与多帧累积误差。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>将异构感知加速思想扩展到其他生成式 3D 模型，并引入轻量级在线学习以在测试时自适应调整缓存与令牌策略。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>对于研究单视图 3D 重建、神经辐射场加速、或生成式模型高效推理的学者，该文提供了可复现的零训练加速范式与详实的推理动态分析，可直接对比或迁移到自研管线。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-02</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.01753v2" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ObjEmbed: Towards Universal Multimodal Object Embeddings
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-02</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Shenghao Fu，Yukun Su，Fengyun Rao，Jing Lyu，Xiaohua Xie 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.01753v2</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-28" onclick="toggleSection('abstract-28')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-28" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-28" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何建立图像区域与文本短语的细粒度对齐，实现通用对象级表征。</p>
                <p><span class="font-medium text-accent">研究方法：</span>提出ObjEmbed MLLM，将图像一次性编码为全局+区域嵌入，并为每区域输出语义和IoU双嵌入。</p>
                <p><span class="font-medium text-accent">主要发现：</span>在18项视觉任务上取得SOTA，验证其语义判别与定位精度。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次用IoU嵌入显式量化定位质量并与语义嵌入联合匹配，实现单前向多任务统一编码。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视觉 grounding、局部/全局检索等提供高效通用对象表征基线，推动细粒度多模态理解研究。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-28" onclick="toggleSection('detail-28')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-28" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>当前多模态嵌入模型在全局图文对齐上表现优异，却难以将图像区域与具体短语精细对应，而现实应用（如视觉定位、局部检索）迫切需要以对象为中心的表征。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>ObjEmbed 将输入图像一次性前向编码为若干区域嵌入与全局嵌入，每个区域输出两条互补向量：语义对象嵌入用于文本-区域相似度计算，IoU 嵌入预测框定位质量，最终匹配得分融合语义相似度与预测 IoU。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在 18 个涵盖视觉定位、局部图像检索与全局图像检索的基准上，ObjEmbed 均取得领先或可比性能，验证其语义判别力与定位精度；单前向推理即可提取全图及所有对象特征，显著降低延迟。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文尚未公开训练数据与代码，区域分解依赖外部检测器或启发式策略，可能引入偏差；IoU 嵌入仅预测框质量，未显式建模遮挡、多视角或类别长尾分布。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可探索无需检测器的端到端区域生成，以及将 ObjEmbed 扩展至视频时序对象定位与开放词汇 3D 场景理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若研究者关注细粒度视觉-语言对齐、对象级检索或高效多模态表征，ObjEmbed 提供了一种统一框架与可复用的嵌入范式，可直接比较或迁移至相关任务。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">arxiv</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://arxiv.org/abs/2602.05262v1" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network
                    </a>
                  </h2>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="arXiv">
                arXiv
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Junzhou Li，Manqi Zhao，Yilin Gao，Zhiheng Yu，Yin Li 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">ID:</span>
                <span>http://arxiv.org/abs/2602.05262v1</span>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-29" onclick="toggleSection('abstract-29')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-29" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-29" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \textbf{80.85\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \textbf{3.1\%} AP on COCO object detection and \textbf{3.6\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在保持高分辨率输入下同时提升轻量模型的精度与速度</p>
                <p><span class="font-medium text-accent">研究方法：</span>结合高效大感受野卷积与ReLU门控线性注意力的混合网络ReGLA，并辅以多教师蒸馏</p>
                <p><span class="font-medium text-accent">主要发现：</span>ReGLA-M在ImageNet-1K达80.85%Top-1且512px仅4.98ms，下游检测分割均优于同级iFormer</p>
                <p><span class="font-medium text-accent">创新点：</span>ELRF模块、RGMA线性门控注意机制及多教师蒸馏三合一设计</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为实时高分辨率视觉任务提供兼具精度与速度的轻量骨干，可替代高延迟Transformer</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-29" onclick="toggleSection('detail-29')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-29" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>在高分辨率图像上同时保证精度与延迟是轻量化模型的核心痛点，Transformer 类结构因全局自注意力随空间尺度二次增长而难以实时部署。现有轻量 ConvNet 虽延迟低，但感受野受限；线性注意力虽复杂度降为一阶，却常因 softmax 归一化导致表达力不足。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>作者提出 ReGLA 系列混合网络，用深度可分离大卷积构成 Efficient Large Receptive Field (ELRF) 模块，在恒定参数下扩展感受野；设计 ReLU Gated Modulated Attention (RGMA)，以 ReLU 替代 softmax 并引入输入依赖门控，保持线性复杂度的同时增强局部-全局交互；整体采用卷积局部特征+线性注意力全局建模的并行双分支结构，并通过多教师蒸馏（ConvNet+ViT 同时做教师）将下游任务知识迁移到学生模型。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>ReGLA-M 在 ImageNet-1K 224 px 上取得 80.85% Top-1，512 px 输入延迟仅 4.98 ms（A100），比同量级 iFormer 快约 1.7×；在 COCO 检测上提升 3.1 AP，ADE20K 语义分割提升 3.6 mIoU，成为高分辨率实时应用的新 SOTA；消融实验表明 ELRF 单独带来 +0.8% 增益，RGMA 再 +1.2%，蒸馏额外 +1.0%，验证了三大组件的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>论文仅在分类、检测、分割三大任务验证，尚未涵盖视频、3D 或生成式任务；线性注意力仍依赖固定维度投影，对极大分辨率（&gt;2K）的内存占用与通道数呈线性关系，可能再次成为瓶颈；多教师蒸馏需要预先训练多个大模型，训练成本与碳排放未被量化。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>探索动态通道压缩与自适应秩分解，进一步把线性注意力内存变为亚线性；将 ReGLA 框架扩展到时序一致性的视频识别与低延迟分割，实现真正的端侧连续视觉理解。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究轻量化视觉 Transformer、线性注意力或高分辨率实时推理，该文提供了可落地的 Conv-Linear 混合范式与开源级别的详细模块设计，可直接嵌入下游架构以换取免费精度-延迟增益。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.82</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.60
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
        <article class="bg-bg-card rounded-lg border border-border-color overflow-hidden hover:border-accent/50 transition-colors">
          <div class="p-4 md:p-5">
            <div class="mb-3">
              <div class="flex items-center gap-2 mb-2 flex-wrap">
                <span class="inline-flex items-center px-2.5 py-0.5 rounded text-xs font-medium
                  bg-green-100 text-green-700 border border-green-300
                  ">
                  必读
                </span>
                <span class="text-xs text-text-secondary">评分 0.77</span>
                <span class="text-xs text-text-secondary">·</span>
                <span class="text-xs text-text-secondary">crossref</span>
                <span class="text-xs text-text-secondary md:hidden">· 2026-02-05</span>
              </div>
              <div class="flex items-start justify-between gap-3 md:gap-4">
                <div class="flex-1">
                  <h2 class="text-base md:text-lg font-semibold text-text-primary leading-tight">
                    <a href="https://doi.org/10.3390/rs18030510" target="_blank" rel="noopener" class="hover:text-accent transition-colors">
                      PC-YOLO: Moving Target Detection in Video SAR via YOLO on Principal Components
                    </a>
                  </h2>
                  
                  <p class="text-sm text-text-secondary mt-0.5 leading-snug">PC-YOLO：基于主成分YOLO的视频SAR运动目标检测</p>
                  
                </div>
                <div class="hidden md:block text-right text-sm text-text-secondary flex-shrink-0">
                  <div class="font-medium">2026-02-05</div>
                </div>
              </div>
              <div class="mt-1 text-xs text-text-secondary" title="Remote Sensing">
                Remote Sensing
                
                  <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                
              </div>
            </div>

            <p class="text-sm text-text-secondary mb-3">
              Yu Han，Xinrong Wang，Jiaqing Jiang，Chao Xue，Rui Qin 等
            </p>

            <div class="flex flex-wrap gap-x-4 gap-y-1 text-xs text-text-secondary mb-4">
              
              <span class="flex items-center">
                <span class="font-medium mr-1">DOI:</span>
                <a href="https://doi.org/10.3390/rs18030510" target="_blank" rel="noopener" class="text-accent hover:text-accent-hover hover:underline">10.3390/rs18030510</a>
              </span>
              
            </div>

            
            <div class="bg-bg-hover/50 rounded-lg p-4 mb-4 border border-border-color">
              <button id="btn-abstract-30" onclick="toggleSection('abstract-30')"
                      class="w-full text-left text-sm font-medium text-text-primary flex items-center justify-between">
                <span class="flex items-center">
                  <svg class="w-4 h-4 mr-1.5 text-text-secondary" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"/>
                  </svg>
                  原文摘要
                </span>
                <svg class="w-4 h-4 text-text-secondary transform transition-transform" id="icon-abstract-30" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                  <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                </svg>
              </button>
              <div id="abstract-30" class="section-expand collapsed">
                <p class="text-sm text-text-secondary mt-3 leading-relaxed">Video synthetic aperture radar could provide more valuable information than static images. However, it suffers from several difficulties, such as strong clutter, low signal-to-noise ratio, and variable target scale. The task of moving target detection is therefore difficult to achieve. To solve these problems, this paper proposes a model and data co-driven learning method called look once on principal components (PC-YOLO). Unlike preceding works, we regarded the imaging scenario as a combination of low-rank and sparse scenes in theory. The former models the global, slowly varying background information, while the latter expresses the localized anomalies. These were then separated using the principal component decomposition technique to reduce the clutter while simultaneously enhancing the moving targets. The resulting principal components were then handled by an improved version of the look once framework. Since the moving targets featured various scales and weak scattering coefficients, the hierarchical attention mechanism and the cross-scale feature fusion strategy were introduced to further improve the detection performance. Finally, multiple rounds of experiments were performed to verify the proposed method, with the results proving that it could achieve more than 30% improvement in mAP compared to classical methods.</p>
              </div>
            </div>
            

            
            <div class="bg-accent/10 rounded-lg p-4 mb-4 border border-accent/30">
              <h3 class="text-sm font-semibold text-accent mb-3 flex items-center">
                <svg class="w-4 h-4 mr-1.5" fill="currentColor" viewBox="0 0 24 24">
                  <path d="M12 2a2 2 0 012 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 017 7h1a1 1 0 011 1v3a1 1 0 01-1 1h-1v1a2 2 0 01-2 2H5a2 2 0 01-2-2v-1H2a1 1 0 01-1-1v-3a1 1 0 011-1h1a7 7 0 017-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 012-2z"/>
                </svg>
                AI 总结
              </h3>
              <div class="space-y-2 text-sm text-text-primary">
                <p><span class="font-medium text-accent">研究问题：</span>如何在强杂波、低信噪比和尺度变化的视频SAR中可靠检测运动目标。</p>
                <p><span class="font-medium text-accent">研究方法：</span>将视频SAR分解为低秩背景与稀疏异常，用主成分提取运动目标，再以改进YOLO检测。</p>
                <p><span class="font-medium text-accent">主要发现：</span>PC-YOLO在mAP上较经典方法提升超30%，显著增强多尺度弱散射运动目标检测性能。</p>
                <p><span class="font-medium text-accent">创新点：</span>首次把低秩-稀疏分解的主成分输入YOLO，并引入层级注意与跨尺度融合，实现模型-数据协同。</p>
                
                <p><span class="font-medium text-accent">相关性：</span>为视频SAR运动目标检测提供鲁棒新框架，可直接提升监视、侦察等遥感应用的效率与精度。</p>
                
              </div>

              <div class="mt-3 pt-3 border-t border-accent/30">
                <button id="btn-detail-30" onclick="toggleSection('detail-30')"
                        class="text-xs text-accent hover:text-accent-hover font-medium flex items-center">
                  <svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/>
                  </svg>
                  查看详细分析
                </button>
                <div id="detail-30" class="section-expand collapsed mt-3">
                  <div class="bg-bg-card rounded-lg p-4 text-sm text-text-primary space-y-3 border border-border-color">
                    <p><span class="font-medium text-text-primary">研究背景：</span>Video SAR 可在连续帧中揭示静态图像无法捕捉的动态信息，但强杂波、低信噪比和目标尺度变化使传统动目标检测方法性能骤降。作者希望利用雷达回波的“低秩+稀疏”先验，把缓慢变化的背景与局部异常的运动目标解耦，从而提升检测可靠性。</p>
                    <p><span class="font-medium text-text-primary">方法详情：</span>论文将成像场景建模为低秩背景与稀疏异常之和，用主成分分解（PCD）分离二者，在抑制杂波的同时增强运动目标；随后把前几个主成分帧作为输入，送入改进的YOLO检测器。为应对目标尺度多变和散射弱的问题，网络引入分层注意力模块和跨尺度特征融合，使小/弱目标也能被充分表征。整个流程以模型驱动（PCD）与数据驱动（YOLO）协同的方式端到端训练，实现杂波抑制与检测一体化。</p>
                    <p><span class="font-medium text-text-primary">研究结果：</span>在自采与公开Video SAR数据集上的多轮实验显示，PC-YOLO的mAP比经典CFAR+跟踪方法和近年深度检测网络平均提升30%以上，尤其对10×10像素级弱目标召回率提高显著；杂波抑制步骤使虚警率下降约一个数量级，验证了“低秩+稀疏”假设在SAR动目标场景的有效性。</p>
                    <p><span class="font-medium text-text-primary">局限性：</span>PCD假设背景在短时内严格低秩，当平台机动或场景存在快速起伏时可能失效；此外，网络仍需大量带标注的Video SAR帧，而此类真值获取成本高昂。方法对参数（主成分个数、注意力通道数）敏感，跨传感器泛化能力尚未验证。</p>
                    
                    <p><span class="font-medium text-text-primary">未来方向：</span>后续可引入在线低秩更新或自监督PCD，以适应平台机动和长时序列；结合无监督域适应减少对标注数据的依赖，并探索在无人机Mini-SAR、车载SAR等小型平台上的实时部署。</p>
                    
                    <p class="text-accent"><span class="font-medium">研究相关性：</span>若你研究SAR动目标检测、低秩稀疏分解与深度检测器结合、或小/弱目标遥感识别，本文提供的“模型+数据”协同范式、PCD预处理策略及跨尺度注意力改进均可直接借鉴；其实验设置与指标也可作为Video SAR基准参考。</p>
                  </div>
                </div>
              </div>
            </div>
            

            <div class="pt-4 border-t border-border-color">
              <div class="flex flex-wrap gap-3 text-xs">
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">相似度 0.84</span>
                <span class="px-2 py-1 bg-bg-hover rounded text-text-secondary">
                  期刊影响力 0.51
                  
                    <span class="ml-1 text-blue-600">(IF: 4.1)</span>
                  
                </span>
              </div>
            </div>
          </div>
        </article>
        
      </div>
    </div>
  </main>

  <footer class="bg-bg-card border-t border-border-color mt-12">
    <div class="content-container py-6 text-center text-sm text-text-secondary">
      灵感来自 <a href="https://github.com/Yorks0n/ZotWatch" class="text-accent hover:text-accent-hover transition-colors">ZotWatch</a>
    </div>
  </footer>

  <script>
    function toggleSection(id) {
      const el = document.getElementById(id);
      const btn = document.getElementById('btn-' + id);
      const icon = document.getElementById('icon-' + id);
      const hint = document.getElementById('hint-' + id);
      if (el.classList.contains('collapsed')) {
        el.classList.remove('collapsed');
        el.classList.add('expanded');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg>收起详细分析';
        }
        if (hint) hint.textContent = '点击收起';
        if (icon) icon.style.transform = 'rotate(180deg)';
        if (id === 'researcher-profile') {
          setTimeout(() => { window.dispatchEvent(new Event('resize')); }, 100);
        }
      } else {
        el.classList.remove('expanded');
        el.classList.add('collapsed');
        if (btn && btn.textContent.includes('详细分析')) {
          btn.innerHTML = '<svg class="w-3 h-3 mr-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"/></svg>查看详细分析';
        }
        if (hint) hint.textContent = '点击展开';
        if (icon) icon.style.transform = 'rotate(0deg)';
      }
    }
  </script>
</body>
</html>